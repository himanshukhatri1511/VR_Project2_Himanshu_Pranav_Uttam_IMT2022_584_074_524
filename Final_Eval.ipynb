{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "501c93be-3e6d-4ab9-9687-a19ddf7d4abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/uttam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.corpus import wordnet\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import transformers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee855d3-36b8-4aad-bd6e-7d022533ca6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Evaluation is  done in the training scripts for all models , this script is to summarize the results using the prediction CSV files produced by the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285b405e-1be4-4afc-a112-ebffcbaf0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine similarity model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bccb294-093d-4a7a-950a-c10263f1f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exact match\n",
    "def exact_match(y_true, y_pred):\n",
    "    return [int(a.strip().lower() == b.strip().lower()) for a, b in zip(y_true, y_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57bdbe80-df71-4bba-babd-63025620fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Match\n",
    "def synonym_match(y_true, y_pred):\n",
    "    def is_synonym(word1, word2):\n",
    "        syns1 = set([lemma.name().lower() for syn in wordnet.synsets(word1) for lemma in syn.lemmas()])\n",
    "        syns2 = set([lemma.name().lower() for syn in wordnet.synsets(word2) for lemma in syn.lemmas()])\n",
    "        return len(syns1.intersection(syns2)) > 0\n",
    "\n",
    "    result = []\n",
    "    for a, b in zip(y_true, y_pred):\n",
    "        a, b = a.strip().lower(), b.strip().lower()\n",
    "        if a == b:\n",
    "            result.append(1)\n",
    "        elif is_synonym(a, b):\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72d0e3e4-e8ac-4a1d-845a-89f4e6563d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Score\n",
    "def rouge_l_score(y_true, y_pred):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(y_true, y_pred)]\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef2f024-b350-4806-a55d-31bc206374dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity score\n",
    "def cosine_similarity_score(y_true, y_pred):\n",
    "    embeddings1 = sbert_model.encode(y_true, convert_to_tensor=True)\n",
    "    embeddings2 = sbert_model.encode(y_pred, convert_to_tensor=True)\n",
    "    cos_sim = util.cos_sim(embeddings1, embeddings2)\n",
    "    return cos_sim.diag().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fa6fbc-22f4-47e9-9290-553d2d36f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y_true = df['true_answer'].astype(str).tolist()\n",
    "    y_pred = df['predicted_answer'].astype(str).tolist()\n",
    "\n",
    "    em = exact_match(y_true, y_pred)\n",
    "    acc = accuracy_score(em, [1]*len(em))\n",
    "    precision = precision_score(em, [1]*len(em), zero_division=0)\n",
    "    recall = recall_score(em, [1]*len(em), zero_division=0)\n",
    "    f1 = f1_score(em, [1]*len(em), zero_division=0)\n",
    "\n",
    "    rouge = rouge_l_score(y_true, y_pred)\n",
    "\n",
    "    syn_acc = accuracy_score(synonym_match(y_true, y_pred), [1]*len(y_true))\n",
    "\n",
    "    # BERTScore\n",
    "    bert_p, bert_r, bert_f1 = bert_score(y_pred, y_true, lang='en', rescale_with_baseline=True)\n",
    "\n",
    "    cos_sim = cosine_similarity_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Exact Match Accuracy\":round(acc * 100, 2),\n",
    "        # Proportion of predictions that exactly match the true answers.\n",
    "        \n",
    "        \"Synonym Accuracy\" :round(syn_acc * 100, 2),\n",
    "        # Proportion of predicted answers that are synonyms of the true answers.\n",
    "\n",
    "        \"Exact Match Precision\":round(precision * 100, 2),\n",
    "        # Proportion of exact matches among all predicted matches.\n",
    "        \n",
    "        \"Exact Match Recall\":round(recall * 100, 2),\n",
    "        # Proportion of exact matches among all true answers.\n",
    "\n",
    "        \"Exact Match F1\":round(f1 * 100, 2),\n",
    "        # Harmonic mean of exact match precision and recall.\n",
    "\n",
    "        \"ROUGE-L F1\" :round(rouge * 100, 2),\n",
    "        # Measures overlap based on the longest common subsequence between predicted and true answers.\n",
    "\n",
    "        \"BERTScore Precision\" :round(bert_p.mean().item() * 100, 2),\n",
    "        # Measures how much of the predicted answer’s meaning matches the true answer using contextual embeddings.\n",
    "\n",
    "        \"BERTScore Recall\" :round(bert_r.mean().item() * 100, 2),\n",
    "        # Measures how much of the true answer’s meaning is captured by the prediction using contextual embeddings.\n",
    "\n",
    "        \"BERTScore F1\"  :round(bert_f1.mean().item() * 100, 2),\n",
    "        # Harmonic mean of BERTScore precision and recall, indicating overall semantic similarity.\n",
    "\n",
    "        \"Cosine Similarity\" : round(cos_sim * 100, 2),\n",
    "        # Cosine of the angle between the embedding vectors of predicted and true answers, representing semantic closeness.\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a067ca8b-3268-4c9c-a1ac-c112fb09851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation/predictions_BLIP_Baseline.csv\n",
      "Exact Match Accuracy     : 46.57%\n",
      "Synonym Accuracy         : 50.71%\n",
      "Exact Match Precision    : 46.57%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 63.54%\n",
      "ROUGE-L F1               : 47.55%\n",
      "BERTScore Precision      : 91.43%\n",
      "BERTScore Recall         : 88.46%\n",
      "BERTScore F1             : 89.78%\n",
      "Cosine Similarity        : 74.36%\n",
      "\n",
      "Evaluation/predictions_Vilt_Baseline.csv\n",
      "Exact Match Accuracy     : 27.53%\n",
      "Synonym Accuracy         : 29.61%\n",
      "Exact Match Precision    : 27.53%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 43.18%\n",
      "ROUGE-L F1               : 28.02%\n",
      "BERTScore Precision      : 90.73%\n",
      "BERTScore Recall         : 88.68%\n",
      "BERTScore F1             : 89.54%\n",
      "Cosine Similarity        : 63.54%\n",
      "\n",
      "Evaluation/predictions_BLIP_r8.csv\n",
      "Exact Match Accuracy     : 62.32%\n",
      "Synonym Accuracy         : 64.57%\n",
      "Exact Match Precision    : 62.32%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 76.78%\n",
      "ROUGE-L F1               : 63.56%\n",
      "BERTScore Precision      : 92.33%\n",
      "BERTScore Recall         : 90.44%\n",
      "BERTScore F1             : 91.24%\n",
      "Cosine Similarity        : 81.62%\n",
      "\n",
      "Evaluation/predictions_BLIP_r16.csv\n",
      "Exact Match Accuracy     : 62.38%\n",
      "Synonym Accuracy         : 64.68%\n",
      "Exact Match Precision    : 62.38%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 76.83%\n",
      "ROUGE-L F1               : 63.67%\n",
      "BERTScore Precision      : 92.38%\n",
      "BERTScore Recall         : 90.45%\n",
      "BERTScore F1             : 91.28%\n",
      "Cosine Similarity        : 81.65%\n",
      "\n",
      "Evaluation/predictions_BLIP_r32.csv\n",
      "Exact Match Accuracy     : 62.48%\n",
      "Synonym Accuracy         : 64.76%\n",
      "Exact Match Precision    : 62.48%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 76.91%\n",
      "ROUGE-L F1               : 63.74%\n",
      "BERTScore Precision      : 92.46%\n",
      "BERTScore Recall         : 90.5%\n",
      "BERTScore F1             : 91.34%\n",
      "Cosine Similarity        : 81.68%\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "csv_files = [\n",
    "    \"Evaluation/predictions_BLIP_Baseline.csv\",\n",
    "    \"Evaluation/predictions_Vilt_Baseline.csv\",\n",
    "    \"Evaluation/predictions_BLIP_r8.csv\",\n",
    "    \"Evaluation/predictions_BLIP_r16.csv\",\n",
    "    \"Evaluation/predictions_BLIP_r32.csv\",\n",
    "   \n",
    "    \n",
    "]\n",
    "\n",
    "# Evaluate and print results\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\n{csv_file}\")\n",
    "    scores = evaluate_file(csv_file)\n",
    "    for metric, value in scores.items():\n",
    "        print(f\"{metric:<25}: {value}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a4102c7-2b89-4091-a3ec-3dcdd49c1769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation/predictions_Blip_LoRa_r16_key.csv\n",
      "Exact Match Accuracy     : 59.6%\n",
      "Synonym Accuracy         : 62.11%\n",
      "Exact Match Precision    : 59.6%\n",
      "Exact Match Recall       : 100.0%\n",
      "Exact Match F1           : 74.69%\n",
      "ROUGE-L F1               : 61.19%\n",
      "BERTScore Precision      : 93.87%\n",
      "BERTScore Recall         : 92.48%\n",
      "BERTScore F1             : 93.07%\n",
      "Cosine Similarity        : 80.69%\n"
     ]
    }
   ],
   "source": [
    "csv_files = [\n",
    "    \"Evaluation/predictions_Blip_LoRa_r16_key.csv\"\n",
    "]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\n{csv_file}\")\n",
    "    scores = evaluate_file(csv_file)\n",
    "    for metric, value in scores.items():\n",
    "        print(f\"{metric:<25}: {value}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab3f8f-4e75-4941-9098-d5fa628573ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea46f89-c1e7-47ef-ab34-b4c9d7aa44ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
