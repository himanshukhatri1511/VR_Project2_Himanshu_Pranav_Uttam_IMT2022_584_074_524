{
  "best_global_step": 72170,
  "best_metric": 0.5446292446292447,
  "best_model_checkpoint": "./results/checkpoint-72170",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 72170,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013856172925038105,
      "grad_norm": 4.698421478271484,
      "learning_rate": 3.999501177774699e-05,
      "loss": 7.6395,
      "step": 10
    },
    {
      "epoch": 0.002771234585007621,
      "grad_norm": 4.994460582733154,
      "learning_rate": 3.9990023555493976e-05,
      "loss": 7.5578,
      "step": 20
    },
    {
      "epoch": 0.004156851877511431,
      "grad_norm": 4.31193208694458,
      "learning_rate": 3.998448108632396e-05,
      "loss": 7.5164,
      "step": 30
    },
    {
      "epoch": 0.005542469170015242,
      "grad_norm": 4.453372955322266,
      "learning_rate": 3.9978938617153947e-05,
      "loss": 7.4514,
      "step": 40
    },
    {
      "epoch": 0.0069280864625190525,
      "grad_norm": 4.899531364440918,
      "learning_rate": 3.997339614798393e-05,
      "loss": 7.3573,
      "step": 50
    },
    {
      "epoch": 0.008313703755022862,
      "grad_norm": 4.310451507568359,
      "learning_rate": 3.996785367881392e-05,
      "loss": 7.3372,
      "step": 60
    },
    {
      "epoch": 0.009699321047526674,
      "grad_norm": 4.8522562980651855,
      "learning_rate": 3.99628654565609e-05,
      "loss": 7.3013,
      "step": 70
    },
    {
      "epoch": 0.011084938340030484,
      "grad_norm": 4.861385345458984,
      "learning_rate": 3.9957322987390885e-05,
      "loss": 7.1064,
      "step": 80
    },
    {
      "epoch": 0.012470555632534294,
      "grad_norm": 5.002529144287109,
      "learning_rate": 3.9951780518220874e-05,
      "loss": 7.1117,
      "step": 90
    },
    {
      "epoch": 0.013856172925038105,
      "grad_norm": 5.768317222595215,
      "learning_rate": 3.9946238049050856e-05,
      "loss": 6.9059,
      "step": 100
    },
    {
      "epoch": 0.015241790217541915,
      "grad_norm": 7.154320240020752,
      "learning_rate": 3.9940695579880844e-05,
      "loss": 6.6757,
      "step": 110
    },
    {
      "epoch": 0.016627407510045725,
      "grad_norm": 6.357460975646973,
      "learning_rate": 3.9935153110710826e-05,
      "loss": 6.6798,
      "step": 120
    },
    {
      "epoch": 0.018013024802549536,
      "grad_norm": 5.77564001083374,
      "learning_rate": 3.992961064154081e-05,
      "loss": 6.4513,
      "step": 130
    },
    {
      "epoch": 0.019398642095053348,
      "grad_norm": 6.755920886993408,
      "learning_rate": 3.99240681723708e-05,
      "loss": 6.3197,
      "step": 140
    },
    {
      "epoch": 0.020784259387557156,
      "grad_norm": 6.869365215301514,
      "learning_rate": 3.991852570320078e-05,
      "loss": 6.3038,
      "step": 150
    },
    {
      "epoch": 0.022169876680060967,
      "grad_norm": 6.20042085647583,
      "learning_rate": 3.991298323403076e-05,
      "loss": 5.9721,
      "step": 160
    },
    {
      "epoch": 0.02355549397256478,
      "grad_norm": 7.478214740753174,
      "learning_rate": 3.990744076486075e-05,
      "loss": 6.1448,
      "step": 170
    },
    {
      "epoch": 0.024941111265068587,
      "grad_norm": 6.858202934265137,
      "learning_rate": 3.990189829569073e-05,
      "loss": 5.5671,
      "step": 180
    },
    {
      "epoch": 0.0263267285575724,
      "grad_norm": 7.230052947998047,
      "learning_rate": 3.989635582652072e-05,
      "loss": 5.7862,
      "step": 190
    },
    {
      "epoch": 0.02771234585007621,
      "grad_norm": 7.3185224533081055,
      "learning_rate": 3.98908133573507e-05,
      "loss": 5.6106,
      "step": 200
    },
    {
      "epoch": 0.029097963142580018,
      "grad_norm": 6.756686687469482,
      "learning_rate": 3.988527088818069e-05,
      "loss": 5.6604,
      "step": 210
    },
    {
      "epoch": 0.03048358043508383,
      "grad_norm": 7.2550249099731445,
      "learning_rate": 3.987972841901067e-05,
      "loss": 5.6908,
      "step": 220
    },
    {
      "epoch": 0.03186919772758764,
      "grad_norm": 6.508905410766602,
      "learning_rate": 3.987418594984066e-05,
      "loss": 5.7181,
      "step": 230
    },
    {
      "epoch": 0.03325481502009145,
      "grad_norm": 6.242406368255615,
      "learning_rate": 3.986864348067064e-05,
      "loss": 5.2569,
      "step": 240
    },
    {
      "epoch": 0.03464043231259526,
      "grad_norm": 7.031116962432861,
      "learning_rate": 3.986310101150063e-05,
      "loss": 5.713,
      "step": 250
    },
    {
      "epoch": 0.03602604960509907,
      "grad_norm": 6.525645732879639,
      "learning_rate": 3.985755854233061e-05,
      "loss": 5.795,
      "step": 260
    },
    {
      "epoch": 0.037411666897602884,
      "grad_norm": 6.913896083831787,
      "learning_rate": 3.9852016073160595e-05,
      "loss": 5.3907,
      "step": 270
    },
    {
      "epoch": 0.038797284190106696,
      "grad_norm": 6.935108661651611,
      "learning_rate": 3.9846473603990583e-05,
      "loss": 5.3682,
      "step": 280
    },
    {
      "epoch": 0.0401829014826105,
      "grad_norm": 6.41327428817749,
      "learning_rate": 3.9840931134820565e-05,
      "loss": 5.6484,
      "step": 290
    },
    {
      "epoch": 0.04156851877511431,
      "grad_norm": 6.442258358001709,
      "learning_rate": 3.983538866565055e-05,
      "loss": 5.3724,
      "step": 300
    },
    {
      "epoch": 0.04295413606761812,
      "grad_norm": 7.17010498046875,
      "learning_rate": 3.9829846196480536e-05,
      "loss": 4.849,
      "step": 310
    },
    {
      "epoch": 0.044339753360121935,
      "grad_norm": 6.36337947845459,
      "learning_rate": 3.982430372731052e-05,
      "loss": 5.1431,
      "step": 320
    },
    {
      "epoch": 0.045725370652625746,
      "grad_norm": 7.180769920349121,
      "learning_rate": 3.9818761258140506e-05,
      "loss": 5.3201,
      "step": 330
    },
    {
      "epoch": 0.04711098794512956,
      "grad_norm": 7.377742290496826,
      "learning_rate": 3.981321878897049e-05,
      "loss": 5.1961,
      "step": 340
    },
    {
      "epoch": 0.04849660523763336,
      "grad_norm": 6.5839643478393555,
      "learning_rate": 3.980767631980048e-05,
      "loss": 5.3366,
      "step": 350
    },
    {
      "epoch": 0.049882222530137174,
      "grad_norm": 6.337095260620117,
      "learning_rate": 3.980213385063046e-05,
      "loss": 5.4154,
      "step": 360
    },
    {
      "epoch": 0.051267839822640986,
      "grad_norm": 6.788588047027588,
      "learning_rate": 3.979659138146045e-05,
      "loss": 5.2801,
      "step": 370
    },
    {
      "epoch": 0.0526534571151448,
      "grad_norm": 7.366186141967773,
      "learning_rate": 3.979104891229043e-05,
      "loss": 5.5956,
      "step": 380
    },
    {
      "epoch": 0.05403907440764861,
      "grad_norm": 7.325521945953369,
      "learning_rate": 3.978550644312041e-05,
      "loss": 5.4698,
      "step": 390
    },
    {
      "epoch": 0.05542469170015242,
      "grad_norm": 6.497098445892334,
      "learning_rate": 3.97799639739504e-05,
      "loss": 4.8944,
      "step": 400
    },
    {
      "epoch": 0.056810308992656225,
      "grad_norm": 6.7516279220581055,
      "learning_rate": 3.977442150478038e-05,
      "loss": 5.4929,
      "step": 410
    },
    {
      "epoch": 0.058195926285160036,
      "grad_norm": 6.914109230041504,
      "learning_rate": 3.9768879035610363e-05,
      "loss": 4.7691,
      "step": 420
    },
    {
      "epoch": 0.05958154357766385,
      "grad_norm": 6.929261684417725,
      "learning_rate": 3.9763890813357357e-05,
      "loss": 4.8271,
      "step": 430
    },
    {
      "epoch": 0.06096716087016766,
      "grad_norm": 6.415261268615723,
      "learning_rate": 3.975834834418734e-05,
      "loss": 5.2995,
      "step": 440
    },
    {
      "epoch": 0.06235277816267147,
      "grad_norm": 6.661591053009033,
      "learning_rate": 3.975280587501732e-05,
      "loss": 4.4313,
      "step": 450
    },
    {
      "epoch": 0.06373839545517528,
      "grad_norm": 6.924159049987793,
      "learning_rate": 3.974726340584731e-05,
      "loss": 5.3199,
      "step": 460
    },
    {
      "epoch": 0.0651240127476791,
      "grad_norm": 6.569519996643066,
      "learning_rate": 3.974172093667729e-05,
      "loss": 5.2263,
      "step": 470
    },
    {
      "epoch": 0.0665096300401829,
      "grad_norm": 6.312114715576172,
      "learning_rate": 3.973617846750727e-05,
      "loss": 5.1658,
      "step": 480
    },
    {
      "epoch": 0.06789524733268672,
      "grad_norm": 7.002020835876465,
      "learning_rate": 3.973063599833726e-05,
      "loss": 5.1075,
      "step": 490
    },
    {
      "epoch": 0.06928086462519052,
      "grad_norm": 6.481775760650635,
      "learning_rate": 3.972509352916725e-05,
      "loss": 4.945,
      "step": 500
    },
    {
      "epoch": 0.07066648191769433,
      "grad_norm": 6.623284339904785,
      "learning_rate": 3.971955105999723e-05,
      "loss": 4.9111,
      "step": 510
    },
    {
      "epoch": 0.07205209921019815,
      "grad_norm": 6.253015518188477,
      "learning_rate": 3.971400859082722e-05,
      "loss": 5.2226,
      "step": 520
    },
    {
      "epoch": 0.07343771650270195,
      "grad_norm": 8.098251342773438,
      "learning_rate": 3.97084661216572e-05,
      "loss": 5.2662,
      "step": 530
    },
    {
      "epoch": 0.07482333379520577,
      "grad_norm": 6.175720691680908,
      "learning_rate": 3.9702923652487184e-05,
      "loss": 5.1432,
      "step": 540
    },
    {
      "epoch": 0.07620895108770957,
      "grad_norm": 7.273942947387695,
      "learning_rate": 3.969738118331717e-05,
      "loss": 4.6462,
      "step": 550
    },
    {
      "epoch": 0.07759456838021339,
      "grad_norm": 5.906149387359619,
      "learning_rate": 3.9691838714147155e-05,
      "loss": 5.0134,
      "step": 560
    },
    {
      "epoch": 0.0789801856727172,
      "grad_norm": 6.184429168701172,
      "learning_rate": 3.968629624497714e-05,
      "loss": 5.3622,
      "step": 570
    },
    {
      "epoch": 0.080365802965221,
      "grad_norm": 7.110500812530518,
      "learning_rate": 3.9680753775807125e-05,
      "loss": 5.2192,
      "step": 580
    },
    {
      "epoch": 0.08175142025772482,
      "grad_norm": 6.520580768585205,
      "learning_rate": 3.967521130663711e-05,
      "loss": 5.0452,
      "step": 590
    },
    {
      "epoch": 0.08313703755022862,
      "grad_norm": 6.576510429382324,
      "learning_rate": 3.9669668837467096e-05,
      "loss": 4.9788,
      "step": 600
    },
    {
      "epoch": 0.08452265484273244,
      "grad_norm": 6.450980186462402,
      "learning_rate": 3.966412636829708e-05,
      "loss": 5.2232,
      "step": 610
    },
    {
      "epoch": 0.08590827213523625,
      "grad_norm": 7.571598529815674,
      "learning_rate": 3.9658583899127066e-05,
      "loss": 4.9145,
      "step": 620
    },
    {
      "epoch": 0.08729388942774007,
      "grad_norm": 6.302317142486572,
      "learning_rate": 3.965304142995705e-05,
      "loss": 4.834,
      "step": 630
    },
    {
      "epoch": 0.08867950672024387,
      "grad_norm": 7.833628177642822,
      "learning_rate": 3.964749896078704e-05,
      "loss": 5.2089,
      "step": 640
    },
    {
      "epoch": 0.09006512401274767,
      "grad_norm": 6.460757255554199,
      "learning_rate": 3.964195649161702e-05,
      "loss": 5.2522,
      "step": 650
    },
    {
      "epoch": 0.09145074130525149,
      "grad_norm": 6.3852105140686035,
      "learning_rate": 3.963641402244701e-05,
      "loss": 4.7796,
      "step": 660
    },
    {
      "epoch": 0.0928363585977553,
      "grad_norm": 7.434102535247803,
      "learning_rate": 3.963087155327699e-05,
      "loss": 4.684,
      "step": 670
    },
    {
      "epoch": 0.09422197589025912,
      "grad_norm": 6.637331485748291,
      "learning_rate": 3.962532908410697e-05,
      "loss": 4.4644,
      "step": 680
    },
    {
      "epoch": 0.09560759318276292,
      "grad_norm": 6.894613265991211,
      "learning_rate": 3.961978661493696e-05,
      "loss": 4.7889,
      "step": 690
    },
    {
      "epoch": 0.09699321047526673,
      "grad_norm": 6.113481521606445,
      "learning_rate": 3.961424414576694e-05,
      "loss": 4.4344,
      "step": 700
    },
    {
      "epoch": 0.09837882776777054,
      "grad_norm": 6.32618522644043,
      "learning_rate": 3.960870167659692e-05,
      "loss": 4.8927,
      "step": 710
    },
    {
      "epoch": 0.09976444506027435,
      "grad_norm": 6.517414569854736,
      "learning_rate": 3.960315920742691e-05,
      "loss": 4.9548,
      "step": 720
    },
    {
      "epoch": 0.10115006235277817,
      "grad_norm": 6.919525623321533,
      "learning_rate": 3.9597616738256894e-05,
      "loss": 5.5255,
      "step": 730
    },
    {
      "epoch": 0.10253567964528197,
      "grad_norm": 6.502464294433594,
      "learning_rate": 3.959207426908688e-05,
      "loss": 4.3335,
      "step": 740
    },
    {
      "epoch": 0.10392129693778579,
      "grad_norm": 7.387646675109863,
      "learning_rate": 3.9586531799916864e-05,
      "loss": 4.6405,
      "step": 750
    },
    {
      "epoch": 0.1053069142302896,
      "grad_norm": 6.139128684997559,
      "learning_rate": 3.958098933074685e-05,
      "loss": 4.8504,
      "step": 760
    },
    {
      "epoch": 0.1066925315227934,
      "grad_norm": 6.760404586791992,
      "learning_rate": 3.9575446861576835e-05,
      "loss": 5.158,
      "step": 770
    },
    {
      "epoch": 0.10807814881529722,
      "grad_norm": 6.445457935333252,
      "learning_rate": 3.956990439240682e-05,
      "loss": 4.5926,
      "step": 780
    },
    {
      "epoch": 0.10946376610780102,
      "grad_norm": 7.126785755157471,
      "learning_rate": 3.9564361923236805e-05,
      "loss": 4.3191,
      "step": 790
    },
    {
      "epoch": 0.11084938340030484,
      "grad_norm": 6.130283355712891,
      "learning_rate": 3.955881945406679e-05,
      "loss": 4.4916,
      "step": 800
    },
    {
      "epoch": 0.11223500069280865,
      "grad_norm": 6.464191913604736,
      "learning_rate": 3.9553276984896776e-05,
      "loss": 4.8049,
      "step": 810
    },
    {
      "epoch": 0.11362061798531245,
      "grad_norm": 6.054614543914795,
      "learning_rate": 3.954773451572676e-05,
      "loss": 4.8174,
      "step": 820
    },
    {
      "epoch": 0.11500623527781627,
      "grad_norm": 6.877565860748291,
      "learning_rate": 3.954219204655674e-05,
      "loss": 4.9063,
      "step": 830
    },
    {
      "epoch": 0.11639185257032007,
      "grad_norm": 7.452534198760986,
      "learning_rate": 3.953664957738673e-05,
      "loss": 4.5877,
      "step": 840
    },
    {
      "epoch": 0.11777746986282389,
      "grad_norm": 6.973615646362305,
      "learning_rate": 3.953110710821671e-05,
      "loss": 4.5131,
      "step": 850
    },
    {
      "epoch": 0.1191630871553277,
      "grad_norm": 7.091983318328857,
      "learning_rate": 3.95255646390467e-05,
      "loss": 4.6652,
      "step": 860
    },
    {
      "epoch": 0.12054870444783151,
      "grad_norm": 8.032571792602539,
      "learning_rate": 3.952002216987669e-05,
      "loss": 4.7755,
      "step": 870
    },
    {
      "epoch": 0.12193432174033532,
      "grad_norm": 5.841087818145752,
      "learning_rate": 3.951447970070667e-05,
      "loss": 4.2896,
      "step": 880
    },
    {
      "epoch": 0.12331993903283912,
      "grad_norm": 5.932678699493408,
      "learning_rate": 3.9509491478453655e-05,
      "loss": 4.5112,
      "step": 890
    },
    {
      "epoch": 0.12470555632534294,
      "grad_norm": 7.940427303314209,
      "learning_rate": 3.950394900928364e-05,
      "loss": 4.7682,
      "step": 900
    },
    {
      "epoch": 0.12609117361784675,
      "grad_norm": 7.595592498779297,
      "learning_rate": 3.9498406540113626e-05,
      "loss": 4.5663,
      "step": 910
    },
    {
      "epoch": 0.12747679091035055,
      "grad_norm": 6.489530086517334,
      "learning_rate": 3.949286407094361e-05,
      "loss": 4.9468,
      "step": 920
    },
    {
      "epoch": 0.12886240820285438,
      "grad_norm": 5.7406535148620605,
      "learning_rate": 3.9487321601773596e-05,
      "loss": 4.4498,
      "step": 930
    },
    {
      "epoch": 0.1302480254953582,
      "grad_norm": 7.372513294219971,
      "learning_rate": 3.948177913260358e-05,
      "loss": 4.3482,
      "step": 940
    },
    {
      "epoch": 0.131633642787862,
      "grad_norm": 7.189676761627197,
      "learning_rate": 3.947623666343357e-05,
      "loss": 4.778,
      "step": 950
    },
    {
      "epoch": 0.1330192600803658,
      "grad_norm": 7.507022857666016,
      "learning_rate": 3.947069419426355e-05,
      "loss": 4.9081,
      "step": 960
    },
    {
      "epoch": 0.1344048773728696,
      "grad_norm": 6.242384910583496,
      "learning_rate": 3.946515172509353e-05,
      "loss": 4.5312,
      "step": 970
    },
    {
      "epoch": 0.13579049466537343,
      "grad_norm": 7.2590012550354,
      "learning_rate": 3.945960925592352e-05,
      "loss": 4.3483,
      "step": 980
    },
    {
      "epoch": 0.13717611195787724,
      "grad_norm": 6.136673450469971,
      "learning_rate": 3.94540667867535e-05,
      "loss": 4.1989,
      "step": 990
    },
    {
      "epoch": 0.13856172925038104,
      "grad_norm": 7.7013468742370605,
      "learning_rate": 3.944852431758348e-05,
      "loss": 4.2683,
      "step": 1000
    },
    {
      "epoch": 0.13994734654288485,
      "grad_norm": 6.885697841644287,
      "learning_rate": 3.944298184841347e-05,
      "loss": 4.6705,
      "step": 1010
    },
    {
      "epoch": 0.14133296383538865,
      "grad_norm": 5.6492719650268555,
      "learning_rate": 3.9437439379243454e-05,
      "loss": 4.5214,
      "step": 1020
    },
    {
      "epoch": 0.14271858112789249,
      "grad_norm": 7.017804145812988,
      "learning_rate": 3.943189691007344e-05,
      "loss": 4.9939,
      "step": 1030
    },
    {
      "epoch": 0.1441041984203963,
      "grad_norm": 6.408788681030273,
      "learning_rate": 3.9426354440903424e-05,
      "loss": 4.5095,
      "step": 1040
    },
    {
      "epoch": 0.1454898157129001,
      "grad_norm": 7.139204502105713,
      "learning_rate": 3.942081197173341e-05,
      "loss": 5.2909,
      "step": 1050
    },
    {
      "epoch": 0.1468754330054039,
      "grad_norm": 8.797179222106934,
      "learning_rate": 3.9415269502563395e-05,
      "loss": 4.4771,
      "step": 1060
    },
    {
      "epoch": 0.14826105029790773,
      "grad_norm": 6.672626972198486,
      "learning_rate": 3.940972703339338e-05,
      "loss": 4.7481,
      "step": 1070
    },
    {
      "epoch": 0.14964666759041154,
      "grad_norm": 6.10399055480957,
      "learning_rate": 3.9404184564223365e-05,
      "loss": 5.302,
      "step": 1080
    },
    {
      "epoch": 0.15103228488291534,
      "grad_norm": 7.217813491821289,
      "learning_rate": 3.939864209505335e-05,
      "loss": 4.6386,
      "step": 1090
    },
    {
      "epoch": 0.15241790217541915,
      "grad_norm": 6.691056728363037,
      "learning_rate": 3.9393099625883336e-05,
      "loss": 4.4883,
      "step": 1100
    },
    {
      "epoch": 0.15380351946792295,
      "grad_norm": 5.8254241943359375,
      "learning_rate": 3.938755715671332e-05,
      "loss": 4.4423,
      "step": 1110
    },
    {
      "epoch": 0.15518913676042678,
      "grad_norm": 6.384988784790039,
      "learning_rate": 3.93820146875433e-05,
      "loss": 4.2413,
      "step": 1120
    },
    {
      "epoch": 0.1565747540529306,
      "grad_norm": 7.959709167480469,
      "learning_rate": 3.937647221837329e-05,
      "loss": 4.3827,
      "step": 1130
    },
    {
      "epoch": 0.1579603713454344,
      "grad_norm": 6.886435508728027,
      "learning_rate": 3.937092974920327e-05,
      "loss": 4.5111,
      "step": 1140
    },
    {
      "epoch": 0.1593459886379382,
      "grad_norm": 8.570540428161621,
      "learning_rate": 3.936538728003326e-05,
      "loss": 4.4282,
      "step": 1150
    },
    {
      "epoch": 0.160731605930442,
      "grad_norm": 6.549566268920898,
      "learning_rate": 3.935984481086324e-05,
      "loss": 4.5898,
      "step": 1160
    },
    {
      "epoch": 0.16211722322294583,
      "grad_norm": 7.832726955413818,
      "learning_rate": 3.935430234169323e-05,
      "loss": 4.4821,
      "step": 1170
    },
    {
      "epoch": 0.16350284051544964,
      "grad_norm": 6.839290618896484,
      "learning_rate": 3.934875987252322e-05,
      "loss": 4.4148,
      "step": 1180
    },
    {
      "epoch": 0.16488845780795344,
      "grad_norm": 6.460943698883057,
      "learning_rate": 3.93432174033532e-05,
      "loss": 4.0884,
      "step": 1190
    },
    {
      "epoch": 0.16627407510045725,
      "grad_norm": 7.377699851989746,
      "learning_rate": 3.933767493418318e-05,
      "loss": 4.5683,
      "step": 1200
    },
    {
      "epoch": 0.16765969239296105,
      "grad_norm": 7.255614280700684,
      "learning_rate": 3.933213246501317e-05,
      "loss": 4.4725,
      "step": 1210
    },
    {
      "epoch": 0.16904530968546488,
      "grad_norm": 6.541706562042236,
      "learning_rate": 3.932658999584315e-05,
      "loss": 4.4131,
      "step": 1220
    },
    {
      "epoch": 0.1704309269779687,
      "grad_norm": 6.53503942489624,
      "learning_rate": 3.9321047526673134e-05,
      "loss": 4.6343,
      "step": 1230
    },
    {
      "epoch": 0.1718165442704725,
      "grad_norm": 8.030116081237793,
      "learning_rate": 3.931550505750312e-05,
      "loss": 4.3988,
      "step": 1240
    },
    {
      "epoch": 0.1732021615629763,
      "grad_norm": 6.523722171783447,
      "learning_rate": 3.9309962588333104e-05,
      "loss": 4.7656,
      "step": 1250
    },
    {
      "epoch": 0.17458777885548013,
      "grad_norm": 7.794316291809082,
      "learning_rate": 3.9304420119163086e-05,
      "loss": 4.7299,
      "step": 1260
    },
    {
      "epoch": 0.17597339614798393,
      "grad_norm": 7.318310737609863,
      "learning_rate": 3.9298877649993075e-05,
      "loss": 4.3498,
      "step": 1270
    },
    {
      "epoch": 0.17735901344048774,
      "grad_norm": 5.6734843254089355,
      "learning_rate": 3.929333518082306e-05,
      "loss": 4.3767,
      "step": 1280
    },
    {
      "epoch": 0.17874463073299154,
      "grad_norm": 8.335746765136719,
      "learning_rate": 3.9287792711653045e-05,
      "loss": 4.1272,
      "step": 1290
    },
    {
      "epoch": 0.18013024802549535,
      "grad_norm": 8.159989356994629,
      "learning_rate": 3.9282250242483034e-05,
      "loss": 4.1206,
      "step": 1300
    },
    {
      "epoch": 0.18151586531799918,
      "grad_norm": 6.679239273071289,
      "learning_rate": 3.9276707773313016e-05,
      "loss": 4.8648,
      "step": 1310
    },
    {
      "epoch": 0.18290148261050299,
      "grad_norm": 7.537233352661133,
      "learning_rate": 3.9271165304143e-05,
      "loss": 4.2992,
      "step": 1320
    },
    {
      "epoch": 0.1842870999030068,
      "grad_norm": 6.815934658050537,
      "learning_rate": 3.9265622834972986e-05,
      "loss": 4.1837,
      "step": 1330
    },
    {
      "epoch": 0.1856727171955106,
      "grad_norm": 8.283483505249023,
      "learning_rate": 3.926008036580297e-05,
      "loss": 4.535,
      "step": 1340
    },
    {
      "epoch": 0.1870583344880144,
      "grad_norm": 6.290595531463623,
      "learning_rate": 3.925453789663295e-05,
      "loss": 4.6974,
      "step": 1350
    },
    {
      "epoch": 0.18844395178051823,
      "grad_norm": 6.6648173332214355,
      "learning_rate": 3.924899542746294e-05,
      "loss": 4.8074,
      "step": 1360
    },
    {
      "epoch": 0.18982956907302204,
      "grad_norm": 7.2099080085754395,
      "learning_rate": 3.924345295829292e-05,
      "loss": 4.1726,
      "step": 1370
    },
    {
      "epoch": 0.19121518636552584,
      "grad_norm": 7.377246856689453,
      "learning_rate": 3.923791048912291e-05,
      "loss": 4.4365,
      "step": 1380
    },
    {
      "epoch": 0.19260080365802965,
      "grad_norm": 6.618297576904297,
      "learning_rate": 3.923236801995289e-05,
      "loss": 4.8659,
      "step": 1390
    },
    {
      "epoch": 0.19398642095053345,
      "grad_norm": 6.957802772521973,
      "learning_rate": 3.922682555078288e-05,
      "loss": 4.5509,
      "step": 1400
    },
    {
      "epoch": 0.19537203824303728,
      "grad_norm": 6.652688026428223,
      "learning_rate": 3.922128308161286e-05,
      "loss": 4.3761,
      "step": 1410
    },
    {
      "epoch": 0.1967576555355411,
      "grad_norm": 6.466408729553223,
      "learning_rate": 3.921574061244285e-05,
      "loss": 4.2392,
      "step": 1420
    },
    {
      "epoch": 0.1981432728280449,
      "grad_norm": 7.305360317230225,
      "learning_rate": 3.921019814327283e-05,
      "loss": 4.2071,
      "step": 1430
    },
    {
      "epoch": 0.1995288901205487,
      "grad_norm": 7.275312900543213,
      "learning_rate": 3.9204655674102814e-05,
      "loss": 4.0505,
      "step": 1440
    },
    {
      "epoch": 0.2009145074130525,
      "grad_norm": 6.068620681762695,
      "learning_rate": 3.91991132049328e-05,
      "loss": 4.6263,
      "step": 1450
    },
    {
      "epoch": 0.20230012470555633,
      "grad_norm": 6.623775005340576,
      "learning_rate": 3.9193570735762784e-05,
      "loss": 4.7495,
      "step": 1460
    },
    {
      "epoch": 0.20368574199806014,
      "grad_norm": 9.10165786743164,
      "learning_rate": 3.918802826659277e-05,
      "loss": 4.2756,
      "step": 1470
    },
    {
      "epoch": 0.20507135929056394,
      "grad_norm": 7.153151512145996,
      "learning_rate": 3.9182485797422755e-05,
      "loss": 4.5305,
      "step": 1480
    },
    {
      "epoch": 0.20645697658306775,
      "grad_norm": 5.377799987792969,
      "learning_rate": 3.9176943328252737e-05,
      "loss": 4.4007,
      "step": 1490
    },
    {
      "epoch": 0.20784259387557158,
      "grad_norm": 6.105747222900391,
      "learning_rate": 3.9171400859082725e-05,
      "loss": 4.2406,
      "step": 1500
    },
    {
      "epoch": 0.20922821116807538,
      "grad_norm": 7.007491588592529,
      "learning_rate": 3.916585838991271e-05,
      "loss": 3.997,
      "step": 1510
    },
    {
      "epoch": 0.2106138284605792,
      "grad_norm": 6.618865013122559,
      "learning_rate": 3.9160315920742696e-05,
      "loss": 3.9776,
      "step": 1520
    },
    {
      "epoch": 0.211999445753083,
      "grad_norm": 8.48931884765625,
      "learning_rate": 3.915477345157268e-05,
      "loss": 4.1037,
      "step": 1530
    },
    {
      "epoch": 0.2133850630455868,
      "grad_norm": 11.30789566040039,
      "learning_rate": 3.9149230982402666e-05,
      "loss": 4.6298,
      "step": 1540
    },
    {
      "epoch": 0.21477068033809063,
      "grad_norm": 6.568777084350586,
      "learning_rate": 3.914368851323265e-05,
      "loss": 4.1926,
      "step": 1550
    },
    {
      "epoch": 0.21615629763059444,
      "grad_norm": 7.466418743133545,
      "learning_rate": 3.913814604406264e-05,
      "loss": 4.4189,
      "step": 1560
    },
    {
      "epoch": 0.21754191492309824,
      "grad_norm": 6.761711597442627,
      "learning_rate": 3.913260357489262e-05,
      "loss": 3.8078,
      "step": 1570
    },
    {
      "epoch": 0.21892753221560204,
      "grad_norm": 5.762843132019043,
      "learning_rate": 3.91270611057226e-05,
      "loss": 4.338,
      "step": 1580
    },
    {
      "epoch": 0.22031314950810585,
      "grad_norm": 5.756577968597412,
      "learning_rate": 3.912151863655259e-05,
      "loss": 3.8381,
      "step": 1590
    },
    {
      "epoch": 0.22169876680060968,
      "grad_norm": 7.1261372566223145,
      "learning_rate": 3.911597616738257e-05,
      "loss": 3.7947,
      "step": 1600
    },
    {
      "epoch": 0.22308438409311349,
      "grad_norm": 6.843912124633789,
      "learning_rate": 3.911043369821255e-05,
      "loss": 4.2009,
      "step": 1610
    },
    {
      "epoch": 0.2244700013856173,
      "grad_norm": 8.06705379486084,
      "learning_rate": 3.910489122904254e-05,
      "loss": 3.6276,
      "step": 1620
    },
    {
      "epoch": 0.2258556186781211,
      "grad_norm": 7.879623889923096,
      "learning_rate": 3.909934875987252e-05,
      "loss": 4.1834,
      "step": 1630
    },
    {
      "epoch": 0.2272412359706249,
      "grad_norm": 6.287467956542969,
      "learning_rate": 3.909380629070251e-05,
      "loss": 4.996,
      "step": 1640
    },
    {
      "epoch": 0.22862685326312873,
      "grad_norm": 5.403460502624512,
      "learning_rate": 3.90882638215325e-05,
      "loss": 4.3197,
      "step": 1650
    },
    {
      "epoch": 0.23001247055563254,
      "grad_norm": 6.907693386077881,
      "learning_rate": 3.908272135236248e-05,
      "loss": 4.201,
      "step": 1660
    },
    {
      "epoch": 0.23139808784813634,
      "grad_norm": 7.5545244216918945,
      "learning_rate": 3.9077178883192464e-05,
      "loss": 4.0061,
      "step": 1670
    },
    {
      "epoch": 0.23278370514064015,
      "grad_norm": 6.102553844451904,
      "learning_rate": 3.907163641402245e-05,
      "loss": 4.2105,
      "step": 1680
    },
    {
      "epoch": 0.23416932243314398,
      "grad_norm": 7.25001335144043,
      "learning_rate": 3.9066093944852435e-05,
      "loss": 4.3035,
      "step": 1690
    },
    {
      "epoch": 0.23555493972564778,
      "grad_norm": 7.272391319274902,
      "learning_rate": 3.906055147568242e-05,
      "loss": 4.2132,
      "step": 1700
    },
    {
      "epoch": 0.2369405570181516,
      "grad_norm": 6.803742408752441,
      "learning_rate": 3.9055009006512405e-05,
      "loss": 4.026,
      "step": 1710
    },
    {
      "epoch": 0.2383261743106554,
      "grad_norm": 6.426242828369141,
      "learning_rate": 3.904946653734239e-05,
      "loss": 3.8594,
      "step": 1720
    },
    {
      "epoch": 0.2397117916031592,
      "grad_norm": 6.59998893737793,
      "learning_rate": 3.904392406817237e-05,
      "loss": 3.9375,
      "step": 1730
    },
    {
      "epoch": 0.24109740889566303,
      "grad_norm": 7.368246078491211,
      "learning_rate": 3.903838159900236e-05,
      "loss": 4.2619,
      "step": 1740
    },
    {
      "epoch": 0.24248302618816683,
      "grad_norm": 8.627985954284668,
      "learning_rate": 3.903283912983234e-05,
      "loss": 4.0885,
      "step": 1750
    },
    {
      "epoch": 0.24386864348067064,
      "grad_norm": 6.405623435974121,
      "learning_rate": 3.902729666066233e-05,
      "loss": 3.7906,
      "step": 1760
    },
    {
      "epoch": 0.24525426077317444,
      "grad_norm": 5.641875743865967,
      "learning_rate": 3.902175419149232e-05,
      "loss": 4.3408,
      "step": 1770
    },
    {
      "epoch": 0.24663987806567825,
      "grad_norm": 5.601477146148682,
      "learning_rate": 3.90162117223223e-05,
      "loss": 4.088,
      "step": 1780
    },
    {
      "epoch": 0.24802549535818208,
      "grad_norm": 6.690592288970947,
      "learning_rate": 3.901066925315229e-05,
      "loss": 4.7073,
      "step": 1790
    },
    {
      "epoch": 0.24941111265068588,
      "grad_norm": 7.838082313537598,
      "learning_rate": 3.900512678398227e-05,
      "loss": 4.2156,
      "step": 1800
    },
    {
      "epoch": 0.2507967299431897,
      "grad_norm": 6.256465911865234,
      "learning_rate": 3.899958431481225e-05,
      "loss": 3.9601,
      "step": 1810
    },
    {
      "epoch": 0.2521823472356935,
      "grad_norm": 6.415787696838379,
      "learning_rate": 3.899404184564224e-05,
      "loss": 3.8325,
      "step": 1820
    },
    {
      "epoch": 0.2535679645281973,
      "grad_norm": 8.496480941772461,
      "learning_rate": 3.898849937647222e-05,
      "loss": 4.2292,
      "step": 1830
    },
    {
      "epoch": 0.2549535818207011,
      "grad_norm": 6.588846206665039,
      "learning_rate": 3.89829569073022e-05,
      "loss": 4.2127,
      "step": 1840
    },
    {
      "epoch": 0.2563391991132049,
      "grad_norm": 6.30315637588501,
      "learning_rate": 3.897741443813219e-05,
      "loss": 3.7926,
      "step": 1850
    },
    {
      "epoch": 0.25772481640570877,
      "grad_norm": 10.380942344665527,
      "learning_rate": 3.8971871968962174e-05,
      "loss": 4.2177,
      "step": 1860
    },
    {
      "epoch": 0.25911043369821257,
      "grad_norm": 6.047898292541504,
      "learning_rate": 3.896632949979216e-05,
      "loss": 4.0715,
      "step": 1870
    },
    {
      "epoch": 0.2604960509907164,
      "grad_norm": 6.7616400718688965,
      "learning_rate": 3.8960787030622144e-05,
      "loss": 3.9952,
      "step": 1880
    },
    {
      "epoch": 0.2618816682832202,
      "grad_norm": 6.316305637359619,
      "learning_rate": 3.895524456145213e-05,
      "loss": 4.3455,
      "step": 1890
    },
    {
      "epoch": 0.263267285575724,
      "grad_norm": 5.73126745223999,
      "learning_rate": 3.8949702092282115e-05,
      "loss": 3.3879,
      "step": 1900
    },
    {
      "epoch": 0.2646529028682278,
      "grad_norm": 6.493295669555664,
      "learning_rate": 3.8944159623112103e-05,
      "loss": 4.2329,
      "step": 1910
    },
    {
      "epoch": 0.2660385201607316,
      "grad_norm": 6.603360652923584,
      "learning_rate": 3.8938617153942085e-05,
      "loss": 3.9271,
      "step": 1920
    },
    {
      "epoch": 0.2674241374532354,
      "grad_norm": 7.151076316833496,
      "learning_rate": 3.893307468477207e-05,
      "loss": 3.8434,
      "step": 1930
    },
    {
      "epoch": 0.2688097547457392,
      "grad_norm": 7.481752395629883,
      "learning_rate": 3.8927532215602056e-05,
      "loss": 4.006,
      "step": 1940
    },
    {
      "epoch": 0.27019537203824306,
      "grad_norm": 7.103095054626465,
      "learning_rate": 3.892198974643204e-05,
      "loss": 4.1819,
      "step": 1950
    },
    {
      "epoch": 0.27158098933074687,
      "grad_norm": 7.350957870483398,
      "learning_rate": 3.891644727726202e-05,
      "loss": 4.5022,
      "step": 1960
    },
    {
      "epoch": 0.2729666066232507,
      "grad_norm": 7.3376007080078125,
      "learning_rate": 3.891090480809201e-05,
      "loss": 4.001,
      "step": 1970
    },
    {
      "epoch": 0.2743522239157545,
      "grad_norm": 7.788269519805908,
      "learning_rate": 3.890536233892199e-05,
      "loss": 3.8568,
      "step": 1980
    },
    {
      "epoch": 0.2757378412082583,
      "grad_norm": 5.618878364562988,
      "learning_rate": 3.889981986975198e-05,
      "loss": 3.6533,
      "step": 1990
    },
    {
      "epoch": 0.2771234585007621,
      "grad_norm": 6.926998615264893,
      "learning_rate": 3.889427740058196e-05,
      "loss": 4.0351,
      "step": 2000
    },
    {
      "epoch": 0.2785090757932659,
      "grad_norm": 7.945760726928711,
      "learning_rate": 3.888873493141195e-05,
      "loss": 3.9653,
      "step": 2010
    },
    {
      "epoch": 0.2798946930857697,
      "grad_norm": 7.261856555938721,
      "learning_rate": 3.888319246224193e-05,
      "loss": 3.838,
      "step": 2020
    },
    {
      "epoch": 0.2812803103782735,
      "grad_norm": 7.671472072601318,
      "learning_rate": 3.887764999307192e-05,
      "loss": 4.2656,
      "step": 2030
    },
    {
      "epoch": 0.2826659276707773,
      "grad_norm": 6.047549247741699,
      "learning_rate": 3.88721075239019e-05,
      "loss": 3.6292,
      "step": 2040
    },
    {
      "epoch": 0.28405154496328117,
      "grad_norm": 6.129972457885742,
      "learning_rate": 3.8866565054731883e-05,
      "loss": 3.595,
      "step": 2050
    },
    {
      "epoch": 0.28543716225578497,
      "grad_norm": 6.95625638961792,
      "learning_rate": 3.886102258556187e-05,
      "loss": 3.6982,
      "step": 2060
    },
    {
      "epoch": 0.2868227795482888,
      "grad_norm": 4.779803276062012,
      "learning_rate": 3.8855480116391854e-05,
      "loss": 4.0659,
      "step": 2070
    },
    {
      "epoch": 0.2882083968407926,
      "grad_norm": 8.125799179077148,
      "learning_rate": 3.884993764722184e-05,
      "loss": 4.4532,
      "step": 2080
    },
    {
      "epoch": 0.2895940141332964,
      "grad_norm": 7.101332187652588,
      "learning_rate": 3.8844395178051824e-05,
      "loss": 4.3751,
      "step": 2090
    },
    {
      "epoch": 0.2909796314258002,
      "grad_norm": 8.303839683532715,
      "learning_rate": 3.8838852708881806e-05,
      "loss": 4.3661,
      "step": 2100
    },
    {
      "epoch": 0.292365248718304,
      "grad_norm": 6.17305326461792,
      "learning_rate": 3.8833310239711795e-05,
      "loss": 3.5666,
      "step": 2110
    },
    {
      "epoch": 0.2937508660108078,
      "grad_norm": 7.327901840209961,
      "learning_rate": 3.882776777054178e-05,
      "loss": 3.9451,
      "step": 2120
    },
    {
      "epoch": 0.2951364833033116,
      "grad_norm": 7.728688716888428,
      "learning_rate": 3.8822225301371765e-05,
      "loss": 4.2283,
      "step": 2130
    },
    {
      "epoch": 0.29652210059581546,
      "grad_norm": 6.37923526763916,
      "learning_rate": 3.8816682832201754e-05,
      "loss": 3.9674,
      "step": 2140
    },
    {
      "epoch": 0.29790771788831927,
      "grad_norm": 6.760735988616943,
      "learning_rate": 3.8811140363031736e-05,
      "loss": 3.7598,
      "step": 2150
    },
    {
      "epoch": 0.2992933351808231,
      "grad_norm": 6.4607415199279785,
      "learning_rate": 3.880559789386172e-05,
      "loss": 3.7275,
      "step": 2160
    },
    {
      "epoch": 0.3006789524733269,
      "grad_norm": 7.758772850036621,
      "learning_rate": 3.8800055424691706e-05,
      "loss": 4.1483,
      "step": 2170
    },
    {
      "epoch": 0.3020645697658307,
      "grad_norm": 7.076984882354736,
      "learning_rate": 3.879451295552169e-05,
      "loss": 4.0914,
      "step": 2180
    },
    {
      "epoch": 0.3034501870583345,
      "grad_norm": 7.0376105308532715,
      "learning_rate": 3.878897048635167e-05,
      "loss": 4.4642,
      "step": 2190
    },
    {
      "epoch": 0.3048358043508383,
      "grad_norm": 6.483504772186279,
      "learning_rate": 3.878342801718166e-05,
      "loss": 3.7202,
      "step": 2200
    },
    {
      "epoch": 0.3062214216433421,
      "grad_norm": 7.592442035675049,
      "learning_rate": 3.877788554801164e-05,
      "loss": 4.1425,
      "step": 2210
    },
    {
      "epoch": 0.3076070389358459,
      "grad_norm": 8.301030158996582,
      "learning_rate": 3.877234307884162e-05,
      "loss": 3.7984,
      "step": 2220
    },
    {
      "epoch": 0.3089926562283497,
      "grad_norm": 6.353024005889893,
      "learning_rate": 3.876680060967161e-05,
      "loss": 3.9292,
      "step": 2230
    },
    {
      "epoch": 0.31037827352085356,
      "grad_norm": 6.102542400360107,
      "learning_rate": 3.87612581405016e-05,
      "loss": 3.7417,
      "step": 2240
    },
    {
      "epoch": 0.31176389081335737,
      "grad_norm": 7.515704154968262,
      "learning_rate": 3.875571567133158e-05,
      "loss": 3.6597,
      "step": 2250
    },
    {
      "epoch": 0.3131495081058612,
      "grad_norm": 7.711704730987549,
      "learning_rate": 3.875017320216157e-05,
      "loss": 4.0536,
      "step": 2260
    },
    {
      "epoch": 0.314535125398365,
      "grad_norm": 7.550965785980225,
      "learning_rate": 3.874463073299155e-05,
      "loss": 4.3775,
      "step": 2270
    },
    {
      "epoch": 0.3159207426908688,
      "grad_norm": 6.910839557647705,
      "learning_rate": 3.8739088263821534e-05,
      "loss": 3.8668,
      "step": 2280
    },
    {
      "epoch": 0.3173063599833726,
      "grad_norm": 6.064583778381348,
      "learning_rate": 3.873354579465152e-05,
      "loss": 3.5674,
      "step": 2290
    },
    {
      "epoch": 0.3186919772758764,
      "grad_norm": 9.089336395263672,
      "learning_rate": 3.8728003325481504e-05,
      "loss": 3.5526,
      "step": 2300
    },
    {
      "epoch": 0.3200775945683802,
      "grad_norm": 6.590825080871582,
      "learning_rate": 3.8722460856311486e-05,
      "loss": 3.795,
      "step": 2310
    },
    {
      "epoch": 0.321463211860884,
      "grad_norm": 8.91767406463623,
      "learning_rate": 3.8716918387141475e-05,
      "loss": 3.692,
      "step": 2320
    },
    {
      "epoch": 0.32284882915338786,
      "grad_norm": 6.520989418029785,
      "learning_rate": 3.871137591797146e-05,
      "loss": 3.9925,
      "step": 2330
    },
    {
      "epoch": 0.32423444644589167,
      "grad_norm": 7.100586414337158,
      "learning_rate": 3.870583344880144e-05,
      "loss": 4.1823,
      "step": 2340
    },
    {
      "epoch": 0.32562006373839547,
      "grad_norm": 6.74662971496582,
      "learning_rate": 3.870029097963143e-05,
      "loss": 3.7687,
      "step": 2350
    },
    {
      "epoch": 0.3270056810308993,
      "grad_norm": 7.3540754318237305,
      "learning_rate": 3.8694748510461416e-05,
      "loss": 4.1972,
      "step": 2360
    },
    {
      "epoch": 0.3283912983234031,
      "grad_norm": 10.142899513244629,
      "learning_rate": 3.86892060412914e-05,
      "loss": 4.1428,
      "step": 2370
    },
    {
      "epoch": 0.3297769156159069,
      "grad_norm": 9.810754776000977,
      "learning_rate": 3.8683663572121386e-05,
      "loss": 4.0195,
      "step": 2380
    },
    {
      "epoch": 0.3311625329084107,
      "grad_norm": 8.015008926391602,
      "learning_rate": 3.867812110295137e-05,
      "loss": 4.0716,
      "step": 2390
    },
    {
      "epoch": 0.3325481502009145,
      "grad_norm": 7.736268043518066,
      "learning_rate": 3.867257863378136e-05,
      "loss": 3.9013,
      "step": 2400
    },
    {
      "epoch": 0.3339337674934183,
      "grad_norm": 7.851137638092041,
      "learning_rate": 3.866703616461134e-05,
      "loss": 4.1547,
      "step": 2410
    },
    {
      "epoch": 0.3353193847859221,
      "grad_norm": 7.57918119430542,
      "learning_rate": 3.866149369544132e-05,
      "loss": 3.8052,
      "step": 2420
    },
    {
      "epoch": 0.33670500207842596,
      "grad_norm": 6.238968372344971,
      "learning_rate": 3.865595122627131e-05,
      "loss": 4.1879,
      "step": 2430
    },
    {
      "epoch": 0.33809061937092977,
      "grad_norm": 6.858716011047363,
      "learning_rate": 3.865040875710129e-05,
      "loss": 3.7836,
      "step": 2440
    },
    {
      "epoch": 0.3394762366634336,
      "grad_norm": 5.690835952758789,
      "learning_rate": 3.864486628793127e-05,
      "loss": 4.3907,
      "step": 2450
    },
    {
      "epoch": 0.3408618539559374,
      "grad_norm": 6.532626152038574,
      "learning_rate": 3.863932381876126e-05,
      "loss": 4.1066,
      "step": 2460
    },
    {
      "epoch": 0.3422474712484412,
      "grad_norm": 7.927560329437256,
      "learning_rate": 3.8633781349591244e-05,
      "loss": 4.0009,
      "step": 2470
    },
    {
      "epoch": 0.343633088540945,
      "grad_norm": 8.993520736694336,
      "learning_rate": 3.862823888042123e-05,
      "loss": 3.5665,
      "step": 2480
    },
    {
      "epoch": 0.3450187058334488,
      "grad_norm": 7.385251522064209,
      "learning_rate": 3.8622696411251214e-05,
      "loss": 4.3373,
      "step": 2490
    },
    {
      "epoch": 0.3464043231259526,
      "grad_norm": 5.732019424438477,
      "learning_rate": 3.86171539420812e-05,
      "loss": 3.3342,
      "step": 2500
    },
    {
      "epoch": 0.3477899404184564,
      "grad_norm": 7.477033615112305,
      "learning_rate": 3.8611611472911185e-05,
      "loss": 3.6644,
      "step": 2510
    },
    {
      "epoch": 0.34917555771096026,
      "grad_norm": 7.932164192199707,
      "learning_rate": 3.860606900374117e-05,
      "loss": 3.9075,
      "step": 2520
    },
    {
      "epoch": 0.35056117500346407,
      "grad_norm": 6.022889137268066,
      "learning_rate": 3.8600526534571155e-05,
      "loss": 3.7099,
      "step": 2530
    },
    {
      "epoch": 0.35194679229596787,
      "grad_norm": 6.158165454864502,
      "learning_rate": 3.859498406540114e-05,
      "loss": 3.7343,
      "step": 2540
    },
    {
      "epoch": 0.3533324095884717,
      "grad_norm": 6.703023910522461,
      "learning_rate": 3.8589441596231126e-05,
      "loss": 3.8617,
      "step": 2550
    },
    {
      "epoch": 0.3547180268809755,
      "grad_norm": 7.777953147888184,
      "learning_rate": 3.858389912706111e-05,
      "loss": 4.0748,
      "step": 2560
    },
    {
      "epoch": 0.3561036441734793,
      "grad_norm": 7.0773420333862305,
      "learning_rate": 3.857835665789109e-05,
      "loss": 4.2225,
      "step": 2570
    },
    {
      "epoch": 0.3574892614659831,
      "grad_norm": 5.902744293212891,
      "learning_rate": 3.857281418872108e-05,
      "loss": 3.5564,
      "step": 2580
    },
    {
      "epoch": 0.3588748787584869,
      "grad_norm": 6.3755927085876465,
      "learning_rate": 3.856727171955106e-05,
      "loss": 3.8545,
      "step": 2590
    },
    {
      "epoch": 0.3602604960509907,
      "grad_norm": 6.03828763961792,
      "learning_rate": 3.856172925038105e-05,
      "loss": 3.7778,
      "step": 2600
    },
    {
      "epoch": 0.3616461133434945,
      "grad_norm": 7.79474401473999,
      "learning_rate": 3.855618678121103e-05,
      "loss": 3.8469,
      "step": 2610
    },
    {
      "epoch": 0.36303173063599836,
      "grad_norm": 7.097156047821045,
      "learning_rate": 3.855064431204102e-05,
      "loss": 3.8958,
      "step": 2620
    },
    {
      "epoch": 0.36441734792850217,
      "grad_norm": 6.394683361053467,
      "learning_rate": 3.8545101842871e-05,
      "loss": 4.3527,
      "step": 2630
    },
    {
      "epoch": 0.36580296522100597,
      "grad_norm": 6.645246982574463,
      "learning_rate": 3.853955937370099e-05,
      "loss": 3.1839,
      "step": 2640
    },
    {
      "epoch": 0.3671885825135098,
      "grad_norm": 7.698002815246582,
      "learning_rate": 3.853401690453097e-05,
      "loss": 4.0087,
      "step": 2650
    },
    {
      "epoch": 0.3685741998060136,
      "grad_norm": 10.491595268249512,
      "learning_rate": 3.852847443536096e-05,
      "loss": 3.9094,
      "step": 2660
    },
    {
      "epoch": 0.3699598170985174,
      "grad_norm": 8.412276268005371,
      "learning_rate": 3.852293196619094e-05,
      "loss": 3.9699,
      "step": 2670
    },
    {
      "epoch": 0.3713454343910212,
      "grad_norm": 8.184427261352539,
      "learning_rate": 3.8517389497020924e-05,
      "loss": 3.9779,
      "step": 2680
    },
    {
      "epoch": 0.372731051683525,
      "grad_norm": 6.67450475692749,
      "learning_rate": 3.851184702785091e-05,
      "loss": 4.0061,
      "step": 2690
    },
    {
      "epoch": 0.3741166689760288,
      "grad_norm": 6.447189807891846,
      "learning_rate": 3.8506304558680894e-05,
      "loss": 3.9554,
      "step": 2700
    },
    {
      "epoch": 0.37550228626853266,
      "grad_norm": 6.399258613586426,
      "learning_rate": 3.8500762089510876e-05,
      "loss": 3.792,
      "step": 2710
    },
    {
      "epoch": 0.37688790356103646,
      "grad_norm": 6.934506416320801,
      "learning_rate": 3.8495219620340865e-05,
      "loss": 4.0786,
      "step": 2720
    },
    {
      "epoch": 0.37827352085354027,
      "grad_norm": 6.55826997756958,
      "learning_rate": 3.848967715117085e-05,
      "loss": 3.7989,
      "step": 2730
    },
    {
      "epoch": 0.3796591381460441,
      "grad_norm": 9.636832237243652,
      "learning_rate": 3.8484134682000835e-05,
      "loss": 3.6998,
      "step": 2740
    },
    {
      "epoch": 0.3810447554385479,
      "grad_norm": 7.053030014038086,
      "learning_rate": 3.8478592212830824e-05,
      "loss": 3.7194,
      "step": 2750
    },
    {
      "epoch": 0.3824303727310517,
      "grad_norm": 6.677851676940918,
      "learning_rate": 3.8473049743660806e-05,
      "loss": 3.6905,
      "step": 2760
    },
    {
      "epoch": 0.3838159900235555,
      "grad_norm": 8.76017951965332,
      "learning_rate": 3.846750727449079e-05,
      "loss": 3.7874,
      "step": 2770
    },
    {
      "epoch": 0.3852016073160593,
      "grad_norm": 7.432733058929443,
      "learning_rate": 3.8461964805320776e-05,
      "loss": 3.8585,
      "step": 2780
    },
    {
      "epoch": 0.3865872246085631,
      "grad_norm": 7.140560150146484,
      "learning_rate": 3.845642233615076e-05,
      "loss": 4.4351,
      "step": 2790
    },
    {
      "epoch": 0.3879728419010669,
      "grad_norm": 8.500883102416992,
      "learning_rate": 3.845087986698074e-05,
      "loss": 4.1264,
      "step": 2800
    },
    {
      "epoch": 0.38935845919357076,
      "grad_norm": 7.517551898956299,
      "learning_rate": 3.844533739781073e-05,
      "loss": 3.3048,
      "step": 2810
    },
    {
      "epoch": 0.39074407648607457,
      "grad_norm": 5.8480377197265625,
      "learning_rate": 3.843979492864071e-05,
      "loss": 3.7234,
      "step": 2820
    },
    {
      "epoch": 0.39212969377857837,
      "grad_norm": 7.891193866729736,
      "learning_rate": 3.84342524594707e-05,
      "loss": 3.3295,
      "step": 2830
    },
    {
      "epoch": 0.3935153110710822,
      "grad_norm": 9.706018447875977,
      "learning_rate": 3.842870999030068e-05,
      "loss": 3.7955,
      "step": 2840
    },
    {
      "epoch": 0.394900928363586,
      "grad_norm": 8.471916198730469,
      "learning_rate": 3.842316752113067e-05,
      "loss": 4.2079,
      "step": 2850
    },
    {
      "epoch": 0.3962865456560898,
      "grad_norm": 6.048375606536865,
      "learning_rate": 3.841762505196065e-05,
      "loss": 3.7526,
      "step": 2860
    },
    {
      "epoch": 0.3976721629485936,
      "grad_norm": 8.295762062072754,
      "learning_rate": 3.841208258279064e-05,
      "loss": 3.4216,
      "step": 2870
    },
    {
      "epoch": 0.3990577802410974,
      "grad_norm": 7.15923547744751,
      "learning_rate": 3.840654011362062e-05,
      "loss": 4.1989,
      "step": 2880
    },
    {
      "epoch": 0.4004433975336012,
      "grad_norm": 7.161533832550049,
      "learning_rate": 3.8400997644450604e-05,
      "loss": 3.7815,
      "step": 2890
    },
    {
      "epoch": 0.401829014826105,
      "grad_norm": 6.698973178863525,
      "learning_rate": 3.839545517528059e-05,
      "loss": 3.8627,
      "step": 2900
    },
    {
      "epoch": 0.40321463211860886,
      "grad_norm": 6.273082256317139,
      "learning_rate": 3.8389912706110574e-05,
      "loss": 3.5215,
      "step": 2910
    },
    {
      "epoch": 0.40460024941111267,
      "grad_norm": 7.697865009307861,
      "learning_rate": 3.8384370236940556e-05,
      "loss": 3.6879,
      "step": 2920
    },
    {
      "epoch": 0.40598586670361647,
      "grad_norm": 6.935979843139648,
      "learning_rate": 3.8378827767770545e-05,
      "loss": 3.5778,
      "step": 2930
    },
    {
      "epoch": 0.4073714839961203,
      "grad_norm": 6.670987129211426,
      "learning_rate": 3.8373285298600527e-05,
      "loss": 3.8358,
      "step": 2940
    },
    {
      "epoch": 0.4087571012886241,
      "grad_norm": 6.9701361656188965,
      "learning_rate": 3.8367742829430515e-05,
      "loss": 3.6473,
      "step": 2950
    },
    {
      "epoch": 0.4101427185811279,
      "grad_norm": 8.406763076782227,
      "learning_rate": 3.83622003602605e-05,
      "loss": 3.7302,
      "step": 2960
    },
    {
      "epoch": 0.4115283358736317,
      "grad_norm": 6.769294738769531,
      "learning_rate": 3.8356657891090486e-05,
      "loss": 3.6206,
      "step": 2970
    },
    {
      "epoch": 0.4129139531661355,
      "grad_norm": 7.914314270019531,
      "learning_rate": 3.835111542192047e-05,
      "loss": 3.8616,
      "step": 2980
    },
    {
      "epoch": 0.4142995704586393,
      "grad_norm": 7.50052547454834,
      "learning_rate": 3.8345572952750456e-05,
      "loss": 3.7183,
      "step": 2990
    },
    {
      "epoch": 0.41568518775114316,
      "grad_norm": 6.50830078125,
      "learning_rate": 3.834003048358044e-05,
      "loss": 4.0166,
      "step": 3000
    },
    {
      "epoch": 0.41707080504364696,
      "grad_norm": 5.600599765777588,
      "learning_rate": 3.833448801441043e-05,
      "loss": 3.9128,
      "step": 3010
    },
    {
      "epoch": 0.41845642233615077,
      "grad_norm": 8.429000854492188,
      "learning_rate": 3.832894554524041e-05,
      "loss": 3.7216,
      "step": 3020
    },
    {
      "epoch": 0.4198420396286546,
      "grad_norm": 5.958381652832031,
      "learning_rate": 3.832340307607039e-05,
      "loss": 3.3839,
      "step": 3030
    },
    {
      "epoch": 0.4212276569211584,
      "grad_norm": 6.290873050689697,
      "learning_rate": 3.831786060690038e-05,
      "loss": 3.5446,
      "step": 3040
    },
    {
      "epoch": 0.4226132742136622,
      "grad_norm": 8.085451126098633,
      "learning_rate": 3.831231813773036e-05,
      "loss": 3.8321,
      "step": 3050
    },
    {
      "epoch": 0.423998891506166,
      "grad_norm": 6.294162273406982,
      "learning_rate": 3.830677566856034e-05,
      "loss": 3.7304,
      "step": 3060
    },
    {
      "epoch": 0.4253845087986698,
      "grad_norm": 6.850571155548096,
      "learning_rate": 3.830123319939033e-05,
      "loss": 3.7787,
      "step": 3070
    },
    {
      "epoch": 0.4267701260911736,
      "grad_norm": 12.391319274902344,
      "learning_rate": 3.829569073022031e-05,
      "loss": 4.0186,
      "step": 3080
    },
    {
      "epoch": 0.4281557433836774,
      "grad_norm": 9.276865005493164,
      "learning_rate": 3.82901482610503e-05,
      "loss": 3.6696,
      "step": 3090
    },
    {
      "epoch": 0.42954136067618126,
      "grad_norm": 7.257948398590088,
      "learning_rate": 3.8284605791880284e-05,
      "loss": 3.594,
      "step": 3100
    },
    {
      "epoch": 0.43092697796868507,
      "grad_norm": 6.571601867675781,
      "learning_rate": 3.827906332271027e-05,
      "loss": 3.4766,
      "step": 3110
    },
    {
      "epoch": 0.43231259526118887,
      "grad_norm": 5.68007755279541,
      "learning_rate": 3.8273520853540254e-05,
      "loss": 3.5194,
      "step": 3120
    },
    {
      "epoch": 0.4336982125536927,
      "grad_norm": 7.862821102142334,
      "learning_rate": 3.826797838437024e-05,
      "loss": 3.3712,
      "step": 3130
    },
    {
      "epoch": 0.4350838298461965,
      "grad_norm": 6.469084739685059,
      "learning_rate": 3.8262435915200225e-05,
      "loss": 3.6236,
      "step": 3140
    },
    {
      "epoch": 0.4364694471387003,
      "grad_norm": 7.104308605194092,
      "learning_rate": 3.825689344603021e-05,
      "loss": 3.8987,
      "step": 3150
    },
    {
      "epoch": 0.4378550644312041,
      "grad_norm": 7.285942554473877,
      "learning_rate": 3.8251350976860195e-05,
      "loss": 4.1457,
      "step": 3160
    },
    {
      "epoch": 0.4392406817237079,
      "grad_norm": 8.32334041595459,
      "learning_rate": 3.824580850769018e-05,
      "loss": 4.1735,
      "step": 3170
    },
    {
      "epoch": 0.4406262990162117,
      "grad_norm": 7.549347400665283,
      "learning_rate": 3.824026603852016e-05,
      "loss": 3.8422,
      "step": 3180
    },
    {
      "epoch": 0.44201191630871556,
      "grad_norm": 9.310481071472168,
      "learning_rate": 3.823472356935015e-05,
      "loss": 3.6637,
      "step": 3190
    },
    {
      "epoch": 0.44339753360121936,
      "grad_norm": 7.033267974853516,
      "learning_rate": 3.822918110018013e-05,
      "loss": 3.6355,
      "step": 3200
    },
    {
      "epoch": 0.44478315089372317,
      "grad_norm": 7.297872543334961,
      "learning_rate": 3.822363863101012e-05,
      "loss": 3.6917,
      "step": 3210
    },
    {
      "epoch": 0.44616876818622697,
      "grad_norm": 7.137946605682373,
      "learning_rate": 3.821809616184011e-05,
      "loss": 3.6931,
      "step": 3220
    },
    {
      "epoch": 0.4475543854787308,
      "grad_norm": 6.240066051483154,
      "learning_rate": 3.821255369267009e-05,
      "loss": 3.6606,
      "step": 3230
    },
    {
      "epoch": 0.4489400027712346,
      "grad_norm": 8.550979614257812,
      "learning_rate": 3.820701122350008e-05,
      "loss": 3.9872,
      "step": 3240
    },
    {
      "epoch": 0.4503256200637384,
      "grad_norm": 8.178872108459473,
      "learning_rate": 3.820146875433006e-05,
      "loss": 4.0573,
      "step": 3250
    },
    {
      "epoch": 0.4517112373562422,
      "grad_norm": 8.749785423278809,
      "learning_rate": 3.819592628516004e-05,
      "loss": 3.726,
      "step": 3260
    },
    {
      "epoch": 0.453096854648746,
      "grad_norm": 7.013505935668945,
      "learning_rate": 3.819038381599003e-05,
      "loss": 4.3771,
      "step": 3270
    },
    {
      "epoch": 0.4544824719412498,
      "grad_norm": 9.830523490905762,
      "learning_rate": 3.818484134682001e-05,
      "loss": 3.9173,
      "step": 3280
    },
    {
      "epoch": 0.45586808923375366,
      "grad_norm": 7.2406206130981445,
      "learning_rate": 3.817929887764999e-05,
      "loss": 4.116,
      "step": 3290
    },
    {
      "epoch": 0.45725370652625746,
      "grad_norm": 9.839254379272461,
      "learning_rate": 3.817375640847998e-05,
      "loss": 3.8633,
      "step": 3300
    },
    {
      "epoch": 0.45863932381876127,
      "grad_norm": 6.356706142425537,
      "learning_rate": 3.8168213939309964e-05,
      "loss": 3.2731,
      "step": 3310
    },
    {
      "epoch": 0.4600249411112651,
      "grad_norm": 7.419363498687744,
      "learning_rate": 3.816267147013995e-05,
      "loss": 3.8906,
      "step": 3320
    },
    {
      "epoch": 0.4614105584037689,
      "grad_norm": 8.772507667541504,
      "learning_rate": 3.8157129000969934e-05,
      "loss": 4.2984,
      "step": 3330
    },
    {
      "epoch": 0.4627961756962727,
      "grad_norm": 7.954538822174072,
      "learning_rate": 3.815158653179992e-05,
      "loss": 3.9639,
      "step": 3340
    },
    {
      "epoch": 0.4641817929887765,
      "grad_norm": 7.341644763946533,
      "learning_rate": 3.8146044062629905e-05,
      "loss": 4.4171,
      "step": 3350
    },
    {
      "epoch": 0.4655674102812803,
      "grad_norm": 6.529458522796631,
      "learning_rate": 3.8140501593459893e-05,
      "loss": 3.9887,
      "step": 3360
    },
    {
      "epoch": 0.4669530275737841,
      "grad_norm": 5.79867696762085,
      "learning_rate": 3.8134959124289875e-05,
      "loss": 3.9284,
      "step": 3370
    },
    {
      "epoch": 0.46833864486628796,
      "grad_norm": 7.088435173034668,
      "learning_rate": 3.812941665511986e-05,
      "loss": 3.5829,
      "step": 3380
    },
    {
      "epoch": 0.46972426215879176,
      "grad_norm": 9.880885124206543,
      "learning_rate": 3.8123874185949846e-05,
      "loss": 4.3887,
      "step": 3390
    },
    {
      "epoch": 0.47110987945129557,
      "grad_norm": 9.772430419921875,
      "learning_rate": 3.811833171677983e-05,
      "loss": 3.2583,
      "step": 3400
    },
    {
      "epoch": 0.47249549674379937,
      "grad_norm": 7.037860870361328,
      "learning_rate": 3.811278924760981e-05,
      "loss": 3.8889,
      "step": 3410
    },
    {
      "epoch": 0.4738811140363032,
      "grad_norm": 7.46946907043457,
      "learning_rate": 3.81072467784398e-05,
      "loss": 3.4778,
      "step": 3420
    },
    {
      "epoch": 0.475266731328807,
      "grad_norm": 10.5236177444458,
      "learning_rate": 3.810170430926978e-05,
      "loss": 3.8694,
      "step": 3430
    },
    {
      "epoch": 0.4766523486213108,
      "grad_norm": 7.567888259887695,
      "learning_rate": 3.809616184009977e-05,
      "loss": 3.5421,
      "step": 3440
    },
    {
      "epoch": 0.4780379659138146,
      "grad_norm": 6.372384548187256,
      "learning_rate": 3.809061937092975e-05,
      "loss": 3.3989,
      "step": 3450
    },
    {
      "epoch": 0.4794235832063184,
      "grad_norm": 8.085339546203613,
      "learning_rate": 3.808507690175974e-05,
      "loss": 3.7037,
      "step": 3460
    },
    {
      "epoch": 0.4808092004988222,
      "grad_norm": 8.549574851989746,
      "learning_rate": 3.807953443258972e-05,
      "loss": 3.5403,
      "step": 3470
    },
    {
      "epoch": 0.48219481779132606,
      "grad_norm": 9.166872024536133,
      "learning_rate": 3.807399196341971e-05,
      "loss": 3.7188,
      "step": 3480
    },
    {
      "epoch": 0.48358043508382986,
      "grad_norm": 10.764431953430176,
      "learning_rate": 3.806844949424969e-05,
      "loss": 3.6276,
      "step": 3490
    },
    {
      "epoch": 0.48496605237633367,
      "grad_norm": 6.01206636428833,
      "learning_rate": 3.8062907025079673e-05,
      "loss": 3.5274,
      "step": 3500
    },
    {
      "epoch": 0.48635166966883747,
      "grad_norm": 7.378100872039795,
      "learning_rate": 3.805736455590966e-05,
      "loss": 3.8297,
      "step": 3510
    },
    {
      "epoch": 0.4877372869613413,
      "grad_norm": 7.588517189025879,
      "learning_rate": 3.8051822086739644e-05,
      "loss": 3.7418,
      "step": 3520
    },
    {
      "epoch": 0.4891229042538451,
      "grad_norm": 7.417191028594971,
      "learning_rate": 3.804627961756963e-05,
      "loss": 3.7142,
      "step": 3530
    },
    {
      "epoch": 0.4905085215463489,
      "grad_norm": 6.891600131988525,
      "learning_rate": 3.8040737148399614e-05,
      "loss": 4.1599,
      "step": 3540
    },
    {
      "epoch": 0.4918941388388527,
      "grad_norm": 6.987326622009277,
      "learning_rate": 3.8035194679229596e-05,
      "loss": 3.6073,
      "step": 3550
    },
    {
      "epoch": 0.4932797561313565,
      "grad_norm": 9.578401565551758,
      "learning_rate": 3.8029652210059585e-05,
      "loss": 3.3504,
      "step": 3560
    },
    {
      "epoch": 0.49466537342386036,
      "grad_norm": 6.336452960968018,
      "learning_rate": 3.802410974088957e-05,
      "loss": 3.5086,
      "step": 3570
    },
    {
      "epoch": 0.49605099071636416,
      "grad_norm": 6.053666591644287,
      "learning_rate": 3.8018567271719555e-05,
      "loss": 3.396,
      "step": 3580
    },
    {
      "epoch": 0.49743660800886796,
      "grad_norm": 8.48862075805664,
      "learning_rate": 3.8013024802549544e-05,
      "loss": 3.6188,
      "step": 3590
    },
    {
      "epoch": 0.49882222530137177,
      "grad_norm": 8.548925399780273,
      "learning_rate": 3.8007482333379526e-05,
      "loss": 3.7624,
      "step": 3600
    },
    {
      "epoch": 0.5002078425938755,
      "grad_norm": 7.45949125289917,
      "learning_rate": 3.800193986420951e-05,
      "loss": 3.5968,
      "step": 3610
    },
    {
      "epoch": 0.5015934598863794,
      "grad_norm": 9.946491241455078,
      "learning_rate": 3.7996397395039496e-05,
      "loss": 3.6852,
      "step": 3620
    },
    {
      "epoch": 0.5029790771788832,
      "grad_norm": 8.581214904785156,
      "learning_rate": 3.799085492586948e-05,
      "loss": 3.4394,
      "step": 3630
    },
    {
      "epoch": 0.504364694471387,
      "grad_norm": 9.048724174499512,
      "learning_rate": 3.798531245669946e-05,
      "loss": 3.6798,
      "step": 3640
    },
    {
      "epoch": 0.5057503117638908,
      "grad_norm": 9.998250007629395,
      "learning_rate": 3.797976998752945e-05,
      "loss": 3.0717,
      "step": 3650
    },
    {
      "epoch": 0.5071359290563946,
      "grad_norm": 7.434495449066162,
      "learning_rate": 3.797422751835943e-05,
      "loss": 3.8545,
      "step": 3660
    },
    {
      "epoch": 0.5085215463488985,
      "grad_norm": 8.666412353515625,
      "learning_rate": 3.796868504918941e-05,
      "loss": 3.6098,
      "step": 3670
    },
    {
      "epoch": 0.5099071636414022,
      "grad_norm": 5.4218430519104,
      "learning_rate": 3.79631425800194e-05,
      "loss": 3.9779,
      "step": 3680
    },
    {
      "epoch": 0.5112927809339061,
      "grad_norm": 7.717753887176514,
      "learning_rate": 3.795760011084938e-05,
      "loss": 4.0232,
      "step": 3690
    },
    {
      "epoch": 0.5126783982264098,
      "grad_norm": 8.259389877319336,
      "learning_rate": 3.795205764167937e-05,
      "loss": 3.3478,
      "step": 3700
    },
    {
      "epoch": 0.5140640155189137,
      "grad_norm": 7.454625606536865,
      "learning_rate": 3.794651517250936e-05,
      "loss": 4.0309,
      "step": 3710
    },
    {
      "epoch": 0.5154496328114175,
      "grad_norm": 11.653630256652832,
      "learning_rate": 3.794097270333934e-05,
      "loss": 3.6329,
      "step": 3720
    },
    {
      "epoch": 0.5168352501039213,
      "grad_norm": 5.337636470794678,
      "learning_rate": 3.7935430234169324e-05,
      "loss": 3.1229,
      "step": 3730
    },
    {
      "epoch": 0.5182208673964251,
      "grad_norm": 10.460890769958496,
      "learning_rate": 3.792988776499931e-05,
      "loss": 3.6865,
      "step": 3740
    },
    {
      "epoch": 0.5196064846889289,
      "grad_norm": 8.268779754638672,
      "learning_rate": 3.7924345295829294e-05,
      "loss": 3.328,
      "step": 3750
    },
    {
      "epoch": 0.5209921019814328,
      "grad_norm": 7.000428199768066,
      "learning_rate": 3.7918802826659276e-05,
      "loss": 3.2629,
      "step": 3760
    },
    {
      "epoch": 0.5223777192739365,
      "grad_norm": 9.151212692260742,
      "learning_rate": 3.7913260357489265e-05,
      "loss": 3.6807,
      "step": 3770
    },
    {
      "epoch": 0.5237633365664404,
      "grad_norm": 9.390297889709473,
      "learning_rate": 3.790771788831925e-05,
      "loss": 3.3948,
      "step": 3780
    },
    {
      "epoch": 0.5251489538589441,
      "grad_norm": 7.5444159507751465,
      "learning_rate": 3.790217541914923e-05,
      "loss": 3.2406,
      "step": 3790
    },
    {
      "epoch": 0.526534571151448,
      "grad_norm": 7.959959506988525,
      "learning_rate": 3.789663294997922e-05,
      "loss": 3.7699,
      "step": 3800
    },
    {
      "epoch": 0.5279201884439518,
      "grad_norm": 8.06016731262207,
      "learning_rate": 3.7891090480809206e-05,
      "loss": 3.5766,
      "step": 3810
    },
    {
      "epoch": 0.5293058057364556,
      "grad_norm": 8.869104385375977,
      "learning_rate": 3.788554801163919e-05,
      "loss": 3.6522,
      "step": 3820
    },
    {
      "epoch": 0.5306914230289594,
      "grad_norm": 7.2064714431762695,
      "learning_rate": 3.7880005542469176e-05,
      "loss": 3.9206,
      "step": 3830
    },
    {
      "epoch": 0.5320770403214632,
      "grad_norm": 9.955928802490234,
      "learning_rate": 3.787446307329916e-05,
      "loss": 3.8033,
      "step": 3840
    },
    {
      "epoch": 0.533462657613967,
      "grad_norm": 6.805974960327148,
      "learning_rate": 3.786892060412915e-05,
      "loss": 3.5243,
      "step": 3850
    },
    {
      "epoch": 0.5348482749064708,
      "grad_norm": 6.901329040527344,
      "learning_rate": 3.786337813495913e-05,
      "loss": 3.5031,
      "step": 3860
    },
    {
      "epoch": 0.5362338921989747,
      "grad_norm": 9.184835433959961,
      "learning_rate": 3.785783566578911e-05,
      "loss": 4.2353,
      "step": 3870
    },
    {
      "epoch": 0.5376195094914784,
      "grad_norm": 9.975028038024902,
      "learning_rate": 3.78522931966191e-05,
      "loss": 3.9128,
      "step": 3880
    },
    {
      "epoch": 0.5390051267839823,
      "grad_norm": 6.185486316680908,
      "learning_rate": 3.784675072744908e-05,
      "loss": 3.6064,
      "step": 3890
    },
    {
      "epoch": 0.5403907440764861,
      "grad_norm": 6.983075141906738,
      "learning_rate": 3.784120825827906e-05,
      "loss": 3.5806,
      "step": 3900
    },
    {
      "epoch": 0.5417763613689899,
      "grad_norm": 6.127877235412598,
      "learning_rate": 3.783566578910905e-05,
      "loss": 4.0072,
      "step": 3910
    },
    {
      "epoch": 0.5431619786614937,
      "grad_norm": 7.925839424133301,
      "learning_rate": 3.7830123319939034e-05,
      "loss": 3.7758,
      "step": 3920
    },
    {
      "epoch": 0.5445475959539975,
      "grad_norm": 6.580527305603027,
      "learning_rate": 3.782458085076902e-05,
      "loss": 3.5634,
      "step": 3930
    },
    {
      "epoch": 0.5459332132465013,
      "grad_norm": 10.683670997619629,
      "learning_rate": 3.7819038381599004e-05,
      "loss": 3.2937,
      "step": 3940
    },
    {
      "epoch": 0.5473188305390051,
      "grad_norm": 6.627694129943848,
      "learning_rate": 3.781349591242899e-05,
      "loss": 3.4124,
      "step": 3950
    },
    {
      "epoch": 0.548704447831509,
      "grad_norm": 7.61181640625,
      "learning_rate": 3.7807953443258975e-05,
      "loss": 3.6448,
      "step": 3960
    },
    {
      "epoch": 0.5500900651240127,
      "grad_norm": 6.80206298828125,
      "learning_rate": 3.780241097408896e-05,
      "loss": 3.6569,
      "step": 3970
    },
    {
      "epoch": 0.5514756824165166,
      "grad_norm": 8.734259605407715,
      "learning_rate": 3.7796868504918945e-05,
      "loss": 3.5117,
      "step": 3980
    },
    {
      "epoch": 0.5528612997090203,
      "grad_norm": 7.6578521728515625,
      "learning_rate": 3.779132603574893e-05,
      "loss": 3.6964,
      "step": 3990
    },
    {
      "epoch": 0.5542469170015242,
      "grad_norm": 10.642303466796875,
      "learning_rate": 3.7785783566578916e-05,
      "loss": 3.5716,
      "step": 4000
    },
    {
      "epoch": 0.555632534294028,
      "grad_norm": 7.64353609085083,
      "learning_rate": 3.77802410974089e-05,
      "loss": 3.7729,
      "step": 4010
    },
    {
      "epoch": 0.5570181515865318,
      "grad_norm": 9.349806785583496,
      "learning_rate": 3.777469862823888e-05,
      "loss": 3.4714,
      "step": 4020
    },
    {
      "epoch": 0.5584037688790356,
      "grad_norm": 8.441808700561523,
      "learning_rate": 3.776915615906887e-05,
      "loss": 3.9393,
      "step": 4030
    },
    {
      "epoch": 0.5597893861715394,
      "grad_norm": 7.424409866333008,
      "learning_rate": 3.776361368989885e-05,
      "loss": 3.4823,
      "step": 4040
    },
    {
      "epoch": 0.5611750034640433,
      "grad_norm": 6.408215045928955,
      "learning_rate": 3.775807122072884e-05,
      "loss": 3.5329,
      "step": 4050
    },
    {
      "epoch": 0.562560620756547,
      "grad_norm": 9.063393592834473,
      "learning_rate": 3.775252875155882e-05,
      "loss": 3.3013,
      "step": 4060
    },
    {
      "epoch": 0.5639462380490509,
      "grad_norm": 8.127518653869629,
      "learning_rate": 3.774698628238881e-05,
      "loss": 3.4415,
      "step": 4070
    },
    {
      "epoch": 0.5653318553415546,
      "grad_norm": 5.932601451873779,
      "learning_rate": 3.774144381321879e-05,
      "loss": 3.6178,
      "step": 4080
    },
    {
      "epoch": 0.5667174726340585,
      "grad_norm": 7.283801555633545,
      "learning_rate": 3.773590134404878e-05,
      "loss": 3.7567,
      "step": 4090
    },
    {
      "epoch": 0.5681030899265623,
      "grad_norm": 5.904545783996582,
      "learning_rate": 3.773035887487876e-05,
      "loss": 3.0815,
      "step": 4100
    },
    {
      "epoch": 0.5694887072190661,
      "grad_norm": 7.050858020782471,
      "learning_rate": 3.772481640570875e-05,
      "loss": 3.2101,
      "step": 4110
    },
    {
      "epoch": 0.5708743245115699,
      "grad_norm": 5.723398208618164,
      "learning_rate": 3.771927393653873e-05,
      "loss": 3.7572,
      "step": 4120
    },
    {
      "epoch": 0.5722599418040737,
      "grad_norm": 7.61580228805542,
      "learning_rate": 3.7713731467368714e-05,
      "loss": 3.3095,
      "step": 4130
    },
    {
      "epoch": 0.5736455590965776,
      "grad_norm": 11.963789939880371,
      "learning_rate": 3.77081889981987e-05,
      "loss": 3.6386,
      "step": 4140
    },
    {
      "epoch": 0.5750311763890813,
      "grad_norm": 6.554729461669922,
      "learning_rate": 3.7702646529028684e-05,
      "loss": 3.9057,
      "step": 4150
    },
    {
      "epoch": 0.5764167936815852,
      "grad_norm": 8.307421684265137,
      "learning_rate": 3.7697104059858666e-05,
      "loss": 3.9368,
      "step": 4160
    },
    {
      "epoch": 0.5778024109740889,
      "grad_norm": 7.160606384277344,
      "learning_rate": 3.7691561590688655e-05,
      "loss": 3.452,
      "step": 4170
    },
    {
      "epoch": 0.5791880282665928,
      "grad_norm": 7.372221946716309,
      "learning_rate": 3.768601912151864e-05,
      "loss": 3.4417,
      "step": 4180
    },
    {
      "epoch": 0.5805736455590966,
      "grad_norm": 8.586249351501465,
      "learning_rate": 3.7680476652348625e-05,
      "loss": 3.3516,
      "step": 4190
    },
    {
      "epoch": 0.5819592628516004,
      "grad_norm": 7.056244373321533,
      "learning_rate": 3.7674934183178614e-05,
      "loss": 3.3302,
      "step": 4200
    },
    {
      "epoch": 0.5833448801441042,
      "grad_norm": 8.204401016235352,
      "learning_rate": 3.7669391714008596e-05,
      "loss": 3.4697,
      "step": 4210
    },
    {
      "epoch": 0.584730497436608,
      "grad_norm": 12.10546875,
      "learning_rate": 3.766384924483858e-05,
      "loss": 3.6236,
      "step": 4220
    },
    {
      "epoch": 0.5861161147291118,
      "grad_norm": 8.878767967224121,
      "learning_rate": 3.7658306775668566e-05,
      "loss": 3.7396,
      "step": 4230
    },
    {
      "epoch": 0.5875017320216156,
      "grad_norm": 8.618685722351074,
      "learning_rate": 3.765276430649855e-05,
      "loss": 3.433,
      "step": 4240
    },
    {
      "epoch": 0.5888873493141195,
      "grad_norm": 7.753961086273193,
      "learning_rate": 3.764722183732853e-05,
      "loss": 3.3649,
      "step": 4250
    },
    {
      "epoch": 0.5902729666066232,
      "grad_norm": 9.343360900878906,
      "learning_rate": 3.764167936815852e-05,
      "loss": 3.9028,
      "step": 4260
    },
    {
      "epoch": 0.5916585838991271,
      "grad_norm": 7.434764385223389,
      "learning_rate": 3.76361368989885e-05,
      "loss": 3.0467,
      "step": 4270
    },
    {
      "epoch": 0.5930442011916309,
      "grad_norm": 9.472795486450195,
      "learning_rate": 3.763059442981848e-05,
      "loss": 3.6932,
      "step": 4280
    },
    {
      "epoch": 0.5944298184841347,
      "grad_norm": 6.242735862731934,
      "learning_rate": 3.762505196064847e-05,
      "loss": 3.3643,
      "step": 4290
    },
    {
      "epoch": 0.5958154357766385,
      "grad_norm": 7.6123552322387695,
      "learning_rate": 3.761950949147846e-05,
      "loss": 3.9294,
      "step": 4300
    },
    {
      "epoch": 0.5972010530691423,
      "grad_norm": 6.654987812042236,
      "learning_rate": 3.761396702230844e-05,
      "loss": 3.8265,
      "step": 4310
    },
    {
      "epoch": 0.5985866703616461,
      "grad_norm": 7.277946949005127,
      "learning_rate": 3.760842455313843e-05,
      "loss": 2.936,
      "step": 4320
    },
    {
      "epoch": 0.5999722876541499,
      "grad_norm": 6.698719501495361,
      "learning_rate": 3.760288208396841e-05,
      "loss": 3.5956,
      "step": 4330
    },
    {
      "epoch": 0.6013579049466538,
      "grad_norm": 6.648393154144287,
      "learning_rate": 3.7597339614798394e-05,
      "loss": 3.596,
      "step": 4340
    },
    {
      "epoch": 0.6027435222391575,
      "grad_norm": 6.744030475616455,
      "learning_rate": 3.759179714562838e-05,
      "loss": 3.5478,
      "step": 4350
    },
    {
      "epoch": 0.6041291395316614,
      "grad_norm": 7.792675971984863,
      "learning_rate": 3.7586254676458364e-05,
      "loss": 3.4697,
      "step": 4360
    },
    {
      "epoch": 0.6055147568241651,
      "grad_norm": 6.473630428314209,
      "learning_rate": 3.7580712207288346e-05,
      "loss": 3.6501,
      "step": 4370
    },
    {
      "epoch": 0.606900374116669,
      "grad_norm": 7.65457010269165,
      "learning_rate": 3.7575169738118335e-05,
      "loss": 3.42,
      "step": 4380
    },
    {
      "epoch": 0.6082859914091728,
      "grad_norm": 6.397698879241943,
      "learning_rate": 3.7569627268948317e-05,
      "loss": 3.3175,
      "step": 4390
    },
    {
      "epoch": 0.6096716087016766,
      "grad_norm": 8.789413452148438,
      "learning_rate": 3.7564084799778305e-05,
      "loss": 3.7918,
      "step": 4400
    },
    {
      "epoch": 0.6110572259941804,
      "grad_norm": 9.276609420776367,
      "learning_rate": 3.755854233060829e-05,
      "loss": 3.2853,
      "step": 4410
    },
    {
      "epoch": 0.6124428432866842,
      "grad_norm": 7.775903224945068,
      "learning_rate": 3.7552999861438276e-05,
      "loss": 3.552,
      "step": 4420
    },
    {
      "epoch": 0.613828460579188,
      "grad_norm": 6.448571681976318,
      "learning_rate": 3.754745739226826e-05,
      "loss": 3.1928,
      "step": 4430
    },
    {
      "epoch": 0.6152140778716918,
      "grad_norm": 6.620352268218994,
      "learning_rate": 3.7541914923098246e-05,
      "loss": 3.1199,
      "step": 4440
    },
    {
      "epoch": 0.6165996951641957,
      "grad_norm": 11.080514907836914,
      "learning_rate": 3.753637245392823e-05,
      "loss": 3.4617,
      "step": 4450
    },
    {
      "epoch": 0.6179853124566994,
      "grad_norm": 9.240049362182617,
      "learning_rate": 3.753082998475822e-05,
      "loss": 3.2792,
      "step": 4460
    },
    {
      "epoch": 0.6193709297492033,
      "grad_norm": 7.859140872955322,
      "learning_rate": 3.75252875155882e-05,
      "loss": 3.3439,
      "step": 4470
    },
    {
      "epoch": 0.6207565470417071,
      "grad_norm": 8.578609466552734,
      "learning_rate": 3.751974504641818e-05,
      "loss": 4.1817,
      "step": 4480
    },
    {
      "epoch": 0.6221421643342109,
      "grad_norm": 7.997689723968506,
      "learning_rate": 3.751420257724817e-05,
      "loss": 3.1808,
      "step": 4490
    },
    {
      "epoch": 0.6235277816267147,
      "grad_norm": 6.311728000640869,
      "learning_rate": 3.750866010807815e-05,
      "loss": 3.2731,
      "step": 4500
    },
    {
      "epoch": 0.6249133989192185,
      "grad_norm": 9.832276344299316,
      "learning_rate": 3.750311763890813e-05,
      "loss": 3.038,
      "step": 4510
    },
    {
      "epoch": 0.6262990162117223,
      "grad_norm": 10.989655494689941,
      "learning_rate": 3.749757516973812e-05,
      "loss": 3.0331,
      "step": 4520
    },
    {
      "epoch": 0.6276846335042261,
      "grad_norm": 8.732982635498047,
      "learning_rate": 3.74920327005681e-05,
      "loss": 3.9848,
      "step": 4530
    },
    {
      "epoch": 0.62907025079673,
      "grad_norm": 8.85763168334961,
      "learning_rate": 3.748649023139809e-05,
      "loss": 3.6105,
      "step": 4540
    },
    {
      "epoch": 0.6304558680892337,
      "grad_norm": 7.461187362670898,
      "learning_rate": 3.7480947762228074e-05,
      "loss": 3.4459,
      "step": 4550
    },
    {
      "epoch": 0.6318414853817376,
      "grad_norm": 7.351160049438477,
      "learning_rate": 3.747540529305806e-05,
      "loss": 3.2467,
      "step": 4560
    },
    {
      "epoch": 0.6332271026742414,
      "grad_norm": 7.852899551391602,
      "learning_rate": 3.7469862823888044e-05,
      "loss": 3.3704,
      "step": 4570
    },
    {
      "epoch": 0.6346127199667452,
      "grad_norm": 7.456647872924805,
      "learning_rate": 3.746432035471803e-05,
      "loss": 3.3397,
      "step": 4580
    },
    {
      "epoch": 0.635998337259249,
      "grad_norm": 7.675103664398193,
      "learning_rate": 3.7458777885548015e-05,
      "loss": 4.0873,
      "step": 4590
    },
    {
      "epoch": 0.6373839545517528,
      "grad_norm": 6.6706342697143555,
      "learning_rate": 3.7453235416378e-05,
      "loss": 3.755,
      "step": 4600
    },
    {
      "epoch": 0.6387695718442566,
      "grad_norm": 8.364500045776367,
      "learning_rate": 3.7447692947207985e-05,
      "loss": 3.1279,
      "step": 4610
    },
    {
      "epoch": 0.6401551891367604,
      "grad_norm": 8.414104461669922,
      "learning_rate": 3.744215047803797e-05,
      "loss": 3.8456,
      "step": 4620
    },
    {
      "epoch": 0.6415408064292643,
      "grad_norm": 9.228647232055664,
      "learning_rate": 3.743660800886795e-05,
      "loss": 3.5666,
      "step": 4630
    },
    {
      "epoch": 0.642926423721768,
      "grad_norm": 9.395487785339355,
      "learning_rate": 3.743106553969794e-05,
      "loss": 3.7508,
      "step": 4640
    },
    {
      "epoch": 0.6443120410142719,
      "grad_norm": 9.663739204406738,
      "learning_rate": 3.742552307052792e-05,
      "loss": 3.6617,
      "step": 4650
    },
    {
      "epoch": 0.6456976583067757,
      "grad_norm": 6.492980003356934,
      "learning_rate": 3.741998060135791e-05,
      "loss": 3.328,
      "step": 4660
    },
    {
      "epoch": 0.6470832755992795,
      "grad_norm": 8.113816261291504,
      "learning_rate": 3.74144381321879e-05,
      "loss": 3.2722,
      "step": 4670
    },
    {
      "epoch": 0.6484688928917833,
      "grad_norm": 11.450458526611328,
      "learning_rate": 3.740889566301788e-05,
      "loss": 3.1604,
      "step": 4680
    },
    {
      "epoch": 0.6498545101842871,
      "grad_norm": 7.290822505950928,
      "learning_rate": 3.740335319384787e-05,
      "loss": 3.4169,
      "step": 4690
    },
    {
      "epoch": 0.6512401274767909,
      "grad_norm": 7.547003269195557,
      "learning_rate": 3.739781072467785e-05,
      "loss": 3.5612,
      "step": 4700
    },
    {
      "epoch": 0.6526257447692947,
      "grad_norm": 7.8197479248046875,
      "learning_rate": 3.739226825550783e-05,
      "loss": 3.6498,
      "step": 4710
    },
    {
      "epoch": 0.6540113620617986,
      "grad_norm": 8.099791526794434,
      "learning_rate": 3.738672578633782e-05,
      "loss": 2.9601,
      "step": 4720
    },
    {
      "epoch": 0.6553969793543023,
      "grad_norm": 8.683972358703613,
      "learning_rate": 3.73811833171678e-05,
      "loss": 3.6298,
      "step": 4730
    },
    {
      "epoch": 0.6567825966468062,
      "grad_norm": 8.17721939086914,
      "learning_rate": 3.737564084799778e-05,
      "loss": 3.3609,
      "step": 4740
    },
    {
      "epoch": 0.6581682139393099,
      "grad_norm": 7.6365461349487305,
      "learning_rate": 3.737009837882777e-05,
      "loss": 3.1804,
      "step": 4750
    },
    {
      "epoch": 0.6595538312318138,
      "grad_norm": 6.931866645812988,
      "learning_rate": 3.7364555909657754e-05,
      "loss": 3.6672,
      "step": 4760
    },
    {
      "epoch": 0.6609394485243176,
      "grad_norm": 8.275534629821777,
      "learning_rate": 3.735901344048774e-05,
      "loss": 3.6722,
      "step": 4770
    },
    {
      "epoch": 0.6623250658168214,
      "grad_norm": 7.894296169281006,
      "learning_rate": 3.7353470971317724e-05,
      "loss": 3.0785,
      "step": 4780
    },
    {
      "epoch": 0.6637106831093252,
      "grad_norm": 7.153256893157959,
      "learning_rate": 3.734792850214771e-05,
      "loss": 3.5425,
      "step": 4790
    },
    {
      "epoch": 0.665096300401829,
      "grad_norm": 7.587490081787109,
      "learning_rate": 3.7342386032977695e-05,
      "loss": 3.495,
      "step": 4800
    },
    {
      "epoch": 0.6664819176943328,
      "grad_norm": 6.854140758514404,
      "learning_rate": 3.7336843563807683e-05,
      "loss": 3.4395,
      "step": 4810
    },
    {
      "epoch": 0.6678675349868366,
      "grad_norm": 13.115524291992188,
      "learning_rate": 3.7331301094637665e-05,
      "loss": 3.3632,
      "step": 4820
    },
    {
      "epoch": 0.6692531522793405,
      "grad_norm": 7.555985450744629,
      "learning_rate": 3.732575862546765e-05,
      "loss": 3.8256,
      "step": 4830
    },
    {
      "epoch": 0.6706387695718442,
      "grad_norm": 8.976499557495117,
      "learning_rate": 3.7320216156297636e-05,
      "loss": 3.5672,
      "step": 4840
    },
    {
      "epoch": 0.6720243868643481,
      "grad_norm": 9.589607238769531,
      "learning_rate": 3.731467368712762e-05,
      "loss": 3.6747,
      "step": 4850
    },
    {
      "epoch": 0.6734100041568519,
      "grad_norm": 7.32848596572876,
      "learning_rate": 3.73091312179576e-05,
      "loss": 3.4437,
      "step": 4860
    },
    {
      "epoch": 0.6747956214493557,
      "grad_norm": 7.672504901885986,
      "learning_rate": 3.730358874878759e-05,
      "loss": 3.0924,
      "step": 4870
    },
    {
      "epoch": 0.6761812387418595,
      "grad_norm": 9.473712921142578,
      "learning_rate": 3.729804627961757e-05,
      "loss": 3.4295,
      "step": 4880
    },
    {
      "epoch": 0.6775668560343633,
      "grad_norm": 7.4663543701171875,
      "learning_rate": 3.729250381044756e-05,
      "loss": 3.5049,
      "step": 4890
    },
    {
      "epoch": 0.6789524733268671,
      "grad_norm": 7.711764812469482,
      "learning_rate": 3.728696134127754e-05,
      "loss": 3.4588,
      "step": 4900
    },
    {
      "epoch": 0.6803380906193709,
      "grad_norm": 7.187010288238525,
      "learning_rate": 3.728141887210753e-05,
      "loss": 3.6121,
      "step": 4910
    },
    {
      "epoch": 0.6817237079118748,
      "grad_norm": 8.389269828796387,
      "learning_rate": 3.727587640293751e-05,
      "loss": 3.2283,
      "step": 4920
    },
    {
      "epoch": 0.6831093252043785,
      "grad_norm": 7.23189115524292,
      "learning_rate": 3.72703339337675e-05,
      "loss": 3.3559,
      "step": 4930
    },
    {
      "epoch": 0.6844949424968824,
      "grad_norm": 8.618982315063477,
      "learning_rate": 3.726479146459748e-05,
      "loss": 3.7159,
      "step": 4940
    },
    {
      "epoch": 0.6858805597893862,
      "grad_norm": 8.39324951171875,
      "learning_rate": 3.7259248995427463e-05,
      "loss": 3.6129,
      "step": 4950
    },
    {
      "epoch": 0.68726617708189,
      "grad_norm": 6.715205669403076,
      "learning_rate": 3.725370652625745e-05,
      "loss": 3.4762,
      "step": 4960
    },
    {
      "epoch": 0.6886517943743938,
      "grad_norm": 6.864809989929199,
      "learning_rate": 3.7248164057087434e-05,
      "loss": 3.3871,
      "step": 4970
    },
    {
      "epoch": 0.6900374116668976,
      "grad_norm": 5.723362922668457,
      "learning_rate": 3.724262158791742e-05,
      "loss": 3.2592,
      "step": 4980
    },
    {
      "epoch": 0.6914230289594014,
      "grad_norm": 8.49991512298584,
      "learning_rate": 3.7237079118747404e-05,
      "loss": 4.0511,
      "step": 4990
    },
    {
      "epoch": 0.6928086462519052,
      "grad_norm": 9.350743293762207,
      "learning_rate": 3.7231536649577386e-05,
      "loss": 3.7596,
      "step": 5000
    },
    {
      "epoch": 0.694194263544409,
      "grad_norm": 6.331125259399414,
      "learning_rate": 3.7225994180407375e-05,
      "loss": 3.3488,
      "step": 5010
    },
    {
      "epoch": 0.6955798808369128,
      "grad_norm": 7.543835639953613,
      "learning_rate": 3.722045171123736e-05,
      "loss": 2.9319,
      "step": 5020
    },
    {
      "epoch": 0.6969654981294167,
      "grad_norm": 11.185614585876465,
      "learning_rate": 3.7214909242067345e-05,
      "loss": 3.5271,
      "step": 5030
    },
    {
      "epoch": 0.6983511154219205,
      "grad_norm": NaN,
      "learning_rate": 3.7209366772897334e-05,
      "loss": 3.0244,
      "step": 5040
    },
    {
      "epoch": 0.6997367327144243,
      "grad_norm": 6.531094074249268,
      "learning_rate": 3.7204378550644314e-05,
      "loss": 3.7806,
      "step": 5050
    },
    {
      "epoch": 0.7011223500069281,
      "grad_norm": 12.948707580566406,
      "learning_rate": 3.7198836081474296e-05,
      "loss": 3.6003,
      "step": 5060
    },
    {
      "epoch": 0.7025079672994319,
      "grad_norm": 6.628715991973877,
      "learning_rate": 3.7193293612304284e-05,
      "loss": 3.4367,
      "step": 5070
    },
    {
      "epoch": 0.7038935845919357,
      "grad_norm": 8.193500518798828,
      "learning_rate": 3.718775114313427e-05,
      "loss": 3.0159,
      "step": 5080
    },
    {
      "epoch": 0.7052792018844395,
      "grad_norm": 8.731579780578613,
      "learning_rate": 3.7182208673964255e-05,
      "loss": 3.1028,
      "step": 5090
    },
    {
      "epoch": 0.7066648191769433,
      "grad_norm": 8.218259811401367,
      "learning_rate": 3.717666620479424e-05,
      "loss": 3.8023,
      "step": 5100
    },
    {
      "epoch": 0.7080504364694471,
      "grad_norm": 7.937808513641357,
      "learning_rate": 3.7171123735624225e-05,
      "loss": 3.0685,
      "step": 5110
    },
    {
      "epoch": 0.709436053761951,
      "grad_norm": 8.275113105773926,
      "learning_rate": 3.716558126645421e-05,
      "loss": 3.3089,
      "step": 5120
    },
    {
      "epoch": 0.7108216710544547,
      "grad_norm": 8.711161613464355,
      "learning_rate": 3.7160038797284196e-05,
      "loss": 3.3171,
      "step": 5130
    },
    {
      "epoch": 0.7122072883469586,
      "grad_norm": 7.481064796447754,
      "learning_rate": 3.715449632811418e-05,
      "loss": 3.3375,
      "step": 5140
    },
    {
      "epoch": 0.7135929056394624,
      "grad_norm": 8.143826484680176,
      "learning_rate": 3.714895385894416e-05,
      "loss": 3.6364,
      "step": 5150
    },
    {
      "epoch": 0.7149785229319662,
      "grad_norm": 8.448323249816895,
      "learning_rate": 3.714341138977415e-05,
      "loss": 3.5336,
      "step": 5160
    },
    {
      "epoch": 0.71636414022447,
      "grad_norm": 8.484060287475586,
      "learning_rate": 3.713786892060413e-05,
      "loss": 3.3172,
      "step": 5170
    },
    {
      "epoch": 0.7177497575169738,
      "grad_norm": 8.930606842041016,
      "learning_rate": 3.713232645143412e-05,
      "loss": 3.7909,
      "step": 5180
    },
    {
      "epoch": 0.7191353748094776,
      "grad_norm": 7.034834384918213,
      "learning_rate": 3.71267839822641e-05,
      "loss": 3.0177,
      "step": 5190
    },
    {
      "epoch": 0.7205209921019814,
      "grad_norm": 6.985345363616943,
      "learning_rate": 3.712124151309409e-05,
      "loss": 4.0106,
      "step": 5200
    },
    {
      "epoch": 0.7219066093944853,
      "grad_norm": 10.692299842834473,
      "learning_rate": 3.711569904392407e-05,
      "loss": 2.957,
      "step": 5210
    },
    {
      "epoch": 0.723292226686989,
      "grad_norm": 8.701611518859863,
      "learning_rate": 3.711015657475406e-05,
      "loss": 3.3866,
      "step": 5220
    },
    {
      "epoch": 0.7246778439794929,
      "grad_norm": 6.989468574523926,
      "learning_rate": 3.710461410558404e-05,
      "loss": 3.6202,
      "step": 5230
    },
    {
      "epoch": 0.7260634612719967,
      "grad_norm": 6.00337028503418,
      "learning_rate": 3.709907163641402e-05,
      "loss": 3.1356,
      "step": 5240
    },
    {
      "epoch": 0.7274490785645005,
      "grad_norm": 11.38056468963623,
      "learning_rate": 3.709352916724401e-05,
      "loss": 4.0089,
      "step": 5250
    },
    {
      "epoch": 0.7288346958570043,
      "grad_norm": 9.710163116455078,
      "learning_rate": 3.7087986698073994e-05,
      "loss": 2.9866,
      "step": 5260
    },
    {
      "epoch": 0.7302203131495081,
      "grad_norm": 7.614070892333984,
      "learning_rate": 3.7082444228903976e-05,
      "loss": 3.5552,
      "step": 5270
    },
    {
      "epoch": 0.7316059304420119,
      "grad_norm": 7.493852138519287,
      "learning_rate": 3.7076901759733964e-05,
      "loss": 3.3475,
      "step": 5280
    },
    {
      "epoch": 0.7329915477345157,
      "grad_norm": 6.675519943237305,
      "learning_rate": 3.7071359290563946e-05,
      "loss": 3.3537,
      "step": 5290
    },
    {
      "epoch": 0.7343771650270196,
      "grad_norm": 11.403499603271484,
      "learning_rate": 3.7065816821393935e-05,
      "loss": 3.2674,
      "step": 5300
    },
    {
      "epoch": 0.7357627823195233,
      "grad_norm": 6.701857566833496,
      "learning_rate": 3.7060274352223917e-05,
      "loss": 3.1403,
      "step": 5310
    },
    {
      "epoch": 0.7371483996120272,
      "grad_norm": 8.382800102233887,
      "learning_rate": 3.70552861299709e-05,
      "loss": 3.7357,
      "step": 5320
    },
    {
      "epoch": 0.738534016904531,
      "grad_norm": 9.55066204071045,
      "learning_rate": 3.704974366080089e-05,
      "loss": 3.2054,
      "step": 5330
    },
    {
      "epoch": 0.7399196341970348,
      "grad_norm": 10.282367706298828,
      "learning_rate": 3.7044201191630873e-05,
      "loss": 3.3891,
      "step": 5340
    },
    {
      "epoch": 0.7413052514895386,
      "grad_norm": 7.772054195404053,
      "learning_rate": 3.7038658722460855e-05,
      "loss": 3.6951,
      "step": 5350
    },
    {
      "epoch": 0.7426908687820424,
      "grad_norm": 7.6931023597717285,
      "learning_rate": 3.7033116253290844e-05,
      "loss": 3.1515,
      "step": 5360
    },
    {
      "epoch": 0.7440764860745462,
      "grad_norm": 6.121262073516846,
      "learning_rate": 3.702757378412083e-05,
      "loss": 3.488,
      "step": 5370
    },
    {
      "epoch": 0.74546210336705,
      "grad_norm": 5.871262073516846,
      "learning_rate": 3.7022031314950814e-05,
      "loss": 3.3188,
      "step": 5380
    },
    {
      "epoch": 0.7468477206595538,
      "grad_norm": 7.3472371101379395,
      "learning_rate": 3.70164888457808e-05,
      "loss": 3.5632,
      "step": 5390
    },
    {
      "epoch": 0.7482333379520576,
      "grad_norm": 6.784160614013672,
      "learning_rate": 3.7010946376610785e-05,
      "loss": 3.7011,
      "step": 5400
    },
    {
      "epoch": 0.7496189552445615,
      "grad_norm": 10.287129402160645,
      "learning_rate": 3.700540390744077e-05,
      "loss": 3.7233,
      "step": 5410
    },
    {
      "epoch": 0.7510045725370653,
      "grad_norm": 5.907973766326904,
      "learning_rate": 3.6999861438270755e-05,
      "loss": 3.7243,
      "step": 5420
    },
    {
      "epoch": 0.7523901898295691,
      "grad_norm": 6.278461933135986,
      "learning_rate": 3.699431896910074e-05,
      "loss": 3.0172,
      "step": 5430
    },
    {
      "epoch": 0.7537758071220729,
      "grad_norm": 7.903363227844238,
      "learning_rate": 3.698877649993072e-05,
      "loss": 3.2555,
      "step": 5440
    },
    {
      "epoch": 0.7551614244145767,
      "grad_norm": 9.25765323638916,
      "learning_rate": 3.698323403076071e-05,
      "loss": 3.3322,
      "step": 5450
    },
    {
      "epoch": 0.7565470417070805,
      "grad_norm": 6.3154120445251465,
      "learning_rate": 3.697769156159069e-05,
      "loss": 3.0663,
      "step": 5460
    },
    {
      "epoch": 0.7579326589995843,
      "grad_norm": 7.857911586761475,
      "learning_rate": 3.697214909242068e-05,
      "loss": 3.4772,
      "step": 5470
    },
    {
      "epoch": 0.7593182762920881,
      "grad_norm": 7.478851318359375,
      "learning_rate": 3.696660662325066e-05,
      "loss": 3.1259,
      "step": 5480
    },
    {
      "epoch": 0.7607038935845919,
      "grad_norm": 10.894887924194336,
      "learning_rate": 3.696106415408065e-05,
      "loss": 3.0655,
      "step": 5490
    },
    {
      "epoch": 0.7620895108770958,
      "grad_norm": 7.9471282958984375,
      "learning_rate": 3.695552168491063e-05,
      "loss": 3.6598,
      "step": 5500
    },
    {
      "epoch": 0.7634751281695995,
      "grad_norm": 6.7472028732299805,
      "learning_rate": 3.694997921574062e-05,
      "loss": 3.5074,
      "step": 5510
    },
    {
      "epoch": 0.7648607454621034,
      "grad_norm": 6.872877597808838,
      "learning_rate": 3.69444367465706e-05,
      "loss": 3.307,
      "step": 5520
    },
    {
      "epoch": 0.7662463627546072,
      "grad_norm": 7.747117042541504,
      "learning_rate": 3.693889427740058e-05,
      "loss": 3.8583,
      "step": 5530
    },
    {
      "epoch": 0.767631980047111,
      "grad_norm": 14.024991035461426,
      "learning_rate": 3.693335180823057e-05,
      "loss": 3.2923,
      "step": 5540
    },
    {
      "epoch": 0.7690175973396148,
      "grad_norm": 7.985296249389648,
      "learning_rate": 3.6927809339060554e-05,
      "loss": 3.6091,
      "step": 5550
    },
    {
      "epoch": 0.7704032146321186,
      "grad_norm": 7.287601470947266,
      "learning_rate": 3.6922266869890535e-05,
      "loss": 3.6836,
      "step": 5560
    },
    {
      "epoch": 0.7717888319246224,
      "grad_norm": 7.7572340965271,
      "learning_rate": 3.6916724400720524e-05,
      "loss": 3.6977,
      "step": 5570
    },
    {
      "epoch": 0.7731744492171262,
      "grad_norm": 5.917104721069336,
      "learning_rate": 3.6911181931550506e-05,
      "loss": 3.3644,
      "step": 5580
    },
    {
      "epoch": 0.77456006650963,
      "grad_norm": 6.214598655700684,
      "learning_rate": 3.6905639462380495e-05,
      "loss": 3.2879,
      "step": 5590
    },
    {
      "epoch": 0.7759456838021338,
      "grad_norm": 9.517342567443848,
      "learning_rate": 3.6900096993210476e-05,
      "loss": 3.8049,
      "step": 5600
    },
    {
      "epoch": 0.7773313010946377,
      "grad_norm": 7.6969170570373535,
      "learning_rate": 3.6894554524040465e-05,
      "loss": 3.7038,
      "step": 5610
    },
    {
      "epoch": 0.7787169183871415,
      "grad_norm": 8.090892791748047,
      "learning_rate": 3.688901205487045e-05,
      "loss": 3.906,
      "step": 5620
    },
    {
      "epoch": 0.7801025356796453,
      "grad_norm": 7.953388214111328,
      "learning_rate": 3.6883469585700436e-05,
      "loss": 3.3241,
      "step": 5630
    },
    {
      "epoch": 0.7814881529721491,
      "grad_norm": 7.632543563842773,
      "learning_rate": 3.687792711653042e-05,
      "loss": 3.401,
      "step": 5640
    },
    {
      "epoch": 0.7828737702646529,
      "grad_norm": 6.711528301239014,
      "learning_rate": 3.6872384647360406e-05,
      "loss": 3.5482,
      "step": 5650
    },
    {
      "epoch": 0.7842593875571567,
      "grad_norm": 7.1154465675354,
      "learning_rate": 3.686684217819039e-05,
      "loss": 2.9201,
      "step": 5660
    },
    {
      "epoch": 0.7856450048496605,
      "grad_norm": 7.633697509765625,
      "learning_rate": 3.686129970902037e-05,
      "loss": 3.4551,
      "step": 5670
    },
    {
      "epoch": 0.7870306221421643,
      "grad_norm": 6.113193988800049,
      "learning_rate": 3.685575723985036e-05,
      "loss": 3.6148,
      "step": 5680
    },
    {
      "epoch": 0.7884162394346681,
      "grad_norm": 7.190479278564453,
      "learning_rate": 3.685021477068034e-05,
      "loss": 3.7339,
      "step": 5690
    },
    {
      "epoch": 0.789801856727172,
      "grad_norm": 6.6243181228637695,
      "learning_rate": 3.684467230151032e-05,
      "loss": 3.3402,
      "step": 5700
    },
    {
      "epoch": 0.7911874740196758,
      "grad_norm": 8.34611701965332,
      "learning_rate": 3.683912983234031e-05,
      "loss": 3.3167,
      "step": 5710
    },
    {
      "epoch": 0.7925730913121796,
      "grad_norm": 9.013060569763184,
      "learning_rate": 3.683358736317029e-05,
      "loss": 3.6733,
      "step": 5720
    },
    {
      "epoch": 0.7939587086046834,
      "grad_norm": 8.379732131958008,
      "learning_rate": 3.682804489400028e-05,
      "loss": 3.5293,
      "step": 5730
    },
    {
      "epoch": 0.7953443258971872,
      "grad_norm": 10.437331199645996,
      "learning_rate": 3.682250242483027e-05,
      "loss": 3.7008,
      "step": 5740
    },
    {
      "epoch": 0.796729943189691,
      "grad_norm": 6.889904499053955,
      "learning_rate": 3.681695995566025e-05,
      "loss": 3.3148,
      "step": 5750
    },
    {
      "epoch": 0.7981155604821948,
      "grad_norm": 6.205615043640137,
      "learning_rate": 3.6811417486490234e-05,
      "loss": 3.8716,
      "step": 5760
    },
    {
      "epoch": 0.7995011777746986,
      "grad_norm": 8.902709007263184,
      "learning_rate": 3.680587501732022e-05,
      "loss": 3.4016,
      "step": 5770
    },
    {
      "epoch": 0.8008867950672024,
      "grad_norm": 8.419744491577148,
      "learning_rate": 3.6800332548150204e-05,
      "loss": 3.0324,
      "step": 5780
    },
    {
      "epoch": 0.8022724123597063,
      "grad_norm": 7.357639312744141,
      "learning_rate": 3.6794790078980186e-05,
      "loss": 3.591,
      "step": 5790
    },
    {
      "epoch": 0.80365802965221,
      "grad_norm": 12.264779090881348,
      "learning_rate": 3.6789247609810175e-05,
      "loss": 3.2631,
      "step": 5800
    },
    {
      "epoch": 0.8050436469447139,
      "grad_norm": 12.411816596984863,
      "learning_rate": 3.6783705140640156e-05,
      "loss": 3.6264,
      "step": 5810
    },
    {
      "epoch": 0.8064292642372177,
      "grad_norm": 8.488903045654297,
      "learning_rate": 3.677816267147014e-05,
      "loss": 3.1439,
      "step": 5820
    },
    {
      "epoch": 0.8078148815297215,
      "grad_norm": 6.748922348022461,
      "learning_rate": 3.677262020230013e-05,
      "loss": 3.1061,
      "step": 5830
    },
    {
      "epoch": 0.8092004988222253,
      "grad_norm": 11.038668632507324,
      "learning_rate": 3.676707773313011e-05,
      "loss": 3.5146,
      "step": 5840
    },
    {
      "epoch": 0.8105861161147291,
      "grad_norm": 8.034549713134766,
      "learning_rate": 3.67615352639601e-05,
      "loss": 3.3613,
      "step": 5850
    },
    {
      "epoch": 0.8119717334072329,
      "grad_norm": 6.786494255065918,
      "learning_rate": 3.6755992794790086e-05,
      "loss": 3.7747,
      "step": 5860
    },
    {
      "epoch": 0.8133573506997367,
      "grad_norm": 6.551872253417969,
      "learning_rate": 3.675045032562007e-05,
      "loss": 3.1687,
      "step": 5870
    },
    {
      "epoch": 0.8147429679922406,
      "grad_norm": 8.852572441101074,
      "learning_rate": 3.674490785645005e-05,
      "loss": 3.0228,
      "step": 5880
    },
    {
      "epoch": 0.8161285852847443,
      "grad_norm": 10.255352020263672,
      "learning_rate": 3.673936538728004e-05,
      "loss": 3.2852,
      "step": 5890
    },
    {
      "epoch": 0.8175142025772482,
      "grad_norm": 6.836305618286133,
      "learning_rate": 3.673382291811002e-05,
      "loss": 3.541,
      "step": 5900
    },
    {
      "epoch": 0.818899819869752,
      "grad_norm": 7.948088645935059,
      "learning_rate": 3.672828044894001e-05,
      "loss": 3.428,
      "step": 5910
    },
    {
      "epoch": 0.8202854371622558,
      "grad_norm": 6.542064666748047,
      "learning_rate": 3.672273797976999e-05,
      "loss": 3.0992,
      "step": 5920
    },
    {
      "epoch": 0.8216710544547596,
      "grad_norm": 8.809285163879395,
      "learning_rate": 3.671719551059997e-05,
      "loss": 3.2539,
      "step": 5930
    },
    {
      "epoch": 0.8230566717472634,
      "grad_norm": 6.870844841003418,
      "learning_rate": 3.671165304142996e-05,
      "loss": 3.0399,
      "step": 5940
    },
    {
      "epoch": 0.8244422890397672,
      "grad_norm": 7.706324577331543,
      "learning_rate": 3.670611057225994e-05,
      "loss": 3.3129,
      "step": 5950
    },
    {
      "epoch": 0.825827906332271,
      "grad_norm": 11.356697082519531,
      "learning_rate": 3.670056810308993e-05,
      "loss": 2.9053,
      "step": 5960
    },
    {
      "epoch": 0.8272135236247748,
      "grad_norm": 7.544647216796875,
      "learning_rate": 3.6695025633919914e-05,
      "loss": 3.3801,
      "step": 5970
    },
    {
      "epoch": 0.8285991409172786,
      "grad_norm": 8.655975341796875,
      "learning_rate": 3.66894831647499e-05,
      "loss": 3.6691,
      "step": 5980
    },
    {
      "epoch": 0.8299847582097825,
      "grad_norm": 7.011889934539795,
      "learning_rate": 3.6683940695579884e-05,
      "loss": 3.0386,
      "step": 5990
    },
    {
      "epoch": 0.8313703755022863,
      "grad_norm": 13.993122100830078,
      "learning_rate": 3.667839822640987e-05,
      "loss": 3.3431,
      "step": 6000
    },
    {
      "epoch": 0.8327559927947901,
      "grad_norm": 8.567042350769043,
      "learning_rate": 3.6672855757239855e-05,
      "loss": 3.3067,
      "step": 6010
    },
    {
      "epoch": 0.8341416100872939,
      "grad_norm": 5.44291877746582,
      "learning_rate": 3.6667313288069837e-05,
      "loss": 2.8729,
      "step": 6020
    },
    {
      "epoch": 0.8355272273797977,
      "grad_norm": 6.842301368713379,
      "learning_rate": 3.6661770818899825e-05,
      "loss": 3.741,
      "step": 6030
    },
    {
      "epoch": 0.8369128446723015,
      "grad_norm": 10.898592948913574,
      "learning_rate": 3.665622834972981e-05,
      "loss": 3.6191,
      "step": 6040
    },
    {
      "epoch": 0.8382984619648053,
      "grad_norm": 9.607603073120117,
      "learning_rate": 3.665068588055979e-05,
      "loss": 4.0206,
      "step": 6050
    },
    {
      "epoch": 0.8396840792573091,
      "grad_norm": 9.780701637268066,
      "learning_rate": 3.664514341138978e-05,
      "loss": 3.1196,
      "step": 6060
    },
    {
      "epoch": 0.8410696965498129,
      "grad_norm": 6.814725399017334,
      "learning_rate": 3.663960094221976e-05,
      "loss": 3.4957,
      "step": 6070
    },
    {
      "epoch": 0.8424553138423168,
      "grad_norm": 7.365749835968018,
      "learning_rate": 3.663405847304975e-05,
      "loss": 2.9822,
      "step": 6080
    },
    {
      "epoch": 0.8438409311348206,
      "grad_norm": 9.017024993896484,
      "learning_rate": 3.662851600387973e-05,
      "loss": 3.4828,
      "step": 6090
    },
    {
      "epoch": 0.8452265484273244,
      "grad_norm": 6.752485752105713,
      "learning_rate": 3.662297353470972e-05,
      "loss": 3.3115,
      "step": 6100
    },
    {
      "epoch": 0.8466121657198282,
      "grad_norm": 7.636025905609131,
      "learning_rate": 3.66174310655397e-05,
      "loss": 3.3106,
      "step": 6110
    },
    {
      "epoch": 0.847997783012332,
      "grad_norm": 7.262976169586182,
      "learning_rate": 3.661188859636969e-05,
      "loss": 3.609,
      "step": 6120
    },
    {
      "epoch": 0.8493834003048358,
      "grad_norm": 7.777398586273193,
      "learning_rate": 3.660634612719967e-05,
      "loss": 3.1305,
      "step": 6130
    },
    {
      "epoch": 0.8507690175973396,
      "grad_norm": 14.144330024719238,
      "learning_rate": 3.660080365802965e-05,
      "loss": 2.9658,
      "step": 6140
    },
    {
      "epoch": 0.8521546348898434,
      "grad_norm": 9.102546691894531,
      "learning_rate": 3.659526118885964e-05,
      "loss": 3.2471,
      "step": 6150
    },
    {
      "epoch": 0.8535402521823472,
      "grad_norm": 6.225594520568848,
      "learning_rate": 3.658971871968962e-05,
      "loss": 2.8747,
      "step": 6160
    },
    {
      "epoch": 0.854925869474851,
      "grad_norm": 10.153277397155762,
      "learning_rate": 3.6584176250519605e-05,
      "loss": 3.3079,
      "step": 6170
    },
    {
      "epoch": 0.8563114867673548,
      "grad_norm": 7.8543548583984375,
      "learning_rate": 3.6578633781349594e-05,
      "loss": 2.727,
      "step": 6180
    },
    {
      "epoch": 0.8576971040598587,
      "grad_norm": 8.446681022644043,
      "learning_rate": 3.6573091312179576e-05,
      "loss": 3.085,
      "step": 6190
    },
    {
      "epoch": 0.8590827213523625,
      "grad_norm": 6.9125285148620605,
      "learning_rate": 3.6567548843009564e-05,
      "loss": 3.6366,
      "step": 6200
    },
    {
      "epoch": 0.8604683386448663,
      "grad_norm": 8.102054595947266,
      "learning_rate": 3.6562006373839546e-05,
      "loss": 3.8375,
      "step": 6210
    },
    {
      "epoch": 0.8618539559373701,
      "grad_norm": 7.556258678436279,
      "learning_rate": 3.6556463904669535e-05,
      "loss": 3.1295,
      "step": 6220
    },
    {
      "epoch": 0.8632395732298739,
      "grad_norm": 6.6558403968811035,
      "learning_rate": 3.655092143549952e-05,
      "loss": 3.363,
      "step": 6230
    },
    {
      "epoch": 0.8646251905223777,
      "grad_norm": 8.507498741149902,
      "learning_rate": 3.6545378966329505e-05,
      "loss": 3.3824,
      "step": 6240
    },
    {
      "epoch": 0.8660108078148815,
      "grad_norm": 8.271937370300293,
      "learning_rate": 3.653983649715949e-05,
      "loss": 3.3717,
      "step": 6250
    },
    {
      "epoch": 0.8673964251073853,
      "grad_norm": 7.678776264190674,
      "learning_rate": 3.6534294027989476e-05,
      "loss": 3.7623,
      "step": 6260
    },
    {
      "epoch": 0.8687820423998891,
      "grad_norm": 8.531659126281738,
      "learning_rate": 3.652875155881946e-05,
      "loss": 3.8445,
      "step": 6270
    },
    {
      "epoch": 0.870167659692393,
      "grad_norm": 5.927853584289551,
      "learning_rate": 3.652320908964944e-05,
      "loss": 3.3122,
      "step": 6280
    },
    {
      "epoch": 0.8715532769848968,
      "grad_norm": 7.509285926818848,
      "learning_rate": 3.651766662047943e-05,
      "loss": 3.8025,
      "step": 6290
    },
    {
      "epoch": 0.8729388942774006,
      "grad_norm": 8.573139190673828,
      "learning_rate": 3.651212415130941e-05,
      "loss": 2.6006,
      "step": 6300
    },
    {
      "epoch": 0.8743245115699044,
      "grad_norm": 8.0862455368042,
      "learning_rate": 3.650658168213939e-05,
      "loss": 3.546,
      "step": 6310
    },
    {
      "epoch": 0.8757101288624082,
      "grad_norm": 8.61689567565918,
      "learning_rate": 3.650103921296938e-05,
      "loss": 2.7111,
      "step": 6320
    },
    {
      "epoch": 0.877095746154912,
      "grad_norm": 6.2634406089782715,
      "learning_rate": 3.649549674379936e-05,
      "loss": 3.3751,
      "step": 6330
    },
    {
      "epoch": 0.8784813634474158,
      "grad_norm": 8.991647720336914,
      "learning_rate": 3.648995427462935e-05,
      "loss": 3.1057,
      "step": 6340
    },
    {
      "epoch": 0.8798669807399196,
      "grad_norm": 9.452376365661621,
      "learning_rate": 3.648441180545934e-05,
      "loss": 2.9467,
      "step": 6350
    },
    {
      "epoch": 0.8812525980324234,
      "grad_norm": 8.238865852355957,
      "learning_rate": 3.647886933628932e-05,
      "loss": 2.9216,
      "step": 6360
    },
    {
      "epoch": 0.8826382153249273,
      "grad_norm": 6.913324356079102,
      "learning_rate": 3.64733268671193e-05,
      "loss": 3.1529,
      "step": 6370
    },
    {
      "epoch": 0.8840238326174311,
      "grad_norm": 8.192204475402832,
      "learning_rate": 3.646778439794929e-05,
      "loss": 3.5492,
      "step": 6380
    },
    {
      "epoch": 0.8854094499099349,
      "grad_norm": 6.150394439697266,
      "learning_rate": 3.6462241928779274e-05,
      "loss": 3.2473,
      "step": 6390
    },
    {
      "epoch": 0.8867950672024387,
      "grad_norm": 8.226541519165039,
      "learning_rate": 3.6456699459609256e-05,
      "loss": 3.0648,
      "step": 6400
    },
    {
      "epoch": 0.8881806844949425,
      "grad_norm": 8.134477615356445,
      "learning_rate": 3.6451156990439244e-05,
      "loss": 3.1717,
      "step": 6410
    },
    {
      "epoch": 0.8895663017874463,
      "grad_norm": 10.262016296386719,
      "learning_rate": 3.6445614521269226e-05,
      "loss": 3.3731,
      "step": 6420
    },
    {
      "epoch": 0.8909519190799501,
      "grad_norm": 11.00949764251709,
      "learning_rate": 3.644007205209921e-05,
      "loss": 3.4091,
      "step": 6430
    },
    {
      "epoch": 0.8923375363724539,
      "grad_norm": 9.994072914123535,
      "learning_rate": 3.64345295829292e-05,
      "loss": 3.2343,
      "step": 6440
    },
    {
      "epoch": 0.8937231536649577,
      "grad_norm": 9.887001037597656,
      "learning_rate": 3.6428987113759185e-05,
      "loss": 3.1342,
      "step": 6450
    },
    {
      "epoch": 0.8951087709574616,
      "grad_norm": 9.808182716369629,
      "learning_rate": 3.642344464458917e-05,
      "loss": 3.1045,
      "step": 6460
    },
    {
      "epoch": 0.8964943882499654,
      "grad_norm": 6.310787677764893,
      "learning_rate": 3.6417902175419156e-05,
      "loss": 3.6659,
      "step": 6470
    },
    {
      "epoch": 0.8978800055424692,
      "grad_norm": 11.430180549621582,
      "learning_rate": 3.641235970624914e-05,
      "loss": 3.0208,
      "step": 6480
    },
    {
      "epoch": 0.899265622834973,
      "grad_norm": 9.354720115661621,
      "learning_rate": 3.640681723707912e-05,
      "loss": 3.0946,
      "step": 6490
    },
    {
      "epoch": 0.9006512401274768,
      "grad_norm": 12.144363403320312,
      "learning_rate": 3.640127476790911e-05,
      "loss": 3.335,
      "step": 6500
    },
    {
      "epoch": 0.9020368574199806,
      "grad_norm": 10.449955940246582,
      "learning_rate": 3.639573229873909e-05,
      "loss": 3.6313,
      "step": 6510
    },
    {
      "epoch": 0.9034224747124844,
      "grad_norm": 9.68331527709961,
      "learning_rate": 3.639018982956908e-05,
      "loss": 2.9644,
      "step": 6520
    },
    {
      "epoch": 0.9048080920049882,
      "grad_norm": 9.853047370910645,
      "learning_rate": 3.638464736039906e-05,
      "loss": 2.9341,
      "step": 6530
    },
    {
      "epoch": 0.906193709297492,
      "grad_norm": 10.981430053710938,
      "learning_rate": 3.637910489122904e-05,
      "loss": 3.2443,
      "step": 6540
    },
    {
      "epoch": 0.9075793265899958,
      "grad_norm": 10.74731731414795,
      "learning_rate": 3.637356242205903e-05,
      "loss": 3.3836,
      "step": 6550
    },
    {
      "epoch": 0.9089649438824996,
      "grad_norm": 11.313959121704102,
      "learning_rate": 3.636801995288901e-05,
      "loss": 3.1336,
      "step": 6560
    },
    {
      "epoch": 0.9103505611750035,
      "grad_norm": 8.581500053405762,
      "learning_rate": 3.6362477483719e-05,
      "loss": 3.2192,
      "step": 6570
    },
    {
      "epoch": 0.9117361784675073,
      "grad_norm": 9.212647438049316,
      "learning_rate": 3.635693501454898e-05,
      "loss": 3.312,
      "step": 6580
    },
    {
      "epoch": 0.9131217957600111,
      "grad_norm": 7.641538143157959,
      "learning_rate": 3.635139254537897e-05,
      "loss": 2.6474,
      "step": 6590
    },
    {
      "epoch": 0.9145074130525149,
      "grad_norm": 7.999475479125977,
      "learning_rate": 3.6345850076208954e-05,
      "loss": 2.8371,
      "step": 6600
    },
    {
      "epoch": 0.9158930303450187,
      "grad_norm": 7.526846885681152,
      "learning_rate": 3.634030760703894e-05,
      "loss": 3.2098,
      "step": 6610
    },
    {
      "epoch": 0.9172786476375225,
      "grad_norm": 7.964606285095215,
      "learning_rate": 3.6334765137868924e-05,
      "loss": 2.9329,
      "step": 6620
    },
    {
      "epoch": 0.9186642649300263,
      "grad_norm": 6.480166912078857,
      "learning_rate": 3.6329222668698906e-05,
      "loss": 3.3207,
      "step": 6630
    },
    {
      "epoch": 0.9200498822225301,
      "grad_norm": 9.203120231628418,
      "learning_rate": 3.6323680199528895e-05,
      "loss": 3.2214,
      "step": 6640
    },
    {
      "epoch": 0.9214354995150339,
      "grad_norm": 7.451114654541016,
      "learning_rate": 3.631813773035888e-05,
      "loss": 3.2363,
      "step": 6650
    },
    {
      "epoch": 0.9228211168075378,
      "grad_norm": 7.1446404457092285,
      "learning_rate": 3.631259526118886e-05,
      "loss": 3.0889,
      "step": 6660
    },
    {
      "epoch": 0.9242067341000416,
      "grad_norm": 7.7212066650390625,
      "learning_rate": 3.630705279201885e-05,
      "loss": 3.2952,
      "step": 6670
    },
    {
      "epoch": 0.9255923513925454,
      "grad_norm": 7.389406681060791,
      "learning_rate": 3.630151032284883e-05,
      "loss": 3.374,
      "step": 6680
    },
    {
      "epoch": 0.9269779686850492,
      "grad_norm": 6.6780476570129395,
      "learning_rate": 3.629596785367882e-05,
      "loss": 3.5865,
      "step": 6690
    },
    {
      "epoch": 0.928363585977553,
      "grad_norm": 10.836283683776855,
      "learning_rate": 3.62904253845088e-05,
      "loss": 2.9894,
      "step": 6700
    },
    {
      "epoch": 0.9297492032700568,
      "grad_norm": 7.688483238220215,
      "learning_rate": 3.628488291533879e-05,
      "loss": 3.5272,
      "step": 6710
    },
    {
      "epoch": 0.9311348205625606,
      "grad_norm": 9.960803985595703,
      "learning_rate": 3.627934044616877e-05,
      "loss": 3.0209,
      "step": 6720
    },
    {
      "epoch": 0.9325204378550644,
      "grad_norm": 6.214895725250244,
      "learning_rate": 3.627379797699876e-05,
      "loss": 2.8262,
      "step": 6730
    },
    {
      "epoch": 0.9339060551475682,
      "grad_norm": 10.442365646362305,
      "learning_rate": 3.626825550782874e-05,
      "loss": 2.6357,
      "step": 6740
    },
    {
      "epoch": 0.935291672440072,
      "grad_norm": 8.700479507446289,
      "learning_rate": 3.626271303865872e-05,
      "loss": 3.5928,
      "step": 6750
    },
    {
      "epoch": 0.9366772897325759,
      "grad_norm": 9.542681694030762,
      "learning_rate": 3.625717056948871e-05,
      "loss": 3.4952,
      "step": 6760
    },
    {
      "epoch": 0.9380629070250797,
      "grad_norm": 7.913937568664551,
      "learning_rate": 3.625162810031869e-05,
      "loss": 3.2384,
      "step": 6770
    },
    {
      "epoch": 0.9394485243175835,
      "grad_norm": 8.027555465698242,
      "learning_rate": 3.6246085631148675e-05,
      "loss": 3.1026,
      "step": 6780
    },
    {
      "epoch": 0.9408341416100873,
      "grad_norm": 12.057660102844238,
      "learning_rate": 3.6240543161978663e-05,
      "loss": 3.073,
      "step": 6790
    },
    {
      "epoch": 0.9422197589025911,
      "grad_norm": 11.25790023803711,
      "learning_rate": 3.6235000692808645e-05,
      "loss": 2.7822,
      "step": 6800
    },
    {
      "epoch": 0.9436053761950949,
      "grad_norm": 5.118574619293213,
      "learning_rate": 3.6229458223638634e-05,
      "loss": 3.4361,
      "step": 6810
    },
    {
      "epoch": 0.9449909934875987,
      "grad_norm": 14.371289253234863,
      "learning_rate": 3.622391575446862e-05,
      "loss": 3.129,
      "step": 6820
    },
    {
      "epoch": 0.9463766107801025,
      "grad_norm": 7.656504154205322,
      "learning_rate": 3.6218373285298604e-05,
      "loss": 2.5833,
      "step": 6830
    },
    {
      "epoch": 0.9477622280726063,
      "grad_norm": 11.602484703063965,
      "learning_rate": 3.621283081612859e-05,
      "loss": 3.3025,
      "step": 6840
    },
    {
      "epoch": 0.9491478453651102,
      "grad_norm": 8.534971237182617,
      "learning_rate": 3.6207288346958575e-05,
      "loss": 3.8033,
      "step": 6850
    },
    {
      "epoch": 0.950533462657614,
      "grad_norm": 8.346203804016113,
      "learning_rate": 3.620174587778856e-05,
      "loss": 3.3758,
      "step": 6860
    },
    {
      "epoch": 0.9519190799501178,
      "grad_norm": 12.78689956665039,
      "learning_rate": 3.6196203408618545e-05,
      "loss": 3.0366,
      "step": 6870
    },
    {
      "epoch": 0.9533046972426216,
      "grad_norm": 8.478840827941895,
      "learning_rate": 3.619066093944853e-05,
      "loss": 3.3595,
      "step": 6880
    },
    {
      "epoch": 0.9546903145351254,
      "grad_norm": 16.0936279296875,
      "learning_rate": 3.618511847027851e-05,
      "loss": 3.3162,
      "step": 6890
    },
    {
      "epoch": 0.9560759318276292,
      "grad_norm": 7.971219062805176,
      "learning_rate": 3.61795760011085e-05,
      "loss": 2.9459,
      "step": 6900
    },
    {
      "epoch": 0.957461549120133,
      "grad_norm": 10.399469375610352,
      "learning_rate": 3.617403353193848e-05,
      "loss": 2.8375,
      "step": 6910
    },
    {
      "epoch": 0.9588471664126368,
      "grad_norm": 7.744691848754883,
      "learning_rate": 3.616849106276846e-05,
      "loss": 2.7087,
      "step": 6920
    },
    {
      "epoch": 0.9602327837051406,
      "grad_norm": 11.692909240722656,
      "learning_rate": 3.616294859359845e-05,
      "loss": 2.9545,
      "step": 6930
    },
    {
      "epoch": 0.9616184009976444,
      "grad_norm": 11.168331146240234,
      "learning_rate": 3.615740612442844e-05,
      "loss": 2.8501,
      "step": 6940
    },
    {
      "epoch": 0.9630040182901483,
      "grad_norm": 11.695939064025879,
      "learning_rate": 3.615186365525842e-05,
      "loss": 3.1725,
      "step": 6950
    },
    {
      "epoch": 0.9643896355826521,
      "grad_norm": 10.4589204788208,
      "learning_rate": 3.614632118608841e-05,
      "loss": 3.3252,
      "step": 6960
    },
    {
      "epoch": 0.9657752528751559,
      "grad_norm": 7.962557792663574,
      "learning_rate": 3.614077871691839e-05,
      "loss": 2.6901,
      "step": 6970
    },
    {
      "epoch": 0.9671608701676597,
      "grad_norm": 8.054793357849121,
      "learning_rate": 3.613523624774837e-05,
      "loss": 2.9521,
      "step": 6980
    },
    {
      "epoch": 0.9685464874601635,
      "grad_norm": 10.799897193908691,
      "learning_rate": 3.612969377857836e-05,
      "loss": 3.0758,
      "step": 6990
    },
    {
      "epoch": 0.9699321047526673,
      "grad_norm": 7.088007926940918,
      "learning_rate": 3.6124151309408344e-05,
      "loss": 2.744,
      "step": 7000
    },
    {
      "epoch": 0.9713177220451711,
      "grad_norm": 5.655190944671631,
      "learning_rate": 3.6118608840238325e-05,
      "loss": 3.7652,
      "step": 7010
    },
    {
      "epoch": 0.9727033393376749,
      "grad_norm": 9.521068572998047,
      "learning_rate": 3.6113066371068314e-05,
      "loss": 3.1585,
      "step": 7020
    },
    {
      "epoch": 0.9740889566301787,
      "grad_norm": 14.280292510986328,
      "learning_rate": 3.6107523901898296e-05,
      "loss": 2.9141,
      "step": 7030
    },
    {
      "epoch": 0.9754745739226826,
      "grad_norm": 8.9537935256958,
      "learning_rate": 3.6101981432728285e-05,
      "loss": 2.9943,
      "step": 7040
    },
    {
      "epoch": 0.9768601912151864,
      "grad_norm": 8.547751426696777,
      "learning_rate": 3.6096438963558266e-05,
      "loss": 3.3482,
      "step": 7050
    },
    {
      "epoch": 0.9782458085076902,
      "grad_norm": 9.746313095092773,
      "learning_rate": 3.6090896494388255e-05,
      "loss": 2.978,
      "step": 7060
    },
    {
      "epoch": 0.979631425800194,
      "grad_norm": 10.198002815246582,
      "learning_rate": 3.608535402521824e-05,
      "loss": 3.0946,
      "step": 7070
    },
    {
      "epoch": 0.9810170430926978,
      "grad_norm": 8.230683326721191,
      "learning_rate": 3.6079811556048226e-05,
      "loss": 3.4966,
      "step": 7080
    },
    {
      "epoch": 0.9824026603852016,
      "grad_norm": 11.137298583984375,
      "learning_rate": 3.607426908687821e-05,
      "loss": 3.5874,
      "step": 7090
    },
    {
      "epoch": 0.9837882776777054,
      "grad_norm": 9.552430152893066,
      "learning_rate": 3.6068726617708196e-05,
      "loss": 2.4361,
      "step": 7100
    },
    {
      "epoch": 0.9851738949702092,
      "grad_norm": 7.649489879608154,
      "learning_rate": 3.606318414853818e-05,
      "loss": 3.0133,
      "step": 7110
    },
    {
      "epoch": 0.986559512262713,
      "grad_norm": 10.035109519958496,
      "learning_rate": 3.605764167936816e-05,
      "loss": 3.1104,
      "step": 7120
    },
    {
      "epoch": 0.9879451295552168,
      "grad_norm": 11.706589698791504,
      "learning_rate": 3.605209921019815e-05,
      "loss": 3.3013,
      "step": 7130
    },
    {
      "epoch": 0.9893307468477207,
      "grad_norm": 7.216526985168457,
      "learning_rate": 3.604655674102813e-05,
      "loss": 3.4056,
      "step": 7140
    },
    {
      "epoch": 0.9907163641402245,
      "grad_norm": 7.542600631713867,
      "learning_rate": 3.604101427185811e-05,
      "loss": 2.7156,
      "step": 7150
    },
    {
      "epoch": 0.9921019814327283,
      "grad_norm": 10.171767234802246,
      "learning_rate": 3.60354718026881e-05,
      "loss": 3.2695,
      "step": 7160
    },
    {
      "epoch": 0.9934875987252321,
      "grad_norm": 7.621974945068359,
      "learning_rate": 3.602992933351808e-05,
      "loss": 3.0411,
      "step": 7170
    },
    {
      "epoch": 0.9948732160177359,
      "grad_norm": 8.05090618133545,
      "learning_rate": 3.602438686434807e-05,
      "loss": 2.7197,
      "step": 7180
    },
    {
      "epoch": 0.9962588333102397,
      "grad_norm": 10.204002380371094,
      "learning_rate": 3.601884439517805e-05,
      "loss": 3.5993,
      "step": 7190
    },
    {
      "epoch": 0.9976444506027435,
      "grad_norm": 13.514372825622559,
      "learning_rate": 3.601330192600804e-05,
      "loss": 3.2776,
      "step": 7200
    },
    {
      "epoch": 0.9990300678952473,
      "grad_norm": 7.089719772338867,
      "learning_rate": 3.6007759456838024e-05,
      "loss": 3.3679,
      "step": 7210
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.3546084546084546,
      "eval_bert_f1": 0.98627108335495,
      "eval_bert_precision": 0.9897926449775696,
      "eval_bert_recall": 0.9832216501235962,
      "eval_f1": 0.012379519195410853,
      "eval_loss": 3.134512424468994,
      "eval_runtime": 286.5847,
      "eval_samples_per_second": 50.352,
      "eval_steps_per_second": 6.295,
      "eval_synonym_accuracy": 0.36486486486486486,
      "step": 7217
    },
    {
      "epoch": 1.000415685187751,
      "grad_norm": 6.566032886505127,
      "learning_rate": 3.600221698766801e-05,
      "loss": 3.358,
      "step": 7220
    },
    {
      "epoch": 1.001801302480255,
      "grad_norm": 7.611103534698486,
      "learning_rate": 3.5996674518497994e-05,
      "loss": 3.012,
      "step": 7230
    },
    {
      "epoch": 1.0031869197727588,
      "grad_norm": 8.525954246520996,
      "learning_rate": 3.5991132049327976e-05,
      "loss": 2.8324,
      "step": 7240
    },
    {
      "epoch": 1.0045725370652625,
      "grad_norm": 7.264875888824463,
      "learning_rate": 3.5985589580157965e-05,
      "loss": 2.6995,
      "step": 7250
    },
    {
      "epoch": 1.0059581543577665,
      "grad_norm": 10.046625137329102,
      "learning_rate": 3.5980047110987946e-05,
      "loss": 3.5242,
      "step": 7260
    },
    {
      "epoch": 1.0073437716502702,
      "grad_norm": 6.2877516746521,
      "learning_rate": 3.597450464181793e-05,
      "loss": 3.1885,
      "step": 7270
    },
    {
      "epoch": 1.008729388942774,
      "grad_norm": 9.622114181518555,
      "learning_rate": 3.596896217264792e-05,
      "loss": 3.1876,
      "step": 7280
    },
    {
      "epoch": 1.0101150062352777,
      "grad_norm": 10.09724235534668,
      "learning_rate": 3.59634197034779e-05,
      "loss": 3.305,
      "step": 7290
    },
    {
      "epoch": 1.0115006235277817,
      "grad_norm": 8.200118064880371,
      "learning_rate": 3.595787723430789e-05,
      "loss": 2.8025,
      "step": 7300
    },
    {
      "epoch": 1.0128862408202854,
      "grad_norm": 9.699362754821777,
      "learning_rate": 3.5952334765137876e-05,
      "loss": 3.1888,
      "step": 7310
    },
    {
      "epoch": 1.0142718581127892,
      "grad_norm": 11.091257095336914,
      "learning_rate": 3.594679229596786e-05,
      "loss": 3.0911,
      "step": 7320
    },
    {
      "epoch": 1.0156574754052932,
      "grad_norm": 8.455394744873047,
      "learning_rate": 3.594124982679784e-05,
      "loss": 2.8292,
      "step": 7330
    },
    {
      "epoch": 1.017043092697797,
      "grad_norm": 7.129735469818115,
      "learning_rate": 3.593570735762783e-05,
      "loss": 3.1911,
      "step": 7340
    },
    {
      "epoch": 1.0184287099903007,
      "grad_norm": 8.658234596252441,
      "learning_rate": 3.593016488845781e-05,
      "loss": 3.1211,
      "step": 7350
    },
    {
      "epoch": 1.0198143272828044,
      "grad_norm": 11.347908020019531,
      "learning_rate": 3.592462241928779e-05,
      "loss": 3.4025,
      "step": 7360
    },
    {
      "epoch": 1.0211999445753084,
      "grad_norm": 6.1162109375,
      "learning_rate": 3.591907995011778e-05,
      "loss": 3.2828,
      "step": 7370
    },
    {
      "epoch": 1.0225855618678121,
      "grad_norm": 8.930700302124023,
      "learning_rate": 3.591353748094776e-05,
      "loss": 2.9059,
      "step": 7380
    },
    {
      "epoch": 1.0239711791603159,
      "grad_norm": 11.092240333557129,
      "learning_rate": 3.590799501177775e-05,
      "loss": 3.2748,
      "step": 7390
    },
    {
      "epoch": 1.0253567964528196,
      "grad_norm": 7.554779052734375,
      "learning_rate": 3.590245254260773e-05,
      "loss": 3.014,
      "step": 7400
    },
    {
      "epoch": 1.0267424137453236,
      "grad_norm": 9.157743453979492,
      "learning_rate": 3.589691007343772e-05,
      "loss": 3.7084,
      "step": 7410
    },
    {
      "epoch": 1.0281280310378274,
      "grad_norm": 9.148473739624023,
      "learning_rate": 3.5891367604267704e-05,
      "loss": 3.1451,
      "step": 7420
    },
    {
      "epoch": 1.029513648330331,
      "grad_norm": 12.273502349853516,
      "learning_rate": 3.588582513509769e-05,
      "loss": 3.032,
      "step": 7430
    },
    {
      "epoch": 1.030899265622835,
      "grad_norm": 6.480156421661377,
      "learning_rate": 3.5880282665927674e-05,
      "loss": 3.2471,
      "step": 7440
    },
    {
      "epoch": 1.0322848829153388,
      "grad_norm": 10.52216625213623,
      "learning_rate": 3.587474019675766e-05,
      "loss": 3.1509,
      "step": 7450
    },
    {
      "epoch": 1.0336705002078426,
      "grad_norm": 7.322123050689697,
      "learning_rate": 3.5869197727587645e-05,
      "loss": 3.1871,
      "step": 7460
    },
    {
      "epoch": 1.0350561175003463,
      "grad_norm": 10.551464080810547,
      "learning_rate": 3.5863655258417627e-05,
      "loss": 3.0652,
      "step": 7470
    },
    {
      "epoch": 1.0364417347928503,
      "grad_norm": 9.23415756225586,
      "learning_rate": 3.5858112789247615e-05,
      "loss": 3.0691,
      "step": 7480
    },
    {
      "epoch": 1.037827352085354,
      "grad_norm": 9.288257598876953,
      "learning_rate": 3.58525703200776e-05,
      "loss": 3.089,
      "step": 7490
    },
    {
      "epoch": 1.0392129693778578,
      "grad_norm": 11.379103660583496,
      "learning_rate": 3.584702785090758e-05,
      "loss": 3.2174,
      "step": 7500
    },
    {
      "epoch": 1.0405985866703618,
      "grad_norm": 9.282526016235352,
      "learning_rate": 3.584148538173757e-05,
      "loss": 3.0422,
      "step": 7510
    },
    {
      "epoch": 1.0419842039628655,
      "grad_norm": 13.752848625183105,
      "learning_rate": 3.583594291256755e-05,
      "loss": 3.2546,
      "step": 7520
    },
    {
      "epoch": 1.0433698212553693,
      "grad_norm": 7.608551979064941,
      "learning_rate": 3.583040044339754e-05,
      "loss": 3.538,
      "step": 7530
    },
    {
      "epoch": 1.044755438547873,
      "grad_norm": 12.301444053649902,
      "learning_rate": 3.582485797422752e-05,
      "loss": 3.1522,
      "step": 7540
    },
    {
      "epoch": 1.046141055840377,
      "grad_norm": 8.015344619750977,
      "learning_rate": 3.581931550505751e-05,
      "loss": 3.0799,
      "step": 7550
    },
    {
      "epoch": 1.0475266731328807,
      "grad_norm": 7.905128479003906,
      "learning_rate": 3.581377303588749e-05,
      "loss": 3.1673,
      "step": 7560
    },
    {
      "epoch": 1.0489122904253845,
      "grad_norm": 13.441083908081055,
      "learning_rate": 3.580823056671748e-05,
      "loss": 3.5102,
      "step": 7570
    },
    {
      "epoch": 1.0502979077178882,
      "grad_norm": 7.8653244972229,
      "learning_rate": 3.580268809754746e-05,
      "loss": 3.1016,
      "step": 7580
    },
    {
      "epoch": 1.0516835250103922,
      "grad_norm": 8.762192726135254,
      "learning_rate": 3.579714562837744e-05,
      "loss": 2.9069,
      "step": 7590
    },
    {
      "epoch": 1.053069142302896,
      "grad_norm": 8.03029727935791,
      "learning_rate": 3.579160315920743e-05,
      "loss": 3.2393,
      "step": 7600
    },
    {
      "epoch": 1.0544547595953997,
      "grad_norm": 6.9604902267456055,
      "learning_rate": 3.578606069003741e-05,
      "loss": 3.0877,
      "step": 7610
    },
    {
      "epoch": 1.0558403768879037,
      "grad_norm": 7.3675971031188965,
      "learning_rate": 3.5780518220867395e-05,
      "loss": 2.9571,
      "step": 7620
    },
    {
      "epoch": 1.0572259941804074,
      "grad_norm": 9.119370460510254,
      "learning_rate": 3.5774975751697384e-05,
      "loss": 2.8903,
      "step": 7630
    },
    {
      "epoch": 1.0586116114729112,
      "grad_norm": 10.840015411376953,
      "learning_rate": 3.576998752944437e-05,
      "loss": 3.0961,
      "step": 7640
    },
    {
      "epoch": 1.059997228765415,
      "grad_norm": 7.3122429847717285,
      "learning_rate": 3.576444506027435e-05,
      "loss": 3.1313,
      "step": 7650
    },
    {
      "epoch": 1.0613828460579189,
      "grad_norm": 8.171784400939941,
      "learning_rate": 3.575890259110434e-05,
      "loss": 2.875,
      "step": 7660
    },
    {
      "epoch": 1.0627684633504226,
      "grad_norm": 10.650419235229492,
      "learning_rate": 3.575336012193432e-05,
      "loss": 3.5999,
      "step": 7670
    },
    {
      "epoch": 1.0641540806429264,
      "grad_norm": 11.93783950805664,
      "learning_rate": 3.5747817652764304e-05,
      "loss": 3.0658,
      "step": 7680
    },
    {
      "epoch": 1.0655396979354301,
      "grad_norm": 7.7236247062683105,
      "learning_rate": 3.574227518359429e-05,
      "loss": 3.2484,
      "step": 7690
    },
    {
      "epoch": 1.066925315227934,
      "grad_norm": 7.218932628631592,
      "learning_rate": 3.5736732714424275e-05,
      "loss": 2.9543,
      "step": 7700
    },
    {
      "epoch": 1.0683109325204379,
      "grad_norm": 7.37408971786499,
      "learning_rate": 3.5731190245254263e-05,
      "loss": 3.2501,
      "step": 7710
    },
    {
      "epoch": 1.0696965498129416,
      "grad_norm": 8.822202682495117,
      "learning_rate": 3.572564777608425e-05,
      "loss": 2.9329,
      "step": 7720
    },
    {
      "epoch": 1.0710821671054456,
      "grad_norm": 9.370698928833008,
      "learning_rate": 3.5720105306914234e-05,
      "loss": 3.2828,
      "step": 7730
    },
    {
      "epoch": 1.0724677843979493,
      "grad_norm": 11.044042587280273,
      "learning_rate": 3.571456283774422e-05,
      "loss": 3.0629,
      "step": 7740
    },
    {
      "epoch": 1.073853401690453,
      "grad_norm": 7.085636138916016,
      "learning_rate": 3.5709020368574204e-05,
      "loss": 3.2153,
      "step": 7750
    },
    {
      "epoch": 1.0752390189829568,
      "grad_norm": 15.351231575012207,
      "learning_rate": 3.5703477899404186e-05,
      "loss": 3.7001,
      "step": 7760
    },
    {
      "epoch": 1.0766246362754608,
      "grad_norm": 4.997162818908691,
      "learning_rate": 3.5697935430234175e-05,
      "loss": 2.4564,
      "step": 7770
    },
    {
      "epoch": 1.0780102535679645,
      "grad_norm": 7.259977340698242,
      "learning_rate": 3.569239296106416e-05,
      "loss": 2.9608,
      "step": 7780
    },
    {
      "epoch": 1.0793958708604683,
      "grad_norm": 9.382122993469238,
      "learning_rate": 3.568685049189414e-05,
      "loss": 3.2916,
      "step": 7790
    },
    {
      "epoch": 1.0807814881529723,
      "grad_norm": 6.719072341918945,
      "learning_rate": 3.568130802272413e-05,
      "loss": 3.1594,
      "step": 7800
    },
    {
      "epoch": 1.082167105445476,
      "grad_norm": 10.041351318359375,
      "learning_rate": 3.567576555355411e-05,
      "loss": 3.0838,
      "step": 7810
    },
    {
      "epoch": 1.0835527227379798,
      "grad_norm": 8.499866485595703,
      "learning_rate": 3.56702230843841e-05,
      "loss": 2.8581,
      "step": 7820
    },
    {
      "epoch": 1.0849383400304835,
      "grad_norm": 13.013249397277832,
      "learning_rate": 3.566468061521408e-05,
      "loss": 3.6777,
      "step": 7830
    },
    {
      "epoch": 1.0863239573229875,
      "grad_norm": 7.295106410980225,
      "learning_rate": 3.565913814604407e-05,
      "loss": 2.7404,
      "step": 7840
    },
    {
      "epoch": 1.0877095746154912,
      "grad_norm": 7.465290069580078,
      "learning_rate": 3.565359567687405e-05,
      "loss": 2.5296,
      "step": 7850
    },
    {
      "epoch": 1.089095191907995,
      "grad_norm": 10.119574546813965,
      "learning_rate": 3.564805320770404e-05,
      "loss": 2.7881,
      "step": 7860
    },
    {
      "epoch": 1.0904808092004987,
      "grad_norm": 8.397679328918457,
      "learning_rate": 3.564251073853402e-05,
      "loss": 2.7166,
      "step": 7870
    },
    {
      "epoch": 1.0918664264930027,
      "grad_norm": 7.502216339111328,
      "learning_rate": 3.5636968269364e-05,
      "loss": 3.3041,
      "step": 7880
    },
    {
      "epoch": 1.0932520437855064,
      "grad_norm": 12.479422569274902,
      "learning_rate": 3.563142580019399e-05,
      "loss": 3.0404,
      "step": 7890
    },
    {
      "epoch": 1.0946376610780102,
      "grad_norm": 7.286232948303223,
      "learning_rate": 3.562588333102397e-05,
      "loss": 2.4496,
      "step": 7900
    },
    {
      "epoch": 1.0960232783705142,
      "grad_norm": 11.956441879272461,
      "learning_rate": 3.5620340861853955e-05,
      "loss": 2.8172,
      "step": 7910
    },
    {
      "epoch": 1.097408895663018,
      "grad_norm": 8.806833267211914,
      "learning_rate": 3.5614798392683944e-05,
      "loss": 2.8413,
      "step": 7920
    },
    {
      "epoch": 1.0987945129555217,
      "grad_norm": 9.261431694030762,
      "learning_rate": 3.5609255923513925e-05,
      "loss": 2.8643,
      "step": 7930
    },
    {
      "epoch": 1.1001801302480254,
      "grad_norm": 8.09997272491455,
      "learning_rate": 3.5603713454343914e-05,
      "loss": 3.0849,
      "step": 7940
    },
    {
      "epoch": 1.1015657475405294,
      "grad_norm": 13.616905212402344,
      "learning_rate": 3.5598170985173896e-05,
      "loss": 2.7496,
      "step": 7950
    },
    {
      "epoch": 1.1029513648330331,
      "grad_norm": 7.7383222579956055,
      "learning_rate": 3.5592628516003885e-05,
      "loss": 2.7428,
      "step": 7960
    },
    {
      "epoch": 1.1043369821255369,
      "grad_norm": 8.361117362976074,
      "learning_rate": 3.5587086046833866e-05,
      "loss": 2.9343,
      "step": 7970
    },
    {
      "epoch": 1.1057225994180406,
      "grad_norm": 10.97084903717041,
      "learning_rate": 3.5581543577663855e-05,
      "loss": 3.0302,
      "step": 7980
    },
    {
      "epoch": 1.1071082167105446,
      "grad_norm": 8.751992225646973,
      "learning_rate": 3.557600110849384e-05,
      "loss": 2.8628,
      "step": 7990
    },
    {
      "epoch": 1.1084938340030484,
      "grad_norm": 12.510834693908691,
      "learning_rate": 3.5570458639323826e-05,
      "loss": 3.1373,
      "step": 8000
    },
    {
      "epoch": 1.109879451295552,
      "grad_norm": 7.771092891693115,
      "learning_rate": 3.556491617015381e-05,
      "loss": 3.4413,
      "step": 8010
    },
    {
      "epoch": 1.111265068588056,
      "grad_norm": 7.766499996185303,
      "learning_rate": 3.555937370098379e-05,
      "loss": 2.684,
      "step": 8020
    },
    {
      "epoch": 1.1126506858805598,
      "grad_norm": 7.563603401184082,
      "learning_rate": 3.555383123181378e-05,
      "loss": 2.8225,
      "step": 8030
    },
    {
      "epoch": 1.1140363031730636,
      "grad_norm": 7.957762241363525,
      "learning_rate": 3.554828876264376e-05,
      "loss": 2.9161,
      "step": 8040
    },
    {
      "epoch": 1.1154219204655673,
      "grad_norm": 9.662696838378906,
      "learning_rate": 3.554274629347374e-05,
      "loss": 3.0012,
      "step": 8050
    },
    {
      "epoch": 1.1168075377580713,
      "grad_norm": 8.800947189331055,
      "learning_rate": 3.553720382430373e-05,
      "loss": 2.8014,
      "step": 8060
    },
    {
      "epoch": 1.118193155050575,
      "grad_norm": 8.89047908782959,
      "learning_rate": 3.553166135513371e-05,
      "loss": 3.2088,
      "step": 8070
    },
    {
      "epoch": 1.1195787723430788,
      "grad_norm": 11.504108428955078,
      "learning_rate": 3.55261188859637e-05,
      "loss": 3.3262,
      "step": 8080
    },
    {
      "epoch": 1.1209643896355828,
      "grad_norm": 6.3101806640625,
      "learning_rate": 3.552057641679369e-05,
      "loss": 2.2564,
      "step": 8090
    },
    {
      "epoch": 1.1223500069280865,
      "grad_norm": 10.31675910949707,
      "learning_rate": 3.551503394762367e-05,
      "loss": 2.9215,
      "step": 8100
    },
    {
      "epoch": 1.1237356242205903,
      "grad_norm": 8.213972091674805,
      "learning_rate": 3.550949147845365e-05,
      "loss": 2.9356,
      "step": 8110
    },
    {
      "epoch": 1.125121241513094,
      "grad_norm": 11.283804893493652,
      "learning_rate": 3.550394900928364e-05,
      "loss": 3.3109,
      "step": 8120
    },
    {
      "epoch": 1.126506858805598,
      "grad_norm": 10.936811447143555,
      "learning_rate": 3.5498406540113624e-05,
      "loss": 3.198,
      "step": 8130
    },
    {
      "epoch": 1.1278924760981017,
      "grad_norm": 13.206802368164062,
      "learning_rate": 3.5492864070943605e-05,
      "loss": 2.9344,
      "step": 8140
    },
    {
      "epoch": 1.1292780933906055,
      "grad_norm": 13.270940780639648,
      "learning_rate": 3.5487321601773594e-05,
      "loss": 2.8927,
      "step": 8150
    },
    {
      "epoch": 1.1306637106831094,
      "grad_norm": 12.019844055175781,
      "learning_rate": 3.5481779132603576e-05,
      "loss": 3.286,
      "step": 8160
    },
    {
      "epoch": 1.1320493279756132,
      "grad_norm": 8.416074752807617,
      "learning_rate": 3.547623666343356e-05,
      "loss": 3.7818,
      "step": 8170
    },
    {
      "epoch": 1.133434945268117,
      "grad_norm": 14.826276779174805,
      "learning_rate": 3.5470694194263546e-05,
      "loss": 3.1016,
      "step": 8180
    },
    {
      "epoch": 1.1348205625606207,
      "grad_norm": 7.868330478668213,
      "learning_rate": 3.5465151725093535e-05,
      "loss": 3.2052,
      "step": 8190
    },
    {
      "epoch": 1.1362061798531244,
      "grad_norm": 12.646601676940918,
      "learning_rate": 3.545960925592352e-05,
      "loss": 2.7163,
      "step": 8200
    },
    {
      "epoch": 1.1375917971456284,
      "grad_norm": 9.649420738220215,
      "learning_rate": 3.5454066786753506e-05,
      "loss": 2.7384,
      "step": 8210
    },
    {
      "epoch": 1.1389774144381322,
      "grad_norm": 7.113203048706055,
      "learning_rate": 3.544852431758349e-05,
      "loss": 2.946,
      "step": 8220
    },
    {
      "epoch": 1.140363031730636,
      "grad_norm": 11.596049308776855,
      "learning_rate": 3.544298184841347e-05,
      "loss": 3.1593,
      "step": 8230
    },
    {
      "epoch": 1.1417486490231399,
      "grad_norm": 10.081303596496582,
      "learning_rate": 3.543743937924346e-05,
      "loss": 2.36,
      "step": 8240
    },
    {
      "epoch": 1.1431342663156436,
      "grad_norm": 9.729161262512207,
      "learning_rate": 3.543189691007344e-05,
      "loss": 3.0381,
      "step": 8250
    },
    {
      "epoch": 1.1445198836081474,
      "grad_norm": 8.033110618591309,
      "learning_rate": 3.542635444090342e-05,
      "loss": 2.88,
      "step": 8260
    },
    {
      "epoch": 1.1459055009006511,
      "grad_norm": 8.255843162536621,
      "learning_rate": 3.542081197173341e-05,
      "loss": 2.873,
      "step": 8270
    },
    {
      "epoch": 1.147291118193155,
      "grad_norm": 7.520106315612793,
      "learning_rate": 3.541526950256339e-05,
      "loss": 3.2047,
      "step": 8280
    },
    {
      "epoch": 1.1486767354856589,
      "grad_norm": 9.016619682312012,
      "learning_rate": 3.540972703339338e-05,
      "loss": 2.6395,
      "step": 8290
    },
    {
      "epoch": 1.1500623527781626,
      "grad_norm": 11.449206352233887,
      "learning_rate": 3.540418456422336e-05,
      "loss": 2.5024,
      "step": 8300
    },
    {
      "epoch": 1.1514479700706666,
      "grad_norm": 8.631501197814941,
      "learning_rate": 3.539864209505335e-05,
      "loss": 2.8573,
      "step": 8310
    },
    {
      "epoch": 1.1528335873631703,
      "grad_norm": 9.949067115783691,
      "learning_rate": 3.539309962588333e-05,
      "loss": 2.8541,
      "step": 8320
    },
    {
      "epoch": 1.154219204655674,
      "grad_norm": 8.92849349975586,
      "learning_rate": 3.538755715671332e-05,
      "loss": 3.2174,
      "step": 8330
    },
    {
      "epoch": 1.1556048219481778,
      "grad_norm": 7.5474371910095215,
      "learning_rate": 3.5382014687543304e-05,
      "loss": 2.9836,
      "step": 8340
    },
    {
      "epoch": 1.1569904392406818,
      "grad_norm": 11.6123685836792,
      "learning_rate": 3.537647221837329e-05,
      "loss": 2.9793,
      "step": 8350
    },
    {
      "epoch": 1.1583760565331855,
      "grad_norm": 10.349030494689941,
      "learning_rate": 3.5370929749203274e-05,
      "loss": 3.2031,
      "step": 8360
    },
    {
      "epoch": 1.1597616738256893,
      "grad_norm": 7.077215671539307,
      "learning_rate": 3.5365387280033256e-05,
      "loss": 2.6286,
      "step": 8370
    },
    {
      "epoch": 1.1611472911181933,
      "grad_norm": 8.348295211791992,
      "learning_rate": 3.5359844810863245e-05,
      "loss": 2.8131,
      "step": 8380
    },
    {
      "epoch": 1.162532908410697,
      "grad_norm": 8.90903377532959,
      "learning_rate": 3.5354302341693227e-05,
      "loss": 3.003,
      "step": 8390
    },
    {
      "epoch": 1.1639185257032008,
      "grad_norm": 6.129230499267578,
      "learning_rate": 3.534875987252321e-05,
      "loss": 3.1383,
      "step": 8400
    },
    {
      "epoch": 1.1653041429957045,
      "grad_norm": 10.076516151428223,
      "learning_rate": 3.53432174033532e-05,
      "loss": 3.1178,
      "step": 8410
    },
    {
      "epoch": 1.1666897602882085,
      "grad_norm": 6.43602991104126,
      "learning_rate": 3.533767493418318e-05,
      "loss": 3.0035,
      "step": 8420
    },
    {
      "epoch": 1.1680753775807122,
      "grad_norm": 9.588386535644531,
      "learning_rate": 3.533213246501317e-05,
      "loss": 2.6915,
      "step": 8430
    },
    {
      "epoch": 1.169460994873216,
      "grad_norm": 10.138537406921387,
      "learning_rate": 3.532658999584315e-05,
      "loss": 2.8533,
      "step": 8440
    },
    {
      "epoch": 1.17084661216572,
      "grad_norm": 10.141081809997559,
      "learning_rate": 3.532104752667314e-05,
      "loss": 3.0714,
      "step": 8450
    },
    {
      "epoch": 1.1722322294582237,
      "grad_norm": 7.095699787139893,
      "learning_rate": 3.531550505750312e-05,
      "loss": 2.9995,
      "step": 8460
    },
    {
      "epoch": 1.1736178467507274,
      "grad_norm": 10.954877853393555,
      "learning_rate": 3.530996258833311e-05,
      "loss": 3.2433,
      "step": 8470
    },
    {
      "epoch": 1.1750034640432312,
      "grad_norm": 10.000688552856445,
      "learning_rate": 3.530442011916309e-05,
      "loss": 2.4391,
      "step": 8480
    },
    {
      "epoch": 1.1763890813357352,
      "grad_norm": 12.796791076660156,
      "learning_rate": 3.529887764999307e-05,
      "loss": 3.3114,
      "step": 8490
    },
    {
      "epoch": 1.177774698628239,
      "grad_norm": 6.724419593811035,
      "learning_rate": 3.529333518082306e-05,
      "loss": 2.9095,
      "step": 8500
    },
    {
      "epoch": 1.1791603159207427,
      "grad_norm": 8.870699882507324,
      "learning_rate": 3.528779271165304e-05,
      "loss": 3.2886,
      "step": 8510
    },
    {
      "epoch": 1.1805459332132464,
      "grad_norm": 10.800013542175293,
      "learning_rate": 3.5282250242483025e-05,
      "loss": 2.7809,
      "step": 8520
    },
    {
      "epoch": 1.1819315505057504,
      "grad_norm": 7.384688377380371,
      "learning_rate": 3.527670777331301e-05,
      "loss": 2.6367,
      "step": 8530
    },
    {
      "epoch": 1.1833171677982541,
      "grad_norm": 10.939983367919922,
      "learning_rate": 3.5271165304142995e-05,
      "loss": 2.7595,
      "step": 8540
    },
    {
      "epoch": 1.1847027850907579,
      "grad_norm": 10.289915084838867,
      "learning_rate": 3.5265622834972984e-05,
      "loss": 3.3064,
      "step": 8550
    },
    {
      "epoch": 1.1860884023832616,
      "grad_norm": 8.915451049804688,
      "learning_rate": 3.5260080365802966e-05,
      "loss": 2.7633,
      "step": 8560
    },
    {
      "epoch": 1.1874740196757656,
      "grad_norm": 9.684062957763672,
      "learning_rate": 3.5254537896632954e-05,
      "loss": 3.0761,
      "step": 8570
    },
    {
      "epoch": 1.1888596369682694,
      "grad_norm": 13.323773384094238,
      "learning_rate": 3.524899542746294e-05,
      "loss": 3.1654,
      "step": 8580
    },
    {
      "epoch": 1.190245254260773,
      "grad_norm": 6.815058708190918,
      "learning_rate": 3.5243452958292925e-05,
      "loss": 3.2184,
      "step": 8590
    },
    {
      "epoch": 1.191630871553277,
      "grad_norm": 6.897209167480469,
      "learning_rate": 3.523791048912291e-05,
      "loss": 2.6099,
      "step": 8600
    },
    {
      "epoch": 1.1930164888457808,
      "grad_norm": 8.323983192443848,
      "learning_rate": 3.5232368019952895e-05,
      "loss": 2.8557,
      "step": 8610
    },
    {
      "epoch": 1.1944021061382846,
      "grad_norm": 6.490217685699463,
      "learning_rate": 3.522682555078288e-05,
      "loss": 2.7345,
      "step": 8620
    },
    {
      "epoch": 1.1957877234307883,
      "grad_norm": 9.086341857910156,
      "learning_rate": 3.522128308161286e-05,
      "loss": 2.9132,
      "step": 8630
    },
    {
      "epoch": 1.1971733407232923,
      "grad_norm": 10.483513832092285,
      "learning_rate": 3.521574061244285e-05,
      "loss": 2.5361,
      "step": 8640
    },
    {
      "epoch": 1.198558958015796,
      "grad_norm": 11.970219612121582,
      "learning_rate": 3.521019814327283e-05,
      "loss": 3.2195,
      "step": 8650
    },
    {
      "epoch": 1.1999445753082998,
      "grad_norm": 7.974437236785889,
      "learning_rate": 3.520465567410281e-05,
      "loss": 3.0352,
      "step": 8660
    },
    {
      "epoch": 1.2013301926008038,
      "grad_norm": 7.038788318634033,
      "learning_rate": 3.51991132049328e-05,
      "loss": 3.0329,
      "step": 8670
    },
    {
      "epoch": 1.2027158098933075,
      "grad_norm": 8.533283233642578,
      "learning_rate": 3.519357073576279e-05,
      "loss": 2.9296,
      "step": 8680
    },
    {
      "epoch": 1.2041014271858113,
      "grad_norm": 7.5934295654296875,
      "learning_rate": 3.518802826659277e-05,
      "loss": 3.4745,
      "step": 8690
    },
    {
      "epoch": 1.205487044478315,
      "grad_norm": 8.73335075378418,
      "learning_rate": 3.518248579742276e-05,
      "loss": 2.7496,
      "step": 8700
    },
    {
      "epoch": 1.206872661770819,
      "grad_norm": 12.68285846710205,
      "learning_rate": 3.517694332825274e-05,
      "loss": 3.3666,
      "step": 8710
    },
    {
      "epoch": 1.2082582790633227,
      "grad_norm": 9.559229850769043,
      "learning_rate": 3.517140085908272e-05,
      "loss": 2.4775,
      "step": 8720
    },
    {
      "epoch": 1.2096438963558265,
      "grad_norm": 7.3025431632995605,
      "learning_rate": 3.516585838991271e-05,
      "loss": 2.7762,
      "step": 8730
    },
    {
      "epoch": 1.2110295136483304,
      "grad_norm": 6.344000816345215,
      "learning_rate": 3.516031592074269e-05,
      "loss": 2.7856,
      "step": 8740
    },
    {
      "epoch": 1.2124151309408342,
      "grad_norm": 9.246227264404297,
      "learning_rate": 3.5154773451572675e-05,
      "loss": 2.8173,
      "step": 8750
    },
    {
      "epoch": 1.213800748233338,
      "grad_norm": 7.330448627471924,
      "learning_rate": 3.5149230982402664e-05,
      "loss": 2.7094,
      "step": 8760
    },
    {
      "epoch": 1.2151863655258417,
      "grad_norm": 10.393023490905762,
      "learning_rate": 3.5143688513232646e-05,
      "loss": 3.1253,
      "step": 8770
    },
    {
      "epoch": 1.2165719828183457,
      "grad_norm": 10.971720695495605,
      "learning_rate": 3.5138146044062634e-05,
      "loss": 2.6517,
      "step": 8780
    },
    {
      "epoch": 1.2179576001108494,
      "grad_norm": 10.406266212463379,
      "learning_rate": 3.5132603574892616e-05,
      "loss": 3.1925,
      "step": 8790
    },
    {
      "epoch": 1.2193432174033532,
      "grad_norm": 8.253168106079102,
      "learning_rate": 3.5127061105722605e-05,
      "loss": 2.7658,
      "step": 8800
    },
    {
      "epoch": 1.220728834695857,
      "grad_norm": 8.372932434082031,
      "learning_rate": 3.512151863655259e-05,
      "loss": 2.7914,
      "step": 8810
    },
    {
      "epoch": 1.2221144519883609,
      "grad_norm": 7.90529203414917,
      "learning_rate": 3.5115976167382575e-05,
      "loss": 3.0279,
      "step": 8820
    },
    {
      "epoch": 1.2235000692808646,
      "grad_norm": 8.676651954650879,
      "learning_rate": 3.511043369821256e-05,
      "loss": 3.3924,
      "step": 8830
    },
    {
      "epoch": 1.2248856865733684,
      "grad_norm": 9.573630332946777,
      "learning_rate": 3.510489122904254e-05,
      "loss": 2.9925,
      "step": 8840
    },
    {
      "epoch": 1.2262713038658721,
      "grad_norm": 7.922731876373291,
      "learning_rate": 3.509934875987253e-05,
      "loss": 2.3845,
      "step": 8850
    },
    {
      "epoch": 1.227656921158376,
      "grad_norm": 9.79373836517334,
      "learning_rate": 3.509380629070251e-05,
      "loss": 3.0436,
      "step": 8860
    },
    {
      "epoch": 1.2290425384508799,
      "grad_norm": 7.4819722175598145,
      "learning_rate": 3.50882638215325e-05,
      "loss": 2.7975,
      "step": 8870
    },
    {
      "epoch": 1.2304281557433836,
      "grad_norm": 6.308804035186768,
      "learning_rate": 3.508272135236248e-05,
      "loss": 2.7705,
      "step": 8880
    },
    {
      "epoch": 1.2318137730358876,
      "grad_norm": 9.939607620239258,
      "learning_rate": 3.507717888319246e-05,
      "loss": 3.2286,
      "step": 8890
    },
    {
      "epoch": 1.2331993903283913,
      "grad_norm": 9.523133277893066,
      "learning_rate": 3.507163641402245e-05,
      "loss": 3.5558,
      "step": 8900
    },
    {
      "epoch": 1.234585007620895,
      "grad_norm": 8.11387825012207,
      "learning_rate": 3.506609394485243e-05,
      "loss": 2.8169,
      "step": 8910
    },
    {
      "epoch": 1.2359706249133988,
      "grad_norm": 6.6394124031066895,
      "learning_rate": 3.506055147568242e-05,
      "loss": 2.9387,
      "step": 8920
    },
    {
      "epoch": 1.2373562422059028,
      "grad_norm": 9.661296844482422,
      "learning_rate": 3.50550090065124e-05,
      "loss": 2.4972,
      "step": 8930
    },
    {
      "epoch": 1.2387418594984065,
      "grad_norm": 7.95088529586792,
      "learning_rate": 3.504946653734239e-05,
      "loss": 2.7023,
      "step": 8940
    },
    {
      "epoch": 1.2401274767909103,
      "grad_norm": 7.543341636657715,
      "learning_rate": 3.5043924068172373e-05,
      "loss": 2.5548,
      "step": 8950
    },
    {
      "epoch": 1.2415130940834143,
      "grad_norm": 9.018522262573242,
      "learning_rate": 3.503838159900236e-05,
      "loss": 2.89,
      "step": 8960
    },
    {
      "epoch": 1.242898711375918,
      "grad_norm": 8.62789249420166,
      "learning_rate": 3.5032839129832344e-05,
      "loss": 2.9804,
      "step": 8970
    },
    {
      "epoch": 1.2442843286684218,
      "grad_norm": 9.81276798248291,
      "learning_rate": 3.5027296660662326e-05,
      "loss": 3.1163,
      "step": 8980
    },
    {
      "epoch": 1.2456699459609255,
      "grad_norm": 8.60411262512207,
      "learning_rate": 3.5021754191492314e-05,
      "loss": 2.7708,
      "step": 8990
    },
    {
      "epoch": 1.2470555632534295,
      "grad_norm": 12.495386123657227,
      "learning_rate": 3.5016211722322296e-05,
      "loss": 3.0163,
      "step": 9000
    },
    {
      "epoch": 1.2484411805459332,
      "grad_norm": 7.588886737823486,
      "learning_rate": 3.501066925315228e-05,
      "loss": 2.8789,
      "step": 9010
    },
    {
      "epoch": 1.249826797838437,
      "grad_norm": 7.204988479614258,
      "learning_rate": 3.500512678398227e-05,
      "loss": 2.7667,
      "step": 9020
    },
    {
      "epoch": 1.251212415130941,
      "grad_norm": 9.442770957946777,
      "learning_rate": 3.499958431481225e-05,
      "loss": 2.9792,
      "step": 9030
    },
    {
      "epoch": 1.2525980324234447,
      "grad_norm": 5.9045867919921875,
      "learning_rate": 3.499404184564224e-05,
      "loss": 3.1566,
      "step": 9040
    },
    {
      "epoch": 1.2539836497159484,
      "grad_norm": 12.695385932922363,
      "learning_rate": 3.498849937647222e-05,
      "loss": 2.276,
      "step": 9050
    },
    {
      "epoch": 1.2553692670084522,
      "grad_norm": 8.237688064575195,
      "learning_rate": 3.498295690730221e-05,
      "loss": 3.3914,
      "step": 9060
    },
    {
      "epoch": 1.256754884300956,
      "grad_norm": 7.8856425285339355,
      "learning_rate": 3.497741443813219e-05,
      "loss": 2.4001,
      "step": 9070
    },
    {
      "epoch": 1.25814050159346,
      "grad_norm": 10.48248291015625,
      "learning_rate": 3.497187196896218e-05,
      "loss": 2.7059,
      "step": 9080
    },
    {
      "epoch": 1.2595261188859637,
      "grad_norm": 8.622054100036621,
      "learning_rate": 3.496632949979216e-05,
      "loss": 3.2391,
      "step": 9090
    },
    {
      "epoch": 1.2609117361784676,
      "grad_norm": 8.050545692443848,
      "learning_rate": 3.496078703062214e-05,
      "loss": 2.7718,
      "step": 9100
    },
    {
      "epoch": 1.2622973534709714,
      "grad_norm": 9.031021118164062,
      "learning_rate": 3.495524456145213e-05,
      "loss": 2.4638,
      "step": 9110
    },
    {
      "epoch": 1.2636829707634751,
      "grad_norm": 10.672514915466309,
      "learning_rate": 3.494970209228211e-05,
      "loss": 2.8875,
      "step": 9120
    },
    {
      "epoch": 1.2650685880559789,
      "grad_norm": 10.51101303100586,
      "learning_rate": 3.4944159623112094e-05,
      "loss": 2.7228,
      "step": 9130
    },
    {
      "epoch": 1.2664542053484826,
      "grad_norm": 10.27071762084961,
      "learning_rate": 3.493861715394208e-05,
      "loss": 3.4225,
      "step": 9140
    },
    {
      "epoch": 1.2678398226409866,
      "grad_norm": 12.059986114501953,
      "learning_rate": 3.4933074684772065e-05,
      "loss": 3.0079,
      "step": 9150
    },
    {
      "epoch": 1.2692254399334904,
      "grad_norm": 9.729545593261719,
      "learning_rate": 3.4927532215602053e-05,
      "loss": 2.6213,
      "step": 9160
    },
    {
      "epoch": 1.270611057225994,
      "grad_norm": 18.952857971191406,
      "learning_rate": 3.492198974643204e-05,
      "loss": 3.0512,
      "step": 9170
    },
    {
      "epoch": 1.271996674518498,
      "grad_norm": 8.391935348510742,
      "learning_rate": 3.4916447277262024e-05,
      "loss": 2.7325,
      "step": 9180
    },
    {
      "epoch": 1.2733822918110018,
      "grad_norm": 10.925630569458008,
      "learning_rate": 3.491090480809201e-05,
      "loss": 2.9667,
      "step": 9190
    },
    {
      "epoch": 1.2747679091035056,
      "grad_norm": 9.42356014251709,
      "learning_rate": 3.4905362338921994e-05,
      "loss": 3.2525,
      "step": 9200
    },
    {
      "epoch": 1.2761535263960093,
      "grad_norm": 8.908784866333008,
      "learning_rate": 3.4899819869751976e-05,
      "loss": 2.7145,
      "step": 9210
    },
    {
      "epoch": 1.2775391436885133,
      "grad_norm": 10.395857810974121,
      "learning_rate": 3.4894277400581965e-05,
      "loss": 2.4556,
      "step": 9220
    },
    {
      "epoch": 1.278924760981017,
      "grad_norm": 7.377101421356201,
      "learning_rate": 3.488873493141195e-05,
      "loss": 2.9302,
      "step": 9230
    },
    {
      "epoch": 1.2803103782735208,
      "grad_norm": 10.115880966186523,
      "learning_rate": 3.488319246224193e-05,
      "loss": 3.0789,
      "step": 9240
    },
    {
      "epoch": 1.2816959955660248,
      "grad_norm": 12.405888557434082,
      "learning_rate": 3.487764999307192e-05,
      "loss": 3.1423,
      "step": 9250
    },
    {
      "epoch": 1.2830816128585285,
      "grad_norm": 8.922002792358398,
      "learning_rate": 3.48721075239019e-05,
      "loss": 2.7336,
      "step": 9260
    },
    {
      "epoch": 1.2844672301510323,
      "grad_norm": 8.679522514343262,
      "learning_rate": 3.486656505473189e-05,
      "loss": 2.6277,
      "step": 9270
    },
    {
      "epoch": 1.285852847443536,
      "grad_norm": 12.392049789428711,
      "learning_rate": 3.486102258556187e-05,
      "loss": 2.4957,
      "step": 9280
    },
    {
      "epoch": 1.28723846473604,
      "grad_norm": 8.373992919921875,
      "learning_rate": 3.485548011639186e-05,
      "loss": 3.1011,
      "step": 9290
    },
    {
      "epoch": 1.2886240820285437,
      "grad_norm": 9.6841459274292,
      "learning_rate": 3.484993764722184e-05,
      "loss": 3.162,
      "step": 9300
    },
    {
      "epoch": 1.2900096993210475,
      "grad_norm": 7.56016731262207,
      "learning_rate": 3.484439517805183e-05,
      "loss": 2.8267,
      "step": 9310
    },
    {
      "epoch": 1.2913953166135514,
      "grad_norm": 7.591274738311768,
      "learning_rate": 3.483885270888181e-05,
      "loss": 2.6793,
      "step": 9320
    },
    {
      "epoch": 1.2927809339060552,
      "grad_norm": 9.429483413696289,
      "learning_rate": 3.483331023971179e-05,
      "loss": 3.1058,
      "step": 9330
    },
    {
      "epoch": 1.294166551198559,
      "grad_norm": 7.861825942993164,
      "learning_rate": 3.482776777054178e-05,
      "loss": 3.1676,
      "step": 9340
    },
    {
      "epoch": 1.2955521684910627,
      "grad_norm": 9.892984390258789,
      "learning_rate": 3.482222530137176e-05,
      "loss": 2.6302,
      "step": 9350
    },
    {
      "epoch": 1.2969377857835664,
      "grad_norm": 10.373063087463379,
      "learning_rate": 3.4816682832201745e-05,
      "loss": 2.5214,
      "step": 9360
    },
    {
      "epoch": 1.2983234030760704,
      "grad_norm": 13.848276138305664,
      "learning_rate": 3.4811140363031734e-05,
      "loss": 2.6844,
      "step": 9370
    },
    {
      "epoch": 1.2997090203685742,
      "grad_norm": 10.27091121673584,
      "learning_rate": 3.4805597893861715e-05,
      "loss": 2.3344,
      "step": 9380
    },
    {
      "epoch": 1.3010946376610781,
      "grad_norm": 10.838960647583008,
      "learning_rate": 3.4800055424691704e-05,
      "loss": 3.0588,
      "step": 9390
    },
    {
      "epoch": 1.3024802549535819,
      "grad_norm": 9.172033309936523,
      "learning_rate": 3.4794512955521686e-05,
      "loss": 3.024,
      "step": 9400
    },
    {
      "epoch": 1.3038658722460856,
      "grad_norm": 8.57127571105957,
      "learning_rate": 3.4788970486351675e-05,
      "loss": 2.8645,
      "step": 9410
    },
    {
      "epoch": 1.3052514895385894,
      "grad_norm": 8.134109497070312,
      "learning_rate": 3.4783428017181656e-05,
      "loss": 2.6733,
      "step": 9420
    },
    {
      "epoch": 1.3066371068310931,
      "grad_norm": 14.698951721191406,
      "learning_rate": 3.4777885548011645e-05,
      "loss": 2.6267,
      "step": 9430
    },
    {
      "epoch": 1.308022724123597,
      "grad_norm": 7.933813095092773,
      "learning_rate": 3.477234307884163e-05,
      "loss": 3.3903,
      "step": 9440
    },
    {
      "epoch": 1.3094083414161009,
      "grad_norm": 11.712095260620117,
      "learning_rate": 3.4766800609671616e-05,
      "loss": 2.789,
      "step": 9450
    },
    {
      "epoch": 1.3107939587086046,
      "grad_norm": 21.490121841430664,
      "learning_rate": 3.47612581405016e-05,
      "loss": 2.8153,
      "step": 9460
    },
    {
      "epoch": 1.3121795760011086,
      "grad_norm": 12.402658462524414,
      "learning_rate": 3.475571567133158e-05,
      "loss": 2.6974,
      "step": 9470
    },
    {
      "epoch": 1.3135651932936123,
      "grad_norm": 7.357696533203125,
      "learning_rate": 3.475017320216157e-05,
      "loss": 2.9745,
      "step": 9480
    },
    {
      "epoch": 1.314950810586116,
      "grad_norm": 11.912210464477539,
      "learning_rate": 3.474463073299155e-05,
      "loss": 2.9836,
      "step": 9490
    },
    {
      "epoch": 1.3163364278786198,
      "grad_norm": 8.838428497314453,
      "learning_rate": 3.473908826382153e-05,
      "loss": 2.751,
      "step": 9500
    },
    {
      "epoch": 1.3177220451711238,
      "grad_norm": 7.736107349395752,
      "learning_rate": 3.473354579465152e-05,
      "loss": 2.9232,
      "step": 9510
    },
    {
      "epoch": 1.3191076624636275,
      "grad_norm": 10.345586776733398,
      "learning_rate": 3.47280033254815e-05,
      "loss": 3.3653,
      "step": 9520
    },
    {
      "epoch": 1.3204932797561313,
      "grad_norm": 11.668227195739746,
      "learning_rate": 3.472246085631149e-05,
      "loss": 3.6031,
      "step": 9530
    },
    {
      "epoch": 1.3218788970486353,
      "grad_norm": 8.228067398071289,
      "learning_rate": 3.471691838714148e-05,
      "loss": 2.3925,
      "step": 9540
    },
    {
      "epoch": 1.323264514341139,
      "grad_norm": 11.158653259277344,
      "learning_rate": 3.471137591797146e-05,
      "loss": 2.6554,
      "step": 9550
    },
    {
      "epoch": 1.3246501316336428,
      "grad_norm": 5.387529373168945,
      "learning_rate": 3.470583344880144e-05,
      "loss": 2.7025,
      "step": 9560
    },
    {
      "epoch": 1.3260357489261465,
      "grad_norm": 9.662882804870605,
      "learning_rate": 3.470029097963143e-05,
      "loss": 2.8405,
      "step": 9570
    },
    {
      "epoch": 1.3274213662186505,
      "grad_norm": 11.042542457580566,
      "learning_rate": 3.4694748510461414e-05,
      "loss": 3.1057,
      "step": 9580
    },
    {
      "epoch": 1.3288069835111542,
      "grad_norm": 7.810927867889404,
      "learning_rate": 3.4689206041291395e-05,
      "loss": 3.327,
      "step": 9590
    },
    {
      "epoch": 1.330192600803658,
      "grad_norm": 11.196972846984863,
      "learning_rate": 3.4683663572121384e-05,
      "loss": 2.8665,
      "step": 9600
    },
    {
      "epoch": 1.331578218096162,
      "grad_norm": 11.70004653930664,
      "learning_rate": 3.4678121102951366e-05,
      "loss": 2.9949,
      "step": 9610
    },
    {
      "epoch": 1.3329638353886657,
      "grad_norm": 9.74608325958252,
      "learning_rate": 3.467257863378135e-05,
      "loss": 2.5994,
      "step": 9620
    },
    {
      "epoch": 1.3343494526811694,
      "grad_norm": 11.640804290771484,
      "learning_rate": 3.4667036164611336e-05,
      "loss": 2.6795,
      "step": 9630
    },
    {
      "epoch": 1.3357350699736732,
      "grad_norm": 12.032580375671387,
      "learning_rate": 3.466149369544132e-05,
      "loss": 2.6503,
      "step": 9640
    },
    {
      "epoch": 1.337120687266177,
      "grad_norm": 11.289548873901367,
      "learning_rate": 3.465595122627131e-05,
      "loss": 2.597,
      "step": 9650
    },
    {
      "epoch": 1.338506304558681,
      "grad_norm": 6.4258880615234375,
      "learning_rate": 3.465096300401829e-05,
      "loss": 2.9607,
      "step": 9660
    },
    {
      "epoch": 1.3398919218511847,
      "grad_norm": 12.520638465881348,
      "learning_rate": 3.4645420534848275e-05,
      "loss": 2.6308,
      "step": 9670
    },
    {
      "epoch": 1.3412775391436886,
      "grad_norm": 9.57197093963623,
      "learning_rate": 3.4639878065678264e-05,
      "loss": 3.3348,
      "step": 9680
    },
    {
      "epoch": 1.3426631564361924,
      "grad_norm": 9.244975090026855,
      "learning_rate": 3.4634335596508246e-05,
      "loss": 2.9582,
      "step": 9690
    },
    {
      "epoch": 1.3440487737286961,
      "grad_norm": 9.549949645996094,
      "learning_rate": 3.4628793127338234e-05,
      "loss": 3.2125,
      "step": 9700
    },
    {
      "epoch": 1.3454343910211999,
      "grad_norm": 9.464808464050293,
      "learning_rate": 3.4623250658168216e-05,
      "loss": 3.2991,
      "step": 9710
    },
    {
      "epoch": 1.3468200083137036,
      "grad_norm": 9.283803939819336,
      "learning_rate": 3.4617708188998205e-05,
      "loss": 3.1323,
      "step": 9720
    },
    {
      "epoch": 1.3482056256062076,
      "grad_norm": 10.198431968688965,
      "learning_rate": 3.461216571982819e-05,
      "loss": 3.0233,
      "step": 9730
    },
    {
      "epoch": 1.3495912428987114,
      "grad_norm": 10.014752388000488,
      "learning_rate": 3.460662325065817e-05,
      "loss": 2.9015,
      "step": 9740
    },
    {
      "epoch": 1.3509768601912153,
      "grad_norm": 7.617334365844727,
      "learning_rate": 3.460108078148816e-05,
      "loss": 2.8241,
      "step": 9750
    },
    {
      "epoch": 1.352362477483719,
      "grad_norm": 9.518512725830078,
      "learning_rate": 3.459553831231814e-05,
      "loss": 3.2154,
      "step": 9760
    },
    {
      "epoch": 1.3537480947762228,
      "grad_norm": 7.127592086791992,
      "learning_rate": 3.458999584314813e-05,
      "loss": 2.9052,
      "step": 9770
    },
    {
      "epoch": 1.3551337120687266,
      "grad_norm": 8.571802139282227,
      "learning_rate": 3.458445337397811e-05,
      "loss": 2.7512,
      "step": 9780
    },
    {
      "epoch": 1.3565193293612303,
      "grad_norm": 11.538677215576172,
      "learning_rate": 3.457891090480809e-05,
      "loss": 3.3493,
      "step": 9790
    },
    {
      "epoch": 1.3579049466537343,
      "grad_norm": 9.279150009155273,
      "learning_rate": 3.457336843563808e-05,
      "loss": 3.0051,
      "step": 9800
    },
    {
      "epoch": 1.359290563946238,
      "grad_norm": 11.531026840209961,
      "learning_rate": 3.456782596646806e-05,
      "loss": 2.5055,
      "step": 9810
    },
    {
      "epoch": 1.3606761812387418,
      "grad_norm": 6.658394813537598,
      "learning_rate": 3.456228349729805e-05,
      "loss": 2.2558,
      "step": 9820
    },
    {
      "epoch": 1.3620617985312458,
      "grad_norm": 13.841444969177246,
      "learning_rate": 3.455674102812803e-05,
      "loss": 2.8639,
      "step": 9830
    },
    {
      "epoch": 1.3634474158237495,
      "grad_norm": 8.758230209350586,
      "learning_rate": 3.455119855895802e-05,
      "loss": 2.4254,
      "step": 9840
    },
    {
      "epoch": 1.3648330331162533,
      "grad_norm": 8.45373249053955,
      "learning_rate": 3.4545656089788e-05,
      "loss": 2.8409,
      "step": 9850
    },
    {
      "epoch": 1.366218650408757,
      "grad_norm": 10.481807708740234,
      "learning_rate": 3.454011362061799e-05,
      "loss": 2.9253,
      "step": 9860
    },
    {
      "epoch": 1.367604267701261,
      "grad_norm": 10.51848030090332,
      "learning_rate": 3.4534571151447973e-05,
      "loss": 2.8771,
      "step": 9870
    },
    {
      "epoch": 1.3689898849937647,
      "grad_norm": 8.544960975646973,
      "learning_rate": 3.4529028682277955e-05,
      "loss": 3.0613,
      "step": 9880
    },
    {
      "epoch": 1.3703755022862685,
      "grad_norm": 9.187289237976074,
      "learning_rate": 3.4523486213107944e-05,
      "loss": 2.9038,
      "step": 9890
    },
    {
      "epoch": 1.3717611195787724,
      "grad_norm": 11.269383430480957,
      "learning_rate": 3.4517943743937926e-05,
      "loss": 3.1663,
      "step": 9900
    },
    {
      "epoch": 1.3731467368712762,
      "grad_norm": 9.976293563842773,
      "learning_rate": 3.451240127476791e-05,
      "loss": 3.2176,
      "step": 9910
    },
    {
      "epoch": 1.37453235416378,
      "grad_norm": 15.107194900512695,
      "learning_rate": 3.4506858805597896e-05,
      "loss": 2.9586,
      "step": 9920
    },
    {
      "epoch": 1.3759179714562837,
      "grad_norm": 6.811245441436768,
      "learning_rate": 3.450131633642788e-05,
      "loss": 2.6512,
      "step": 9930
    },
    {
      "epoch": 1.3773035887487877,
      "grad_norm": 11.612129211425781,
      "learning_rate": 3.449577386725787e-05,
      "loss": 3.0546,
      "step": 9940
    },
    {
      "epoch": 1.3786892060412914,
      "grad_norm": 10.611662864685059,
      "learning_rate": 3.4490231398087855e-05,
      "loss": 2.5698,
      "step": 9950
    },
    {
      "epoch": 1.3800748233337952,
      "grad_norm": 11.515256881713867,
      "learning_rate": 3.448468892891784e-05,
      "loss": 2.7348,
      "step": 9960
    },
    {
      "epoch": 1.3814604406262991,
      "grad_norm": 15.125904083251953,
      "learning_rate": 3.447914645974782e-05,
      "loss": 3.0663,
      "step": 9970
    },
    {
      "epoch": 1.3828460579188029,
      "grad_norm": 9.183638572692871,
      "learning_rate": 3.447360399057781e-05,
      "loss": 2.8963,
      "step": 9980
    },
    {
      "epoch": 1.3842316752113066,
      "grad_norm": 9.123579978942871,
      "learning_rate": 3.446806152140779e-05,
      "loss": 3.154,
      "step": 9990
    },
    {
      "epoch": 1.3856172925038104,
      "grad_norm": 13.217728614807129,
      "learning_rate": 3.446251905223777e-05,
      "loss": 3.9072,
      "step": 10000
    },
    {
      "epoch": 1.3870029097963141,
      "grad_norm": 13.939703941345215,
      "learning_rate": 3.445697658306776e-05,
      "loss": 2.9497,
      "step": 10010
    },
    {
      "epoch": 1.388388527088818,
      "grad_norm": 15.411192893981934,
      "learning_rate": 3.445143411389774e-05,
      "loss": 2.8173,
      "step": 10020
    },
    {
      "epoch": 1.3897741443813219,
      "grad_norm": 11.371112823486328,
      "learning_rate": 3.4445891644727724e-05,
      "loss": 2.7438,
      "step": 10030
    },
    {
      "epoch": 1.3911597616738258,
      "grad_norm": 7.641732215881348,
      "learning_rate": 3.444034917555771e-05,
      "loss": 2.7197,
      "step": 10040
    },
    {
      "epoch": 1.3925453789663296,
      "grad_norm": 7.031378746032715,
      "learning_rate": 3.44348067063877e-05,
      "loss": 2.4634,
      "step": 10050
    },
    {
      "epoch": 1.3939309962588333,
      "grad_norm": 16.531198501586914,
      "learning_rate": 3.442926423721768e-05,
      "loss": 2.6695,
      "step": 10060
    },
    {
      "epoch": 1.395316613551337,
      "grad_norm": 7.73068904876709,
      "learning_rate": 3.442372176804767e-05,
      "loss": 3.1965,
      "step": 10070
    },
    {
      "epoch": 1.3967022308438408,
      "grad_norm": 11.780901908874512,
      "learning_rate": 3.4418179298877654e-05,
      "loss": 3.1281,
      "step": 10080
    },
    {
      "epoch": 1.3980878481363448,
      "grad_norm": 7.115349769592285,
      "learning_rate": 3.441263682970764e-05,
      "loss": 2.4542,
      "step": 10090
    },
    {
      "epoch": 1.3994734654288485,
      "grad_norm": 7.5277018547058105,
      "learning_rate": 3.4407094360537624e-05,
      "loss": 2.6005,
      "step": 10100
    },
    {
      "epoch": 1.4008590827213523,
      "grad_norm": 11.757545471191406,
      "learning_rate": 3.4401551891367606e-05,
      "loss": 2.6718,
      "step": 10110
    },
    {
      "epoch": 1.4022447000138563,
      "grad_norm": 7.339056968688965,
      "learning_rate": 3.4396009422197594e-05,
      "loss": 2.9074,
      "step": 10120
    },
    {
      "epoch": 1.40363031730636,
      "grad_norm": 11.873178482055664,
      "learning_rate": 3.4390466953027576e-05,
      "loss": 3.1208,
      "step": 10130
    },
    {
      "epoch": 1.4050159345988638,
      "grad_norm": 11.126131057739258,
      "learning_rate": 3.438492448385756e-05,
      "loss": 2.6419,
      "step": 10140
    },
    {
      "epoch": 1.4064015518913675,
      "grad_norm": 11.658809661865234,
      "learning_rate": 3.437938201468755e-05,
      "loss": 2.8643,
      "step": 10150
    },
    {
      "epoch": 1.4077871691838715,
      "grad_norm": 8.473893165588379,
      "learning_rate": 3.437383954551753e-05,
      "loss": 2.7714,
      "step": 10160
    },
    {
      "epoch": 1.4091727864763752,
      "grad_norm": 11.375481605529785,
      "learning_rate": 3.436829707634752e-05,
      "loss": 2.6926,
      "step": 10170
    },
    {
      "epoch": 1.410558403768879,
      "grad_norm": 11.079219818115234,
      "learning_rate": 3.43627546071775e-05,
      "loss": 2.6832,
      "step": 10180
    },
    {
      "epoch": 1.411944021061383,
      "grad_norm": 6.417253017425537,
      "learning_rate": 3.435721213800749e-05,
      "loss": 2.9417,
      "step": 10190
    },
    {
      "epoch": 1.4133296383538867,
      "grad_norm": 7.946660041809082,
      "learning_rate": 3.435166966883747e-05,
      "loss": 2.5661,
      "step": 10200
    },
    {
      "epoch": 1.4147152556463904,
      "grad_norm": 5.659088134765625,
      "learning_rate": 3.434612719966746e-05,
      "loss": 3.1985,
      "step": 10210
    },
    {
      "epoch": 1.4161008729388942,
      "grad_norm": 9.690787315368652,
      "learning_rate": 3.434058473049744e-05,
      "loss": 3.1603,
      "step": 10220
    },
    {
      "epoch": 1.4174864902313982,
      "grad_norm": 17.438486099243164,
      "learning_rate": 3.433504226132742e-05,
      "loss": 3.0198,
      "step": 10230
    },
    {
      "epoch": 1.418872107523902,
      "grad_norm": 11.783604621887207,
      "learning_rate": 3.432949979215741e-05,
      "loss": 2.6244,
      "step": 10240
    },
    {
      "epoch": 1.4202577248164057,
      "grad_norm": 9.393012046813965,
      "learning_rate": 3.432395732298739e-05,
      "loss": 3.0126,
      "step": 10250
    },
    {
      "epoch": 1.4216433421089096,
      "grad_norm": 11.060099601745605,
      "learning_rate": 3.4318414853817374e-05,
      "loss": 2.3736,
      "step": 10260
    },
    {
      "epoch": 1.4230289594014134,
      "grad_norm": 7.999692916870117,
      "learning_rate": 3.431287238464736e-05,
      "loss": 2.5066,
      "step": 10270
    },
    {
      "epoch": 1.4244145766939171,
      "grad_norm": 8.819729804992676,
      "learning_rate": 3.4307329915477345e-05,
      "loss": 2.883,
      "step": 10280
    },
    {
      "epoch": 1.4258001939864209,
      "grad_norm": 8.828059196472168,
      "learning_rate": 3.4301787446307334e-05,
      "loss": 2.9368,
      "step": 10290
    },
    {
      "epoch": 1.4271858112789246,
      "grad_norm": 12.482786178588867,
      "learning_rate": 3.4296244977137315e-05,
      "loss": 3.405,
      "step": 10300
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 8.408697128295898,
      "learning_rate": 3.4290702507967304e-05,
      "loss": 2.9311,
      "step": 10310
    },
    {
      "epoch": 1.4299570458639324,
      "grad_norm": 9.325382232666016,
      "learning_rate": 3.4285160038797286e-05,
      "loss": 2.6466,
      "step": 10320
    },
    {
      "epoch": 1.4313426631564363,
      "grad_norm": 6.402508735656738,
      "learning_rate": 3.4279617569627275e-05,
      "loss": 3.0381,
      "step": 10330
    },
    {
      "epoch": 1.43272828044894,
      "grad_norm": 8.635882377624512,
      "learning_rate": 3.4274075100457256e-05,
      "loss": 2.7751,
      "step": 10340
    },
    {
      "epoch": 1.4341138977414438,
      "grad_norm": 10.960564613342285,
      "learning_rate": 3.426853263128724e-05,
      "loss": 2.9293,
      "step": 10350
    },
    {
      "epoch": 1.4354995150339476,
      "grad_norm": 10.053631782531738,
      "learning_rate": 3.426299016211723e-05,
      "loss": 3.3148,
      "step": 10360
    },
    {
      "epoch": 1.4368851323264513,
      "grad_norm": 8.877311706542969,
      "learning_rate": 3.425744769294721e-05,
      "loss": 2.7776,
      "step": 10370
    },
    {
      "epoch": 1.4382707496189553,
      "grad_norm": 10.538008689880371,
      "learning_rate": 3.42519052237772e-05,
      "loss": 2.7449,
      "step": 10380
    },
    {
      "epoch": 1.439656366911459,
      "grad_norm": 11.189780235290527,
      "learning_rate": 3.424636275460718e-05,
      "loss": 2.6393,
      "step": 10390
    },
    {
      "epoch": 1.4410419842039628,
      "grad_norm": 6.350564956665039,
      "learning_rate": 3.424082028543716e-05,
      "loss": 2.6191,
      "step": 10400
    },
    {
      "epoch": 1.4424276014964668,
      "grad_norm": 8.893223762512207,
      "learning_rate": 3.423527781626715e-05,
      "loss": 2.42,
      "step": 10410
    },
    {
      "epoch": 1.4438132187889705,
      "grad_norm": 11.11877155303955,
      "learning_rate": 3.422973534709713e-05,
      "loss": 2.7996,
      "step": 10420
    },
    {
      "epoch": 1.4451988360814743,
      "grad_norm": 9.70274543762207,
      "learning_rate": 3.422419287792712e-05,
      "loss": 3.337,
      "step": 10430
    },
    {
      "epoch": 1.446584453373978,
      "grad_norm": 10.567179679870605,
      "learning_rate": 3.421865040875711e-05,
      "loss": 2.8477,
      "step": 10440
    },
    {
      "epoch": 1.447970070666482,
      "grad_norm": 15.657992362976074,
      "learning_rate": 3.421310793958709e-05,
      "loss": 2.3834,
      "step": 10450
    },
    {
      "epoch": 1.4493556879589857,
      "grad_norm": 10.14991569519043,
      "learning_rate": 3.420756547041707e-05,
      "loss": 2.5576,
      "step": 10460
    },
    {
      "epoch": 1.4507413052514895,
      "grad_norm": 13.208224296569824,
      "learning_rate": 3.420202300124706e-05,
      "loss": 3.2157,
      "step": 10470
    },
    {
      "epoch": 1.4521269225439934,
      "grad_norm": 12.323226928710938,
      "learning_rate": 3.419648053207704e-05,
      "loss": 2.3295,
      "step": 10480
    },
    {
      "epoch": 1.4535125398364972,
      "grad_norm": 12.8388090133667,
      "learning_rate": 3.4190938062907025e-05,
      "loss": 3.3021,
      "step": 10490
    },
    {
      "epoch": 1.454898157129001,
      "grad_norm": 8.794177055358887,
      "learning_rate": 3.4185395593737014e-05,
      "loss": 2.7253,
      "step": 10500
    },
    {
      "epoch": 1.4562837744215047,
      "grad_norm": 9.059990882873535,
      "learning_rate": 3.4179853124566996e-05,
      "loss": 2.6577,
      "step": 10510
    },
    {
      "epoch": 1.4576693917140087,
      "grad_norm": 8.241386413574219,
      "learning_rate": 3.417431065539698e-05,
      "loss": 2.3638,
      "step": 10520
    },
    {
      "epoch": 1.4590550090065124,
      "grad_norm": 8.882030487060547,
      "learning_rate": 3.4168768186226966e-05,
      "loss": 2.6781,
      "step": 10530
    },
    {
      "epoch": 1.4604406262990162,
      "grad_norm": 14.087506294250488,
      "learning_rate": 3.4163225717056955e-05,
      "loss": 2.5459,
      "step": 10540
    },
    {
      "epoch": 1.4618262435915201,
      "grad_norm": 8.465873718261719,
      "learning_rate": 3.4157683247886936e-05,
      "loss": 2.51,
      "step": 10550
    },
    {
      "epoch": 1.4632118608840239,
      "grad_norm": 8.714884757995605,
      "learning_rate": 3.4152140778716925e-05,
      "loss": 3.26,
      "step": 10560
    },
    {
      "epoch": 1.4645974781765276,
      "grad_norm": 14.654704093933105,
      "learning_rate": 3.414659830954691e-05,
      "loss": 3.6484,
      "step": 10570
    },
    {
      "epoch": 1.4659830954690314,
      "grad_norm": 17.4462947845459,
      "learning_rate": 3.414105584037689e-05,
      "loss": 2.5523,
      "step": 10580
    },
    {
      "epoch": 1.4673687127615351,
      "grad_norm": 7.138838291168213,
      "learning_rate": 3.413551337120688e-05,
      "loss": 2.9798,
      "step": 10590
    },
    {
      "epoch": 1.468754330054039,
      "grad_norm": 14.841306686401367,
      "learning_rate": 3.412997090203686e-05,
      "loss": 2.7531,
      "step": 10600
    },
    {
      "epoch": 1.4701399473465429,
      "grad_norm": 7.746425151824951,
      "learning_rate": 3.412442843286684e-05,
      "loss": 2.8626,
      "step": 10610
    },
    {
      "epoch": 1.4715255646390468,
      "grad_norm": 13.395692825317383,
      "learning_rate": 3.411888596369683e-05,
      "loss": 3.0877,
      "step": 10620
    },
    {
      "epoch": 1.4729111819315506,
      "grad_norm": 7.480428695678711,
      "learning_rate": 3.411334349452681e-05,
      "loss": 3.1951,
      "step": 10630
    },
    {
      "epoch": 1.4742967992240543,
      "grad_norm": 8.771763801574707,
      "learning_rate": 3.41078010253568e-05,
      "loss": 2.6152,
      "step": 10640
    },
    {
      "epoch": 1.475682416516558,
      "grad_norm": 14.378461837768555,
      "learning_rate": 3.410225855618678e-05,
      "loss": 2.6248,
      "step": 10650
    },
    {
      "epoch": 1.4770680338090618,
      "grad_norm": 8.789307594299316,
      "learning_rate": 3.409671608701677e-05,
      "loss": 2.9592,
      "step": 10660
    },
    {
      "epoch": 1.4784536511015658,
      "grad_norm": 9.894025802612305,
      "learning_rate": 3.409117361784675e-05,
      "loss": 3.1817,
      "step": 10670
    },
    {
      "epoch": 1.4798392683940695,
      "grad_norm": 8.969942092895508,
      "learning_rate": 3.408563114867674e-05,
      "loss": 2.9096,
      "step": 10680
    },
    {
      "epoch": 1.4812248856865733,
      "grad_norm": 12.342723846435547,
      "learning_rate": 3.408008867950672e-05,
      "loss": 2.8186,
      "step": 10690
    },
    {
      "epoch": 1.4826105029790773,
      "grad_norm": 10.094937324523926,
      "learning_rate": 3.407454621033671e-05,
      "loss": 3.0835,
      "step": 10700
    },
    {
      "epoch": 1.483996120271581,
      "grad_norm": 9.2767915725708,
      "learning_rate": 3.4069003741166694e-05,
      "loss": 3.2419,
      "step": 10710
    },
    {
      "epoch": 1.4853817375640848,
      "grad_norm": 10.769364356994629,
      "learning_rate": 3.4063461271996676e-05,
      "loss": 2.7599,
      "step": 10720
    },
    {
      "epoch": 1.4867673548565885,
      "grad_norm": 15.01574420928955,
      "learning_rate": 3.4057918802826664e-05,
      "loss": 2.8931,
      "step": 10730
    },
    {
      "epoch": 1.4881529721490925,
      "grad_norm": 8.980159759521484,
      "learning_rate": 3.4052376333656646e-05,
      "loss": 2.8132,
      "step": 10740
    },
    {
      "epoch": 1.4895385894415962,
      "grad_norm": 8.128779411315918,
      "learning_rate": 3.404683386448663e-05,
      "loss": 2.794,
      "step": 10750
    },
    {
      "epoch": 1.4909242067341,
      "grad_norm": 12.313043594360352,
      "learning_rate": 3.4041291395316617e-05,
      "loss": 2.7899,
      "step": 10760
    },
    {
      "epoch": 1.492309824026604,
      "grad_norm": 9.42667007446289,
      "learning_rate": 3.40357489261466e-05,
      "loss": 2.934,
      "step": 10770
    },
    {
      "epoch": 1.4936954413191077,
      "grad_norm": 10.361920356750488,
      "learning_rate": 3.403020645697659e-05,
      "loss": 2.3883,
      "step": 10780
    },
    {
      "epoch": 1.4950810586116114,
      "grad_norm": 11.407939910888672,
      "learning_rate": 3.402466398780657e-05,
      "loss": 3.1117,
      "step": 10790
    },
    {
      "epoch": 1.4964666759041152,
      "grad_norm": 8.474381446838379,
      "learning_rate": 3.401912151863656e-05,
      "loss": 2.6544,
      "step": 10800
    },
    {
      "epoch": 1.4978522931966192,
      "grad_norm": 11.722577095031738,
      "learning_rate": 3.401357904946654e-05,
      "loss": 2.7904,
      "step": 10810
    },
    {
      "epoch": 1.499237910489123,
      "grad_norm": 8.622462272644043,
      "learning_rate": 3.400803658029653e-05,
      "loss": 2.617,
      "step": 10820
    },
    {
      "epoch": 1.5006235277816267,
      "grad_norm": 8.51187515258789,
      "learning_rate": 3.400249411112651e-05,
      "loss": 2.7129,
      "step": 10830
    },
    {
      "epoch": 1.5020091450741306,
      "grad_norm": 10.584322929382324,
      "learning_rate": 3.399695164195649e-05,
      "loss": 2.9665,
      "step": 10840
    },
    {
      "epoch": 1.5033947623666344,
      "grad_norm": 10.4657564163208,
      "learning_rate": 3.399140917278648e-05,
      "loss": 2.7361,
      "step": 10850
    },
    {
      "epoch": 1.5047803796591381,
      "grad_norm": 10.79537582397461,
      "learning_rate": 3.398586670361646e-05,
      "loss": 2.9969,
      "step": 10860
    },
    {
      "epoch": 1.5061659969516419,
      "grad_norm": 7.224458694458008,
      "learning_rate": 3.3980324234446444e-05,
      "loss": 2.6658,
      "step": 10870
    },
    {
      "epoch": 1.5075516142441456,
      "grad_norm": 7.157652378082275,
      "learning_rate": 3.397478176527643e-05,
      "loss": 3.0936,
      "step": 10880
    },
    {
      "epoch": 1.5089372315366496,
      "grad_norm": 9.671306610107422,
      "learning_rate": 3.3969239296106415e-05,
      "loss": 2.8599,
      "step": 10890
    },
    {
      "epoch": 1.5103228488291534,
      "grad_norm": 9.288142204284668,
      "learning_rate": 3.39636968269364e-05,
      "loss": 2.6606,
      "step": 10900
    },
    {
      "epoch": 1.5117084661216573,
      "grad_norm": 8.955260276794434,
      "learning_rate": 3.395815435776639e-05,
      "loss": 2.8072,
      "step": 10910
    },
    {
      "epoch": 1.513094083414161,
      "grad_norm": 8.182401657104492,
      "learning_rate": 3.3952611888596374e-05,
      "loss": 2.854,
      "step": 10920
    },
    {
      "epoch": 1.5144797007066648,
      "grad_norm": 8.84386157989502,
      "learning_rate": 3.3947069419426356e-05,
      "loss": 2.8392,
      "step": 10930
    },
    {
      "epoch": 1.5158653179991686,
      "grad_norm": 11.409454345703125,
      "learning_rate": 3.3941526950256344e-05,
      "loss": 2.4832,
      "step": 10940
    },
    {
      "epoch": 1.5172509352916723,
      "grad_norm": 9.88207721710205,
      "learning_rate": 3.3935984481086326e-05,
      "loss": 3.0073,
      "step": 10950
    },
    {
      "epoch": 1.5186365525841763,
      "grad_norm": 11.858380317687988,
      "learning_rate": 3.3930442011916315e-05,
      "loss": 3.1904,
      "step": 10960
    },
    {
      "epoch": 1.52002216987668,
      "grad_norm": 10.39583683013916,
      "learning_rate": 3.39248995427463e-05,
      "loss": 2.9255,
      "step": 10970
    },
    {
      "epoch": 1.521407787169184,
      "grad_norm": 9.556205749511719,
      "learning_rate": 3.391935707357628e-05,
      "loss": 2.9385,
      "step": 10980
    },
    {
      "epoch": 1.5227934044616878,
      "grad_norm": 7.246410846710205,
      "learning_rate": 3.391381460440627e-05,
      "loss": 2.8426,
      "step": 10990
    },
    {
      "epoch": 1.5241790217541915,
      "grad_norm": 9.836027145385742,
      "learning_rate": 3.390827213523625e-05,
      "loss": 3.1103,
      "step": 11000
    },
    {
      "epoch": 1.5255646390466953,
      "grad_norm": 12.38597297668457,
      "learning_rate": 3.390272966606623e-05,
      "loss": 3.015,
      "step": 11010
    },
    {
      "epoch": 1.526950256339199,
      "grad_norm": 7.611448764801025,
      "learning_rate": 3.389718719689622e-05,
      "loss": 2.5139,
      "step": 11020
    },
    {
      "epoch": 1.528335873631703,
      "grad_norm": 11.589893341064453,
      "learning_rate": 3.389164472772621e-05,
      "loss": 3.0345,
      "step": 11030
    },
    {
      "epoch": 1.5297214909242067,
      "grad_norm": 15.103222846984863,
      "learning_rate": 3.388610225855619e-05,
      "loss": 2.7409,
      "step": 11040
    },
    {
      "epoch": 1.5311071082167107,
      "grad_norm": 14.961952209472656,
      "learning_rate": 3.388055978938618e-05,
      "loss": 2.6553,
      "step": 11050
    },
    {
      "epoch": 1.5324927255092144,
      "grad_norm": 9.261817932128906,
      "learning_rate": 3.387501732021616e-05,
      "loss": 3.2167,
      "step": 11060
    },
    {
      "epoch": 1.5338783428017182,
      "grad_norm": 10.584914207458496,
      "learning_rate": 3.386947485104614e-05,
      "loss": 3.7025,
      "step": 11070
    },
    {
      "epoch": 1.535263960094222,
      "grad_norm": 9.987833023071289,
      "learning_rate": 3.386393238187613e-05,
      "loss": 2.7678,
      "step": 11080
    },
    {
      "epoch": 1.5366495773867257,
      "grad_norm": 11.175190925598145,
      "learning_rate": 3.385838991270611e-05,
      "loss": 2.4799,
      "step": 11090
    },
    {
      "epoch": 1.5380351946792294,
      "grad_norm": 10.616533279418945,
      "learning_rate": 3.3852847443536095e-05,
      "loss": 2.1574,
      "step": 11100
    },
    {
      "epoch": 1.5394208119717334,
      "grad_norm": 9.015523910522461,
      "learning_rate": 3.384730497436608e-05,
      "loss": 2.5459,
      "step": 11110
    },
    {
      "epoch": 1.5408064292642372,
      "grad_norm": 9.236474990844727,
      "learning_rate": 3.3841762505196065e-05,
      "loss": 2.8193,
      "step": 11120
    },
    {
      "epoch": 1.5421920465567411,
      "grad_norm": 22.749357223510742,
      "learning_rate": 3.3836220036026054e-05,
      "loss": 2.4946,
      "step": 11130
    },
    {
      "epoch": 1.5435776638492449,
      "grad_norm": 14.992863655090332,
      "learning_rate": 3.3830677566856036e-05,
      "loss": 2.8537,
      "step": 11140
    },
    {
      "epoch": 1.5449632811417486,
      "grad_norm": 9.800978660583496,
      "learning_rate": 3.3825135097686024e-05,
      "loss": 2.4755,
      "step": 11150
    },
    {
      "epoch": 1.5463488984342524,
      "grad_norm": 6.217576026916504,
      "learning_rate": 3.3819592628516006e-05,
      "loss": 2.9077,
      "step": 11160
    },
    {
      "epoch": 1.5477345157267561,
      "grad_norm": 6.896458148956299,
      "learning_rate": 3.3814050159345995e-05,
      "loss": 2.4963,
      "step": 11170
    },
    {
      "epoch": 1.54912013301926,
      "grad_norm": 9.409454345703125,
      "learning_rate": 3.380850769017598e-05,
      "loss": 2.6125,
      "step": 11180
    },
    {
      "epoch": 1.5505057503117639,
      "grad_norm": 8.481416702270508,
      "learning_rate": 3.380296522100596e-05,
      "loss": 2.6651,
      "step": 11190
    },
    {
      "epoch": 1.5518913676042678,
      "grad_norm": 11.923765182495117,
      "learning_rate": 3.379742275183595e-05,
      "loss": 2.7335,
      "step": 11200
    },
    {
      "epoch": 1.5532769848967716,
      "grad_norm": 8.629791259765625,
      "learning_rate": 3.379188028266593e-05,
      "loss": 2.6859,
      "step": 11210
    },
    {
      "epoch": 1.5546626021892753,
      "grad_norm": 15.997905731201172,
      "learning_rate": 3.378633781349591e-05,
      "loss": 2.7433,
      "step": 11220
    },
    {
      "epoch": 1.556048219481779,
      "grad_norm": 7.332687854766846,
      "learning_rate": 3.37807953443259e-05,
      "loss": 2.8152,
      "step": 11230
    },
    {
      "epoch": 1.5574338367742828,
      "grad_norm": 9.514026641845703,
      "learning_rate": 3.377525287515588e-05,
      "loss": 2.7522,
      "step": 11240
    },
    {
      "epoch": 1.5588194540667868,
      "grad_norm": 13.213603973388672,
      "learning_rate": 3.376971040598587e-05,
      "loss": 2.3398,
      "step": 11250
    },
    {
      "epoch": 1.5602050713592905,
      "grad_norm": 12.78331470489502,
      "learning_rate": 3.376416793681585e-05,
      "loss": 2.6226,
      "step": 11260
    },
    {
      "epoch": 1.5615906886517945,
      "grad_norm": 14.621721267700195,
      "learning_rate": 3.375862546764584e-05,
      "loss": 3.4152,
      "step": 11270
    },
    {
      "epoch": 1.5629763059442983,
      "grad_norm": 9.904077529907227,
      "learning_rate": 3.375308299847582e-05,
      "loss": 3.0173,
      "step": 11280
    },
    {
      "epoch": 1.564361923236802,
      "grad_norm": 15.103865623474121,
      "learning_rate": 3.374754052930581e-05,
      "loss": 2.554,
      "step": 11290
    },
    {
      "epoch": 1.5657475405293058,
      "grad_norm": 9.524561882019043,
      "learning_rate": 3.374199806013579e-05,
      "loss": 2.7417,
      "step": 11300
    },
    {
      "epoch": 1.5671331578218095,
      "grad_norm": 8.426526069641113,
      "learning_rate": 3.373645559096578e-05,
      "loss": 2.8914,
      "step": 11310
    },
    {
      "epoch": 1.5685187751143135,
      "grad_norm": 12.424688339233398,
      "learning_rate": 3.3730913121795763e-05,
      "loss": 2.6294,
      "step": 11320
    },
    {
      "epoch": 1.5699043924068172,
      "grad_norm": 11.615653038024902,
      "learning_rate": 3.3725370652625745e-05,
      "loss": 2.6832,
      "step": 11330
    },
    {
      "epoch": 1.5712900096993212,
      "grad_norm": 16.011537551879883,
      "learning_rate": 3.3719828183455734e-05,
      "loss": 3.0023,
      "step": 11340
    },
    {
      "epoch": 1.572675626991825,
      "grad_norm": 14.205948829650879,
      "learning_rate": 3.3714285714285716e-05,
      "loss": 2.8003,
      "step": 11350
    },
    {
      "epoch": 1.5740612442843287,
      "grad_norm": 14.088571548461914,
      "learning_rate": 3.37087432451157e-05,
      "loss": 2.7229,
      "step": 11360
    },
    {
      "epoch": 1.5754468615768324,
      "grad_norm": 15.782712936401367,
      "learning_rate": 3.3703200775945686e-05,
      "loss": 2.8444,
      "step": 11370
    },
    {
      "epoch": 1.5768324788693362,
      "grad_norm": 8.822161674499512,
      "learning_rate": 3.369765830677567e-05,
      "loss": 2.9622,
      "step": 11380
    },
    {
      "epoch": 1.57821809616184,
      "grad_norm": 9.936013221740723,
      "learning_rate": 3.369211583760566e-05,
      "loss": 2.9944,
      "step": 11390
    },
    {
      "epoch": 1.579603713454344,
      "grad_norm": 12.3937406539917,
      "learning_rate": 3.3686573368435645e-05,
      "loss": 2.9801,
      "step": 11400
    },
    {
      "epoch": 1.5809893307468477,
      "grad_norm": 13.897103309631348,
      "learning_rate": 3.368103089926563e-05,
      "loss": 2.7979,
      "step": 11410
    },
    {
      "epoch": 1.5823749480393516,
      "grad_norm": 8.806368827819824,
      "learning_rate": 3.367548843009561e-05,
      "loss": 2.5279,
      "step": 11420
    },
    {
      "epoch": 1.5837605653318554,
      "grad_norm": 12.23784351348877,
      "learning_rate": 3.36699459609256e-05,
      "loss": 2.2216,
      "step": 11430
    },
    {
      "epoch": 1.5851461826243591,
      "grad_norm": 6.454760551452637,
      "learning_rate": 3.366440349175558e-05,
      "loss": 2.4871,
      "step": 11440
    },
    {
      "epoch": 1.5865317999168629,
      "grad_norm": 8.385232925415039,
      "learning_rate": 3.365886102258556e-05,
      "loss": 2.7351,
      "step": 11450
    },
    {
      "epoch": 1.5879174172093666,
      "grad_norm": 6.415119171142578,
      "learning_rate": 3.365331855341555e-05,
      "loss": 2.7033,
      "step": 11460
    },
    {
      "epoch": 1.5893030345018706,
      "grad_norm": 14.06595230102539,
      "learning_rate": 3.364777608424553e-05,
      "loss": 3.0396,
      "step": 11470
    },
    {
      "epoch": 1.5906886517943744,
      "grad_norm": 13.721826553344727,
      "learning_rate": 3.3642233615075514e-05,
      "loss": 2.8783,
      "step": 11480
    },
    {
      "epoch": 1.5920742690868783,
      "grad_norm": 13.289291381835938,
      "learning_rate": 3.36366911459055e-05,
      "loss": 3.2543,
      "step": 11490
    },
    {
      "epoch": 1.593459886379382,
      "grad_norm": 7.284567832946777,
      "learning_rate": 3.363114867673549e-05,
      "loss": 2.7653,
      "step": 11500
    },
    {
      "epoch": 1.5948455036718858,
      "grad_norm": 11.800519943237305,
      "learning_rate": 3.362560620756547e-05,
      "loss": 2.5199,
      "step": 11510
    },
    {
      "epoch": 1.5962311209643896,
      "grad_norm": 8.804855346679688,
      "learning_rate": 3.362006373839546e-05,
      "loss": 3.0776,
      "step": 11520
    },
    {
      "epoch": 1.5976167382568933,
      "grad_norm": 11.302433967590332,
      "learning_rate": 3.3614521269225443e-05,
      "loss": 2.5605,
      "step": 11530
    },
    {
      "epoch": 1.5990023555493973,
      "grad_norm": 9.78136920928955,
      "learning_rate": 3.360897880005543e-05,
      "loss": 3.0459,
      "step": 11540
    },
    {
      "epoch": 1.600387972841901,
      "grad_norm": 9.125375747680664,
      "learning_rate": 3.3603436330885414e-05,
      "loss": 2.6235,
      "step": 11550
    },
    {
      "epoch": 1.601773590134405,
      "grad_norm": 7.969781398773193,
      "learning_rate": 3.3597893861715396e-05,
      "loss": 2.7254,
      "step": 11560
    },
    {
      "epoch": 1.6031592074269088,
      "grad_norm": 7.340554714202881,
      "learning_rate": 3.3592351392545384e-05,
      "loss": 3.071,
      "step": 11570
    },
    {
      "epoch": 1.6045448247194125,
      "grad_norm": 12.573564529418945,
      "learning_rate": 3.3586808923375366e-05,
      "loss": 3.158,
      "step": 11580
    },
    {
      "epoch": 1.6059304420119163,
      "grad_norm": 7.832365989685059,
      "learning_rate": 3.358126645420535e-05,
      "loss": 2.898,
      "step": 11590
    },
    {
      "epoch": 1.60731605930442,
      "grad_norm": 11.713406562805176,
      "learning_rate": 3.357572398503534e-05,
      "loss": 2.1792,
      "step": 11600
    },
    {
      "epoch": 1.608701676596924,
      "grad_norm": 12.87327766418457,
      "learning_rate": 3.357018151586532e-05,
      "loss": 2.5826,
      "step": 11610
    },
    {
      "epoch": 1.6100872938894277,
      "grad_norm": 8.492715835571289,
      "learning_rate": 3.356463904669531e-05,
      "loss": 2.6147,
      "step": 11620
    },
    {
      "epoch": 1.6114729111819317,
      "grad_norm": 10.536844253540039,
      "learning_rate": 3.355909657752529e-05,
      "loss": 2.6653,
      "step": 11630
    },
    {
      "epoch": 1.6128585284744354,
      "grad_norm": 12.513327598571777,
      "learning_rate": 3.355355410835528e-05,
      "loss": 2.6746,
      "step": 11640
    },
    {
      "epoch": 1.6142441457669392,
      "grad_norm": 10.28134536743164,
      "learning_rate": 3.354801163918526e-05,
      "loss": 3.3537,
      "step": 11650
    },
    {
      "epoch": 1.615629763059443,
      "grad_norm": 10.5737943649292,
      "learning_rate": 3.354246917001525e-05,
      "loss": 2.828,
      "step": 11660
    },
    {
      "epoch": 1.6170153803519467,
      "grad_norm": 11.080171585083008,
      "learning_rate": 3.353692670084523e-05,
      "loss": 3.0908,
      "step": 11670
    },
    {
      "epoch": 1.6184009976444504,
      "grad_norm": 6.987707138061523,
      "learning_rate": 3.353138423167521e-05,
      "loss": 2.6339,
      "step": 11680
    },
    {
      "epoch": 1.6197866149369544,
      "grad_norm": 6.713537216186523,
      "learning_rate": 3.35258417625052e-05,
      "loss": 3.3229,
      "step": 11690
    },
    {
      "epoch": 1.6211722322294584,
      "grad_norm": 8.809020042419434,
      "learning_rate": 3.352029929333518e-05,
      "loss": 2.9608,
      "step": 11700
    },
    {
      "epoch": 1.6225578495219621,
      "grad_norm": 8.050456047058105,
      "learning_rate": 3.3514756824165164e-05,
      "loss": 2.7981,
      "step": 11710
    },
    {
      "epoch": 1.6239434668144659,
      "grad_norm": 12.067269325256348,
      "learning_rate": 3.350921435499515e-05,
      "loss": 2.9287,
      "step": 11720
    },
    {
      "epoch": 1.6253290841069696,
      "grad_norm": 13.230852127075195,
      "learning_rate": 3.3503671885825135e-05,
      "loss": 2.4688,
      "step": 11730
    },
    {
      "epoch": 1.6267147013994734,
      "grad_norm": 9.310401916503906,
      "learning_rate": 3.3498129416655124e-05,
      "loss": 3.0798,
      "step": 11740
    },
    {
      "epoch": 1.6281003186919771,
      "grad_norm": 6.827785015106201,
      "learning_rate": 3.3492586947485105e-05,
      "loss": 2.2717,
      "step": 11750
    },
    {
      "epoch": 1.629485935984481,
      "grad_norm": 7.887416362762451,
      "learning_rate": 3.3487044478315094e-05,
      "loss": 2.3217,
      "step": 11760
    },
    {
      "epoch": 1.6308715532769849,
      "grad_norm": 16.680736541748047,
      "learning_rate": 3.3481502009145076e-05,
      "loss": 2.9033,
      "step": 11770
    },
    {
      "epoch": 1.6322571705694888,
      "grad_norm": 9.050968170166016,
      "learning_rate": 3.3475959539975065e-05,
      "loss": 2.6848,
      "step": 11780
    },
    {
      "epoch": 1.6336427878619926,
      "grad_norm": 10.156821250915527,
      "learning_rate": 3.3470417070805046e-05,
      "loss": 2.6154,
      "step": 11790
    },
    {
      "epoch": 1.6350284051544963,
      "grad_norm": 7.0137810707092285,
      "learning_rate": 3.346487460163503e-05,
      "loss": 2.6136,
      "step": 11800
    },
    {
      "epoch": 1.636414022447,
      "grad_norm": 8.736451148986816,
      "learning_rate": 3.345933213246502e-05,
      "loss": 2.6775,
      "step": 11810
    },
    {
      "epoch": 1.6377996397395038,
      "grad_norm": 9.722742080688477,
      "learning_rate": 3.3453789663295e-05,
      "loss": 2.6844,
      "step": 11820
    },
    {
      "epoch": 1.6391852570320078,
      "grad_norm": 15.547310829162598,
      "learning_rate": 3.344824719412499e-05,
      "loss": 2.8116,
      "step": 11830
    },
    {
      "epoch": 1.6405708743245115,
      "grad_norm": 18.25741958618164,
      "learning_rate": 3.344270472495497e-05,
      "loss": 2.3902,
      "step": 11840
    },
    {
      "epoch": 1.6419564916170155,
      "grad_norm": 7.808305263519287,
      "learning_rate": 3.343716225578495e-05,
      "loss": 3.126,
      "step": 11850
    },
    {
      "epoch": 1.6433421089095193,
      "grad_norm": 7.241561412811279,
      "learning_rate": 3.343161978661494e-05,
      "loss": 2.062,
      "step": 11860
    },
    {
      "epoch": 1.644727726202023,
      "grad_norm": 10.536227226257324,
      "learning_rate": 3.342607731744492e-05,
      "loss": 2.7066,
      "step": 11870
    },
    {
      "epoch": 1.6461133434945268,
      "grad_norm": 9.552335739135742,
      "learning_rate": 3.342053484827491e-05,
      "loss": 3.075,
      "step": 11880
    },
    {
      "epoch": 1.6474989607870305,
      "grad_norm": 10.482284545898438,
      "learning_rate": 3.34149923791049e-05,
      "loss": 2.6901,
      "step": 11890
    },
    {
      "epoch": 1.6488845780795345,
      "grad_norm": 10.400503158569336,
      "learning_rate": 3.340944990993488e-05,
      "loss": 2.9028,
      "step": 11900
    },
    {
      "epoch": 1.6502701953720382,
      "grad_norm": 10.769071578979492,
      "learning_rate": 3.340390744076486e-05,
      "loss": 2.4875,
      "step": 11910
    },
    {
      "epoch": 1.6516558126645422,
      "grad_norm": 9.370641708374023,
      "learning_rate": 3.339836497159485e-05,
      "loss": 2.8717,
      "step": 11920
    },
    {
      "epoch": 1.653041429957046,
      "grad_norm": 10.620837211608887,
      "learning_rate": 3.339282250242483e-05,
      "loss": 2.847,
      "step": 11930
    },
    {
      "epoch": 1.6544270472495497,
      "grad_norm": 10.987754821777344,
      "learning_rate": 3.3387280033254815e-05,
      "loss": 2.5091,
      "step": 11940
    },
    {
      "epoch": 1.6558126645420534,
      "grad_norm": 8.87768840789795,
      "learning_rate": 3.3381737564084804e-05,
      "loss": 2.5879,
      "step": 11950
    },
    {
      "epoch": 1.6571982818345572,
      "grad_norm": 10.323704719543457,
      "learning_rate": 3.3376195094914785e-05,
      "loss": 2.7818,
      "step": 11960
    },
    {
      "epoch": 1.658583899127061,
      "grad_norm": 8.805593490600586,
      "learning_rate": 3.337065262574477e-05,
      "loss": 2.5098,
      "step": 11970
    },
    {
      "epoch": 1.659969516419565,
      "grad_norm": 7.120449066162109,
      "learning_rate": 3.3365110156574756e-05,
      "loss": 2.8559,
      "step": 11980
    },
    {
      "epoch": 1.6613551337120689,
      "grad_norm": 10.623530387878418,
      "learning_rate": 3.3359567687404745e-05,
      "loss": 3.4384,
      "step": 11990
    },
    {
      "epoch": 1.6627407510045726,
      "grad_norm": 13.331549644470215,
      "learning_rate": 3.3354025218234726e-05,
      "loss": 2.7971,
      "step": 12000
    },
    {
      "epoch": 1.6641263682970764,
      "grad_norm": 8.450297355651855,
      "learning_rate": 3.3348482749064715e-05,
      "loss": 2.5335,
      "step": 12010
    },
    {
      "epoch": 1.6655119855895801,
      "grad_norm": 7.242631435394287,
      "learning_rate": 3.33429402798947e-05,
      "loss": 2.52,
      "step": 12020
    },
    {
      "epoch": 1.6668976028820839,
      "grad_norm": 8.393942832946777,
      "learning_rate": 3.333739781072468e-05,
      "loss": 2.4015,
      "step": 12030
    },
    {
      "epoch": 1.6682832201745876,
      "grad_norm": 12.550067901611328,
      "learning_rate": 3.333185534155467e-05,
      "loss": 2.62,
      "step": 12040
    },
    {
      "epoch": 1.6696688374670916,
      "grad_norm": 10.568719863891602,
      "learning_rate": 3.332631287238465e-05,
      "loss": 2.1422,
      "step": 12050
    },
    {
      "epoch": 1.6710544547595954,
      "grad_norm": 12.73823070526123,
      "learning_rate": 3.332077040321463e-05,
      "loss": 3.1043,
      "step": 12060
    },
    {
      "epoch": 1.6724400720520993,
      "grad_norm": 10.345648765563965,
      "learning_rate": 3.331522793404462e-05,
      "loss": 3.3226,
      "step": 12070
    },
    {
      "epoch": 1.673825689344603,
      "grad_norm": 8.921656608581543,
      "learning_rate": 3.33096854648746e-05,
      "loss": 2.7781,
      "step": 12080
    },
    {
      "epoch": 1.6752113066371068,
      "grad_norm": 17.44655418395996,
      "learning_rate": 3.330414299570459e-05,
      "loss": 2.8886,
      "step": 12090
    },
    {
      "epoch": 1.6765969239296106,
      "grad_norm": 11.335043907165527,
      "learning_rate": 3.329860052653457e-05,
      "loss": 3.0814,
      "step": 12100
    },
    {
      "epoch": 1.6779825412221143,
      "grad_norm": 9.87883186340332,
      "learning_rate": 3.329305805736456e-05,
      "loss": 3.3708,
      "step": 12110
    },
    {
      "epoch": 1.6793681585146183,
      "grad_norm": 7.427059650421143,
      "learning_rate": 3.328751558819454e-05,
      "loss": 2.8054,
      "step": 12120
    },
    {
      "epoch": 1.680753775807122,
      "grad_norm": 12.163814544677734,
      "learning_rate": 3.328197311902453e-05,
      "loss": 2.8021,
      "step": 12130
    },
    {
      "epoch": 1.682139393099626,
      "grad_norm": 6.526882648468018,
      "learning_rate": 3.327643064985451e-05,
      "loss": 2.8254,
      "step": 12140
    },
    {
      "epoch": 1.6835250103921298,
      "grad_norm": 12.273174285888672,
      "learning_rate": 3.32708881806845e-05,
      "loss": 2.6085,
      "step": 12150
    },
    {
      "epoch": 1.6849106276846335,
      "grad_norm": 16.94613265991211,
      "learning_rate": 3.3265345711514484e-05,
      "loss": 2.7205,
      "step": 12160
    },
    {
      "epoch": 1.6862962449771373,
      "grad_norm": 9.60678482055664,
      "learning_rate": 3.3259803242344466e-05,
      "loss": 2.8722,
      "step": 12170
    },
    {
      "epoch": 1.687681862269641,
      "grad_norm": 9.9055757522583,
      "learning_rate": 3.3254260773174454e-05,
      "loss": 2.6879,
      "step": 12180
    },
    {
      "epoch": 1.689067479562145,
      "grad_norm": 10.955833435058594,
      "learning_rate": 3.3248718304004436e-05,
      "loss": 2.7745,
      "step": 12190
    },
    {
      "epoch": 1.6904530968546487,
      "grad_norm": 11.768448829650879,
      "learning_rate": 3.324317583483442e-05,
      "loss": 2.622,
      "step": 12200
    },
    {
      "epoch": 1.6918387141471527,
      "grad_norm": 15.393449783325195,
      "learning_rate": 3.3237633365664407e-05,
      "loss": 2.2164,
      "step": 12210
    },
    {
      "epoch": 1.6932243314396564,
      "grad_norm": 7.538621425628662,
      "learning_rate": 3.323209089649439e-05,
      "loss": 2.4852,
      "step": 12220
    },
    {
      "epoch": 1.6946099487321602,
      "grad_norm": 7.876250267028809,
      "learning_rate": 3.322654842732438e-05,
      "loss": 2.9318,
      "step": 12230
    },
    {
      "epoch": 1.695995566024664,
      "grad_norm": 11.655598640441895,
      "learning_rate": 3.322100595815436e-05,
      "loss": 2.6782,
      "step": 12240
    },
    {
      "epoch": 1.6973811833171677,
      "grad_norm": 10.363037109375,
      "learning_rate": 3.321546348898435e-05,
      "loss": 2.294,
      "step": 12250
    },
    {
      "epoch": 1.6987668006096714,
      "grad_norm": 8.873441696166992,
      "learning_rate": 3.320992101981433e-05,
      "loss": 2.6548,
      "step": 12260
    },
    {
      "epoch": 1.7001524179021754,
      "grad_norm": 8.099831581115723,
      "learning_rate": 3.320437855064432e-05,
      "loss": 3.0136,
      "step": 12270
    },
    {
      "epoch": 1.7015380351946794,
      "grad_norm": 10.320172309875488,
      "learning_rate": 3.31988360814743e-05,
      "loss": 2.9755,
      "step": 12280
    },
    {
      "epoch": 1.7029236524871831,
      "grad_norm": 12.382791519165039,
      "learning_rate": 3.319329361230428e-05,
      "loss": 2.3083,
      "step": 12290
    },
    {
      "epoch": 1.7043092697796869,
      "grad_norm": 11.669082641601562,
      "learning_rate": 3.318775114313427e-05,
      "loss": 2.8089,
      "step": 12300
    },
    {
      "epoch": 1.7056948870721906,
      "grad_norm": 7.098870277404785,
      "learning_rate": 3.318220867396425e-05,
      "loss": 2.9318,
      "step": 12310
    },
    {
      "epoch": 1.7070805043646944,
      "grad_norm": 18.00049591064453,
      "learning_rate": 3.3176666204794234e-05,
      "loss": 2.8399,
      "step": 12320
    },
    {
      "epoch": 1.7084661216571981,
      "grad_norm": 8.896541595458984,
      "learning_rate": 3.317112373562422e-05,
      "loss": 2.7195,
      "step": 12330
    },
    {
      "epoch": 1.709851738949702,
      "grad_norm": 6.211198329925537,
      "learning_rate": 3.3165581266454205e-05,
      "loss": 2.4448,
      "step": 12340
    },
    {
      "epoch": 1.7112373562422059,
      "grad_norm": 11.004204750061035,
      "learning_rate": 3.316003879728419e-05,
      "loss": 2.7219,
      "step": 12350
    },
    {
      "epoch": 1.7126229735347098,
      "grad_norm": 8.041102409362793,
      "learning_rate": 3.3154496328114175e-05,
      "loss": 2.9589,
      "step": 12360
    },
    {
      "epoch": 1.7140085908272136,
      "grad_norm": 8.636136054992676,
      "learning_rate": 3.3148953858944164e-05,
      "loss": 2.6664,
      "step": 12370
    },
    {
      "epoch": 1.7153942081197173,
      "grad_norm": 12.54254150390625,
      "learning_rate": 3.3143411389774146e-05,
      "loss": 2.9886,
      "step": 12380
    },
    {
      "epoch": 1.716779825412221,
      "grad_norm": 8.56125259399414,
      "learning_rate": 3.3137868920604134e-05,
      "loss": 2.7032,
      "step": 12390
    },
    {
      "epoch": 1.7181654427047248,
      "grad_norm": 7.640599250793457,
      "learning_rate": 3.3132326451434116e-05,
      "loss": 2.7226,
      "step": 12400
    },
    {
      "epoch": 1.7195510599972288,
      "grad_norm": 10.676054000854492,
      "learning_rate": 3.3126783982264105e-05,
      "loss": 2.7964,
      "step": 12410
    },
    {
      "epoch": 1.7209366772897325,
      "grad_norm": 8.963845252990723,
      "learning_rate": 3.312124151309409e-05,
      "loss": 2.3414,
      "step": 12420
    },
    {
      "epoch": 1.7223222945822365,
      "grad_norm": 13.630471229553223,
      "learning_rate": 3.311569904392407e-05,
      "loss": 2.4294,
      "step": 12430
    },
    {
      "epoch": 1.7237079118747403,
      "grad_norm": 18.31825828552246,
      "learning_rate": 3.311015657475406e-05,
      "loss": 2.4834,
      "step": 12440
    },
    {
      "epoch": 1.725093529167244,
      "grad_norm": 12.617010116577148,
      "learning_rate": 3.310461410558404e-05,
      "loss": 2.5214,
      "step": 12450
    },
    {
      "epoch": 1.7264791464597478,
      "grad_norm": 9.232375144958496,
      "learning_rate": 3.309907163641402e-05,
      "loss": 2.719,
      "step": 12460
    },
    {
      "epoch": 1.7278647637522515,
      "grad_norm": 11.716996192932129,
      "learning_rate": 3.309352916724401e-05,
      "loss": 2.5933,
      "step": 12470
    },
    {
      "epoch": 1.7292503810447555,
      "grad_norm": 11.77146053314209,
      "learning_rate": 3.3087986698074e-05,
      "loss": 2.94,
      "step": 12480
    },
    {
      "epoch": 1.7306359983372592,
      "grad_norm": 8.378121376037598,
      "learning_rate": 3.308244422890398e-05,
      "loss": 2.215,
      "step": 12490
    },
    {
      "epoch": 1.7320216156297632,
      "grad_norm": 9.411421775817871,
      "learning_rate": 3.307690175973397e-05,
      "loss": 2.7004,
      "step": 12500
    },
    {
      "epoch": 1.733407232922267,
      "grad_norm": 8.13158893585205,
      "learning_rate": 3.307135929056395e-05,
      "loss": 2.5577,
      "step": 12510
    },
    {
      "epoch": 1.7347928502147707,
      "grad_norm": 15.399581909179688,
      "learning_rate": 3.306581682139393e-05,
      "loss": 2.3747,
      "step": 12520
    },
    {
      "epoch": 1.7361784675072744,
      "grad_norm": 10.416385650634766,
      "learning_rate": 3.306027435222392e-05,
      "loss": 2.4728,
      "step": 12530
    },
    {
      "epoch": 1.7375640847997782,
      "grad_norm": 8.604655265808105,
      "learning_rate": 3.30547318830539e-05,
      "loss": 2.3983,
      "step": 12540
    },
    {
      "epoch": 1.738949702092282,
      "grad_norm": 7.828896522521973,
      "learning_rate": 3.3049189413883885e-05,
      "loss": 2.2046,
      "step": 12550
    },
    {
      "epoch": 1.740335319384786,
      "grad_norm": 8.302142143249512,
      "learning_rate": 3.304364694471387e-05,
      "loss": 2.5591,
      "step": 12560
    },
    {
      "epoch": 1.7417209366772899,
      "grad_norm": 9.2819242477417,
      "learning_rate": 3.3038104475543855e-05,
      "loss": 2.3661,
      "step": 12570
    },
    {
      "epoch": 1.7431065539697936,
      "grad_norm": 8.207131385803223,
      "learning_rate": 3.3032562006373844e-05,
      "loss": 2.5153,
      "step": 12580
    },
    {
      "epoch": 1.7444921712622974,
      "grad_norm": 18.602434158325195,
      "learning_rate": 3.3027019537203826e-05,
      "loss": 2.8439,
      "step": 12590
    },
    {
      "epoch": 1.7458777885548011,
      "grad_norm": 10.694381713867188,
      "learning_rate": 3.3021477068033814e-05,
      "loss": 2.398,
      "step": 12600
    },
    {
      "epoch": 1.7472634058473049,
      "grad_norm": 10.193424224853516,
      "learning_rate": 3.3015934598863796e-05,
      "loss": 2.2955,
      "step": 12610
    },
    {
      "epoch": 1.7486490231398086,
      "grad_norm": 9.836966514587402,
      "learning_rate": 3.3010392129693785e-05,
      "loss": 2.6554,
      "step": 12620
    },
    {
      "epoch": 1.7500346404323126,
      "grad_norm": 11.050158500671387,
      "learning_rate": 3.300484966052377e-05,
      "loss": 2.2576,
      "step": 12630
    },
    {
      "epoch": 1.7514202577248164,
      "grad_norm": 10.328012466430664,
      "learning_rate": 3.299930719135375e-05,
      "loss": 2.6425,
      "step": 12640
    },
    {
      "epoch": 1.7528058750173203,
      "grad_norm": 11.108524322509766,
      "learning_rate": 3.299376472218374e-05,
      "loss": 2.718,
      "step": 12650
    },
    {
      "epoch": 1.754191492309824,
      "grad_norm": 8.164558410644531,
      "learning_rate": 3.298822225301372e-05,
      "loss": 2.7883,
      "step": 12660
    },
    {
      "epoch": 1.7555771096023278,
      "grad_norm": 9.701446533203125,
      "learning_rate": 3.29826797838437e-05,
      "loss": 3.0989,
      "step": 12670
    },
    {
      "epoch": 1.7569627268948316,
      "grad_norm": 6.480656147003174,
      "learning_rate": 3.297713731467369e-05,
      "loss": 2.7722,
      "step": 12680
    },
    {
      "epoch": 1.7583483441873353,
      "grad_norm": 10.382833480834961,
      "learning_rate": 3.297159484550367e-05,
      "loss": 2.8679,
      "step": 12690
    },
    {
      "epoch": 1.7597339614798393,
      "grad_norm": 12.961421012878418,
      "learning_rate": 3.296605237633366e-05,
      "loss": 2.9577,
      "step": 12700
    },
    {
      "epoch": 1.761119578772343,
      "grad_norm": 10.956274032592773,
      "learning_rate": 3.296050990716364e-05,
      "loss": 2.9166,
      "step": 12710
    },
    {
      "epoch": 1.762505196064847,
      "grad_norm": 8.711929321289062,
      "learning_rate": 3.295496743799363e-05,
      "loss": 2.4834,
      "step": 12720
    },
    {
      "epoch": 1.7638908133573508,
      "grad_norm": 9.806092262268066,
      "learning_rate": 3.294942496882361e-05,
      "loss": 2.7826,
      "step": 12730
    },
    {
      "epoch": 1.7652764306498545,
      "grad_norm": 9.318656921386719,
      "learning_rate": 3.29438824996536e-05,
      "loss": 2.0823,
      "step": 12740
    },
    {
      "epoch": 1.7666620479423583,
      "grad_norm": 5.8231916427612305,
      "learning_rate": 3.293834003048358e-05,
      "loss": 2.248,
      "step": 12750
    },
    {
      "epoch": 1.768047665234862,
      "grad_norm": 14.473774909973145,
      "learning_rate": 3.293279756131357e-05,
      "loss": 2.4328,
      "step": 12760
    },
    {
      "epoch": 1.769433282527366,
      "grad_norm": 11.025506019592285,
      "learning_rate": 3.2927255092143553e-05,
      "loss": 3.0977,
      "step": 12770
    },
    {
      "epoch": 1.7708188998198697,
      "grad_norm": 9.897382736206055,
      "learning_rate": 3.2921712622973535e-05,
      "loss": 2.9816,
      "step": 12780
    },
    {
      "epoch": 1.7722045171123737,
      "grad_norm": 12.013745307922363,
      "learning_rate": 3.2916170153803524e-05,
      "loss": 2.9105,
      "step": 12790
    },
    {
      "epoch": 1.7735901344048774,
      "grad_norm": 6.272558212280273,
      "learning_rate": 3.2910627684633506e-05,
      "loss": 2.4651,
      "step": 12800
    },
    {
      "epoch": 1.7749757516973812,
      "grad_norm": 14.096415519714355,
      "learning_rate": 3.290508521546349e-05,
      "loss": 2.8179,
      "step": 12810
    },
    {
      "epoch": 1.776361368989885,
      "grad_norm": 9.180330276489258,
      "learning_rate": 3.2899542746293476e-05,
      "loss": 2.371,
      "step": 12820
    },
    {
      "epoch": 1.7777469862823887,
      "grad_norm": 12.11335277557373,
      "learning_rate": 3.289400027712346e-05,
      "loss": 2.5055,
      "step": 12830
    },
    {
      "epoch": 1.7791326035748927,
      "grad_norm": 7.414041042327881,
      "learning_rate": 3.288845780795345e-05,
      "loss": 3.1635,
      "step": 12840
    },
    {
      "epoch": 1.7805182208673964,
      "grad_norm": 11.133369445800781,
      "learning_rate": 3.2882915338783435e-05,
      "loss": 2.7609,
      "step": 12850
    },
    {
      "epoch": 1.7819038381599004,
      "grad_norm": 7.415423393249512,
      "learning_rate": 3.287737286961342e-05,
      "loss": 2.4505,
      "step": 12860
    },
    {
      "epoch": 1.7832894554524041,
      "grad_norm": 10.109167098999023,
      "learning_rate": 3.28718304004434e-05,
      "loss": 2.5225,
      "step": 12870
    },
    {
      "epoch": 1.7846750727449079,
      "grad_norm": 6.85125732421875,
      "learning_rate": 3.286628793127339e-05,
      "loss": 1.9247,
      "step": 12880
    },
    {
      "epoch": 1.7860606900374116,
      "grad_norm": 7.633820056915283,
      "learning_rate": 3.286074546210337e-05,
      "loss": 2.5913,
      "step": 12890
    },
    {
      "epoch": 1.7874463073299154,
      "grad_norm": 8.270208358764648,
      "learning_rate": 3.285520299293335e-05,
      "loss": 2.7794,
      "step": 12900
    },
    {
      "epoch": 1.7888319246224191,
      "grad_norm": 10.063952445983887,
      "learning_rate": 3.284966052376334e-05,
      "loss": 3.0179,
      "step": 12910
    },
    {
      "epoch": 1.790217541914923,
      "grad_norm": 10.44800090789795,
      "learning_rate": 3.284411805459332e-05,
      "loss": 2.7337,
      "step": 12920
    },
    {
      "epoch": 1.7916031592074269,
      "grad_norm": 11.465353012084961,
      "learning_rate": 3.2838575585423304e-05,
      "loss": 2.7654,
      "step": 12930
    },
    {
      "epoch": 1.7929887764999308,
      "grad_norm": 6.168772220611572,
      "learning_rate": 3.283303311625329e-05,
      "loss": 2.9336,
      "step": 12940
    },
    {
      "epoch": 1.7943743937924346,
      "grad_norm": 10.019332885742188,
      "learning_rate": 3.2827490647083274e-05,
      "loss": 2.5642,
      "step": 12950
    },
    {
      "epoch": 1.7957600110849383,
      "grad_norm": 11.061628341674805,
      "learning_rate": 3.282194817791326e-05,
      "loss": 2.7187,
      "step": 12960
    },
    {
      "epoch": 1.797145628377442,
      "grad_norm": 12.026262283325195,
      "learning_rate": 3.281640570874325e-05,
      "loss": 2.8153,
      "step": 12970
    },
    {
      "epoch": 1.7985312456699458,
      "grad_norm": 9.080832481384277,
      "learning_rate": 3.2810863239573233e-05,
      "loss": 2.7507,
      "step": 12980
    },
    {
      "epoch": 1.7999168629624498,
      "grad_norm": 13.343154907226562,
      "learning_rate": 3.280532077040322e-05,
      "loss": 2.8593,
      "step": 12990
    },
    {
      "epoch": 1.8013024802549535,
      "grad_norm": 9.477029800415039,
      "learning_rate": 3.2799778301233204e-05,
      "loss": 2.8823,
      "step": 13000
    },
    {
      "epoch": 1.8026880975474575,
      "grad_norm": 8.253156661987305,
      "learning_rate": 3.2794235832063186e-05,
      "loss": 2.3196,
      "step": 13010
    },
    {
      "epoch": 1.8040737148399613,
      "grad_norm": 8.97461223602295,
      "learning_rate": 3.2788693362893174e-05,
      "loss": 2.3852,
      "step": 13020
    },
    {
      "epoch": 1.805459332132465,
      "grad_norm": 8.596467018127441,
      "learning_rate": 3.2783150893723156e-05,
      "loss": 2.3118,
      "step": 13030
    },
    {
      "epoch": 1.8068449494249688,
      "grad_norm": 15.71645450592041,
      "learning_rate": 3.277760842455314e-05,
      "loss": 2.6822,
      "step": 13040
    },
    {
      "epoch": 1.8082305667174725,
      "grad_norm": 6.306035995483398,
      "learning_rate": 3.277206595538313e-05,
      "loss": 2.4913,
      "step": 13050
    },
    {
      "epoch": 1.8096161840099765,
      "grad_norm": 11.827776908874512,
      "learning_rate": 3.276652348621311e-05,
      "loss": 2.3477,
      "step": 13060
    },
    {
      "epoch": 1.8110018013024802,
      "grad_norm": 7.310844898223877,
      "learning_rate": 3.27609810170431e-05,
      "loss": 2.6845,
      "step": 13070
    },
    {
      "epoch": 1.8123874185949842,
      "grad_norm": 11.169146537780762,
      "learning_rate": 3.275543854787308e-05,
      "loss": 2.5088,
      "step": 13080
    },
    {
      "epoch": 1.813773035887488,
      "grad_norm": 8.469392776489258,
      "learning_rate": 3.274989607870307e-05,
      "loss": 2.5484,
      "step": 13090
    },
    {
      "epoch": 1.8151586531799917,
      "grad_norm": 9.102058410644531,
      "learning_rate": 3.274435360953305e-05,
      "loss": 3.5,
      "step": 13100
    },
    {
      "epoch": 1.8165442704724954,
      "grad_norm": 8.67410945892334,
      "learning_rate": 3.273881114036304e-05,
      "loss": 2.2529,
      "step": 13110
    },
    {
      "epoch": 1.8179298877649992,
      "grad_norm": 16.88705825805664,
      "learning_rate": 3.273326867119302e-05,
      "loss": 3.1511,
      "step": 13120
    },
    {
      "epoch": 1.8193155050575032,
      "grad_norm": 10.780107498168945,
      "learning_rate": 3.2727726202023e-05,
      "loss": 2.4256,
      "step": 13130
    },
    {
      "epoch": 1.820701122350007,
      "grad_norm": 8.803180694580078,
      "learning_rate": 3.272218373285299e-05,
      "loss": 2.5075,
      "step": 13140
    },
    {
      "epoch": 1.8220867396425109,
      "grad_norm": 8.221818923950195,
      "learning_rate": 3.271664126368297e-05,
      "loss": 2.1532,
      "step": 13150
    },
    {
      "epoch": 1.8234723569350146,
      "grad_norm": 8.02804183959961,
      "learning_rate": 3.2711098794512954e-05,
      "loss": 2.5315,
      "step": 13160
    },
    {
      "epoch": 1.8248579742275184,
      "grad_norm": 10.308833122253418,
      "learning_rate": 3.270555632534294e-05,
      "loss": 2.6379,
      "step": 13170
    },
    {
      "epoch": 1.8262435915200221,
      "grad_norm": 6.010453224182129,
      "learning_rate": 3.2700013856172925e-05,
      "loss": 2.7594,
      "step": 13180
    },
    {
      "epoch": 1.8276292088125259,
      "grad_norm": 15.422931671142578,
      "learning_rate": 3.2694471387002914e-05,
      "loss": 2.8421,
      "step": 13190
    },
    {
      "epoch": 1.8290148261050296,
      "grad_norm": 11.094666481018066,
      "learning_rate": 3.2688928917832895e-05,
      "loss": 3.0222,
      "step": 13200
    },
    {
      "epoch": 1.8304004433975336,
      "grad_norm": 16.69727897644043,
      "learning_rate": 3.2683386448662884e-05,
      "loss": 2.8337,
      "step": 13210
    },
    {
      "epoch": 1.8317860606900376,
      "grad_norm": 12.984622955322266,
      "learning_rate": 3.2677843979492866e-05,
      "loss": 2.4985,
      "step": 13220
    },
    {
      "epoch": 1.8331716779825413,
      "grad_norm": 13.104071617126465,
      "learning_rate": 3.2672301510322855e-05,
      "loss": 2.6651,
      "step": 13230
    },
    {
      "epoch": 1.834557295275045,
      "grad_norm": 9.850946426391602,
      "learning_rate": 3.2666759041152836e-05,
      "loss": 2.884,
      "step": 13240
    },
    {
      "epoch": 1.8359429125675488,
      "grad_norm": 12.54727840423584,
      "learning_rate": 3.266121657198282e-05,
      "loss": 2.7098,
      "step": 13250
    },
    {
      "epoch": 1.8373285298600526,
      "grad_norm": 9.924593925476074,
      "learning_rate": 3.265567410281281e-05,
      "loss": 2.2175,
      "step": 13260
    },
    {
      "epoch": 1.8387141471525563,
      "grad_norm": 11.96859359741211,
      "learning_rate": 3.265013163364279e-05,
      "loss": 2.2566,
      "step": 13270
    },
    {
      "epoch": 1.8400997644450603,
      "grad_norm": 9.954166412353516,
      "learning_rate": 3.264458916447278e-05,
      "loss": 2.5492,
      "step": 13280
    },
    {
      "epoch": 1.841485381737564,
      "grad_norm": 8.866412162780762,
      "learning_rate": 3.263904669530276e-05,
      "loss": 2.9486,
      "step": 13290
    },
    {
      "epoch": 1.842870999030068,
      "grad_norm": 9.988935470581055,
      "learning_rate": 3.263350422613274e-05,
      "loss": 2.7419,
      "step": 13300
    },
    {
      "epoch": 1.8442566163225718,
      "grad_norm": 9.264094352722168,
      "learning_rate": 3.262796175696273e-05,
      "loss": 2.9126,
      "step": 13310
    },
    {
      "epoch": 1.8456422336150755,
      "grad_norm": 7.718641757965088,
      "learning_rate": 3.262241928779271e-05,
      "loss": 2.8627,
      "step": 13320
    },
    {
      "epoch": 1.8470278509075793,
      "grad_norm": 12.112585067749023,
      "learning_rate": 3.26168768186227e-05,
      "loss": 2.458,
      "step": 13330
    },
    {
      "epoch": 1.848413468200083,
      "grad_norm": 7.710023403167725,
      "learning_rate": 3.261133434945269e-05,
      "loss": 2.7703,
      "step": 13340
    },
    {
      "epoch": 1.849799085492587,
      "grad_norm": 7.989736080169678,
      "learning_rate": 3.260579188028267e-05,
      "loss": 2.3163,
      "step": 13350
    },
    {
      "epoch": 1.8511847027850907,
      "grad_norm": 9.313522338867188,
      "learning_rate": 3.260024941111265e-05,
      "loss": 2.697,
      "step": 13360
    },
    {
      "epoch": 1.8525703200775947,
      "grad_norm": 7.763256549835205,
      "learning_rate": 3.259470694194264e-05,
      "loss": 2.2681,
      "step": 13370
    },
    {
      "epoch": 1.8539559373700985,
      "grad_norm": 8.985881805419922,
      "learning_rate": 3.258916447277262e-05,
      "loss": 2.7745,
      "step": 13380
    },
    {
      "epoch": 1.8553415546626022,
      "grad_norm": 9.514872550964355,
      "learning_rate": 3.2583622003602605e-05,
      "loss": 2.4593,
      "step": 13390
    },
    {
      "epoch": 1.856727171955106,
      "grad_norm": 9.62941837310791,
      "learning_rate": 3.2578079534432594e-05,
      "loss": 2.7162,
      "step": 13400
    },
    {
      "epoch": 1.8581127892476097,
      "grad_norm": 10.06219482421875,
      "learning_rate": 3.2572537065262575e-05,
      "loss": 2.4657,
      "step": 13410
    },
    {
      "epoch": 1.8594984065401137,
      "grad_norm": 12.759358406066895,
      "learning_rate": 3.256699459609256e-05,
      "loss": 2.4882,
      "step": 13420
    },
    {
      "epoch": 1.8608840238326174,
      "grad_norm": 12.924290657043457,
      "learning_rate": 3.2561452126922546e-05,
      "loss": 2.5743,
      "step": 13430
    },
    {
      "epoch": 1.8622696411251214,
      "grad_norm": 15.338258743286133,
      "learning_rate": 3.2555909657752535e-05,
      "loss": 2.062,
      "step": 13440
    },
    {
      "epoch": 1.8636552584176251,
      "grad_norm": 8.749621391296387,
      "learning_rate": 3.2550367188582516e-05,
      "loss": 2.5166,
      "step": 13450
    },
    {
      "epoch": 1.8650408757101289,
      "grad_norm": 11.271146774291992,
      "learning_rate": 3.2544824719412505e-05,
      "loss": 2.3848,
      "step": 13460
    },
    {
      "epoch": 1.8664264930026326,
      "grad_norm": 7.816531658172607,
      "learning_rate": 3.253928225024249e-05,
      "loss": 2.755,
      "step": 13470
    },
    {
      "epoch": 1.8678121102951364,
      "grad_norm": 11.835660934448242,
      "learning_rate": 3.253373978107247e-05,
      "loss": 3.0299,
      "step": 13480
    },
    {
      "epoch": 1.8691977275876401,
      "grad_norm": 12.905661582946777,
      "learning_rate": 3.252819731190246e-05,
      "loss": 2.7852,
      "step": 13490
    },
    {
      "epoch": 1.870583344880144,
      "grad_norm": 12.64249324798584,
      "learning_rate": 3.252265484273244e-05,
      "loss": 2.2009,
      "step": 13500
    },
    {
      "epoch": 1.871968962172648,
      "grad_norm": 9.331552505493164,
      "learning_rate": 3.251711237356242e-05,
      "loss": 2.4427,
      "step": 13510
    },
    {
      "epoch": 1.8733545794651518,
      "grad_norm": 11.061405181884766,
      "learning_rate": 3.251156990439241e-05,
      "loss": 2.3736,
      "step": 13520
    },
    {
      "epoch": 1.8747401967576556,
      "grad_norm": 10.46109676361084,
      "learning_rate": 3.250602743522239e-05,
      "loss": 3.2726,
      "step": 13530
    },
    {
      "epoch": 1.8761258140501593,
      "grad_norm": 12.362323760986328,
      "learning_rate": 3.2500484966052374e-05,
      "loss": 2.3487,
      "step": 13540
    },
    {
      "epoch": 1.877511431342663,
      "grad_norm": 10.650396347045898,
      "learning_rate": 3.249494249688236e-05,
      "loss": 2.2673,
      "step": 13550
    },
    {
      "epoch": 1.8788970486351668,
      "grad_norm": 13.25784683227539,
      "learning_rate": 3.248940002771235e-05,
      "loss": 2.5862,
      "step": 13560
    },
    {
      "epoch": 1.8802826659276708,
      "grad_norm": 14.9487943649292,
      "learning_rate": 3.248385755854233e-05,
      "loss": 2.1509,
      "step": 13570
    },
    {
      "epoch": 1.8816682832201745,
      "grad_norm": 9.618526458740234,
      "learning_rate": 3.247831508937232e-05,
      "loss": 2.673,
      "step": 13580
    },
    {
      "epoch": 1.8830539005126785,
      "grad_norm": 8.0975980758667,
      "learning_rate": 3.24727726202023e-05,
      "loss": 3.0567,
      "step": 13590
    },
    {
      "epoch": 1.8844395178051823,
      "grad_norm": 13.686517715454102,
      "learning_rate": 3.246723015103229e-05,
      "loss": 2.6041,
      "step": 13600
    },
    {
      "epoch": 1.885825135097686,
      "grad_norm": 10.58201789855957,
      "learning_rate": 3.2461687681862274e-05,
      "loss": 2.3216,
      "step": 13610
    },
    {
      "epoch": 1.8872107523901898,
      "grad_norm": 9.604233741760254,
      "learning_rate": 3.2456145212692256e-05,
      "loss": 2.8795,
      "step": 13620
    },
    {
      "epoch": 1.8885963696826935,
      "grad_norm": 11.260682106018066,
      "learning_rate": 3.2450602743522244e-05,
      "loss": 2.6697,
      "step": 13630
    },
    {
      "epoch": 1.8899819869751975,
      "grad_norm": 11.852075576782227,
      "learning_rate": 3.2445060274352226e-05,
      "loss": 2.4731,
      "step": 13640
    },
    {
      "epoch": 1.8913676042677012,
      "grad_norm": 12.122878074645996,
      "learning_rate": 3.243951780518221e-05,
      "loss": 2.3507,
      "step": 13650
    },
    {
      "epoch": 1.8927532215602052,
      "grad_norm": 11.91281509399414,
      "learning_rate": 3.2433975336012197e-05,
      "loss": 3.1379,
      "step": 13660
    },
    {
      "epoch": 1.894138838852709,
      "grad_norm": 7.766002655029297,
      "learning_rate": 3.242843286684218e-05,
      "loss": 3.0733,
      "step": 13670
    },
    {
      "epoch": 1.8955244561452127,
      "grad_norm": 10.663771629333496,
      "learning_rate": 3.242289039767217e-05,
      "loss": 2.3781,
      "step": 13680
    },
    {
      "epoch": 1.8969100734377164,
      "grad_norm": 16.12685775756836,
      "learning_rate": 3.241734792850215e-05,
      "loss": 2.1978,
      "step": 13690
    },
    {
      "epoch": 1.8982956907302202,
      "grad_norm": 13.629496574401855,
      "learning_rate": 3.241180545933214e-05,
      "loss": 2.7185,
      "step": 13700
    },
    {
      "epoch": 1.8996813080227242,
      "grad_norm": 8.853620529174805,
      "learning_rate": 3.240626299016212e-05,
      "loss": 2.8372,
      "step": 13710
    },
    {
      "epoch": 1.901066925315228,
      "grad_norm": 15.30765151977539,
      "learning_rate": 3.240072052099211e-05,
      "loss": 3.1436,
      "step": 13720
    },
    {
      "epoch": 1.902452542607732,
      "grad_norm": 22.52938461303711,
      "learning_rate": 3.239517805182209e-05,
      "loss": 2.8345,
      "step": 13730
    },
    {
      "epoch": 1.9038381599002356,
      "grad_norm": 9.056476593017578,
      "learning_rate": 3.238963558265207e-05,
      "loss": 2.6211,
      "step": 13740
    },
    {
      "epoch": 1.9052237771927394,
      "grad_norm": 13.895670890808105,
      "learning_rate": 3.238409311348206e-05,
      "loss": 2.3948,
      "step": 13750
    },
    {
      "epoch": 1.9066093944852431,
      "grad_norm": 16.17829704284668,
      "learning_rate": 3.237855064431204e-05,
      "loss": 2.8694,
      "step": 13760
    },
    {
      "epoch": 1.9079950117777469,
      "grad_norm": 10.054945945739746,
      "learning_rate": 3.2373008175142024e-05,
      "loss": 2.7139,
      "step": 13770
    },
    {
      "epoch": 1.9093806290702506,
      "grad_norm": 8.502073287963867,
      "learning_rate": 3.236746570597201e-05,
      "loss": 2.8256,
      "step": 13780
    },
    {
      "epoch": 1.9107662463627546,
      "grad_norm": 11.494939804077148,
      "learning_rate": 3.2361923236801995e-05,
      "loss": 2.8225,
      "step": 13790
    },
    {
      "epoch": 1.9121518636552586,
      "grad_norm": 11.781054496765137,
      "learning_rate": 3.235638076763198e-05,
      "loss": 2.8419,
      "step": 13800
    },
    {
      "epoch": 1.9135374809477623,
      "grad_norm": 10.60799789428711,
      "learning_rate": 3.2350838298461965e-05,
      "loss": 2.48,
      "step": 13810
    },
    {
      "epoch": 1.914923098240266,
      "grad_norm": 10.322105407714844,
      "learning_rate": 3.2345295829291954e-05,
      "loss": 2.5706,
      "step": 13820
    },
    {
      "epoch": 1.9163087155327698,
      "grad_norm": 9.726056098937988,
      "learning_rate": 3.2339753360121936e-05,
      "loss": 2.6335,
      "step": 13830
    },
    {
      "epoch": 1.9176943328252736,
      "grad_norm": 14.355993270874023,
      "learning_rate": 3.2334210890951924e-05,
      "loss": 2.7504,
      "step": 13840
    },
    {
      "epoch": 1.9190799501177773,
      "grad_norm": 12.536765098571777,
      "learning_rate": 3.2328668421781906e-05,
      "loss": 2.5139,
      "step": 13850
    },
    {
      "epoch": 1.9204655674102813,
      "grad_norm": 18.57935333251953,
      "learning_rate": 3.2323125952611895e-05,
      "loss": 2.8934,
      "step": 13860
    },
    {
      "epoch": 1.921851184702785,
      "grad_norm": 9.217418670654297,
      "learning_rate": 3.231758348344188e-05,
      "loss": 2.6278,
      "step": 13870
    },
    {
      "epoch": 1.923236801995289,
      "grad_norm": 11.735069274902344,
      "learning_rate": 3.231204101427186e-05,
      "loss": 2.428,
      "step": 13880
    },
    {
      "epoch": 1.9246224192877928,
      "grad_norm": 10.673284530639648,
      "learning_rate": 3.230649854510185e-05,
      "loss": 2.4677,
      "step": 13890
    },
    {
      "epoch": 1.9260080365802965,
      "grad_norm": 7.076624393463135,
      "learning_rate": 3.230095607593183e-05,
      "loss": 2.674,
      "step": 13900
    },
    {
      "epoch": 1.9273936538728003,
      "grad_norm": 16.066978454589844,
      "learning_rate": 3.229541360676181e-05,
      "loss": 2.5968,
      "step": 13910
    },
    {
      "epoch": 1.928779271165304,
      "grad_norm": 12.062894821166992,
      "learning_rate": 3.22898711375918e-05,
      "loss": 2.9136,
      "step": 13920
    },
    {
      "epoch": 1.930164888457808,
      "grad_norm": 10.26628303527832,
      "learning_rate": 3.228432866842179e-05,
      "loss": 2.0386,
      "step": 13930
    },
    {
      "epoch": 1.9315505057503117,
      "grad_norm": 12.440877914428711,
      "learning_rate": 3.227878619925177e-05,
      "loss": 2.5159,
      "step": 13940
    },
    {
      "epoch": 1.9329361230428157,
      "grad_norm": 6.708882808685303,
      "learning_rate": 3.227324373008176e-05,
      "loss": 2.696,
      "step": 13950
    },
    {
      "epoch": 1.9343217403353195,
      "grad_norm": 10.292496681213379,
      "learning_rate": 3.226770126091174e-05,
      "loss": 3.0801,
      "step": 13960
    },
    {
      "epoch": 1.9357073576278232,
      "grad_norm": 4.432198524475098,
      "learning_rate": 3.226215879174172e-05,
      "loss": 2.4639,
      "step": 13970
    },
    {
      "epoch": 1.937092974920327,
      "grad_norm": 8.83458137512207,
      "learning_rate": 3.225661632257171e-05,
      "loss": 2.4301,
      "step": 13980
    },
    {
      "epoch": 1.9384785922128307,
      "grad_norm": 9.903244018554688,
      "learning_rate": 3.225107385340169e-05,
      "loss": 2.5085,
      "step": 13990
    },
    {
      "epoch": 1.9398642095053347,
      "grad_norm": 7.934494972229004,
      "learning_rate": 3.2245531384231675e-05,
      "loss": 2.511,
      "step": 14000
    },
    {
      "epoch": 1.9412498267978384,
      "grad_norm": 10.248775482177734,
      "learning_rate": 3.223998891506166e-05,
      "loss": 2.5985,
      "step": 14010
    },
    {
      "epoch": 1.9426354440903424,
      "grad_norm": 5.490228176116943,
      "learning_rate": 3.2234446445891645e-05,
      "loss": 2.2444,
      "step": 14020
    },
    {
      "epoch": 1.9440210613828461,
      "grad_norm": 9.826055526733398,
      "learning_rate": 3.2228903976721634e-05,
      "loss": 2.4381,
      "step": 14030
    },
    {
      "epoch": 1.9454066786753499,
      "grad_norm": 15.882282257080078,
      "learning_rate": 3.2223361507551616e-05,
      "loss": 2.7018,
      "step": 14040
    },
    {
      "epoch": 1.9467922959678536,
      "grad_norm": 6.272987365722656,
      "learning_rate": 3.2217819038381604e-05,
      "loss": 2.8662,
      "step": 14050
    },
    {
      "epoch": 1.9481779132603574,
      "grad_norm": 10.474952697753906,
      "learning_rate": 3.2212276569211586e-05,
      "loss": 2.7994,
      "step": 14060
    },
    {
      "epoch": 1.9495635305528611,
      "grad_norm": 9.940046310424805,
      "learning_rate": 3.2206734100041575e-05,
      "loss": 2.7134,
      "step": 14070
    },
    {
      "epoch": 1.950949147845365,
      "grad_norm": 13.963980674743652,
      "learning_rate": 3.220119163087156e-05,
      "loss": 2.6492,
      "step": 14080
    },
    {
      "epoch": 1.952334765137869,
      "grad_norm": 11.488363265991211,
      "learning_rate": 3.219564916170154e-05,
      "loss": 3.0363,
      "step": 14090
    },
    {
      "epoch": 1.9537203824303728,
      "grad_norm": 17.677772521972656,
      "learning_rate": 3.219010669253153e-05,
      "loss": 3.0652,
      "step": 14100
    },
    {
      "epoch": 1.9551059997228766,
      "grad_norm": 12.330751419067383,
      "learning_rate": 3.218456422336151e-05,
      "loss": 2.1399,
      "step": 14110
    },
    {
      "epoch": 1.9564916170153803,
      "grad_norm": 10.072078704833984,
      "learning_rate": 3.217902175419149e-05,
      "loss": 2.3349,
      "step": 14120
    },
    {
      "epoch": 1.957877234307884,
      "grad_norm": 12.372808456420898,
      "learning_rate": 3.217347928502148e-05,
      "loss": 2.6401,
      "step": 14130
    },
    {
      "epoch": 1.9592628516003878,
      "grad_norm": 13.52066421508789,
      "learning_rate": 3.216793681585146e-05,
      "loss": 2.8004,
      "step": 14140
    },
    {
      "epoch": 1.9606484688928918,
      "grad_norm": 10.705986022949219,
      "learning_rate": 3.216239434668145e-05,
      "loss": 2.4548,
      "step": 14150
    },
    {
      "epoch": 1.9620340861853955,
      "grad_norm": 10.705577850341797,
      "learning_rate": 3.215685187751143e-05,
      "loss": 2.7397,
      "step": 14160
    },
    {
      "epoch": 1.9634197034778995,
      "grad_norm": 8.632405281066895,
      "learning_rate": 3.215130940834142e-05,
      "loss": 2.4134,
      "step": 14170
    },
    {
      "epoch": 1.9648053207704033,
      "grad_norm": 8.071569442749023,
      "learning_rate": 3.21457669391714e-05,
      "loss": 2.5253,
      "step": 14180
    },
    {
      "epoch": 1.966190938062907,
      "grad_norm": 9.3974027633667,
      "learning_rate": 3.214022447000139e-05,
      "loss": 2.4138,
      "step": 14190
    },
    {
      "epoch": 1.9675765553554108,
      "grad_norm": 9.007184028625488,
      "learning_rate": 3.213468200083137e-05,
      "loss": 2.4789,
      "step": 14200
    },
    {
      "epoch": 1.9689621726479145,
      "grad_norm": 11.694626808166504,
      "learning_rate": 3.212913953166136e-05,
      "loss": 2.9303,
      "step": 14210
    },
    {
      "epoch": 1.9703477899404185,
      "grad_norm": 8.54691219329834,
      "learning_rate": 3.2123597062491343e-05,
      "loss": 2.975,
      "step": 14220
    },
    {
      "epoch": 1.9717334072329222,
      "grad_norm": 14.613280296325684,
      "learning_rate": 3.2118054593321325e-05,
      "loss": 2.5674,
      "step": 14230
    },
    {
      "epoch": 1.9731190245254262,
      "grad_norm": 10.950945854187012,
      "learning_rate": 3.2112512124151314e-05,
      "loss": 2.6319,
      "step": 14240
    },
    {
      "epoch": 1.97450464181793,
      "grad_norm": 8.905519485473633,
      "learning_rate": 3.2106969654981296e-05,
      "loss": 2.5262,
      "step": 14250
    },
    {
      "epoch": 1.9758902591104337,
      "grad_norm": 9.169200897216797,
      "learning_rate": 3.210198143272828e-05,
      "loss": 2.6977,
      "step": 14260
    },
    {
      "epoch": 1.9772758764029374,
      "grad_norm": 10.15938949584961,
      "learning_rate": 3.209643896355827e-05,
      "loss": 3.1852,
      "step": 14270
    },
    {
      "epoch": 1.9786614936954412,
      "grad_norm": 10.669086456298828,
      "learning_rate": 3.209089649438825e-05,
      "loss": 2.6399,
      "step": 14280
    },
    {
      "epoch": 1.9800471109879452,
      "grad_norm": 12.363873481750488,
      "learning_rate": 3.2085354025218235e-05,
      "loss": 3.2361,
      "step": 14290
    },
    {
      "epoch": 1.981432728280449,
      "grad_norm": 8.793366432189941,
      "learning_rate": 3.207981155604822e-05,
      "loss": 2.5561,
      "step": 14300
    },
    {
      "epoch": 1.982818345572953,
      "grad_norm": 9.248319625854492,
      "learning_rate": 3.2074269086878205e-05,
      "loss": 2.5272,
      "step": 14310
    },
    {
      "epoch": 1.9842039628654566,
      "grad_norm": 5.04386568069458,
      "learning_rate": 3.206872661770819e-05,
      "loss": 2.4485,
      "step": 14320
    },
    {
      "epoch": 1.9855895801579604,
      "grad_norm": 7.494180679321289,
      "learning_rate": 3.2063184148538176e-05,
      "loss": 2.4297,
      "step": 14330
    },
    {
      "epoch": 1.9869751974504641,
      "grad_norm": 13.133407592773438,
      "learning_rate": 3.2057641679368164e-05,
      "loss": 2.4333,
      "step": 14340
    },
    {
      "epoch": 1.9883608147429679,
      "grad_norm": 13.957475662231445,
      "learning_rate": 3.2052099210198146e-05,
      "loss": 2.7452,
      "step": 14350
    },
    {
      "epoch": 1.9897464320354719,
      "grad_norm": 15.090617179870605,
      "learning_rate": 3.2046556741028135e-05,
      "loss": 3.4521,
      "step": 14360
    },
    {
      "epoch": 1.9911320493279756,
      "grad_norm": 10.718873023986816,
      "learning_rate": 3.2041014271858117e-05,
      "loss": 2.9792,
      "step": 14370
    },
    {
      "epoch": 1.9925176666204796,
      "grad_norm": 8.554403305053711,
      "learning_rate": 3.20354718026881e-05,
      "loss": 2.6972,
      "step": 14380
    },
    {
      "epoch": 1.9939032839129833,
      "grad_norm": 7.5762763023376465,
      "learning_rate": 3.202992933351809e-05,
      "loss": 2.8788,
      "step": 14390
    },
    {
      "epoch": 1.995288901205487,
      "grad_norm": 10.934276580810547,
      "learning_rate": 3.202438686434807e-05,
      "loss": 2.9509,
      "step": 14400
    },
    {
      "epoch": 1.9966745184979908,
      "grad_norm": 17.303373336791992,
      "learning_rate": 3.201884439517805e-05,
      "loss": 2.8593,
      "step": 14410
    },
    {
      "epoch": 1.9980601357904946,
      "grad_norm": 6.709728240966797,
      "learning_rate": 3.201330192600804e-05,
      "loss": 2.7636,
      "step": 14420
    },
    {
      "epoch": 1.9994457530829983,
      "grad_norm": 12.880925178527832,
      "learning_rate": 3.200775945683802e-05,
      "loss": 2.5625,
      "step": 14430
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.4352044352044352,
      "eval_bert_f1": 0.986363410949707,
      "eval_bert_precision": 0.9892053604125977,
      "eval_bert_recall": 0.9839882850646973,
      "eval_f1": 0.023262668132108765,
      "eval_loss": 2.699765920639038,
      "eval_runtime": 279.0949,
      "eval_samples_per_second": 51.703,
      "eval_steps_per_second": 6.464,
      "eval_synonym_accuracy": 0.44504504504504505,
      "step": 14434
    },
    {
      "epoch": 2.000831370375502,
      "grad_norm": 14.7081937789917,
      "learning_rate": 3.200221698766801e-05,
      "loss": 3.3373,
      "step": 14440
    },
    {
      "epoch": 2.0022169876680063,
      "grad_norm": 11.747176170349121,
      "learning_rate": 3.199667451849799e-05,
      "loss": 2.3803,
      "step": 14450
    },
    {
      "epoch": 2.00360260496051,
      "grad_norm": 11.555294036865234,
      "learning_rate": 3.199113204932798e-05,
      "loss": 2.6035,
      "step": 14460
    },
    {
      "epoch": 2.0049882222530138,
      "grad_norm": 10.900683403015137,
      "learning_rate": 3.198558958015796e-05,
      "loss": 3.0133,
      "step": 14470
    },
    {
      "epoch": 2.0063738395455175,
      "grad_norm": 12.455163955688477,
      "learning_rate": 3.198004711098795e-05,
      "loss": 2.6278,
      "step": 14480
    },
    {
      "epoch": 2.0077594568380213,
      "grad_norm": 9.707042694091797,
      "learning_rate": 3.197450464181793e-05,
      "loss": 2.487,
      "step": 14490
    },
    {
      "epoch": 2.009145074130525,
      "grad_norm": 9.59013557434082,
      "learning_rate": 3.196896217264792e-05,
      "loss": 2.7341,
      "step": 14500
    },
    {
      "epoch": 2.0105306914230288,
      "grad_norm": 6.2417521476745605,
      "learning_rate": 3.19634197034779e-05,
      "loss": 2.1657,
      "step": 14510
    },
    {
      "epoch": 2.011916308715533,
      "grad_norm": 6.1125922203063965,
      "learning_rate": 3.1957877234307885e-05,
      "loss": 2.4206,
      "step": 14520
    },
    {
      "epoch": 2.0133019260080367,
      "grad_norm": 9.370382308959961,
      "learning_rate": 3.1952334765137874e-05,
      "loss": 2.7608,
      "step": 14530
    },
    {
      "epoch": 2.0146875433005405,
      "grad_norm": 12.0237398147583,
      "learning_rate": 3.1946792295967856e-05,
      "loss": 2.6728,
      "step": 14540
    },
    {
      "epoch": 2.016073160593044,
      "grad_norm": 7.664720058441162,
      "learning_rate": 3.194124982679784e-05,
      "loss": 2.4051,
      "step": 14550
    },
    {
      "epoch": 2.017458777885548,
      "grad_norm": 14.36408519744873,
      "learning_rate": 3.1935707357627826e-05,
      "loss": 2.3083,
      "step": 14560
    },
    {
      "epoch": 2.0188443951780517,
      "grad_norm": 10.303605079650879,
      "learning_rate": 3.193016488845781e-05,
      "loss": 2.2956,
      "step": 14570
    },
    {
      "epoch": 2.0202300124705554,
      "grad_norm": 11.938963890075684,
      "learning_rate": 3.1924622419287797e-05,
      "loss": 2.5338,
      "step": 14580
    },
    {
      "epoch": 2.0216156297630596,
      "grad_norm": 7.260564804077148,
      "learning_rate": 3.191907995011778e-05,
      "loss": 2.9687,
      "step": 14590
    },
    {
      "epoch": 2.0230012470555634,
      "grad_norm": 10.326412200927734,
      "learning_rate": 3.191353748094777e-05,
      "loss": 2.704,
      "step": 14600
    },
    {
      "epoch": 2.024386864348067,
      "grad_norm": 12.123194694519043,
      "learning_rate": 3.190799501177775e-05,
      "loss": 2.172,
      "step": 14610
    },
    {
      "epoch": 2.025772481640571,
      "grad_norm": 19.81793212890625,
      "learning_rate": 3.190245254260774e-05,
      "loss": 2.1397,
      "step": 14620
    },
    {
      "epoch": 2.0271580989330746,
      "grad_norm": 12.274709701538086,
      "learning_rate": 3.189691007343772e-05,
      "loss": 2.4742,
      "step": 14630
    },
    {
      "epoch": 2.0285437162255784,
      "grad_norm": 12.499055862426758,
      "learning_rate": 3.18913676042677e-05,
      "loss": 2.6579,
      "step": 14640
    },
    {
      "epoch": 2.029929333518082,
      "grad_norm": 8.20168685913086,
      "learning_rate": 3.188582513509769e-05,
      "loss": 2.136,
      "step": 14650
    },
    {
      "epoch": 2.0313149508105863,
      "grad_norm": 11.705074310302734,
      "learning_rate": 3.188028266592767e-05,
      "loss": 2.3319,
      "step": 14660
    },
    {
      "epoch": 2.03270056810309,
      "grad_norm": 17.355703353881836,
      "learning_rate": 3.1874740196757654e-05,
      "loss": 2.0832,
      "step": 14670
    },
    {
      "epoch": 2.034086185395594,
      "grad_norm": 11.919839859008789,
      "learning_rate": 3.186919772758764e-05,
      "loss": 2.4507,
      "step": 14680
    },
    {
      "epoch": 2.0354718026880976,
      "grad_norm": 11.225255012512207,
      "learning_rate": 3.1863655258417624e-05,
      "loss": 2.3501,
      "step": 14690
    },
    {
      "epoch": 2.0368574199806013,
      "grad_norm": 11.229471206665039,
      "learning_rate": 3.185811278924761e-05,
      "loss": 2.692,
      "step": 14700
    },
    {
      "epoch": 2.038243037273105,
      "grad_norm": 9.348270416259766,
      "learning_rate": 3.18525703200776e-05,
      "loss": 2.8613,
      "step": 14710
    },
    {
      "epoch": 2.039628654565609,
      "grad_norm": 13.105420112609863,
      "learning_rate": 3.184702785090758e-05,
      "loss": 2.5714,
      "step": 14720
    },
    {
      "epoch": 2.0410142718581126,
      "grad_norm": 9.24390697479248,
      "learning_rate": 3.1841485381737565e-05,
      "loss": 2.0366,
      "step": 14730
    },
    {
      "epoch": 2.0423998891506168,
      "grad_norm": 10.696292877197266,
      "learning_rate": 3.1835942912567554e-05,
      "loss": 2.1687,
      "step": 14740
    },
    {
      "epoch": 2.0437855064431205,
      "grad_norm": 15.976495742797852,
      "learning_rate": 3.1830400443397536e-05,
      "loss": 2.2384,
      "step": 14750
    },
    {
      "epoch": 2.0451711237356243,
      "grad_norm": 7.33494234085083,
      "learning_rate": 3.182485797422752e-05,
      "loss": 2.3143,
      "step": 14760
    },
    {
      "epoch": 2.046556741028128,
      "grad_norm": 7.969905376434326,
      "learning_rate": 3.1819315505057506e-05,
      "loss": 2.7763,
      "step": 14770
    },
    {
      "epoch": 2.0479423583206318,
      "grad_norm": 11.447005271911621,
      "learning_rate": 3.181377303588749e-05,
      "loss": 2.8695,
      "step": 14780
    },
    {
      "epoch": 2.0493279756131355,
      "grad_norm": 12.33297061920166,
      "learning_rate": 3.180823056671748e-05,
      "loss": 2.6929,
      "step": 14790
    },
    {
      "epoch": 2.0507135929056393,
      "grad_norm": 7.981356620788574,
      "learning_rate": 3.180268809754746e-05,
      "loss": 2.2622,
      "step": 14800
    },
    {
      "epoch": 2.0520992101981435,
      "grad_norm": 13.586545944213867,
      "learning_rate": 3.179714562837745e-05,
      "loss": 2.7127,
      "step": 14810
    },
    {
      "epoch": 2.053484827490647,
      "grad_norm": 8.66601276397705,
      "learning_rate": 3.179160315920743e-05,
      "loss": 2.1965,
      "step": 14820
    },
    {
      "epoch": 2.054870444783151,
      "grad_norm": 13.956151008605957,
      "learning_rate": 3.178606069003742e-05,
      "loss": 2.2791,
      "step": 14830
    },
    {
      "epoch": 2.0562560620756547,
      "grad_norm": 10.358901023864746,
      "learning_rate": 3.17805182208674e-05,
      "loss": 2.1566,
      "step": 14840
    },
    {
      "epoch": 2.0576416793681584,
      "grad_norm": 9.279581069946289,
      "learning_rate": 3.177497575169739e-05,
      "loss": 2.5723,
      "step": 14850
    },
    {
      "epoch": 2.059027296660662,
      "grad_norm": 11.6570405960083,
      "learning_rate": 3.176943328252737e-05,
      "loss": 3.0599,
      "step": 14860
    },
    {
      "epoch": 2.060412913953166,
      "grad_norm": 17.569316864013672,
      "learning_rate": 3.176389081335735e-05,
      "loss": 2.4516,
      "step": 14870
    },
    {
      "epoch": 2.06179853124567,
      "grad_norm": 15.458852767944336,
      "learning_rate": 3.175834834418734e-05,
      "loss": 2.6765,
      "step": 14880
    },
    {
      "epoch": 2.063184148538174,
      "grad_norm": 8.570833206176758,
      "learning_rate": 3.175280587501732e-05,
      "loss": 2.844,
      "step": 14890
    },
    {
      "epoch": 2.0645697658306776,
      "grad_norm": 9.181151390075684,
      "learning_rate": 3.1747263405847304e-05,
      "loss": 2.2734,
      "step": 14900
    },
    {
      "epoch": 2.0659553831231814,
      "grad_norm": 10.384247779846191,
      "learning_rate": 3.174172093667729e-05,
      "loss": 2.2339,
      "step": 14910
    },
    {
      "epoch": 2.067341000415685,
      "grad_norm": 8.537264823913574,
      "learning_rate": 3.1736178467507275e-05,
      "loss": 2.391,
      "step": 14920
    },
    {
      "epoch": 2.068726617708189,
      "grad_norm": 6.548811912536621,
      "learning_rate": 3.173063599833726e-05,
      "loss": 2.1058,
      "step": 14930
    },
    {
      "epoch": 2.0701122350006926,
      "grad_norm": 6.152245044708252,
      "learning_rate": 3.1725093529167245e-05,
      "loss": 2.7438,
      "step": 14940
    },
    {
      "epoch": 2.071497852293197,
      "grad_norm": 11.644094467163086,
      "learning_rate": 3.1719551059997234e-05,
      "loss": 2.2228,
      "step": 14950
    },
    {
      "epoch": 2.0728834695857006,
      "grad_norm": 11.423564910888672,
      "learning_rate": 3.1714008590827216e-05,
      "loss": 2.4492,
      "step": 14960
    },
    {
      "epoch": 2.0742690868782043,
      "grad_norm": 8.568133354187012,
      "learning_rate": 3.1708466121657204e-05,
      "loss": 2.8962,
      "step": 14970
    },
    {
      "epoch": 2.075654704170708,
      "grad_norm": 9.179442405700684,
      "learning_rate": 3.1702923652487186e-05,
      "loss": 2.6979,
      "step": 14980
    },
    {
      "epoch": 2.077040321463212,
      "grad_norm": 7.52274751663208,
      "learning_rate": 3.169738118331717e-05,
      "loss": 2.3749,
      "step": 14990
    },
    {
      "epoch": 2.0784259387557156,
      "grad_norm": 8.297975540161133,
      "learning_rate": 3.169183871414716e-05,
      "loss": 2.565,
      "step": 15000
    },
    {
      "epoch": 2.0798115560482193,
      "grad_norm": 10.168027877807617,
      "learning_rate": 3.168629624497714e-05,
      "loss": 2.5841,
      "step": 15010
    },
    {
      "epoch": 2.0811971733407235,
      "grad_norm": 6.883472919464111,
      "learning_rate": 3.168075377580712e-05,
      "loss": 2.3479,
      "step": 15020
    },
    {
      "epoch": 2.0825827906332273,
      "grad_norm": 7.308030128479004,
      "learning_rate": 3.167521130663711e-05,
      "loss": 2.691,
      "step": 15030
    },
    {
      "epoch": 2.083968407925731,
      "grad_norm": 8.56451416015625,
      "learning_rate": 3.166966883746709e-05,
      "loss": 2.7797,
      "step": 15040
    },
    {
      "epoch": 2.0853540252182348,
      "grad_norm": 14.521835327148438,
      "learning_rate": 3.166412636829708e-05,
      "loss": 2.4611,
      "step": 15050
    },
    {
      "epoch": 2.0867396425107385,
      "grad_norm": 16.441030502319336,
      "learning_rate": 3.165858389912706e-05,
      "loss": 2.6296,
      "step": 15060
    },
    {
      "epoch": 2.0881252598032423,
      "grad_norm": 12.082393646240234,
      "learning_rate": 3.165304142995705e-05,
      "loss": 2.2752,
      "step": 15070
    },
    {
      "epoch": 2.089510877095746,
      "grad_norm": 9.124143600463867,
      "learning_rate": 3.164749896078703e-05,
      "loss": 2.6288,
      "step": 15080
    },
    {
      "epoch": 2.0908964943882498,
      "grad_norm": 7.681831359863281,
      "learning_rate": 3.164195649161702e-05,
      "loss": 2.2334,
      "step": 15090
    },
    {
      "epoch": 2.092282111680754,
      "grad_norm": 9.328718185424805,
      "learning_rate": 3.1636414022447e-05,
      "loss": 2.8687,
      "step": 15100
    },
    {
      "epoch": 2.0936677289732577,
      "grad_norm": 12.209601402282715,
      "learning_rate": 3.163087155327699e-05,
      "loss": 2.768,
      "step": 15110
    },
    {
      "epoch": 2.0950533462657615,
      "grad_norm": 17.08673667907715,
      "learning_rate": 3.162532908410697e-05,
      "loss": 2.6864,
      "step": 15120
    },
    {
      "epoch": 2.096438963558265,
      "grad_norm": 5.851841449737549,
      "learning_rate": 3.1619786614936955e-05,
      "loss": 2.7249,
      "step": 15130
    },
    {
      "epoch": 2.097824580850769,
      "grad_norm": 6.304830551147461,
      "learning_rate": 3.1614244145766943e-05,
      "loss": 2.734,
      "step": 15140
    },
    {
      "epoch": 2.0992101981432727,
      "grad_norm": 6.867762088775635,
      "learning_rate": 3.1608701676596925e-05,
      "loss": 2.2251,
      "step": 15150
    },
    {
      "epoch": 2.1005958154357764,
      "grad_norm": 10.26071834564209,
      "learning_rate": 3.160315920742691e-05,
      "loss": 2.5167,
      "step": 15160
    },
    {
      "epoch": 2.1019814327282806,
      "grad_norm": 20.16290855407715,
      "learning_rate": 3.1597616738256896e-05,
      "loss": 2.4415,
      "step": 15170
    },
    {
      "epoch": 2.1033670500207844,
      "grad_norm": 9.810373306274414,
      "learning_rate": 3.159207426908688e-05,
      "loss": 2.8081,
      "step": 15180
    },
    {
      "epoch": 2.104752667313288,
      "grad_norm": 14.351442337036133,
      "learning_rate": 3.1586531799916866e-05,
      "loss": 3.1683,
      "step": 15190
    },
    {
      "epoch": 2.106138284605792,
      "grad_norm": 11.581548690795898,
      "learning_rate": 3.1580989330746855e-05,
      "loss": 2.4931,
      "step": 15200
    },
    {
      "epoch": 2.1075239018982956,
      "grad_norm": 10.741095542907715,
      "learning_rate": 3.157544686157684e-05,
      "loss": 3.1505,
      "step": 15210
    },
    {
      "epoch": 2.1089095191907994,
      "grad_norm": 8.886513710021973,
      "learning_rate": 3.156990439240682e-05,
      "loss": 2.3447,
      "step": 15220
    },
    {
      "epoch": 2.110295136483303,
      "grad_norm": 12.333236694335938,
      "learning_rate": 3.156436192323681e-05,
      "loss": 3.0815,
      "step": 15230
    },
    {
      "epoch": 2.1116807537758073,
      "grad_norm": 8.383318901062012,
      "learning_rate": 3.155881945406679e-05,
      "loss": 2.2426,
      "step": 15240
    },
    {
      "epoch": 2.113066371068311,
      "grad_norm": 11.741024017333984,
      "learning_rate": 3.155327698489677e-05,
      "loss": 3.0823,
      "step": 15250
    },
    {
      "epoch": 2.114451988360815,
      "grad_norm": 11.3124361038208,
      "learning_rate": 3.154773451572676e-05,
      "loss": 2.5271,
      "step": 15260
    },
    {
      "epoch": 2.1158376056533186,
      "grad_norm": 7.388031959533691,
      "learning_rate": 3.154219204655674e-05,
      "loss": 2.831,
      "step": 15270
    },
    {
      "epoch": 2.1172232229458223,
      "grad_norm": 16.69904327392578,
      "learning_rate": 3.153664957738672e-05,
      "loss": 2.564,
      "step": 15280
    },
    {
      "epoch": 2.118608840238326,
      "grad_norm": 14.829103469848633,
      "learning_rate": 3.153110710821671e-05,
      "loss": 2.475,
      "step": 15290
    },
    {
      "epoch": 2.11999445753083,
      "grad_norm": 10.715605735778809,
      "learning_rate": 3.15255646390467e-05,
      "loss": 2.8081,
      "step": 15300
    },
    {
      "epoch": 2.1213800748233336,
      "grad_norm": 14.806035995483398,
      "learning_rate": 3.152002216987668e-05,
      "loss": 3.0603,
      "step": 15310
    },
    {
      "epoch": 2.1227656921158378,
      "grad_norm": 9.48941707611084,
      "learning_rate": 3.151447970070667e-05,
      "loss": 2.3865,
      "step": 15320
    },
    {
      "epoch": 2.1241513094083415,
      "grad_norm": 14.990924835205078,
      "learning_rate": 3.150893723153665e-05,
      "loss": 2.9429,
      "step": 15330
    },
    {
      "epoch": 2.1255369267008453,
      "grad_norm": 7.6205878257751465,
      "learning_rate": 3.1503394762366635e-05,
      "loss": 2.3445,
      "step": 15340
    },
    {
      "epoch": 2.126922543993349,
      "grad_norm": 16.65733528137207,
      "learning_rate": 3.1497852293196624e-05,
      "loss": 2.6051,
      "step": 15350
    },
    {
      "epoch": 2.1283081612858528,
      "grad_norm": 10.478102684020996,
      "learning_rate": 3.1492309824026605e-05,
      "loss": 2.4386,
      "step": 15360
    },
    {
      "epoch": 2.1296937785783565,
      "grad_norm": 12.625683784484863,
      "learning_rate": 3.1486767354856594e-05,
      "loss": 3.1485,
      "step": 15370
    },
    {
      "epoch": 2.1310793958708603,
      "grad_norm": 11.49833869934082,
      "learning_rate": 3.1481224885686576e-05,
      "loss": 2.1987,
      "step": 15380
    },
    {
      "epoch": 2.1324650131633645,
      "grad_norm": 14.902149200439453,
      "learning_rate": 3.147568241651656e-05,
      "loss": 2.6461,
      "step": 15390
    },
    {
      "epoch": 2.133850630455868,
      "grad_norm": 12.137024879455566,
      "learning_rate": 3.1470139947346546e-05,
      "loss": 2.4271,
      "step": 15400
    },
    {
      "epoch": 2.135236247748372,
      "grad_norm": 5.37546443939209,
      "learning_rate": 3.146459747817653e-05,
      "loss": 2.0314,
      "step": 15410
    },
    {
      "epoch": 2.1366218650408757,
      "grad_norm": 11.090145111083984,
      "learning_rate": 3.145905500900652e-05,
      "loss": 2.0987,
      "step": 15420
    },
    {
      "epoch": 2.1380074823333794,
      "grad_norm": 8.98601245880127,
      "learning_rate": 3.14535125398365e-05,
      "loss": 2.3454,
      "step": 15430
    },
    {
      "epoch": 2.139393099625883,
      "grad_norm": 8.918978691101074,
      "learning_rate": 3.144797007066649e-05,
      "loss": 2.1576,
      "step": 15440
    },
    {
      "epoch": 2.140778716918387,
      "grad_norm": 12.313754081726074,
      "learning_rate": 3.144242760149647e-05,
      "loss": 2.314,
      "step": 15450
    },
    {
      "epoch": 2.142164334210891,
      "grad_norm": 9.34688949584961,
      "learning_rate": 3.143688513232646e-05,
      "loss": 2.6914,
      "step": 15460
    },
    {
      "epoch": 2.143549951503395,
      "grad_norm": 9.860145568847656,
      "learning_rate": 3.143134266315644e-05,
      "loss": 2.1795,
      "step": 15470
    },
    {
      "epoch": 2.1449355687958986,
      "grad_norm": 7.9230475425720215,
      "learning_rate": 3.142580019398642e-05,
      "loss": 2.3249,
      "step": 15480
    },
    {
      "epoch": 2.1463211860884024,
      "grad_norm": 12.660534858703613,
      "learning_rate": 3.142025772481641e-05,
      "loss": 2.5561,
      "step": 15490
    },
    {
      "epoch": 2.147706803380906,
      "grad_norm": 9.039558410644531,
      "learning_rate": 3.141471525564639e-05,
      "loss": 2.8982,
      "step": 15500
    },
    {
      "epoch": 2.14909242067341,
      "grad_norm": 9.65811824798584,
      "learning_rate": 3.1409172786476374e-05,
      "loss": 1.8443,
      "step": 15510
    },
    {
      "epoch": 2.1504780379659136,
      "grad_norm": 8.902141571044922,
      "learning_rate": 3.140363031730636e-05,
      "loss": 2.2691,
      "step": 15520
    },
    {
      "epoch": 2.151863655258418,
      "grad_norm": 9.834905624389648,
      "learning_rate": 3.1398087848136344e-05,
      "loss": 2.4923,
      "step": 15530
    },
    {
      "epoch": 2.1532492725509216,
      "grad_norm": 11.484163284301758,
      "learning_rate": 3.139254537896633e-05,
      "loss": 2.4543,
      "step": 15540
    },
    {
      "epoch": 2.1546348898434253,
      "grad_norm": 8.552738189697266,
      "learning_rate": 3.1387002909796315e-05,
      "loss": 2.414,
      "step": 15550
    },
    {
      "epoch": 2.156020507135929,
      "grad_norm": 7.664861679077148,
      "learning_rate": 3.1381460440626304e-05,
      "loss": 2.2224,
      "step": 15560
    },
    {
      "epoch": 2.157406124428433,
      "grad_norm": 7.934942722320557,
      "learning_rate": 3.1375917971456285e-05,
      "loss": 2.1438,
      "step": 15570
    },
    {
      "epoch": 2.1587917417209366,
      "grad_norm": 9.075918197631836,
      "learning_rate": 3.1370375502286274e-05,
      "loss": 2.4792,
      "step": 15580
    },
    {
      "epoch": 2.1601773590134403,
      "grad_norm": 7.054211616516113,
      "learning_rate": 3.1364833033116256e-05,
      "loss": 2.4937,
      "step": 15590
    },
    {
      "epoch": 2.1615629763059445,
      "grad_norm": 10.773124694824219,
      "learning_rate": 3.135929056394624e-05,
      "loss": 2.5343,
      "step": 15600
    },
    {
      "epoch": 2.1629485935984483,
      "grad_norm": 10.996044158935547,
      "learning_rate": 3.1353748094776226e-05,
      "loss": 2.4482,
      "step": 15610
    },
    {
      "epoch": 2.164334210890952,
      "grad_norm": 9.869823455810547,
      "learning_rate": 3.134820562560621e-05,
      "loss": 2.1562,
      "step": 15620
    },
    {
      "epoch": 2.1657198281834558,
      "grad_norm": 8.354503631591797,
      "learning_rate": 3.134266315643619e-05,
      "loss": 2.3137,
      "step": 15630
    },
    {
      "epoch": 2.1671054454759595,
      "grad_norm": 7.8808417320251465,
      "learning_rate": 3.133712068726618e-05,
      "loss": 2.5571,
      "step": 15640
    },
    {
      "epoch": 2.1684910627684633,
      "grad_norm": 8.497885704040527,
      "learning_rate": 3.133157821809616e-05,
      "loss": 2.4708,
      "step": 15650
    },
    {
      "epoch": 2.169876680060967,
      "grad_norm": 9.449082374572754,
      "learning_rate": 3.132603574892615e-05,
      "loss": 2.0972,
      "step": 15660
    },
    {
      "epoch": 2.171262297353471,
      "grad_norm": 7.132951736450195,
      "learning_rate": 3.132049327975613e-05,
      "loss": 2.7195,
      "step": 15670
    },
    {
      "epoch": 2.172647914645975,
      "grad_norm": 8.917630195617676,
      "learning_rate": 3.131495081058612e-05,
      "loss": 2.6578,
      "step": 15680
    },
    {
      "epoch": 2.1740335319384787,
      "grad_norm": 9.38337516784668,
      "learning_rate": 3.130940834141611e-05,
      "loss": 2.3036,
      "step": 15690
    },
    {
      "epoch": 2.1754191492309825,
      "grad_norm": 7.809995174407959,
      "learning_rate": 3.130386587224609e-05,
      "loss": 2.6206,
      "step": 15700
    },
    {
      "epoch": 2.176804766523486,
      "grad_norm": 11.437470436096191,
      "learning_rate": 3.129832340307607e-05,
      "loss": 3.104,
      "step": 15710
    },
    {
      "epoch": 2.17819038381599,
      "grad_norm": 9.096810340881348,
      "learning_rate": 3.129278093390606e-05,
      "loss": 2.6322,
      "step": 15720
    },
    {
      "epoch": 2.1795760011084937,
      "grad_norm": 10.532246589660645,
      "learning_rate": 3.128723846473604e-05,
      "loss": 2.2804,
      "step": 15730
    },
    {
      "epoch": 2.1809616184009974,
      "grad_norm": 7.399022102355957,
      "learning_rate": 3.1281695995566025e-05,
      "loss": 2.6472,
      "step": 15740
    },
    {
      "epoch": 2.1823472356935016,
      "grad_norm": 11.30093765258789,
      "learning_rate": 3.127615352639601e-05,
      "loss": 2.6719,
      "step": 15750
    },
    {
      "epoch": 2.1837328529860054,
      "grad_norm": 12.113174438476562,
      "learning_rate": 3.1270611057225995e-05,
      "loss": 2.6422,
      "step": 15760
    },
    {
      "epoch": 2.185118470278509,
      "grad_norm": 9.137757301330566,
      "learning_rate": 3.126506858805598e-05,
      "loss": 2.1797,
      "step": 15770
    },
    {
      "epoch": 2.186504087571013,
      "grad_norm": 9.718701362609863,
      "learning_rate": 3.1259526118885966e-05,
      "loss": 2.4215,
      "step": 15780
    },
    {
      "epoch": 2.1878897048635166,
      "grad_norm": 7.798741817474365,
      "learning_rate": 3.1253983649715954e-05,
      "loss": 2.5773,
      "step": 15790
    },
    {
      "epoch": 2.1892753221560204,
      "grad_norm": 11.115755081176758,
      "learning_rate": 3.1248441180545936e-05,
      "loss": 2.6067,
      "step": 15800
    },
    {
      "epoch": 2.190660939448524,
      "grad_norm": 16.008262634277344,
      "learning_rate": 3.1242898711375925e-05,
      "loss": 2.2756,
      "step": 15810
    },
    {
      "epoch": 2.1920465567410283,
      "grad_norm": 11.458650588989258,
      "learning_rate": 3.1237356242205907e-05,
      "loss": 2.3042,
      "step": 15820
    },
    {
      "epoch": 2.193432174033532,
      "grad_norm": 7.865445613861084,
      "learning_rate": 3.123181377303589e-05,
      "loss": 3.0344,
      "step": 15830
    },
    {
      "epoch": 2.194817791326036,
      "grad_norm": 12.167381286621094,
      "learning_rate": 3.122627130386588e-05,
      "loss": 2.869,
      "step": 15840
    },
    {
      "epoch": 2.1962034086185396,
      "grad_norm": 12.782064437866211,
      "learning_rate": 3.122072883469586e-05,
      "loss": 2.4214,
      "step": 15850
    },
    {
      "epoch": 2.1975890259110433,
      "grad_norm": 15.168356895446777,
      "learning_rate": 3.121518636552584e-05,
      "loss": 2.4883,
      "step": 15860
    },
    {
      "epoch": 2.198974643203547,
      "grad_norm": 13.416139602661133,
      "learning_rate": 3.120964389635583e-05,
      "loss": 3.0272,
      "step": 15870
    },
    {
      "epoch": 2.200360260496051,
      "grad_norm": 13.243340492248535,
      "learning_rate": 3.120410142718581e-05,
      "loss": 2.5918,
      "step": 15880
    },
    {
      "epoch": 2.2017458777885546,
      "grad_norm": 10.060728073120117,
      "learning_rate": 3.11985589580158e-05,
      "loss": 2.6475,
      "step": 15890
    },
    {
      "epoch": 2.2031314950810588,
      "grad_norm": 9.49746036529541,
      "learning_rate": 3.119301648884578e-05,
      "loss": 2.8944,
      "step": 15900
    },
    {
      "epoch": 2.2045171123735625,
      "grad_norm": 8.75903606414795,
      "learning_rate": 3.118747401967577e-05,
      "loss": 2.4024,
      "step": 15910
    },
    {
      "epoch": 2.2059027296660663,
      "grad_norm": 15.050468444824219,
      "learning_rate": 3.118193155050575e-05,
      "loss": 2.4793,
      "step": 15920
    },
    {
      "epoch": 2.20728834695857,
      "grad_norm": 11.220147132873535,
      "learning_rate": 3.117638908133574e-05,
      "loss": 2.2878,
      "step": 15930
    },
    {
      "epoch": 2.2086739642510738,
      "grad_norm": 15.863922119140625,
      "learning_rate": 3.117084661216572e-05,
      "loss": 2.1956,
      "step": 15940
    },
    {
      "epoch": 2.2100595815435775,
      "grad_norm": 11.646565437316895,
      "learning_rate": 3.116530414299571e-05,
      "loss": 2.3363,
      "step": 15950
    },
    {
      "epoch": 2.2114451988360813,
      "grad_norm": 11.150350570678711,
      "learning_rate": 3.115976167382569e-05,
      "loss": 2.0504,
      "step": 15960
    },
    {
      "epoch": 2.2128308161285855,
      "grad_norm": 17.61549186706543,
      "learning_rate": 3.1154219204655675e-05,
      "loss": 2.2099,
      "step": 15970
    },
    {
      "epoch": 2.214216433421089,
      "grad_norm": 16.787158966064453,
      "learning_rate": 3.1148676735485664e-05,
      "loss": 3.1179,
      "step": 15980
    },
    {
      "epoch": 2.215602050713593,
      "grad_norm": 10.580994606018066,
      "learning_rate": 3.1143134266315646e-05,
      "loss": 3.0812,
      "step": 15990
    },
    {
      "epoch": 2.2169876680060967,
      "grad_norm": 13.485642433166504,
      "learning_rate": 3.113759179714563e-05,
      "loss": 2.3923,
      "step": 16000
    },
    {
      "epoch": 2.2183732852986005,
      "grad_norm": 15.995393753051758,
      "learning_rate": 3.1132049327975616e-05,
      "loss": 2.6204,
      "step": 16010
    },
    {
      "epoch": 2.219758902591104,
      "grad_norm": 8.856996536254883,
      "learning_rate": 3.11265068588056e-05,
      "loss": 2.5845,
      "step": 16020
    },
    {
      "epoch": 2.221144519883608,
      "grad_norm": 6.672215938568115,
      "learning_rate": 3.1120964389635587e-05,
      "loss": 2.4851,
      "step": 16030
    },
    {
      "epoch": 2.222530137176112,
      "grad_norm": 11.749842643737793,
      "learning_rate": 3.111542192046557e-05,
      "loss": 2.4572,
      "step": 16040
    },
    {
      "epoch": 2.223915754468616,
      "grad_norm": 5.014955997467041,
      "learning_rate": 3.110987945129556e-05,
      "loss": 2.5132,
      "step": 16050
    },
    {
      "epoch": 2.2253013717611196,
      "grad_norm": 12.891745567321777,
      "learning_rate": 3.110433698212554e-05,
      "loss": 2.7751,
      "step": 16060
    },
    {
      "epoch": 2.2266869890536234,
      "grad_norm": 11.746278762817383,
      "learning_rate": 3.109879451295553e-05,
      "loss": 2.2806,
      "step": 16070
    },
    {
      "epoch": 2.228072606346127,
      "grad_norm": 9.035892486572266,
      "learning_rate": 3.109325204378551e-05,
      "loss": 2.3449,
      "step": 16080
    },
    {
      "epoch": 2.229458223638631,
      "grad_norm": 12.871953964233398,
      "learning_rate": 3.108770957461549e-05,
      "loss": 2.5657,
      "step": 16090
    },
    {
      "epoch": 2.2308438409311346,
      "grad_norm": 10.096248626708984,
      "learning_rate": 3.108216710544548e-05,
      "loss": 2.7323,
      "step": 16100
    },
    {
      "epoch": 2.232229458223639,
      "grad_norm": 12.391213417053223,
      "learning_rate": 3.107662463627546e-05,
      "loss": 2.9933,
      "step": 16110
    },
    {
      "epoch": 2.2336150755161426,
      "grad_norm": 9.216938972473145,
      "learning_rate": 3.1071082167105444e-05,
      "loss": 2.3518,
      "step": 16120
    },
    {
      "epoch": 2.2350006928086463,
      "grad_norm": 15.105727195739746,
      "learning_rate": 3.106553969793543e-05,
      "loss": 2.1902,
      "step": 16130
    },
    {
      "epoch": 2.23638631010115,
      "grad_norm": 6.172408580780029,
      "learning_rate": 3.1059997228765414e-05,
      "loss": 2.4626,
      "step": 16140
    },
    {
      "epoch": 2.237771927393654,
      "grad_norm": 9.006555557250977,
      "learning_rate": 3.10544547595954e-05,
      "loss": 2.6034,
      "step": 16150
    },
    {
      "epoch": 2.2391575446861576,
      "grad_norm": 13.72258186340332,
      "learning_rate": 3.104891229042539e-05,
      "loss": 2.5505,
      "step": 16160
    },
    {
      "epoch": 2.2405431619786613,
      "grad_norm": 6.828301429748535,
      "learning_rate": 3.104336982125537e-05,
      "loss": 2.4139,
      "step": 16170
    },
    {
      "epoch": 2.2419287792711655,
      "grad_norm": 8.378934860229492,
      "learning_rate": 3.1037827352085355e-05,
      "loss": 3.1022,
      "step": 16180
    },
    {
      "epoch": 2.2433143965636693,
      "grad_norm": 12.700705528259277,
      "learning_rate": 3.1032284882915344e-05,
      "loss": 2.2902,
      "step": 16190
    },
    {
      "epoch": 2.244700013856173,
      "grad_norm": 7.972536563873291,
      "learning_rate": 3.1026742413745326e-05,
      "loss": 2.0765,
      "step": 16200
    },
    {
      "epoch": 2.2460856311486768,
      "grad_norm": 6.890265464782715,
      "learning_rate": 3.102119994457531e-05,
      "loss": 2.3699,
      "step": 16210
    },
    {
      "epoch": 2.2474712484411805,
      "grad_norm": 12.260189056396484,
      "learning_rate": 3.1015657475405296e-05,
      "loss": 2.1193,
      "step": 16220
    },
    {
      "epoch": 2.2488568657336843,
      "grad_norm": 16.67074203491211,
      "learning_rate": 3.101011500623528e-05,
      "loss": 2.6285,
      "step": 16230
    },
    {
      "epoch": 2.250242483026188,
      "grad_norm": 11.53359603881836,
      "learning_rate": 3.100457253706527e-05,
      "loss": 2.3122,
      "step": 16240
    },
    {
      "epoch": 2.251628100318692,
      "grad_norm": 10.143793106079102,
      "learning_rate": 3.099903006789525e-05,
      "loss": 2.688,
      "step": 16250
    },
    {
      "epoch": 2.253013717611196,
      "grad_norm": 9.513973236083984,
      "learning_rate": 3.099348759872523e-05,
      "loss": 2.5424,
      "step": 16260
    },
    {
      "epoch": 2.2543993349036997,
      "grad_norm": 7.456992149353027,
      "learning_rate": 3.098794512955522e-05,
      "loss": 2.3036,
      "step": 16270
    },
    {
      "epoch": 2.2557849521962035,
      "grad_norm": 9.981148719787598,
      "learning_rate": 3.098240266038521e-05,
      "loss": 2.261,
      "step": 16280
    },
    {
      "epoch": 2.257170569488707,
      "grad_norm": 13.396782875061035,
      "learning_rate": 3.097686019121519e-05,
      "loss": 2.9811,
      "step": 16290
    },
    {
      "epoch": 2.258556186781211,
      "grad_norm": 15.57819938659668,
      "learning_rate": 3.097131772204518e-05,
      "loss": 2.5473,
      "step": 16300
    },
    {
      "epoch": 2.2599418040737147,
      "grad_norm": 11.948893547058105,
      "learning_rate": 3.096577525287516e-05,
      "loss": 2.5212,
      "step": 16310
    },
    {
      "epoch": 2.261327421366219,
      "grad_norm": 6.7391862869262695,
      "learning_rate": 3.096023278370514e-05,
      "loss": 2.4435,
      "step": 16320
    },
    {
      "epoch": 2.2627130386587226,
      "grad_norm": 7.16084623336792,
      "learning_rate": 3.095469031453513e-05,
      "loss": 2.3628,
      "step": 16330
    },
    {
      "epoch": 2.2640986559512264,
      "grad_norm": 10.31927490234375,
      "learning_rate": 3.094914784536511e-05,
      "loss": 2.5317,
      "step": 16340
    },
    {
      "epoch": 2.26548427324373,
      "grad_norm": 7.523336410522461,
      "learning_rate": 3.0943605376195094e-05,
      "loss": 2.5101,
      "step": 16350
    },
    {
      "epoch": 2.266869890536234,
      "grad_norm": 24.890640258789062,
      "learning_rate": 3.093806290702508e-05,
      "loss": 2.4765,
      "step": 16360
    },
    {
      "epoch": 2.2682555078287376,
      "grad_norm": 14.476017951965332,
      "learning_rate": 3.0932520437855065e-05,
      "loss": 2.3072,
      "step": 16370
    },
    {
      "epoch": 2.2696411251212414,
      "grad_norm": 12.599332809448242,
      "learning_rate": 3.092697796868505e-05,
      "loss": 2.7367,
      "step": 16380
    },
    {
      "epoch": 2.271026742413745,
      "grad_norm": 7.249706745147705,
      "learning_rate": 3.0921435499515035e-05,
      "loss": 2.5219,
      "step": 16390
    },
    {
      "epoch": 2.272412359706249,
      "grad_norm": 20.846233367919922,
      "learning_rate": 3.0915893030345024e-05,
      "loss": 2.6096,
      "step": 16400
    },
    {
      "epoch": 2.273797976998753,
      "grad_norm": 11.741032600402832,
      "learning_rate": 3.0910350561175006e-05,
      "loss": 2.0116,
      "step": 16410
    },
    {
      "epoch": 2.275183594291257,
      "grad_norm": 9.90388298034668,
      "learning_rate": 3.0904808092004994e-05,
      "loss": 2.2071,
      "step": 16420
    },
    {
      "epoch": 2.2765692115837606,
      "grad_norm": 11.129968643188477,
      "learning_rate": 3.0899265622834976e-05,
      "loss": 2.1428,
      "step": 16430
    },
    {
      "epoch": 2.2779548288762643,
      "grad_norm": 10.411087036132812,
      "learning_rate": 3.089372315366496e-05,
      "loss": 2.3638,
      "step": 16440
    },
    {
      "epoch": 2.279340446168768,
      "grad_norm": 15.545716285705566,
      "learning_rate": 3.088818068449495e-05,
      "loss": 2.2356,
      "step": 16450
    },
    {
      "epoch": 2.280726063461272,
      "grad_norm": 10.16054916381836,
      "learning_rate": 3.088263821532493e-05,
      "loss": 2.4313,
      "step": 16460
    },
    {
      "epoch": 2.2821116807537756,
      "grad_norm": 8.697606086730957,
      "learning_rate": 3.0877649993071915e-05,
      "loss": 2.377,
      "step": 16470
    },
    {
      "epoch": 2.2834972980462798,
      "grad_norm": 5.888217926025391,
      "learning_rate": 3.0872107523901904e-05,
      "loss": 2.2306,
      "step": 16480
    },
    {
      "epoch": 2.2848829153387835,
      "grad_norm": 10.773604393005371,
      "learning_rate": 3.0866565054731885e-05,
      "loss": 2.7616,
      "step": 16490
    },
    {
      "epoch": 2.2862685326312873,
      "grad_norm": 8.500377655029297,
      "learning_rate": 3.086102258556187e-05,
      "loss": 2.6553,
      "step": 16500
    },
    {
      "epoch": 2.287654149923791,
      "grad_norm": 8.365767478942871,
      "learning_rate": 3.0855480116391856e-05,
      "loss": 2.2258,
      "step": 16510
    },
    {
      "epoch": 2.2890397672162948,
      "grad_norm": 14.457158088684082,
      "learning_rate": 3.084993764722184e-05,
      "loss": 2.6757,
      "step": 16520
    },
    {
      "epoch": 2.2904253845087985,
      "grad_norm": 16.007774353027344,
      "learning_rate": 3.084439517805182e-05,
      "loss": 2.6977,
      "step": 16530
    },
    {
      "epoch": 2.2918110018013023,
      "grad_norm": 10.723617553710938,
      "learning_rate": 3.083885270888181e-05,
      "loss": 2.8087,
      "step": 16540
    },
    {
      "epoch": 2.2931966190938065,
      "grad_norm": 9.528544425964355,
      "learning_rate": 3.083331023971179e-05,
      "loss": 2.3383,
      "step": 16550
    },
    {
      "epoch": 2.29458223638631,
      "grad_norm": 9.113424301147461,
      "learning_rate": 3.082776777054178e-05,
      "loss": 2.606,
      "step": 16560
    },
    {
      "epoch": 2.295967853678814,
      "grad_norm": 15.75671100616455,
      "learning_rate": 3.082222530137177e-05,
      "loss": 2.5261,
      "step": 16570
    },
    {
      "epoch": 2.2973534709713177,
      "grad_norm": 13.973138809204102,
      "learning_rate": 3.081668283220175e-05,
      "loss": 2.2936,
      "step": 16580
    },
    {
      "epoch": 2.2987390882638215,
      "grad_norm": 8.608307838439941,
      "learning_rate": 3.081114036303174e-05,
      "loss": 2.3748,
      "step": 16590
    },
    {
      "epoch": 2.300124705556325,
      "grad_norm": 13.477620124816895,
      "learning_rate": 3.080559789386172e-05,
      "loss": 2.7486,
      "step": 16600
    },
    {
      "epoch": 2.301510322848829,
      "grad_norm": 9.891220092773438,
      "learning_rate": 3.08000554246917e-05,
      "loss": 2.6167,
      "step": 16610
    },
    {
      "epoch": 2.302895940141333,
      "grad_norm": 10.686735153198242,
      "learning_rate": 3.079451295552169e-05,
      "loss": 2.431,
      "step": 16620
    },
    {
      "epoch": 2.304281557433837,
      "grad_norm": 7.4269232749938965,
      "learning_rate": 3.078897048635167e-05,
      "loss": 2.4486,
      "step": 16630
    },
    {
      "epoch": 2.3056671747263406,
      "grad_norm": 7.903440952301025,
      "learning_rate": 3.0783428017181654e-05,
      "loss": 2.5642,
      "step": 16640
    },
    {
      "epoch": 2.3070527920188444,
      "grad_norm": 12.290229797363281,
      "learning_rate": 3.077788554801164e-05,
      "loss": 2.5635,
      "step": 16650
    },
    {
      "epoch": 2.308438409311348,
      "grad_norm": 12.90865707397461,
      "learning_rate": 3.0772343078841625e-05,
      "loss": 2.3381,
      "step": 16660
    },
    {
      "epoch": 2.309824026603852,
      "grad_norm": 13.237085342407227,
      "learning_rate": 3.076680060967161e-05,
      "loss": 2.5509,
      "step": 16670
    },
    {
      "epoch": 2.3112096438963556,
      "grad_norm": 8.758783340454102,
      "learning_rate": 3.0761258140501595e-05,
      "loss": 2.4169,
      "step": 16680
    },
    {
      "epoch": 2.31259526118886,
      "grad_norm": 15.072258949279785,
      "learning_rate": 3.0755715671331584e-05,
      "loss": 1.9644,
      "step": 16690
    },
    {
      "epoch": 2.3139808784813636,
      "grad_norm": 8.981100082397461,
      "learning_rate": 3.0750173202161566e-05,
      "loss": 2.5043,
      "step": 16700
    },
    {
      "epoch": 2.3153664957738673,
      "grad_norm": 11.070026397705078,
      "learning_rate": 3.0744630732991554e-05,
      "loss": 2.2817,
      "step": 16710
    },
    {
      "epoch": 2.316752113066371,
      "grad_norm": 11.772947311401367,
      "learning_rate": 3.0739088263821536e-05,
      "loss": 2.7929,
      "step": 16720
    },
    {
      "epoch": 2.318137730358875,
      "grad_norm": 10.653103828430176,
      "learning_rate": 3.073354579465152e-05,
      "loss": 2.5582,
      "step": 16730
    },
    {
      "epoch": 2.3195233476513786,
      "grad_norm": 9.71946907043457,
      "learning_rate": 3.0728003325481507e-05,
      "loss": 2.9294,
      "step": 16740
    },
    {
      "epoch": 2.3209089649438823,
      "grad_norm": 9.624415397644043,
      "learning_rate": 3.072246085631149e-05,
      "loss": 2.3261,
      "step": 16750
    },
    {
      "epoch": 2.3222945822363865,
      "grad_norm": 7.434267997741699,
      "learning_rate": 3.071691838714147e-05,
      "loss": 2.5931,
      "step": 16760
    },
    {
      "epoch": 2.3236801995288903,
      "grad_norm": 8.08291244506836,
      "learning_rate": 3.071137591797146e-05,
      "loss": 2.7518,
      "step": 16770
    },
    {
      "epoch": 2.325065816821394,
      "grad_norm": 9.120258331298828,
      "learning_rate": 3.070583344880144e-05,
      "loss": 2.2427,
      "step": 16780
    },
    {
      "epoch": 2.3264514341138978,
      "grad_norm": 11.081244468688965,
      "learning_rate": 3.070029097963143e-05,
      "loss": 2.2694,
      "step": 16790
    },
    {
      "epoch": 2.3278370514064015,
      "grad_norm": 9.456782341003418,
      "learning_rate": 3.069474851046141e-05,
      "loss": 2.4676,
      "step": 16800
    },
    {
      "epoch": 2.3292226686989053,
      "grad_norm": 9.133485794067383,
      "learning_rate": 3.06892060412914e-05,
      "loss": 2.3749,
      "step": 16810
    },
    {
      "epoch": 2.330608285991409,
      "grad_norm": 14.937589645385742,
      "learning_rate": 3.068366357212138e-05,
      "loss": 2.5721,
      "step": 16820
    },
    {
      "epoch": 2.331993903283913,
      "grad_norm": 9.140617370605469,
      "learning_rate": 3.067812110295137e-05,
      "loss": 2.5271,
      "step": 16830
    },
    {
      "epoch": 2.333379520576417,
      "grad_norm": 7.248733997344971,
      "learning_rate": 3.067257863378135e-05,
      "loss": 2.0734,
      "step": 16840
    },
    {
      "epoch": 2.3347651378689207,
      "grad_norm": 5.897427082061768,
      "learning_rate": 3.066703616461134e-05,
      "loss": 2.3713,
      "step": 16850
    },
    {
      "epoch": 2.3361507551614245,
      "grad_norm": 16.60118865966797,
      "learning_rate": 3.066149369544132e-05,
      "loss": 2.5225,
      "step": 16860
    },
    {
      "epoch": 2.337536372453928,
      "grad_norm": 11.676332473754883,
      "learning_rate": 3.0655951226271305e-05,
      "loss": 2.2171,
      "step": 16870
    },
    {
      "epoch": 2.338921989746432,
      "grad_norm": 8.961499214172363,
      "learning_rate": 3.065040875710129e-05,
      "loss": 2.7157,
      "step": 16880
    },
    {
      "epoch": 2.3403076070389357,
      "grad_norm": 11.131946563720703,
      "learning_rate": 3.0644866287931275e-05,
      "loss": 2.3358,
      "step": 16890
    },
    {
      "epoch": 2.34169322433144,
      "grad_norm": 6.922090530395508,
      "learning_rate": 3.063932381876126e-05,
      "loss": 2.2327,
      "step": 16900
    },
    {
      "epoch": 2.3430788416239436,
      "grad_norm": 9.479480743408203,
      "learning_rate": 3.0633781349591246e-05,
      "loss": 2.3851,
      "step": 16910
    },
    {
      "epoch": 2.3444644589164474,
      "grad_norm": 8.775184631347656,
      "learning_rate": 3.062823888042123e-05,
      "loss": 2.6271,
      "step": 16920
    },
    {
      "epoch": 2.345850076208951,
      "grad_norm": 8.07295036315918,
      "learning_rate": 3.0622696411251216e-05,
      "loss": 2.5043,
      "step": 16930
    },
    {
      "epoch": 2.347235693501455,
      "grad_norm": 14.148488998413086,
      "learning_rate": 3.0617153942081205e-05,
      "loss": 2.7377,
      "step": 16940
    },
    {
      "epoch": 2.3486213107939586,
      "grad_norm": 6.361992359161377,
      "learning_rate": 3.0611611472911187e-05,
      "loss": 2.5955,
      "step": 16950
    },
    {
      "epoch": 2.3500069280864624,
      "grad_norm": 12.841928482055664,
      "learning_rate": 3.060606900374117e-05,
      "loss": 2.1231,
      "step": 16960
    },
    {
      "epoch": 2.351392545378966,
      "grad_norm": 15.255315780639648,
      "learning_rate": 3.060052653457116e-05,
      "loss": 2.3463,
      "step": 16970
    },
    {
      "epoch": 2.3527781626714703,
      "grad_norm": 11.708540916442871,
      "learning_rate": 3.059498406540114e-05,
      "loss": 2.9084,
      "step": 16980
    },
    {
      "epoch": 2.354163779963974,
      "grad_norm": 10.40744400024414,
      "learning_rate": 3.058944159623112e-05,
      "loss": 2.741,
      "step": 16990
    },
    {
      "epoch": 2.355549397256478,
      "grad_norm": 9.044754981994629,
      "learning_rate": 3.058389912706111e-05,
      "loss": 2.9695,
      "step": 17000
    },
    {
      "epoch": 2.3569350145489816,
      "grad_norm": 20.305875778198242,
      "learning_rate": 3.057835665789109e-05,
      "loss": 2.5134,
      "step": 17010
    },
    {
      "epoch": 2.3583206318414853,
      "grad_norm": 15.027204513549805,
      "learning_rate": 3.057281418872107e-05,
      "loss": 2.2555,
      "step": 17020
    },
    {
      "epoch": 2.359706249133989,
      "grad_norm": 10.054710388183594,
      "learning_rate": 3.056727171955106e-05,
      "loss": 2.4154,
      "step": 17030
    },
    {
      "epoch": 2.361091866426493,
      "grad_norm": 7.209543228149414,
      "learning_rate": 3.0561729250381044e-05,
      "loss": 2.3334,
      "step": 17040
    },
    {
      "epoch": 2.3624774837189966,
      "grad_norm": 7.355217933654785,
      "learning_rate": 3.055618678121103e-05,
      "loss": 2.8397,
      "step": 17050
    },
    {
      "epoch": 2.3638631010115008,
      "grad_norm": 12.332898139953613,
      "learning_rate": 3.055064431204102e-05,
      "loss": 2.3546,
      "step": 17060
    },
    {
      "epoch": 2.3652487183040045,
      "grad_norm": 15.454670906066895,
      "learning_rate": 3.0545101842871e-05,
      "loss": 2.4585,
      "step": 17070
    },
    {
      "epoch": 2.3666343355965083,
      "grad_norm": 10.317573547363281,
      "learning_rate": 3.0539559373700985e-05,
      "loss": 2.1812,
      "step": 17080
    },
    {
      "epoch": 2.368019952889012,
      "grad_norm": 9.90349006652832,
      "learning_rate": 3.053401690453097e-05,
      "loss": 2.5842,
      "step": 17090
    },
    {
      "epoch": 2.3694055701815158,
      "grad_norm": 5.33489465713501,
      "learning_rate": 3.0528474435360955e-05,
      "loss": 2.5113,
      "step": 17100
    },
    {
      "epoch": 2.3707911874740195,
      "grad_norm": 10.038074493408203,
      "learning_rate": 3.052293196619094e-05,
      "loss": 2.9241,
      "step": 17110
    },
    {
      "epoch": 2.3721768047665233,
      "grad_norm": 15.301932334899902,
      "learning_rate": 3.0517389497020926e-05,
      "loss": 2.7162,
      "step": 17120
    },
    {
      "epoch": 2.3735624220590275,
      "grad_norm": 6.13732385635376,
      "learning_rate": 3.051184702785091e-05,
      "loss": 2.0078,
      "step": 17130
    },
    {
      "epoch": 2.374948039351531,
      "grad_norm": 14.583281517028809,
      "learning_rate": 3.0506304558680896e-05,
      "loss": 2.4412,
      "step": 17140
    },
    {
      "epoch": 2.376333656644035,
      "grad_norm": 9.983065605163574,
      "learning_rate": 3.050076208951088e-05,
      "loss": 2.271,
      "step": 17150
    },
    {
      "epoch": 2.3777192739365387,
      "grad_norm": 10.216312408447266,
      "learning_rate": 3.0495219620340863e-05,
      "loss": 2.4519,
      "step": 17160
    },
    {
      "epoch": 2.3791048912290425,
      "grad_norm": 10.712532043457031,
      "learning_rate": 3.0489677151170852e-05,
      "loss": 2.2737,
      "step": 17170
    },
    {
      "epoch": 2.380490508521546,
      "grad_norm": 10.139025688171387,
      "learning_rate": 3.0484134682000834e-05,
      "loss": 2.9434,
      "step": 17180
    },
    {
      "epoch": 2.38187612581405,
      "grad_norm": 10.21037483215332,
      "learning_rate": 3.047859221283082e-05,
      "loss": 2.3572,
      "step": 17190
    },
    {
      "epoch": 2.383261743106554,
      "grad_norm": 17.77339744567871,
      "learning_rate": 3.0473049743660804e-05,
      "loss": 2.726,
      "step": 17200
    },
    {
      "epoch": 2.384647360399058,
      "grad_norm": 12.066919326782227,
      "learning_rate": 3.046750727449079e-05,
      "loss": 1.8721,
      "step": 17210
    },
    {
      "epoch": 2.3860329776915616,
      "grad_norm": 8.763294219970703,
      "learning_rate": 3.046196480532077e-05,
      "loss": 2.7493,
      "step": 17220
    },
    {
      "epoch": 2.3874185949840654,
      "grad_norm": 6.637188911437988,
      "learning_rate": 3.045642233615076e-05,
      "loss": 2.188,
      "step": 17230
    },
    {
      "epoch": 2.388804212276569,
      "grad_norm": 13.771286010742188,
      "learning_rate": 3.0450879866980742e-05,
      "loss": 2.5843,
      "step": 17240
    },
    {
      "epoch": 2.390189829569073,
      "grad_norm": 13.7254638671875,
      "learning_rate": 3.0445337397810727e-05,
      "loss": 2.4714,
      "step": 17250
    },
    {
      "epoch": 2.3915754468615766,
      "grad_norm": 9.576545715332031,
      "learning_rate": 3.0439794928640712e-05,
      "loss": 2.4278,
      "step": 17260
    },
    {
      "epoch": 2.392961064154081,
      "grad_norm": 8.583932876586914,
      "learning_rate": 3.0434252459470698e-05,
      "loss": 2.2778,
      "step": 17270
    },
    {
      "epoch": 2.3943466814465846,
      "grad_norm": 8.746700286865234,
      "learning_rate": 3.042870999030068e-05,
      "loss": 2.7287,
      "step": 17280
    },
    {
      "epoch": 2.3957322987390883,
      "grad_norm": 7.646458625793457,
      "learning_rate": 3.0423167521130668e-05,
      "loss": 2.3901,
      "step": 17290
    },
    {
      "epoch": 2.397117916031592,
      "grad_norm": 10.942044258117676,
      "learning_rate": 3.041762505196065e-05,
      "loss": 2.3715,
      "step": 17300
    },
    {
      "epoch": 2.398503533324096,
      "grad_norm": 10.348411560058594,
      "learning_rate": 3.0412082582790635e-05,
      "loss": 2.7807,
      "step": 17310
    },
    {
      "epoch": 2.3998891506165996,
      "grad_norm": 6.923431873321533,
      "learning_rate": 3.040654011362062e-05,
      "loss": 2.4654,
      "step": 17320
    },
    {
      "epoch": 2.4012747679091033,
      "grad_norm": 11.880636215209961,
      "learning_rate": 3.0400997644450606e-05,
      "loss": 2.7023,
      "step": 17330
    },
    {
      "epoch": 2.4026603852016075,
      "grad_norm": 9.674811363220215,
      "learning_rate": 3.0395455175280588e-05,
      "loss": 2.5629,
      "step": 17340
    },
    {
      "epoch": 2.4040460024941113,
      "grad_norm": 13.267107009887695,
      "learning_rate": 3.0389912706110576e-05,
      "loss": 2.2867,
      "step": 17350
    },
    {
      "epoch": 2.405431619786615,
      "grad_norm": 10.927294731140137,
      "learning_rate": 3.0384370236940558e-05,
      "loss": 2.2399,
      "step": 17360
    },
    {
      "epoch": 2.4068172370791188,
      "grad_norm": 8.07908821105957,
      "learning_rate": 3.0378827767770543e-05,
      "loss": 2.5162,
      "step": 17370
    },
    {
      "epoch": 2.4082028543716225,
      "grad_norm": 12.486738204956055,
      "learning_rate": 3.037328529860053e-05,
      "loss": 2.5744,
      "step": 17380
    },
    {
      "epoch": 2.4095884716641263,
      "grad_norm": 10.730386734008789,
      "learning_rate": 3.0367742829430514e-05,
      "loss": 2.3558,
      "step": 17390
    },
    {
      "epoch": 2.41097408895663,
      "grad_norm": 12.155187606811523,
      "learning_rate": 3.0362200360260496e-05,
      "loss": 2.4448,
      "step": 17400
    },
    {
      "epoch": 2.412359706249134,
      "grad_norm": 10.618178367614746,
      "learning_rate": 3.0356657891090484e-05,
      "loss": 2.3176,
      "step": 17410
    },
    {
      "epoch": 2.413745323541638,
      "grad_norm": 10.877549171447754,
      "learning_rate": 3.0351115421920466e-05,
      "loss": 2.7697,
      "step": 17420
    },
    {
      "epoch": 2.4151309408341417,
      "grad_norm": 16.014841079711914,
      "learning_rate": 3.0345572952750455e-05,
      "loss": 2.0733,
      "step": 17430
    },
    {
      "epoch": 2.4165165581266455,
      "grad_norm": 10.16993522644043,
      "learning_rate": 3.0340030483580437e-05,
      "loss": 2.3512,
      "step": 17440
    },
    {
      "epoch": 2.417902175419149,
      "grad_norm": 17.37448501586914,
      "learning_rate": 3.0334488014410422e-05,
      "loss": 2.3876,
      "step": 17450
    },
    {
      "epoch": 2.419287792711653,
      "grad_norm": 9.07644271850586,
      "learning_rate": 3.032894554524041e-05,
      "loss": 1.8412,
      "step": 17460
    },
    {
      "epoch": 2.4206734100041567,
      "grad_norm": 7.884594440460205,
      "learning_rate": 3.0323403076070392e-05,
      "loss": 2.9197,
      "step": 17470
    },
    {
      "epoch": 2.422059027296661,
      "grad_norm": 13.755578994750977,
      "learning_rate": 3.0317860606900374e-05,
      "loss": 2.8556,
      "step": 17480
    },
    {
      "epoch": 2.4234446445891646,
      "grad_norm": 10.045595169067383,
      "learning_rate": 3.0312318137730363e-05,
      "loss": 2.3816,
      "step": 17490
    },
    {
      "epoch": 2.4248302618816684,
      "grad_norm": 11.550833702087402,
      "learning_rate": 3.0306775668560345e-05,
      "loss": 3.204,
      "step": 17500
    },
    {
      "epoch": 2.426215879174172,
      "grad_norm": 15.539336204528809,
      "learning_rate": 3.030123319939033e-05,
      "loss": 2.2513,
      "step": 17510
    },
    {
      "epoch": 2.427601496466676,
      "grad_norm": 14.235586166381836,
      "learning_rate": 3.029569073022032e-05,
      "loss": 2.2967,
      "step": 17520
    },
    {
      "epoch": 2.4289871137591796,
      "grad_norm": 13.738117218017578,
      "learning_rate": 3.02901482610503e-05,
      "loss": 2.2236,
      "step": 17530
    },
    {
      "epoch": 2.4303727310516834,
      "grad_norm": 14.606142044067383,
      "learning_rate": 3.0284605791880282e-05,
      "loss": 2.7195,
      "step": 17540
    },
    {
      "epoch": 2.4317583483441876,
      "grad_norm": 11.902767181396484,
      "learning_rate": 3.027906332271027e-05,
      "loss": 2.3806,
      "step": 17550
    },
    {
      "epoch": 2.4331439656366913,
      "grad_norm": 9.326058387756348,
      "learning_rate": 3.0273520853540256e-05,
      "loss": 2.4169,
      "step": 17560
    },
    {
      "epoch": 2.434529582929195,
      "grad_norm": 12.741636276245117,
      "learning_rate": 3.0267978384370238e-05,
      "loss": 2.0035,
      "step": 17570
    },
    {
      "epoch": 2.435915200221699,
      "grad_norm": 14.886106491088867,
      "learning_rate": 3.0262435915200227e-05,
      "loss": 1.8874,
      "step": 17580
    },
    {
      "epoch": 2.4373008175142026,
      "grad_norm": 10.925786972045898,
      "learning_rate": 3.025689344603021e-05,
      "loss": 2.5077,
      "step": 17590
    },
    {
      "epoch": 2.4386864348067063,
      "grad_norm": 10.277271270751953,
      "learning_rate": 3.025135097686019e-05,
      "loss": 2.8033,
      "step": 17600
    },
    {
      "epoch": 2.44007205209921,
      "grad_norm": 10.252482414245605,
      "learning_rate": 3.024580850769018e-05,
      "loss": 2.2976,
      "step": 17610
    },
    {
      "epoch": 2.441457669391714,
      "grad_norm": 12.630245208740234,
      "learning_rate": 3.0240266038520164e-05,
      "loss": 2.2154,
      "step": 17620
    },
    {
      "epoch": 2.4428432866842176,
      "grad_norm": 18.935781478881836,
      "learning_rate": 3.0234723569350146e-05,
      "loss": 2.2444,
      "step": 17630
    },
    {
      "epoch": 2.4442289039767218,
      "grad_norm": 21.81167221069336,
      "learning_rate": 3.0229735347097136e-05,
      "loss": 2.5021,
      "step": 17640
    },
    {
      "epoch": 2.4456145212692255,
      "grad_norm": 15.82117748260498,
      "learning_rate": 3.0224192877927118e-05,
      "loss": 2.5352,
      "step": 17650
    },
    {
      "epoch": 2.4470001385617293,
      "grad_norm": 16.68500518798828,
      "learning_rate": 3.0218650408757103e-05,
      "loss": 2.3848,
      "step": 17660
    },
    {
      "epoch": 2.448385755854233,
      "grad_norm": 12.89641284942627,
      "learning_rate": 3.021310793958709e-05,
      "loss": 2.0622,
      "step": 17670
    },
    {
      "epoch": 2.4497713731467368,
      "grad_norm": 13.111610412597656,
      "learning_rate": 3.0207565470417074e-05,
      "loss": 2.7733,
      "step": 17680
    },
    {
      "epoch": 2.4511569904392405,
      "grad_norm": 11.616905212402344,
      "learning_rate": 3.0202023001247056e-05,
      "loss": 2.3833,
      "step": 17690
    },
    {
      "epoch": 2.4525426077317443,
      "grad_norm": 19.434101104736328,
      "learning_rate": 3.0196480532077044e-05,
      "loss": 2.3226,
      "step": 17700
    },
    {
      "epoch": 2.4539282250242485,
      "grad_norm": 12.54909896850586,
      "learning_rate": 3.0190938062907026e-05,
      "loss": 1.8588,
      "step": 17710
    },
    {
      "epoch": 2.455313842316752,
      "grad_norm": 12.119122505187988,
      "learning_rate": 3.018539559373701e-05,
      "loss": 1.9434,
      "step": 17720
    },
    {
      "epoch": 2.456699459609256,
      "grad_norm": 11.517288208007812,
      "learning_rate": 3.0179853124566997e-05,
      "loss": 2.3839,
      "step": 17730
    },
    {
      "epoch": 2.4580850769017597,
      "grad_norm": 11.791476249694824,
      "learning_rate": 3.0174310655396982e-05,
      "loss": 2.8294,
      "step": 17740
    },
    {
      "epoch": 2.4594706941942635,
      "grad_norm": 15.141037940979004,
      "learning_rate": 3.016876818622697e-05,
      "loss": 2.3733,
      "step": 17750
    },
    {
      "epoch": 2.460856311486767,
      "grad_norm": 13.577332496643066,
      "learning_rate": 3.0163225717056952e-05,
      "loss": 2.7688,
      "step": 17760
    },
    {
      "epoch": 2.462241928779271,
      "grad_norm": 13.287530899047852,
      "learning_rate": 3.0157683247886934e-05,
      "loss": 2.4677,
      "step": 17770
    },
    {
      "epoch": 2.463627546071775,
      "grad_norm": 9.709625244140625,
      "learning_rate": 3.0152140778716923e-05,
      "loss": 2.1836,
      "step": 17780
    },
    {
      "epoch": 2.465013163364279,
      "grad_norm": 14.048952102661133,
      "learning_rate": 3.0146598309546905e-05,
      "loss": 3.3502,
      "step": 17790
    },
    {
      "epoch": 2.4663987806567826,
      "grad_norm": 16.086149215698242,
      "learning_rate": 3.014105584037689e-05,
      "loss": 2.8353,
      "step": 17800
    },
    {
      "epoch": 2.4677843979492864,
      "grad_norm": 8.941368103027344,
      "learning_rate": 3.013551337120688e-05,
      "loss": 1.9929,
      "step": 17810
    },
    {
      "epoch": 2.46917001524179,
      "grad_norm": 11.279109001159668,
      "learning_rate": 3.012997090203686e-05,
      "loss": 2.4583,
      "step": 17820
    },
    {
      "epoch": 2.470555632534294,
      "grad_norm": 14.942954063415527,
      "learning_rate": 3.0124428432866842e-05,
      "loss": 2.4171,
      "step": 17830
    },
    {
      "epoch": 2.4719412498267976,
      "grad_norm": 12.362049102783203,
      "learning_rate": 3.011888596369683e-05,
      "loss": 2.4106,
      "step": 17840
    },
    {
      "epoch": 2.473326867119302,
      "grad_norm": 15.809988975524902,
      "learning_rate": 3.0113343494526813e-05,
      "loss": 2.9975,
      "step": 17850
    },
    {
      "epoch": 2.4747124844118056,
      "grad_norm": 7.354685306549072,
      "learning_rate": 3.0107801025356798e-05,
      "loss": 2.3915,
      "step": 17860
    },
    {
      "epoch": 2.4760981017043093,
      "grad_norm": 9.593871116638184,
      "learning_rate": 3.0102258556186787e-05,
      "loss": 2.4325,
      "step": 17870
    },
    {
      "epoch": 2.477483718996813,
      "grad_norm": 12.541213989257812,
      "learning_rate": 3.009671608701677e-05,
      "loss": 2.0725,
      "step": 17880
    },
    {
      "epoch": 2.478869336289317,
      "grad_norm": 18.780487060546875,
      "learning_rate": 3.009117361784675e-05,
      "loss": 2.5358,
      "step": 17890
    },
    {
      "epoch": 2.4802549535818206,
      "grad_norm": 8.462728500366211,
      "learning_rate": 3.008563114867674e-05,
      "loss": 2.4618,
      "step": 17900
    },
    {
      "epoch": 2.4816405708743243,
      "grad_norm": 5.509677886962891,
      "learning_rate": 3.0080088679506724e-05,
      "loss": 2.525,
      "step": 17910
    },
    {
      "epoch": 2.4830261881668285,
      "grad_norm": 12.852605819702148,
      "learning_rate": 3.0074546210336706e-05,
      "loss": 2.4917,
      "step": 17920
    },
    {
      "epoch": 2.4844118054593323,
      "grad_norm": 10.472273826599121,
      "learning_rate": 3.0069003741166695e-05,
      "loss": 2.6465,
      "step": 17930
    },
    {
      "epoch": 2.485797422751836,
      "grad_norm": 11.1388578414917,
      "learning_rate": 3.0063461271996677e-05,
      "loss": 2.7021,
      "step": 17940
    },
    {
      "epoch": 2.4871830400443398,
      "grad_norm": 8.198322296142578,
      "learning_rate": 3.005791880282666e-05,
      "loss": 2.766,
      "step": 17950
    },
    {
      "epoch": 2.4885686573368435,
      "grad_norm": 9.345236778259277,
      "learning_rate": 3.0052376333656647e-05,
      "loss": 1.9804,
      "step": 17960
    },
    {
      "epoch": 2.4899542746293473,
      "grad_norm": 12.896048545837402,
      "learning_rate": 3.0046833864486632e-05,
      "loss": 2.2818,
      "step": 17970
    },
    {
      "epoch": 2.491339891921851,
      "grad_norm": 15.979498863220215,
      "learning_rate": 3.0041291395316614e-05,
      "loss": 2.3247,
      "step": 17980
    },
    {
      "epoch": 2.492725509214355,
      "grad_norm": 17.990694046020508,
      "learning_rate": 3.0035748926146603e-05,
      "loss": 2.2369,
      "step": 17990
    },
    {
      "epoch": 2.494111126506859,
      "grad_norm": 11.756139755249023,
      "learning_rate": 3.0030206456976585e-05,
      "loss": 2.8052,
      "step": 18000
    },
    {
      "epoch": 2.4954967437993627,
      "grad_norm": 10.578543663024902,
      "learning_rate": 3.0024663987806567e-05,
      "loss": 2.5509,
      "step": 18010
    },
    {
      "epoch": 2.4968823610918665,
      "grad_norm": 11.013649940490723,
      "learning_rate": 3.0019121518636555e-05,
      "loss": 2.5791,
      "step": 18020
    },
    {
      "epoch": 2.49826797838437,
      "grad_norm": 8.748638153076172,
      "learning_rate": 3.001357904946654e-05,
      "loss": 2.63,
      "step": 18030
    },
    {
      "epoch": 2.499653595676874,
      "grad_norm": 20.28583526611328,
      "learning_rate": 3.0008036580296526e-05,
      "loss": 2.6848,
      "step": 18040
    },
    {
      "epoch": 2.5010392129693777,
      "grad_norm": 11.497008323669434,
      "learning_rate": 3.000249411112651e-05,
      "loss": 2.7433,
      "step": 18050
    },
    {
      "epoch": 2.502424830261882,
      "grad_norm": 10.541352272033691,
      "learning_rate": 2.9996951641956493e-05,
      "loss": 2.1993,
      "step": 18060
    },
    {
      "epoch": 2.5038104475543856,
      "grad_norm": 10.560134887695312,
      "learning_rate": 2.999140917278648e-05,
      "loss": 2.7866,
      "step": 18070
    },
    {
      "epoch": 2.5051960648468894,
      "grad_norm": 14.490334510803223,
      "learning_rate": 2.9985866703616463e-05,
      "loss": 2.5283,
      "step": 18080
    },
    {
      "epoch": 2.506581682139393,
      "grad_norm": 17.14607810974121,
      "learning_rate": 2.998032423444645e-05,
      "loss": 2.6312,
      "step": 18090
    },
    {
      "epoch": 2.507967299431897,
      "grad_norm": 11.213701248168945,
      "learning_rate": 2.9974781765276434e-05,
      "loss": 2.4582,
      "step": 18100
    },
    {
      "epoch": 2.5093529167244006,
      "grad_norm": 15.670195579528809,
      "learning_rate": 2.996923929610642e-05,
      "loss": 2.3471,
      "step": 18110
    },
    {
      "epoch": 2.5107385340169044,
      "grad_norm": 8.975439071655273,
      "learning_rate": 2.99636968269364e-05,
      "loss": 2.1942,
      "step": 18120
    },
    {
      "epoch": 2.5121241513094086,
      "grad_norm": 6.931324481964111,
      "learning_rate": 2.995815435776639e-05,
      "loss": 2.415,
      "step": 18130
    },
    {
      "epoch": 2.513509768601912,
      "grad_norm": 11.966822624206543,
      "learning_rate": 2.995261188859637e-05,
      "loss": 1.9589,
      "step": 18140
    },
    {
      "epoch": 2.514895385894416,
      "grad_norm": 12.48922061920166,
      "learning_rate": 2.9947069419426357e-05,
      "loss": 2.553,
      "step": 18150
    },
    {
      "epoch": 2.51628100318692,
      "grad_norm": 10.730128288269043,
      "learning_rate": 2.9941526950256342e-05,
      "loss": 2.3844,
      "step": 18160
    },
    {
      "epoch": 2.5176666204794236,
      "grad_norm": 24.302627563476562,
      "learning_rate": 2.9935984481086327e-05,
      "loss": 2.331,
      "step": 18170
    },
    {
      "epoch": 2.5190522377719273,
      "grad_norm": 7.831603050231934,
      "learning_rate": 2.993044201191631e-05,
      "loss": 2.6582,
      "step": 18180
    },
    {
      "epoch": 2.520437855064431,
      "grad_norm": 5.559858798980713,
      "learning_rate": 2.9924899542746298e-05,
      "loss": 2.2887,
      "step": 18190
    },
    {
      "epoch": 2.5218234723569353,
      "grad_norm": 8.465137481689453,
      "learning_rate": 2.991935707357628e-05,
      "loss": 2.8632,
      "step": 18200
    },
    {
      "epoch": 2.5232090896494386,
      "grad_norm": 15.226795196533203,
      "learning_rate": 2.9913814604406265e-05,
      "loss": 2.7976,
      "step": 18210
    },
    {
      "epoch": 2.5245947069419428,
      "grad_norm": 9.554261207580566,
      "learning_rate": 2.990827213523625e-05,
      "loss": 2.41,
      "step": 18220
    },
    {
      "epoch": 2.5259803242344465,
      "grad_norm": 16.861923217773438,
      "learning_rate": 2.9902729666066235e-05,
      "loss": 2.4952,
      "step": 18230
    },
    {
      "epoch": 2.5273659415269503,
      "grad_norm": 14.753901481628418,
      "learning_rate": 2.9897187196896217e-05,
      "loss": 2.4627,
      "step": 18240
    },
    {
      "epoch": 2.528751558819454,
      "grad_norm": 17.09401512145996,
      "learning_rate": 2.9891644727726206e-05,
      "loss": 2.6478,
      "step": 18250
    },
    {
      "epoch": 2.5301371761119578,
      "grad_norm": 15.958691596984863,
      "learning_rate": 2.9886102258556188e-05,
      "loss": 2.8257,
      "step": 18260
    },
    {
      "epoch": 2.531522793404462,
      "grad_norm": 7.796380519866943,
      "learning_rate": 2.9880559789386173e-05,
      "loss": 3.0803,
      "step": 18270
    },
    {
      "epoch": 2.5329084106969653,
      "grad_norm": 7.296705722808838,
      "learning_rate": 2.9875017320216158e-05,
      "loss": 2.6201,
      "step": 18280
    },
    {
      "epoch": 2.5342940279894695,
      "grad_norm": 7.887547969818115,
      "learning_rate": 2.9869474851046143e-05,
      "loss": 2.0812,
      "step": 18290
    },
    {
      "epoch": 2.535679645281973,
      "grad_norm": 8.993698120117188,
      "learning_rate": 2.9863932381876125e-05,
      "loss": 2.4158,
      "step": 18300
    },
    {
      "epoch": 2.537065262574477,
      "grad_norm": 9.655014991760254,
      "learning_rate": 2.9858389912706114e-05,
      "loss": 2.3423,
      "step": 18310
    },
    {
      "epoch": 2.5384508798669807,
      "grad_norm": 13.37005615234375,
      "learning_rate": 2.9852847443536096e-05,
      "loss": 2.9163,
      "step": 18320
    },
    {
      "epoch": 2.5398364971594845,
      "grad_norm": 8.588884353637695,
      "learning_rate": 2.984730497436608e-05,
      "loss": 2.317,
      "step": 18330
    },
    {
      "epoch": 2.541222114451988,
      "grad_norm": 8.826887130737305,
      "learning_rate": 2.984176250519607e-05,
      "loss": 2.6227,
      "step": 18340
    },
    {
      "epoch": 2.542607731744492,
      "grad_norm": 11.55893611907959,
      "learning_rate": 2.983622003602605e-05,
      "loss": 2.3748,
      "step": 18350
    },
    {
      "epoch": 2.543993349036996,
      "grad_norm": 12.220918655395508,
      "learning_rate": 2.983067756685604e-05,
      "loss": 3.2088,
      "step": 18360
    },
    {
      "epoch": 2.5453789663295,
      "grad_norm": 8.440755844116211,
      "learning_rate": 2.9825135097686022e-05,
      "loss": 2.2541,
      "step": 18370
    },
    {
      "epoch": 2.5467645836220036,
      "grad_norm": 9.992646217346191,
      "learning_rate": 2.9819592628516004e-05,
      "loss": 3.1749,
      "step": 18380
    },
    {
      "epoch": 2.5481502009145074,
      "grad_norm": 10.927729606628418,
      "learning_rate": 2.9814050159345992e-05,
      "loss": 2.2255,
      "step": 18390
    },
    {
      "epoch": 2.549535818207011,
      "grad_norm": 8.98062801361084,
      "learning_rate": 2.9808507690175978e-05,
      "loss": 2.5148,
      "step": 18400
    },
    {
      "epoch": 2.550921435499515,
      "grad_norm": 9.037629127502441,
      "learning_rate": 2.980296522100596e-05,
      "loss": 2.1991,
      "step": 18410
    },
    {
      "epoch": 2.5523070527920186,
      "grad_norm": 17.2246150970459,
      "learning_rate": 2.9797422751835948e-05,
      "loss": 2.8422,
      "step": 18420
    },
    {
      "epoch": 2.553692670084523,
      "grad_norm": 10.199365615844727,
      "learning_rate": 2.979188028266593e-05,
      "loss": 2.4828,
      "step": 18430
    },
    {
      "epoch": 2.5550782873770266,
      "grad_norm": 7.212472915649414,
      "learning_rate": 2.9786337813495912e-05,
      "loss": 2.3681,
      "step": 18440
    },
    {
      "epoch": 2.5564639046695303,
      "grad_norm": 7.576082229614258,
      "learning_rate": 2.97807953443259e-05,
      "loss": 2.0053,
      "step": 18450
    },
    {
      "epoch": 2.557849521962034,
      "grad_norm": 9.894791603088379,
      "learning_rate": 2.9775252875155886e-05,
      "loss": 2.2184,
      "step": 18460
    },
    {
      "epoch": 2.559235139254538,
      "grad_norm": 9.219325065612793,
      "learning_rate": 2.9769710405985868e-05,
      "loss": 2.5278,
      "step": 18470
    },
    {
      "epoch": 2.5606207565470416,
      "grad_norm": 6.154848098754883,
      "learning_rate": 2.9764167936815856e-05,
      "loss": 2.4705,
      "step": 18480
    },
    {
      "epoch": 2.5620063738395453,
      "grad_norm": 10.214339256286621,
      "learning_rate": 2.9758625467645838e-05,
      "loss": 2.5128,
      "step": 18490
    },
    {
      "epoch": 2.5633919911320495,
      "grad_norm": 10.467339515686035,
      "learning_rate": 2.9753082998475823e-05,
      "loss": 2.6248,
      "step": 18500
    },
    {
      "epoch": 2.5647776084245533,
      "grad_norm": 8.881903648376465,
      "learning_rate": 2.974754052930581e-05,
      "loss": 2.393,
      "step": 18510
    },
    {
      "epoch": 2.566163225717057,
      "grad_norm": 13.701312065124512,
      "learning_rate": 2.9741998060135794e-05,
      "loss": 2.6039,
      "step": 18520
    },
    {
      "epoch": 2.5675488430095608,
      "grad_norm": 16.054353713989258,
      "learning_rate": 2.9736455590965776e-05,
      "loss": 2.4928,
      "step": 18530
    },
    {
      "epoch": 2.5689344603020645,
      "grad_norm": 9.5592041015625,
      "learning_rate": 2.9730913121795764e-05,
      "loss": 2.5857,
      "step": 18540
    },
    {
      "epoch": 2.5703200775945683,
      "grad_norm": 9.888906478881836,
      "learning_rate": 2.9725370652625746e-05,
      "loss": 2.3468,
      "step": 18550
    },
    {
      "epoch": 2.571705694887072,
      "grad_norm": 13.111400604248047,
      "learning_rate": 2.971982818345573e-05,
      "loss": 2.4545,
      "step": 18560
    },
    {
      "epoch": 2.573091312179576,
      "grad_norm": 7.885849952697754,
      "learning_rate": 2.9714285714285717e-05,
      "loss": 2.5945,
      "step": 18570
    },
    {
      "epoch": 2.57447692947208,
      "grad_norm": 6.785879611968994,
      "learning_rate": 2.9708743245115702e-05,
      "loss": 2.6001,
      "step": 18580
    },
    {
      "epoch": 2.5758625467645837,
      "grad_norm": 11.19408893585205,
      "learning_rate": 2.9703200775945684e-05,
      "loss": 2.8269,
      "step": 18590
    },
    {
      "epoch": 2.5772481640570875,
      "grad_norm": 14.457737922668457,
      "learning_rate": 2.9697658306775673e-05,
      "loss": 2.2692,
      "step": 18600
    },
    {
      "epoch": 2.578633781349591,
      "grad_norm": 12.89730167388916,
      "learning_rate": 2.9692115837605654e-05,
      "loss": 1.8969,
      "step": 18610
    },
    {
      "epoch": 2.580019398642095,
      "grad_norm": 13.49263858795166,
      "learning_rate": 2.968657336843564e-05,
      "loss": 2.5511,
      "step": 18620
    },
    {
      "epoch": 2.5814050159345987,
      "grad_norm": 11.764554023742676,
      "learning_rate": 2.9681030899265625e-05,
      "loss": 2.5724,
      "step": 18630
    },
    {
      "epoch": 2.582790633227103,
      "grad_norm": 8.391739845275879,
      "learning_rate": 2.967548843009561e-05,
      "loss": 2.7136,
      "step": 18640
    },
    {
      "epoch": 2.5841762505196066,
      "grad_norm": 10.651941299438477,
      "learning_rate": 2.9669945960925595e-05,
      "loss": 2.3146,
      "step": 18650
    },
    {
      "epoch": 2.5855618678121104,
      "grad_norm": 12.093308448791504,
      "learning_rate": 2.966440349175558e-05,
      "loss": 2.1566,
      "step": 18660
    },
    {
      "epoch": 2.586947485104614,
      "grad_norm": 8.693252563476562,
      "learning_rate": 2.9658861022585563e-05,
      "loss": 2.8091,
      "step": 18670
    },
    {
      "epoch": 2.588333102397118,
      "grad_norm": 11.420647621154785,
      "learning_rate": 2.965331855341555e-05,
      "loss": 2.4399,
      "step": 18680
    },
    {
      "epoch": 2.5897187196896216,
      "grad_norm": 13.001436233520508,
      "learning_rate": 2.9647776084245533e-05,
      "loss": 2.3795,
      "step": 18690
    },
    {
      "epoch": 2.5911043369821254,
      "grad_norm": 8.654043197631836,
      "learning_rate": 2.9642233615075518e-05,
      "loss": 2.2704,
      "step": 18700
    },
    {
      "epoch": 2.5924899542746296,
      "grad_norm": 9.651684761047363,
      "learning_rate": 2.9636691145905504e-05,
      "loss": 2.0537,
      "step": 18710
    },
    {
      "epoch": 2.593875571567133,
      "grad_norm": 15.539101600646973,
      "learning_rate": 2.963114867673549e-05,
      "loss": 2.2361,
      "step": 18720
    },
    {
      "epoch": 2.595261188859637,
      "grad_norm": 11.860458374023438,
      "learning_rate": 2.962560620756547e-05,
      "loss": 1.9744,
      "step": 18730
    },
    {
      "epoch": 2.596646806152141,
      "grad_norm": 8.531838417053223,
      "learning_rate": 2.962006373839546e-05,
      "loss": 2.2445,
      "step": 18740
    },
    {
      "epoch": 2.5980324234446446,
      "grad_norm": 11.852973937988281,
      "learning_rate": 2.961452126922544e-05,
      "loss": 2.7995,
      "step": 18750
    },
    {
      "epoch": 2.5994180407371483,
      "grad_norm": 14.55167293548584,
      "learning_rate": 2.9608978800055426e-05,
      "loss": 2.3832,
      "step": 18760
    },
    {
      "epoch": 2.600803658029652,
      "grad_norm": 9.267720222473145,
      "learning_rate": 2.9603436330885415e-05,
      "loss": 2.4017,
      "step": 18770
    },
    {
      "epoch": 2.6021892753221563,
      "grad_norm": 12.67919635772705,
      "learning_rate": 2.9597893861715397e-05,
      "loss": 2.7818,
      "step": 18780
    },
    {
      "epoch": 2.6035748926146596,
      "grad_norm": 7.764073371887207,
      "learning_rate": 2.959235139254538e-05,
      "loss": 2.4822,
      "step": 18790
    },
    {
      "epoch": 2.6049605099071638,
      "grad_norm": 10.726007461547852,
      "learning_rate": 2.9586808923375367e-05,
      "loss": 2.0336,
      "step": 18800
    },
    {
      "epoch": 2.6063461271996675,
      "grad_norm": 8.95570182800293,
      "learning_rate": 2.958126645420535e-05,
      "loss": 2.5502,
      "step": 18810
    },
    {
      "epoch": 2.6077317444921713,
      "grad_norm": 10.809972763061523,
      "learning_rate": 2.9575723985035334e-05,
      "loss": 2.3295,
      "step": 18820
    },
    {
      "epoch": 2.609117361784675,
      "grad_norm": 10.626615524291992,
      "learning_rate": 2.9570181515865323e-05,
      "loss": 2.8714,
      "step": 18830
    },
    {
      "epoch": 2.6105029790771788,
      "grad_norm": 12.862190246582031,
      "learning_rate": 2.9564639046695305e-05,
      "loss": 2.901,
      "step": 18840
    },
    {
      "epoch": 2.611888596369683,
      "grad_norm": 14.420570373535156,
      "learning_rate": 2.9559096577525287e-05,
      "loss": 2.4543,
      "step": 18850
    },
    {
      "epoch": 2.6132742136621863,
      "grad_norm": 5.9937357902526855,
      "learning_rate": 2.9553554108355275e-05,
      "loss": 2.5579,
      "step": 18860
    },
    {
      "epoch": 2.6146598309546905,
      "grad_norm": 6.694873809814453,
      "learning_rate": 2.9548011639185257e-05,
      "loss": 2.4465,
      "step": 18870
    },
    {
      "epoch": 2.616045448247194,
      "grad_norm": 11.664791107177734,
      "learning_rate": 2.9542469170015243e-05,
      "loss": 2.864,
      "step": 18880
    },
    {
      "epoch": 2.617431065539698,
      "grad_norm": 12.318331718444824,
      "learning_rate": 2.953692670084523e-05,
      "loss": 2.6404,
      "step": 18890
    },
    {
      "epoch": 2.6188166828322017,
      "grad_norm": 9.197300910949707,
      "learning_rate": 2.9531384231675213e-05,
      "loss": 2.1205,
      "step": 18900
    },
    {
      "epoch": 2.6202023001247055,
      "grad_norm": 11.20983600616455,
      "learning_rate": 2.9525841762505195e-05,
      "loss": 2.3112,
      "step": 18910
    },
    {
      "epoch": 2.621587917417209,
      "grad_norm": 12.993922233581543,
      "learning_rate": 2.9520299293335184e-05,
      "loss": 2.3742,
      "step": 18920
    },
    {
      "epoch": 2.622973534709713,
      "grad_norm": 7.392845630645752,
      "learning_rate": 2.951475682416517e-05,
      "loss": 2.1593,
      "step": 18930
    },
    {
      "epoch": 2.624359152002217,
      "grad_norm": 13.993948936462402,
      "learning_rate": 2.9509214354995154e-05,
      "loss": 2.5033,
      "step": 18940
    },
    {
      "epoch": 2.625744769294721,
      "grad_norm": 10.698718070983887,
      "learning_rate": 2.950367188582514e-05,
      "loss": 2.1976,
      "step": 18950
    },
    {
      "epoch": 2.6271303865872246,
      "grad_norm": 10.577301979064941,
      "learning_rate": 2.949812941665512e-05,
      "loss": 2.4144,
      "step": 18960
    },
    {
      "epoch": 2.6285160038797284,
      "grad_norm": 16.584077835083008,
      "learning_rate": 2.949258694748511e-05,
      "loss": 2.9956,
      "step": 18970
    },
    {
      "epoch": 2.629901621172232,
      "grad_norm": 7.114412307739258,
      "learning_rate": 2.9487044478315092e-05,
      "loss": 1.9779,
      "step": 18980
    },
    {
      "epoch": 2.631287238464736,
      "grad_norm": 8.028803825378418,
      "learning_rate": 2.9481502009145077e-05,
      "loss": 2.4187,
      "step": 18990
    },
    {
      "epoch": 2.6326728557572396,
      "grad_norm": 9.40269660949707,
      "learning_rate": 2.9475959539975062e-05,
      "loss": 2.8689,
      "step": 19000
    },
    {
      "epoch": 2.634058473049744,
      "grad_norm": 8.064932823181152,
      "learning_rate": 2.9470417070805047e-05,
      "loss": 2.3088,
      "step": 19010
    },
    {
      "epoch": 2.6354440903422476,
      "grad_norm": 9.120002746582031,
      "learning_rate": 2.946487460163503e-05,
      "loss": 2.2475,
      "step": 19020
    },
    {
      "epoch": 2.6368297076347513,
      "grad_norm": 7.809818267822266,
      "learning_rate": 2.9459332132465018e-05,
      "loss": 2.2326,
      "step": 19030
    },
    {
      "epoch": 2.638215324927255,
      "grad_norm": 12.630860328674316,
      "learning_rate": 2.9453789663295e-05,
      "loss": 2.1218,
      "step": 19040
    },
    {
      "epoch": 2.639600942219759,
      "grad_norm": 13.561924934387207,
      "learning_rate": 2.9448247194124985e-05,
      "loss": 2.6928,
      "step": 19050
    },
    {
      "epoch": 2.6409865595122626,
      "grad_norm": 10.767511367797852,
      "learning_rate": 2.944270472495497e-05,
      "loss": 2.4789,
      "step": 19060
    },
    {
      "epoch": 2.6423721768047663,
      "grad_norm": 12.040867805480957,
      "learning_rate": 2.9437162255784956e-05,
      "loss": 2.243,
      "step": 19070
    },
    {
      "epoch": 2.6437577940972705,
      "grad_norm": 8.17800521850586,
      "learning_rate": 2.9431619786614937e-05,
      "loss": 2.3692,
      "step": 19080
    },
    {
      "epoch": 2.6451434113897743,
      "grad_norm": 18.384347915649414,
      "learning_rate": 2.9426077317444926e-05,
      "loss": 1.9717,
      "step": 19090
    },
    {
      "epoch": 2.646529028682278,
      "grad_norm": 20.881427764892578,
      "learning_rate": 2.9420534848274908e-05,
      "loss": 2.3794,
      "step": 19100
    },
    {
      "epoch": 2.6479146459747818,
      "grad_norm": 7.434497833251953,
      "learning_rate": 2.9414992379104893e-05,
      "loss": 1.9954,
      "step": 19110
    },
    {
      "epoch": 2.6493002632672855,
      "grad_norm": 10.385059356689453,
      "learning_rate": 2.940944990993488e-05,
      "loss": 2.3786,
      "step": 19120
    },
    {
      "epoch": 2.6506858805597893,
      "grad_norm": 6.744446277618408,
      "learning_rate": 2.9403907440764864e-05,
      "loss": 2.155,
      "step": 19130
    },
    {
      "epoch": 2.652071497852293,
      "grad_norm": 6.06703519821167,
      "learning_rate": 2.9398364971594846e-05,
      "loss": 2.1954,
      "step": 19140
    },
    {
      "epoch": 2.653457115144797,
      "grad_norm": 16.026805877685547,
      "learning_rate": 2.9392822502424834e-05,
      "loss": 2.2818,
      "step": 19150
    },
    {
      "epoch": 2.654842732437301,
      "grad_norm": 9.374561309814453,
      "learning_rate": 2.9387280033254816e-05,
      "loss": 2.3176,
      "step": 19160
    },
    {
      "epoch": 2.6562283497298047,
      "grad_norm": 15.12103271484375,
      "learning_rate": 2.93817375640848e-05,
      "loss": 2.5429,
      "step": 19170
    },
    {
      "epoch": 2.6576139670223085,
      "grad_norm": 7.739707946777344,
      "learning_rate": 2.9376195094914787e-05,
      "loss": 2.2411,
      "step": 19180
    },
    {
      "epoch": 2.658999584314812,
      "grad_norm": 12.734787940979004,
      "learning_rate": 2.9370652625744772e-05,
      "loss": 2.5079,
      "step": 19190
    },
    {
      "epoch": 2.660385201607316,
      "grad_norm": 14.268142700195312,
      "learning_rate": 2.9365110156574754e-05,
      "loss": 2.5539,
      "step": 19200
    },
    {
      "epoch": 2.6617708188998197,
      "grad_norm": 12.05359172821045,
      "learning_rate": 2.9359567687404742e-05,
      "loss": 2.3008,
      "step": 19210
    },
    {
      "epoch": 2.663156436192324,
      "grad_norm": 8.553901672363281,
      "learning_rate": 2.9354025218234724e-05,
      "loss": 1.9542,
      "step": 19220
    },
    {
      "epoch": 2.6645420534848276,
      "grad_norm": 7.3893141746521,
      "learning_rate": 2.9348482749064713e-05,
      "loss": 2.2003,
      "step": 19230
    },
    {
      "epoch": 2.6659276707773314,
      "grad_norm": 7.755525588989258,
      "learning_rate": 2.9342940279894695e-05,
      "loss": 2.7472,
      "step": 19240
    },
    {
      "epoch": 2.667313288069835,
      "grad_norm": 11.367480278015137,
      "learning_rate": 2.933739781072468e-05,
      "loss": 1.7232,
      "step": 19250
    },
    {
      "epoch": 2.668698905362339,
      "grad_norm": 9.15428352355957,
      "learning_rate": 2.933185534155467e-05,
      "loss": 2.403,
      "step": 19260
    },
    {
      "epoch": 2.6700845226548426,
      "grad_norm": 12.058780670166016,
      "learning_rate": 2.932631287238465e-05,
      "loss": 3.0165,
      "step": 19270
    },
    {
      "epoch": 2.6714701399473464,
      "grad_norm": 11.127094268798828,
      "learning_rate": 2.9320770403214632e-05,
      "loss": 2.4114,
      "step": 19280
    },
    {
      "epoch": 2.6728557572398506,
      "grad_norm": 10.234183311462402,
      "learning_rate": 2.931522793404462e-05,
      "loss": 2.0409,
      "step": 19290
    },
    {
      "epoch": 2.674241374532354,
      "grad_norm": 14.937939643859863,
      "learning_rate": 2.9309685464874603e-05,
      "loss": 2.4393,
      "step": 19300
    },
    {
      "epoch": 2.675626991824858,
      "grad_norm": 7.731569290161133,
      "learning_rate": 2.9304142995704588e-05,
      "loss": 2.0391,
      "step": 19310
    },
    {
      "epoch": 2.677012609117362,
      "grad_norm": 11.215995788574219,
      "learning_rate": 2.9298600526534577e-05,
      "loss": 2.801,
      "step": 19320
    },
    {
      "epoch": 2.6783982264098656,
      "grad_norm": 8.642868041992188,
      "learning_rate": 2.929305805736456e-05,
      "loss": 2.5396,
      "step": 19330
    },
    {
      "epoch": 2.6797838437023693,
      "grad_norm": 11.137104034423828,
      "learning_rate": 2.928751558819454e-05,
      "loss": 2.2415,
      "step": 19340
    },
    {
      "epoch": 2.681169460994873,
      "grad_norm": 12.691980361938477,
      "learning_rate": 2.928197311902453e-05,
      "loss": 2.3753,
      "step": 19350
    },
    {
      "epoch": 2.6825550782873773,
      "grad_norm": 13.361161231994629,
      "learning_rate": 2.9276430649854514e-05,
      "loss": 2.6014,
      "step": 19360
    },
    {
      "epoch": 2.6839406955798806,
      "grad_norm": 11.833600997924805,
      "learning_rate": 2.9270888180684496e-05,
      "loss": 2.8951,
      "step": 19370
    },
    {
      "epoch": 2.6853263128723848,
      "grad_norm": 11.726247787475586,
      "learning_rate": 2.9265345711514485e-05,
      "loss": 2.1795,
      "step": 19380
    },
    {
      "epoch": 2.6867119301648885,
      "grad_norm": 10.753046989440918,
      "learning_rate": 2.9259803242344467e-05,
      "loss": 2.3751,
      "step": 19390
    },
    {
      "epoch": 2.6880975474573923,
      "grad_norm": 5.381998538970947,
      "learning_rate": 2.925426077317445e-05,
      "loss": 2.2351,
      "step": 19400
    },
    {
      "epoch": 2.689483164749896,
      "grad_norm": 11.084362030029297,
      "learning_rate": 2.9248718304004437e-05,
      "loss": 2.8021,
      "step": 19410
    },
    {
      "epoch": 2.6908687820423998,
      "grad_norm": 14.511907577514648,
      "learning_rate": 2.9243175834834422e-05,
      "loss": 2.3402,
      "step": 19420
    },
    {
      "epoch": 2.692254399334904,
      "grad_norm": 9.938057899475098,
      "learning_rate": 2.9237633365664404e-05,
      "loss": 2.8135,
      "step": 19430
    },
    {
      "epoch": 2.6936400166274073,
      "grad_norm": 10.70934772491455,
      "learning_rate": 2.9232090896494393e-05,
      "loss": 2.4071,
      "step": 19440
    },
    {
      "epoch": 2.6950256339199115,
      "grad_norm": 10.128851890563965,
      "learning_rate": 2.9226548427324375e-05,
      "loss": 2.1554,
      "step": 19450
    },
    {
      "epoch": 2.696411251212415,
      "grad_norm": 12.476244926452637,
      "learning_rate": 2.9221005958154357e-05,
      "loss": 2.5167,
      "step": 19460
    },
    {
      "epoch": 2.697796868504919,
      "grad_norm": 13.876054763793945,
      "learning_rate": 2.9215463488984345e-05,
      "loss": 2.3373,
      "step": 19470
    },
    {
      "epoch": 2.6991824857974227,
      "grad_norm": 13.872900009155273,
      "learning_rate": 2.920992101981433e-05,
      "loss": 2.3969,
      "step": 19480
    },
    {
      "epoch": 2.7005681030899265,
      "grad_norm": 11.321188926696777,
      "learning_rate": 2.9204378550644312e-05,
      "loss": 1.848,
      "step": 19490
    },
    {
      "epoch": 2.7019537203824306,
      "grad_norm": 8.402496337890625,
      "learning_rate": 2.91988360814743e-05,
      "loss": 2.4519,
      "step": 19500
    },
    {
      "epoch": 2.703339337674934,
      "grad_norm": 12.793472290039062,
      "learning_rate": 2.9193293612304283e-05,
      "loss": 2.1921,
      "step": 19510
    },
    {
      "epoch": 2.704724954967438,
      "grad_norm": 11.829218864440918,
      "learning_rate": 2.918775114313427e-05,
      "loss": 2.526,
      "step": 19520
    },
    {
      "epoch": 2.706110572259942,
      "grad_norm": 10.405355453491211,
      "learning_rate": 2.9182208673964253e-05,
      "loss": 2.579,
      "step": 19530
    },
    {
      "epoch": 2.7074961895524456,
      "grad_norm": 10.882805824279785,
      "learning_rate": 2.917666620479424e-05,
      "loss": 2.4294,
      "step": 19540
    },
    {
      "epoch": 2.7088818068449494,
      "grad_norm": 10.379504203796387,
      "learning_rate": 2.9171123735624224e-05,
      "loss": 1.4984,
      "step": 19550
    },
    {
      "epoch": 2.710267424137453,
      "grad_norm": 14.218513488769531,
      "learning_rate": 2.916558126645421e-05,
      "loss": 2.5094,
      "step": 19560
    },
    {
      "epoch": 2.711653041429957,
      "grad_norm": 12.151850700378418,
      "learning_rate": 2.916003879728419e-05,
      "loss": 2.6499,
      "step": 19570
    },
    {
      "epoch": 2.7130386587224606,
      "grad_norm": 10.41191291809082,
      "learning_rate": 2.915449632811418e-05,
      "loss": 2.1881,
      "step": 19580
    },
    {
      "epoch": 2.714424276014965,
      "grad_norm": 11.213546752929688,
      "learning_rate": 2.914895385894416e-05,
      "loss": 2.5602,
      "step": 19590
    },
    {
      "epoch": 2.7158098933074686,
      "grad_norm": 16.64159393310547,
      "learning_rate": 2.9143411389774147e-05,
      "loss": 2.2879,
      "step": 19600
    },
    {
      "epoch": 2.7171955105999723,
      "grad_norm": 10.472146987915039,
      "learning_rate": 2.9137868920604132e-05,
      "loss": 2.7208,
      "step": 19610
    },
    {
      "epoch": 2.718581127892476,
      "grad_norm": 10.122679710388184,
      "learning_rate": 2.9132326451434117e-05,
      "loss": 2.1271,
      "step": 19620
    },
    {
      "epoch": 2.71996674518498,
      "grad_norm": 9.390633583068848,
      "learning_rate": 2.91267839822641e-05,
      "loss": 2.2376,
      "step": 19630
    },
    {
      "epoch": 2.7213523624774836,
      "grad_norm": 14.822667121887207,
      "learning_rate": 2.9121241513094088e-05,
      "loss": 2.6262,
      "step": 19640
    },
    {
      "epoch": 2.7227379797699873,
      "grad_norm": 10.989548683166504,
      "learning_rate": 2.911569904392407e-05,
      "loss": 2.4533,
      "step": 19650
    },
    {
      "epoch": 2.7241235970624915,
      "grad_norm": 17.96879005432129,
      "learning_rate": 2.9110156574754055e-05,
      "loss": 2.3929,
      "step": 19660
    },
    {
      "epoch": 2.7255092143549953,
      "grad_norm": 10.44373607635498,
      "learning_rate": 2.910461410558404e-05,
      "loss": 2.3214,
      "step": 19670
    },
    {
      "epoch": 2.726894831647499,
      "grad_norm": 9.387604713439941,
      "learning_rate": 2.9099071636414025e-05,
      "loss": 2.6103,
      "step": 19680
    },
    {
      "epoch": 2.7282804489400028,
      "grad_norm": 13.05915641784668,
      "learning_rate": 2.9093529167244007e-05,
      "loss": 2.5111,
      "step": 19690
    },
    {
      "epoch": 2.7296660662325065,
      "grad_norm": 16.264009475708008,
      "learning_rate": 2.9087986698073996e-05,
      "loss": 2.5697,
      "step": 19700
    },
    {
      "epoch": 2.7310516835250103,
      "grad_norm": 7.914224147796631,
      "learning_rate": 2.9082444228903978e-05,
      "loss": 2.6425,
      "step": 19710
    },
    {
      "epoch": 2.732437300817514,
      "grad_norm": 8.218631744384766,
      "learning_rate": 2.9076901759733963e-05,
      "loss": 2.7803,
      "step": 19720
    },
    {
      "epoch": 2.733822918110018,
      "grad_norm": 14.931734085083008,
      "learning_rate": 2.9071359290563948e-05,
      "loss": 2.6795,
      "step": 19730
    },
    {
      "epoch": 2.735208535402522,
      "grad_norm": 12.188051223754883,
      "learning_rate": 2.9065816821393933e-05,
      "loss": 2.4892,
      "step": 19740
    },
    {
      "epoch": 2.7365941526950257,
      "grad_norm": 11.246580123901367,
      "learning_rate": 2.9060274352223915e-05,
      "loss": 2.3577,
      "step": 19750
    },
    {
      "epoch": 2.7379797699875295,
      "grad_norm": 12.348184585571289,
      "learning_rate": 2.9054731883053904e-05,
      "loss": 2.1826,
      "step": 19760
    },
    {
      "epoch": 2.739365387280033,
      "grad_norm": 16.446653366088867,
      "learning_rate": 2.9049189413883886e-05,
      "loss": 2.6921,
      "step": 19770
    },
    {
      "epoch": 2.740751004572537,
      "grad_norm": 11.7190523147583,
      "learning_rate": 2.904364694471387e-05,
      "loss": 1.9481,
      "step": 19780
    },
    {
      "epoch": 2.7421366218650407,
      "grad_norm": 17.38855743408203,
      "learning_rate": 2.9038104475543856e-05,
      "loss": 2.396,
      "step": 19790
    },
    {
      "epoch": 2.743522239157545,
      "grad_norm": 27.663686752319336,
      "learning_rate": 2.903256200637384e-05,
      "loss": 2.6229,
      "step": 19800
    },
    {
      "epoch": 2.7449078564500486,
      "grad_norm": 12.664624214172363,
      "learning_rate": 2.902701953720383e-05,
      "loss": 1.9797,
      "step": 19810
    },
    {
      "epoch": 2.7462934737425524,
      "grad_norm": 6.472981929779053,
      "learning_rate": 2.9021477068033812e-05,
      "loss": 2.2891,
      "step": 19820
    },
    {
      "epoch": 2.747679091035056,
      "grad_norm": 10.510619163513184,
      "learning_rate": 2.9015934598863794e-05,
      "loss": 2.7289,
      "step": 19830
    },
    {
      "epoch": 2.74906470832756,
      "grad_norm": 7.587552070617676,
      "learning_rate": 2.9010392129693782e-05,
      "loss": 2.0653,
      "step": 19840
    },
    {
      "epoch": 2.7504503256200636,
      "grad_norm": 14.296626091003418,
      "learning_rate": 2.9004849660523768e-05,
      "loss": 2.4469,
      "step": 19850
    },
    {
      "epoch": 2.7518359429125674,
      "grad_norm": 7.342351913452148,
      "learning_rate": 2.899930719135375e-05,
      "loss": 2.755,
      "step": 19860
    },
    {
      "epoch": 2.7532215602050716,
      "grad_norm": 7.296130180358887,
      "learning_rate": 2.8993764722183738e-05,
      "loss": 2.5022,
      "step": 19870
    },
    {
      "epoch": 2.7546071774975753,
      "grad_norm": 9.101055145263672,
      "learning_rate": 2.898822225301372e-05,
      "loss": 2.3941,
      "step": 19880
    },
    {
      "epoch": 2.755992794790079,
      "grad_norm": 9.410805702209473,
      "learning_rate": 2.8982679783843702e-05,
      "loss": 2.8448,
      "step": 19890
    },
    {
      "epoch": 2.757378412082583,
      "grad_norm": 12.319916725158691,
      "learning_rate": 2.897713731467369e-05,
      "loss": 2.4693,
      "step": 19900
    },
    {
      "epoch": 2.7587640293750866,
      "grad_norm": 9.818327903747559,
      "learning_rate": 2.8971594845503676e-05,
      "loss": 2.0476,
      "step": 19910
    },
    {
      "epoch": 2.7601496466675903,
      "grad_norm": 5.61179780960083,
      "learning_rate": 2.8966052376333658e-05,
      "loss": 2.567,
      "step": 19920
    },
    {
      "epoch": 2.761535263960094,
      "grad_norm": 10.523404121398926,
      "learning_rate": 2.8960509907163646e-05,
      "loss": 2.5312,
      "step": 19930
    },
    {
      "epoch": 2.7629208812525983,
      "grad_norm": 11.02111530303955,
      "learning_rate": 2.8954967437993628e-05,
      "loss": 2.3281,
      "step": 19940
    },
    {
      "epoch": 2.7643064985451016,
      "grad_norm": 9.267327308654785,
      "learning_rate": 2.894942496882361e-05,
      "loss": 2.4246,
      "step": 19950
    },
    {
      "epoch": 2.7656921158376058,
      "grad_norm": 8.877650260925293,
      "learning_rate": 2.89438824996536e-05,
      "loss": 2.2207,
      "step": 19960
    },
    {
      "epoch": 2.7670777331301095,
      "grad_norm": 8.307772636413574,
      "learning_rate": 2.8938340030483584e-05,
      "loss": 2.6091,
      "step": 19970
    },
    {
      "epoch": 2.7684633504226133,
      "grad_norm": 10.665313720703125,
      "learning_rate": 2.8932797561313566e-05,
      "loss": 2.7986,
      "step": 19980
    },
    {
      "epoch": 2.769848967715117,
      "grad_norm": 11.9450101852417,
      "learning_rate": 2.8927255092143554e-05,
      "loss": 2.2181,
      "step": 19990
    },
    {
      "epoch": 2.7712345850076208,
      "grad_norm": 15.965169906616211,
      "learning_rate": 2.8921712622973536e-05,
      "loss": 2.4527,
      "step": 20000
    },
    {
      "epoch": 2.772620202300125,
      "grad_norm": 13.66320514678955,
      "learning_rate": 2.891617015380352e-05,
      "loss": 2.6041,
      "step": 20010
    },
    {
      "epoch": 2.7740058195926283,
      "grad_norm": 14.309725761413574,
      "learning_rate": 2.8910627684633507e-05,
      "loss": 2.7352,
      "step": 20020
    },
    {
      "epoch": 2.7753914368851325,
      "grad_norm": 19.836223602294922,
      "learning_rate": 2.8905085215463492e-05,
      "loss": 2.2099,
      "step": 20030
    },
    {
      "epoch": 2.776777054177636,
      "grad_norm": 15.591846466064453,
      "learning_rate": 2.8899542746293474e-05,
      "loss": 2.1726,
      "step": 20040
    },
    {
      "epoch": 2.77816267147014,
      "grad_norm": 13.072507858276367,
      "learning_rate": 2.8894000277123463e-05,
      "loss": 2.8226,
      "step": 20050
    },
    {
      "epoch": 2.7795482887626437,
      "grad_norm": 15.291146278381348,
      "learning_rate": 2.8888457807953444e-05,
      "loss": 2.4775,
      "step": 20060
    },
    {
      "epoch": 2.7809339060551475,
      "grad_norm": 8.026991844177246,
      "learning_rate": 2.888291533878343e-05,
      "loss": 2.5596,
      "step": 20070
    },
    {
      "epoch": 2.7823195233476516,
      "grad_norm": 14.032660484313965,
      "learning_rate": 2.8877372869613415e-05,
      "loss": 2.4305,
      "step": 20080
    },
    {
      "epoch": 2.783705140640155,
      "grad_norm": 8.769218444824219,
      "learning_rate": 2.88718304004434e-05,
      "loss": 2.1945,
      "step": 20090
    },
    {
      "epoch": 2.785090757932659,
      "grad_norm": 8.679795265197754,
      "learning_rate": 2.8866287931273385e-05,
      "loss": 2.2564,
      "step": 20100
    },
    {
      "epoch": 2.786476375225163,
      "grad_norm": 9.239171028137207,
      "learning_rate": 2.886074546210337e-05,
      "loss": 2.5497,
      "step": 20110
    },
    {
      "epoch": 2.7878619925176666,
      "grad_norm": 17.272764205932617,
      "learning_rate": 2.8855202992933353e-05,
      "loss": 2.1986,
      "step": 20120
    },
    {
      "epoch": 2.7892476098101704,
      "grad_norm": 12.447118759155273,
      "learning_rate": 2.884966052376334e-05,
      "loss": 2.6674,
      "step": 20130
    },
    {
      "epoch": 2.790633227102674,
      "grad_norm": 19.401594161987305,
      "learning_rate": 2.8844118054593323e-05,
      "loss": 2.5506,
      "step": 20140
    },
    {
      "epoch": 2.792018844395178,
      "grad_norm": 13.108048439025879,
      "learning_rate": 2.8838575585423308e-05,
      "loss": 2.5848,
      "step": 20150
    },
    {
      "epoch": 2.7934044616876816,
      "grad_norm": 7.2194037437438965,
      "learning_rate": 2.8833033116253294e-05,
      "loss": 2.0722,
      "step": 20160
    },
    {
      "epoch": 2.794790078980186,
      "grad_norm": 6.9764275550842285,
      "learning_rate": 2.882749064708328e-05,
      "loss": 2.1936,
      "step": 20170
    },
    {
      "epoch": 2.7961756962726896,
      "grad_norm": 11.195098876953125,
      "learning_rate": 2.882194817791326e-05,
      "loss": 2.2577,
      "step": 20180
    },
    {
      "epoch": 2.7975613135651933,
      "grad_norm": 10.919931411743164,
      "learning_rate": 2.881640570874325e-05,
      "loss": 2.2123,
      "step": 20190
    },
    {
      "epoch": 2.798946930857697,
      "grad_norm": 12.526541709899902,
      "learning_rate": 2.881086323957323e-05,
      "loss": 1.9246,
      "step": 20200
    },
    {
      "epoch": 2.800332548150201,
      "grad_norm": 16.608604431152344,
      "learning_rate": 2.8805320770403216e-05,
      "loss": 2.6575,
      "step": 20210
    },
    {
      "epoch": 2.8017181654427046,
      "grad_norm": 10.295538902282715,
      "learning_rate": 2.87997783012332e-05,
      "loss": 2.5665,
      "step": 20220
    },
    {
      "epoch": 2.8031037827352083,
      "grad_norm": 10.384454727172852,
      "learning_rate": 2.8794235832063187e-05,
      "loss": 1.9914,
      "step": 20230
    },
    {
      "epoch": 2.8044894000277125,
      "grad_norm": 8.651732444763184,
      "learning_rate": 2.878869336289317e-05,
      "loss": 2.4215,
      "step": 20240
    },
    {
      "epoch": 2.8058750173202163,
      "grad_norm": 9.20456600189209,
      "learning_rate": 2.8783150893723157e-05,
      "loss": 2.5416,
      "step": 20250
    },
    {
      "epoch": 2.80726063461272,
      "grad_norm": 7.611724376678467,
      "learning_rate": 2.877760842455314e-05,
      "loss": 2.3496,
      "step": 20260
    },
    {
      "epoch": 2.8086462519052238,
      "grad_norm": 6.294947624206543,
      "learning_rate": 2.8772065955383124e-05,
      "loss": 2.3991,
      "step": 20270
    },
    {
      "epoch": 2.8100318691977275,
      "grad_norm": 15.22191047668457,
      "learning_rate": 2.8766523486213113e-05,
      "loss": 2.5433,
      "step": 20280
    },
    {
      "epoch": 2.8114174864902313,
      "grad_norm": 13.163363456726074,
      "learning_rate": 2.8760981017043095e-05,
      "loss": 2.251,
      "step": 20290
    },
    {
      "epoch": 2.812803103782735,
      "grad_norm": 7.061553955078125,
      "learning_rate": 2.8755438547873077e-05,
      "loss": 2.3817,
      "step": 20300
    },
    {
      "epoch": 2.814188721075239,
      "grad_norm": 16.492094039916992,
      "learning_rate": 2.8749896078703065e-05,
      "loss": 2.7636,
      "step": 20310
    },
    {
      "epoch": 2.815574338367743,
      "grad_norm": 7.57879638671875,
      "learning_rate": 2.8744353609533047e-05,
      "loss": 2.3163,
      "step": 20320
    },
    {
      "epoch": 2.8169599556602467,
      "grad_norm": 14.648384094238281,
      "learning_rate": 2.8738811140363033e-05,
      "loss": 2.2384,
      "step": 20330
    },
    {
      "epoch": 2.8183455729527505,
      "grad_norm": 8.5714693069458,
      "learning_rate": 2.873326867119302e-05,
      "loss": 2.3051,
      "step": 20340
    },
    {
      "epoch": 2.819731190245254,
      "grad_norm": 16.378751754760742,
      "learning_rate": 2.8727726202023003e-05,
      "loss": 2.6992,
      "step": 20350
    },
    {
      "epoch": 2.821116807537758,
      "grad_norm": 11.981922149658203,
      "learning_rate": 2.8722183732852985e-05,
      "loss": 2.2286,
      "step": 20360
    },
    {
      "epoch": 2.8225024248302617,
      "grad_norm": 13.86722469329834,
      "learning_rate": 2.8716641263682974e-05,
      "loss": 1.9378,
      "step": 20370
    },
    {
      "epoch": 2.823888042122766,
      "grad_norm": 19.23330307006836,
      "learning_rate": 2.8711098794512955e-05,
      "loss": 2.4388,
      "step": 20380
    },
    {
      "epoch": 2.8252736594152696,
      "grad_norm": 9.645398139953613,
      "learning_rate": 2.8705556325342944e-05,
      "loss": 2.4755,
      "step": 20390
    },
    {
      "epoch": 2.8266592767077734,
      "grad_norm": 13.994779586791992,
      "learning_rate": 2.870001385617293e-05,
      "loss": 2.6793,
      "step": 20400
    },
    {
      "epoch": 2.828044894000277,
      "grad_norm": 8.136711120605469,
      "learning_rate": 2.869447138700291e-05,
      "loss": 2.1739,
      "step": 20410
    },
    {
      "epoch": 2.829430511292781,
      "grad_norm": 10.893265724182129,
      "learning_rate": 2.86889289178329e-05,
      "loss": 2.1912,
      "step": 20420
    },
    {
      "epoch": 2.8308161285852846,
      "grad_norm": 17.7503662109375,
      "learning_rate": 2.8683386448662882e-05,
      "loss": 2.5708,
      "step": 20430
    },
    {
      "epoch": 2.8322017458777884,
      "grad_norm": 11.094133377075195,
      "learning_rate": 2.8677843979492867e-05,
      "loss": 2.5931,
      "step": 20440
    },
    {
      "epoch": 2.8335873631702926,
      "grad_norm": 17.311336517333984,
      "learning_rate": 2.8672301510322852e-05,
      "loss": 2.6436,
      "step": 20450
    },
    {
      "epoch": 2.8349729804627963,
      "grad_norm": 18.044843673706055,
      "learning_rate": 2.8666759041152837e-05,
      "loss": 2.0884,
      "step": 20460
    },
    {
      "epoch": 2.8363585977553,
      "grad_norm": 16.64591407775879,
      "learning_rate": 2.866121657198282e-05,
      "loss": 2.8819,
      "step": 20470
    },
    {
      "epoch": 2.837744215047804,
      "grad_norm": 11.992178916931152,
      "learning_rate": 2.8655674102812808e-05,
      "loss": 2.4378,
      "step": 20480
    },
    {
      "epoch": 2.8391298323403076,
      "grad_norm": 17.14875602722168,
      "learning_rate": 2.865013163364279e-05,
      "loss": 1.9077,
      "step": 20490
    },
    {
      "epoch": 2.8405154496328113,
      "grad_norm": 10.523604393005371,
      "learning_rate": 2.8644589164472775e-05,
      "loss": 2.4057,
      "step": 20500
    },
    {
      "epoch": 2.841901066925315,
      "grad_norm": 13.772650718688965,
      "learning_rate": 2.863904669530276e-05,
      "loss": 2.3633,
      "step": 20510
    },
    {
      "epoch": 2.8432866842178193,
      "grad_norm": 10.545575141906738,
      "learning_rate": 2.8633504226132746e-05,
      "loss": 2.9855,
      "step": 20520
    },
    {
      "epoch": 2.8446723015103226,
      "grad_norm": 11.676778793334961,
      "learning_rate": 2.8627961756962727e-05,
      "loss": 2.4018,
      "step": 20530
    },
    {
      "epoch": 2.8460579188028268,
      "grad_norm": 16.464139938354492,
      "learning_rate": 2.8622419287792716e-05,
      "loss": 2.7186,
      "step": 20540
    },
    {
      "epoch": 2.8474435360953305,
      "grad_norm": 8.921801567077637,
      "learning_rate": 2.8616876818622698e-05,
      "loss": 2.5083,
      "step": 20550
    },
    {
      "epoch": 2.8488291533878343,
      "grad_norm": 7.896288871765137,
      "learning_rate": 2.8611334349452683e-05,
      "loss": 2.2046,
      "step": 20560
    },
    {
      "epoch": 2.850214770680338,
      "grad_norm": 10.645797729492188,
      "learning_rate": 2.860579188028267e-05,
      "loss": 2.388,
      "step": 20570
    },
    {
      "epoch": 2.8516003879728418,
      "grad_norm": 9.071394920349121,
      "learning_rate": 2.8600249411112654e-05,
      "loss": 2.3007,
      "step": 20580
    },
    {
      "epoch": 2.852986005265346,
      "grad_norm": 12.25483226776123,
      "learning_rate": 2.8594706941942636e-05,
      "loss": 2.5839,
      "step": 20590
    },
    {
      "epoch": 2.8543716225578493,
      "grad_norm": 9.865202903747559,
      "learning_rate": 2.8589164472772624e-05,
      "loss": 1.8004,
      "step": 20600
    },
    {
      "epoch": 2.8557572398503535,
      "grad_norm": 22.14539909362793,
      "learning_rate": 2.8583622003602606e-05,
      "loss": 2.3818,
      "step": 20610
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 18.06641387939453,
      "learning_rate": 2.857807953443259e-05,
      "loss": 2.7673,
      "step": 20620
    },
    {
      "epoch": 2.858528474435361,
      "grad_norm": 12.014544486999512,
      "learning_rate": 2.8572537065262577e-05,
      "loss": 2.8078,
      "step": 20630
    },
    {
      "epoch": 2.8599140917278647,
      "grad_norm": 13.28575325012207,
      "learning_rate": 2.8566994596092562e-05,
      "loss": 2.5989,
      "step": 20640
    },
    {
      "epoch": 2.8612997090203685,
      "grad_norm": 6.151064395904541,
      "learning_rate": 2.8561452126922544e-05,
      "loss": 2.6093,
      "step": 20650
    },
    {
      "epoch": 2.8626853263128726,
      "grad_norm": 14.85142993927002,
      "learning_rate": 2.8555909657752532e-05,
      "loss": 2.7392,
      "step": 20660
    },
    {
      "epoch": 2.864070943605376,
      "grad_norm": 9.523777961730957,
      "learning_rate": 2.8550367188582514e-05,
      "loss": 2.4403,
      "step": 20670
    },
    {
      "epoch": 2.86545656089788,
      "grad_norm": 9.11623764038086,
      "learning_rate": 2.8544824719412503e-05,
      "loss": 2.1384,
      "step": 20680
    },
    {
      "epoch": 2.866842178190384,
      "grad_norm": 11.600807189941406,
      "learning_rate": 2.8539282250242485e-05,
      "loss": 1.9988,
      "step": 20690
    },
    {
      "epoch": 2.8682277954828876,
      "grad_norm": 7.710055828094482,
      "learning_rate": 2.853373978107247e-05,
      "loss": 2.1598,
      "step": 20700
    },
    {
      "epoch": 2.8696134127753914,
      "grad_norm": 10.9356050491333,
      "learning_rate": 2.852819731190246e-05,
      "loss": 2.337,
      "step": 20710
    },
    {
      "epoch": 2.870999030067895,
      "grad_norm": 13.697674751281738,
      "learning_rate": 2.852265484273244e-05,
      "loss": 2.4986,
      "step": 20720
    },
    {
      "epoch": 2.8723846473603993,
      "grad_norm": 7.586243629455566,
      "learning_rate": 2.8517112373562422e-05,
      "loss": 2.4295,
      "step": 20730
    },
    {
      "epoch": 2.8737702646529026,
      "grad_norm": 8.760944366455078,
      "learning_rate": 2.851156990439241e-05,
      "loss": 2.5057,
      "step": 20740
    },
    {
      "epoch": 2.875155881945407,
      "grad_norm": 7.966065883636475,
      "learning_rate": 2.8506027435222393e-05,
      "loss": 2.3025,
      "step": 20750
    },
    {
      "epoch": 2.8765414992379106,
      "grad_norm": 10.983858108520508,
      "learning_rate": 2.8500484966052378e-05,
      "loss": 2.8684,
      "step": 20760
    },
    {
      "epoch": 2.8779271165304143,
      "grad_norm": 15.52717113494873,
      "learning_rate": 2.8494942496882367e-05,
      "loss": 1.9202,
      "step": 20770
    },
    {
      "epoch": 2.879312733822918,
      "grad_norm": 9.84118938446045,
      "learning_rate": 2.848940002771235e-05,
      "loss": 2.1891,
      "step": 20780
    },
    {
      "epoch": 2.880698351115422,
      "grad_norm": 14.361778259277344,
      "learning_rate": 2.848385755854233e-05,
      "loss": 2.2483,
      "step": 20790
    },
    {
      "epoch": 2.8820839684079256,
      "grad_norm": 8.156375885009766,
      "learning_rate": 2.847831508937232e-05,
      "loss": 2.458,
      "step": 20800
    },
    {
      "epoch": 2.8834695857004293,
      "grad_norm": 9.27065658569336,
      "learning_rate": 2.84727726202023e-05,
      "loss": 2.1816,
      "step": 20810
    },
    {
      "epoch": 2.8848552029929335,
      "grad_norm": 6.518765449523926,
      "learning_rate": 2.8467230151032286e-05,
      "loss": 2.4093,
      "step": 20820
    },
    {
      "epoch": 2.8862408202854373,
      "grad_norm": 7.260569095611572,
      "learning_rate": 2.8461687681862275e-05,
      "loss": 2.4043,
      "step": 20830
    },
    {
      "epoch": 2.887626437577941,
      "grad_norm": 15.778969764709473,
      "learning_rate": 2.8456145212692257e-05,
      "loss": 2.5894,
      "step": 20840
    },
    {
      "epoch": 2.8890120548704448,
      "grad_norm": 10.455549240112305,
      "learning_rate": 2.845060274352224e-05,
      "loss": 3.0595,
      "step": 20850
    },
    {
      "epoch": 2.8903976721629485,
      "grad_norm": 6.886279106140137,
      "learning_rate": 2.8445060274352227e-05,
      "loss": 2.2379,
      "step": 20860
    },
    {
      "epoch": 2.8917832894554523,
      "grad_norm": 14.747968673706055,
      "learning_rate": 2.8439517805182212e-05,
      "loss": 2.7131,
      "step": 20870
    },
    {
      "epoch": 2.893168906747956,
      "grad_norm": 10.410295486450195,
      "learning_rate": 2.8433975336012194e-05,
      "loss": 2.4237,
      "step": 20880
    },
    {
      "epoch": 2.89455452404046,
      "grad_norm": 15.285572052001953,
      "learning_rate": 2.8428432866842183e-05,
      "loss": 2.455,
      "step": 20890
    },
    {
      "epoch": 2.895940141332964,
      "grad_norm": 11.263843536376953,
      "learning_rate": 2.8422890397672165e-05,
      "loss": 2.6243,
      "step": 20900
    },
    {
      "epoch": 2.8973257586254677,
      "grad_norm": 9.850135803222656,
      "learning_rate": 2.8417347928502147e-05,
      "loss": 1.9663,
      "step": 20910
    },
    {
      "epoch": 2.8987113759179715,
      "grad_norm": 16.11832046508789,
      "learning_rate": 2.8411805459332135e-05,
      "loss": 2.0537,
      "step": 20920
    },
    {
      "epoch": 2.900096993210475,
      "grad_norm": 10.35551643371582,
      "learning_rate": 2.840626299016212e-05,
      "loss": 2.3206,
      "step": 20930
    },
    {
      "epoch": 2.901482610502979,
      "grad_norm": 9.933387756347656,
      "learning_rate": 2.8400720520992102e-05,
      "loss": 2.3657,
      "step": 20940
    },
    {
      "epoch": 2.9028682277954827,
      "grad_norm": 10.641752243041992,
      "learning_rate": 2.839517805182209e-05,
      "loss": 2.2961,
      "step": 20950
    },
    {
      "epoch": 2.904253845087987,
      "grad_norm": 9.961933135986328,
      "learning_rate": 2.8389635582652073e-05,
      "loss": 2.5908,
      "step": 20960
    },
    {
      "epoch": 2.9056394623804906,
      "grad_norm": 10.522932052612305,
      "learning_rate": 2.838409311348206e-05,
      "loss": 2.5946,
      "step": 20970
    },
    {
      "epoch": 2.9070250796729944,
      "grad_norm": 15.1561918258667,
      "learning_rate": 2.8378550644312043e-05,
      "loss": 2.0667,
      "step": 20980
    },
    {
      "epoch": 2.908410696965498,
      "grad_norm": 12.061894416809082,
      "learning_rate": 2.837300817514203e-05,
      "loss": 2.12,
      "step": 20990
    },
    {
      "epoch": 2.909796314258002,
      "grad_norm": 11.189412117004395,
      "learning_rate": 2.8367465705972014e-05,
      "loss": 2.0842,
      "step": 21000
    },
    {
      "epoch": 2.9111819315505056,
      "grad_norm": 11.83027172088623,
      "learning_rate": 2.8361923236802e-05,
      "loss": 2.3387,
      "step": 21010
    },
    {
      "epoch": 2.9125675488430094,
      "grad_norm": 9.55556869506836,
      "learning_rate": 2.835638076763198e-05,
      "loss": 2.0596,
      "step": 21020
    },
    {
      "epoch": 2.9139531661355136,
      "grad_norm": 11.860064506530762,
      "learning_rate": 2.835083829846197e-05,
      "loss": 2.4142,
      "step": 21030
    },
    {
      "epoch": 2.9153387834280173,
      "grad_norm": 8.59759521484375,
      "learning_rate": 2.834529582929195e-05,
      "loss": 2.5496,
      "step": 21040
    },
    {
      "epoch": 2.916724400720521,
      "grad_norm": 13.500588417053223,
      "learning_rate": 2.8339753360121937e-05,
      "loss": 2.2539,
      "step": 21050
    },
    {
      "epoch": 2.918110018013025,
      "grad_norm": 11.626177787780762,
      "learning_rate": 2.8334210890951922e-05,
      "loss": 2.4115,
      "step": 21060
    },
    {
      "epoch": 2.9194956353055286,
      "grad_norm": 9.140294075012207,
      "learning_rate": 2.8328668421781907e-05,
      "loss": 2.0839,
      "step": 21070
    },
    {
      "epoch": 2.9208812525980323,
      "grad_norm": 13.071407318115234,
      "learning_rate": 2.832312595261189e-05,
      "loss": 2.3427,
      "step": 21080
    },
    {
      "epoch": 2.922266869890536,
      "grad_norm": 9.464086532592773,
      "learning_rate": 2.8317583483441878e-05,
      "loss": 2.2934,
      "step": 21090
    },
    {
      "epoch": 2.9236524871830403,
      "grad_norm": 8.805212020874023,
      "learning_rate": 2.831204101427186e-05,
      "loss": 3.0572,
      "step": 21100
    },
    {
      "epoch": 2.925038104475544,
      "grad_norm": 19.82843017578125,
      "learning_rate": 2.8306498545101845e-05,
      "loss": 2.5138,
      "step": 21110
    },
    {
      "epoch": 2.9264237217680478,
      "grad_norm": 9.886517524719238,
      "learning_rate": 2.830095607593183e-05,
      "loss": 2.5005,
      "step": 21120
    },
    {
      "epoch": 2.9278093390605515,
      "grad_norm": 9.416308403015137,
      "learning_rate": 2.8295413606761815e-05,
      "loss": 2.3819,
      "step": 21130
    },
    {
      "epoch": 2.9291949563530553,
      "grad_norm": 12.477458000183105,
      "learning_rate": 2.8289871137591797e-05,
      "loss": 2.0888,
      "step": 21140
    },
    {
      "epoch": 2.930580573645559,
      "grad_norm": 8.573802947998047,
      "learning_rate": 2.8284328668421786e-05,
      "loss": 1.8308,
      "step": 21150
    },
    {
      "epoch": 2.9319661909380628,
      "grad_norm": 14.833256721496582,
      "learning_rate": 2.8278786199251768e-05,
      "loss": 2.3455,
      "step": 21160
    },
    {
      "epoch": 2.933351808230567,
      "grad_norm": 12.472312927246094,
      "learning_rate": 2.8273243730081753e-05,
      "loss": 2.0643,
      "step": 21170
    },
    {
      "epoch": 2.9347374255230703,
      "grad_norm": 9.850049018859863,
      "learning_rate": 2.8267701260911738e-05,
      "loss": 2.5539,
      "step": 21180
    },
    {
      "epoch": 2.9361230428155745,
      "grad_norm": 12.359981536865234,
      "learning_rate": 2.8262158791741723e-05,
      "loss": 2.2118,
      "step": 21190
    },
    {
      "epoch": 2.937508660108078,
      "grad_norm": 12.634939193725586,
      "learning_rate": 2.8256616322571705e-05,
      "loss": 2.1634,
      "step": 21200
    },
    {
      "epoch": 2.938894277400582,
      "grad_norm": 10.259799003601074,
      "learning_rate": 2.8251073853401694e-05,
      "loss": 2.7835,
      "step": 21210
    },
    {
      "epoch": 2.9402798946930857,
      "grad_norm": 13.340636253356934,
      "learning_rate": 2.8245531384231676e-05,
      "loss": 2.5528,
      "step": 21220
    },
    {
      "epoch": 2.9416655119855895,
      "grad_norm": 9.521430015563965,
      "learning_rate": 2.823998891506166e-05,
      "loss": 2.92,
      "step": 21230
    },
    {
      "epoch": 2.9430511292780936,
      "grad_norm": 9.464517593383789,
      "learning_rate": 2.8234446445891646e-05,
      "loss": 2.3671,
      "step": 21240
    },
    {
      "epoch": 2.944436746570597,
      "grad_norm": 10.872076988220215,
      "learning_rate": 2.822890397672163e-05,
      "loss": 2.3486,
      "step": 21250
    },
    {
      "epoch": 2.945822363863101,
      "grad_norm": 7.599025249481201,
      "learning_rate": 2.822336150755162e-05,
      "loss": 2.3959,
      "step": 21260
    },
    {
      "epoch": 2.947207981155605,
      "grad_norm": 14.13825798034668,
      "learning_rate": 2.8217819038381602e-05,
      "loss": 2.6405,
      "step": 21270
    },
    {
      "epoch": 2.9485935984481086,
      "grad_norm": 7.961889266967773,
      "learning_rate": 2.8212276569211584e-05,
      "loss": 2.632,
      "step": 21280
    },
    {
      "epoch": 2.9499792157406124,
      "grad_norm": 10.12601089477539,
      "learning_rate": 2.8206734100041572e-05,
      "loss": 2.1385,
      "step": 21290
    },
    {
      "epoch": 2.951364833033116,
      "grad_norm": 12.106246948242188,
      "learning_rate": 2.8201191630871558e-05,
      "loss": 2.8385,
      "step": 21300
    },
    {
      "epoch": 2.9527504503256203,
      "grad_norm": 11.885068893432617,
      "learning_rate": 2.819564916170154e-05,
      "loss": 2.539,
      "step": 21310
    },
    {
      "epoch": 2.9541360676181236,
      "grad_norm": 18.21230125427246,
      "learning_rate": 2.8190106692531528e-05,
      "loss": 2.3227,
      "step": 21320
    },
    {
      "epoch": 2.955521684910628,
      "grad_norm": 16.136981964111328,
      "learning_rate": 2.818456422336151e-05,
      "loss": 1.9544,
      "step": 21330
    },
    {
      "epoch": 2.9569073022031316,
      "grad_norm": 8.818042755126953,
      "learning_rate": 2.8179021754191492e-05,
      "loss": 2.7434,
      "step": 21340
    },
    {
      "epoch": 2.9582929194956353,
      "grad_norm": 9.944960594177246,
      "learning_rate": 2.817347928502148e-05,
      "loss": 2.1984,
      "step": 21350
    },
    {
      "epoch": 2.959678536788139,
      "grad_norm": 11.213134765625,
      "learning_rate": 2.8167936815851466e-05,
      "loss": 2.6394,
      "step": 21360
    },
    {
      "epoch": 2.961064154080643,
      "grad_norm": 8.747196197509766,
      "learning_rate": 2.8162394346681448e-05,
      "loss": 2.3798,
      "step": 21370
    },
    {
      "epoch": 2.9624497713731466,
      "grad_norm": 5.253321170806885,
      "learning_rate": 2.8156851877511436e-05,
      "loss": 2.0735,
      "step": 21380
    },
    {
      "epoch": 2.9638353886656503,
      "grad_norm": 10.182853698730469,
      "learning_rate": 2.8151309408341418e-05,
      "loss": 2.4748,
      "step": 21390
    },
    {
      "epoch": 2.9652210059581545,
      "grad_norm": 11.649346351623535,
      "learning_rate": 2.81457669391714e-05,
      "loss": 2.5488,
      "step": 21400
    },
    {
      "epoch": 2.9666066232506583,
      "grad_norm": 14.602184295654297,
      "learning_rate": 2.814022447000139e-05,
      "loss": 2.2823,
      "step": 21410
    },
    {
      "epoch": 2.967992240543162,
      "grad_norm": 15.509187698364258,
      "learning_rate": 2.8134682000831374e-05,
      "loss": 2.5045,
      "step": 21420
    },
    {
      "epoch": 2.9693778578356658,
      "grad_norm": 10.626014709472656,
      "learning_rate": 2.8129139531661356e-05,
      "loss": 2.151,
      "step": 21430
    },
    {
      "epoch": 2.9707634751281695,
      "grad_norm": 8.10281753540039,
      "learning_rate": 2.8123597062491344e-05,
      "loss": 2.1092,
      "step": 21440
    },
    {
      "epoch": 2.9721490924206733,
      "grad_norm": 10.310012817382812,
      "learning_rate": 2.8118054593321326e-05,
      "loss": 2.56,
      "step": 21450
    },
    {
      "epoch": 2.973534709713177,
      "grad_norm": 16.433412551879883,
      "learning_rate": 2.811251212415131e-05,
      "loss": 2.3564,
      "step": 21460
    },
    {
      "epoch": 2.974920327005681,
      "grad_norm": 14.578264236450195,
      "learning_rate": 2.8106969654981297e-05,
      "loss": 2.2083,
      "step": 21470
    },
    {
      "epoch": 2.976305944298185,
      "grad_norm": 10.640156745910645,
      "learning_rate": 2.8101427185811282e-05,
      "loss": 2.3728,
      "step": 21480
    },
    {
      "epoch": 2.9776915615906887,
      "grad_norm": 7.446831226348877,
      "learning_rate": 2.8095884716641264e-05,
      "loss": 2.0984,
      "step": 21490
    },
    {
      "epoch": 2.9790771788831925,
      "grad_norm": 9.403730392456055,
      "learning_rate": 2.8090342247471253e-05,
      "loss": 2.3439,
      "step": 21500
    },
    {
      "epoch": 2.980462796175696,
      "grad_norm": 15.193869590759277,
      "learning_rate": 2.8084799778301234e-05,
      "loss": 2.1167,
      "step": 21510
    },
    {
      "epoch": 2.9818484134682,
      "grad_norm": 13.827360153198242,
      "learning_rate": 2.807925730913122e-05,
      "loss": 2.066,
      "step": 21520
    },
    {
      "epoch": 2.9832340307607037,
      "grad_norm": 8.023019790649414,
      "learning_rate": 2.8073714839961205e-05,
      "loss": 2.1833,
      "step": 21530
    },
    {
      "epoch": 2.984619648053208,
      "grad_norm": 10.303498268127441,
      "learning_rate": 2.806817237079119e-05,
      "loss": 2.2998,
      "step": 21540
    },
    {
      "epoch": 2.9860052653457116,
      "grad_norm": 10.583783149719238,
      "learning_rate": 2.8062629901621175e-05,
      "loss": 2.042,
      "step": 21550
    },
    {
      "epoch": 2.9873908826382154,
      "grad_norm": 11.979146957397461,
      "learning_rate": 2.805708743245116e-05,
      "loss": 2.7331,
      "step": 21560
    },
    {
      "epoch": 2.988776499930719,
      "grad_norm": 10.718086242675781,
      "learning_rate": 2.8051544963281143e-05,
      "loss": 2.5989,
      "step": 21570
    },
    {
      "epoch": 2.990162117223223,
      "grad_norm": 11.468650817871094,
      "learning_rate": 2.804600249411113e-05,
      "loss": 2.0855,
      "step": 21580
    },
    {
      "epoch": 2.9915477345157266,
      "grad_norm": 14.055280685424805,
      "learning_rate": 2.8040460024941113e-05,
      "loss": 2.426,
      "step": 21590
    },
    {
      "epoch": 2.9929333518082304,
      "grad_norm": 13.776942253112793,
      "learning_rate": 2.8034917555771098e-05,
      "loss": 2.3045,
      "step": 21600
    },
    {
      "epoch": 2.9943189691007346,
      "grad_norm": 11.092177391052246,
      "learning_rate": 2.8029375086601084e-05,
      "loss": 2.0175,
      "step": 21610
    },
    {
      "epoch": 2.9957045863932383,
      "grad_norm": 19.125244140625,
      "learning_rate": 2.802383261743107e-05,
      "loss": 2.1911,
      "step": 21620
    },
    {
      "epoch": 2.997090203685742,
      "grad_norm": 14.656669616699219,
      "learning_rate": 2.801829014826105e-05,
      "loss": 2.34,
      "step": 21630
    },
    {
      "epoch": 2.998475820978246,
      "grad_norm": 10.621291160583496,
      "learning_rate": 2.801274767909104e-05,
      "loss": 2.3801,
      "step": 21640
    },
    {
      "epoch": 2.9998614382707496,
      "grad_norm": 11.473626136779785,
      "learning_rate": 2.800720520992102e-05,
      "loss": 3.0015,
      "step": 21650
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.4688842688842689,
      "eval_bert_f1": 0.9866654872894287,
      "eval_bert_precision": 0.9890410304069519,
      "eval_bert_recall": 0.9847609400749207,
      "eval_f1": 0.03584573289973186,
      "eval_loss": 2.467087984085083,
      "eval_runtime": 289.1821,
      "eval_samples_per_second": 49.899,
      "eval_steps_per_second": 6.238,
      "eval_synonym_accuracy": 0.47976437976437974,
      "step": 21651
    },
    {
      "epoch": 3.0012470555632533,
      "grad_norm": 11.84343433380127,
      "learning_rate": 2.8001662740751006e-05,
      "loss": 2.2534,
      "step": 21660
    },
    {
      "epoch": 3.002632672855757,
      "grad_norm": 13.641266822814941,
      "learning_rate": 2.799612027158099e-05,
      "loss": 2.7007,
      "step": 21670
    },
    {
      "epoch": 3.0040182901482613,
      "grad_norm": 9.144071578979492,
      "learning_rate": 2.7990577802410977e-05,
      "loss": 2.5949,
      "step": 21680
    },
    {
      "epoch": 3.005403907440765,
      "grad_norm": 11.628033638000488,
      "learning_rate": 2.798503533324096e-05,
      "loss": 2.7486,
      "step": 21690
    },
    {
      "epoch": 3.0067895247332688,
      "grad_norm": 6.540714263916016,
      "learning_rate": 2.7979492864070947e-05,
      "loss": 2.3905,
      "step": 21700
    },
    {
      "epoch": 3.0081751420257725,
      "grad_norm": 10.890460968017578,
      "learning_rate": 2.797395039490093e-05,
      "loss": 2.1505,
      "step": 21710
    },
    {
      "epoch": 3.0095607593182763,
      "grad_norm": 10.37345027923584,
      "learning_rate": 2.7968407925730914e-05,
      "loss": 2.0732,
      "step": 21720
    },
    {
      "epoch": 3.01094637661078,
      "grad_norm": 8.43645191192627,
      "learning_rate": 2.7962865456560903e-05,
      "loss": 2.2455,
      "step": 21730
    },
    {
      "epoch": 3.0123319939032838,
      "grad_norm": 7.283044815063477,
      "learning_rate": 2.7957322987390885e-05,
      "loss": 2.2243,
      "step": 21740
    },
    {
      "epoch": 3.0137176111957875,
      "grad_norm": 5.07418966293335,
      "learning_rate": 2.7951780518220867e-05,
      "loss": 2.104,
      "step": 21750
    },
    {
      "epoch": 3.0151032284882917,
      "grad_norm": 13.273102760314941,
      "learning_rate": 2.7946238049050855e-05,
      "loss": 2.3595,
      "step": 21760
    },
    {
      "epoch": 3.0164888457807955,
      "grad_norm": 13.384699821472168,
      "learning_rate": 2.7940695579880837e-05,
      "loss": 2.312,
      "step": 21770
    },
    {
      "epoch": 3.017874463073299,
      "grad_norm": 16.48860740661621,
      "learning_rate": 2.7935153110710823e-05,
      "loss": 2.4924,
      "step": 21780
    },
    {
      "epoch": 3.019260080365803,
      "grad_norm": 16.90248680114746,
      "learning_rate": 2.792961064154081e-05,
      "loss": 2.3562,
      "step": 21790
    },
    {
      "epoch": 3.0206456976583067,
      "grad_norm": 12.130797386169434,
      "learning_rate": 2.7924068172370793e-05,
      "loss": 2.3033,
      "step": 21800
    },
    {
      "epoch": 3.0220313149508105,
      "grad_norm": 11.857510566711426,
      "learning_rate": 2.7918525703200775e-05,
      "loss": 1.9218,
      "step": 21810
    },
    {
      "epoch": 3.023416932243314,
      "grad_norm": 10.30793571472168,
      "learning_rate": 2.7912983234030764e-05,
      "loss": 2.4428,
      "step": 21820
    },
    {
      "epoch": 3.0248025495358184,
      "grad_norm": 9.722111701965332,
      "learning_rate": 2.7907440764860745e-05,
      "loss": 2.4101,
      "step": 21830
    },
    {
      "epoch": 3.026188166828322,
      "grad_norm": 14.461034774780273,
      "learning_rate": 2.7901898295690734e-05,
      "loss": 2.2151,
      "step": 21840
    },
    {
      "epoch": 3.027573784120826,
      "grad_norm": 10.729384422302246,
      "learning_rate": 2.789635582652072e-05,
      "loss": 2.2117,
      "step": 21850
    },
    {
      "epoch": 3.0289594014133296,
      "grad_norm": 10.718619346618652,
      "learning_rate": 2.78908133573507e-05,
      "loss": 2.3681,
      "step": 21860
    },
    {
      "epoch": 3.0303450187058334,
      "grad_norm": 14.443426132202148,
      "learning_rate": 2.788527088818069e-05,
      "loss": 2.2513,
      "step": 21870
    },
    {
      "epoch": 3.031730635998337,
      "grad_norm": 15.681926727294922,
      "learning_rate": 2.7879728419010672e-05,
      "loss": 2.2505,
      "step": 21880
    },
    {
      "epoch": 3.033116253290841,
      "grad_norm": 13.152424812316895,
      "learning_rate": 2.7874185949840657e-05,
      "loss": 2.493,
      "step": 21890
    },
    {
      "epoch": 3.034501870583345,
      "grad_norm": 11.177556037902832,
      "learning_rate": 2.7868643480670642e-05,
      "loss": 2.1715,
      "step": 21900
    },
    {
      "epoch": 3.035887487875849,
      "grad_norm": 13.0880126953125,
      "learning_rate": 2.7863101011500627e-05,
      "loss": 1.7531,
      "step": 21910
    },
    {
      "epoch": 3.0372731051683526,
      "grad_norm": 11.287715911865234,
      "learning_rate": 2.785755854233061e-05,
      "loss": 2.2616,
      "step": 21920
    },
    {
      "epoch": 3.0386587224608563,
      "grad_norm": 7.573007106781006,
      "learning_rate": 2.7852016073160598e-05,
      "loss": 2.3397,
      "step": 21930
    },
    {
      "epoch": 3.04004433975336,
      "grad_norm": 8.200765609741211,
      "learning_rate": 2.784647360399058e-05,
      "loss": 2.4193,
      "step": 21940
    },
    {
      "epoch": 3.041429957045864,
      "grad_norm": 8.283549308776855,
      "learning_rate": 2.7840931134820565e-05,
      "loss": 2.5799,
      "step": 21950
    },
    {
      "epoch": 3.0428155743383676,
      "grad_norm": 13.607211112976074,
      "learning_rate": 2.783538866565055e-05,
      "loss": 2.5529,
      "step": 21960
    },
    {
      "epoch": 3.0442011916308713,
      "grad_norm": 18.593055725097656,
      "learning_rate": 2.7829846196480536e-05,
      "loss": 2.1019,
      "step": 21970
    },
    {
      "epoch": 3.0455868089233755,
      "grad_norm": 9.552421569824219,
      "learning_rate": 2.7824303727310517e-05,
      "loss": 2.5461,
      "step": 21980
    },
    {
      "epoch": 3.0469724262158793,
      "grad_norm": 14.163605690002441,
      "learning_rate": 2.7818761258140506e-05,
      "loss": 2.2178,
      "step": 21990
    },
    {
      "epoch": 3.048358043508383,
      "grad_norm": 10.769640922546387,
      "learning_rate": 2.7813218788970488e-05,
      "loss": 2.2747,
      "step": 22000
    },
    {
      "epoch": 3.0497436608008868,
      "grad_norm": 9.156599044799805,
      "learning_rate": 2.7807676319800473e-05,
      "loss": 2.0616,
      "step": 22010
    },
    {
      "epoch": 3.0511292780933905,
      "grad_norm": 11.981550216674805,
      "learning_rate": 2.780213385063046e-05,
      "loss": 2.259,
      "step": 22020
    },
    {
      "epoch": 3.0525148953858943,
      "grad_norm": 11.432483673095703,
      "learning_rate": 2.7796591381460444e-05,
      "loss": 2.2617,
      "step": 22030
    },
    {
      "epoch": 3.053900512678398,
      "grad_norm": 8.566886901855469,
      "learning_rate": 2.7791048912290426e-05,
      "loss": 2.0663,
      "step": 22040
    },
    {
      "epoch": 3.055286129970902,
      "grad_norm": 17.206995010375977,
      "learning_rate": 2.7785506443120414e-05,
      "loss": 2.6019,
      "step": 22050
    },
    {
      "epoch": 3.056671747263406,
      "grad_norm": 11.315211296081543,
      "learning_rate": 2.7779963973950396e-05,
      "loss": 2.5812,
      "step": 22060
    },
    {
      "epoch": 3.0580573645559097,
      "grad_norm": 12.736930847167969,
      "learning_rate": 2.777442150478038e-05,
      "loss": 1.9754,
      "step": 22070
    },
    {
      "epoch": 3.0594429818484135,
      "grad_norm": 8.917783737182617,
      "learning_rate": 2.7768879035610367e-05,
      "loss": 2.3199,
      "step": 22080
    },
    {
      "epoch": 3.060828599140917,
      "grad_norm": 10.290660858154297,
      "learning_rate": 2.7763336566440352e-05,
      "loss": 2.1922,
      "step": 22090
    },
    {
      "epoch": 3.062214216433421,
      "grad_norm": 13.568755149841309,
      "learning_rate": 2.7757794097270334e-05,
      "loss": 2.1958,
      "step": 22100
    },
    {
      "epoch": 3.0635998337259247,
      "grad_norm": 9.00823974609375,
      "learning_rate": 2.7752251628100322e-05,
      "loss": 2.4373,
      "step": 22110
    },
    {
      "epoch": 3.064985451018429,
      "grad_norm": 21.283700942993164,
      "learning_rate": 2.7746709158930304e-05,
      "loss": 2.2911,
      "step": 22120
    },
    {
      "epoch": 3.0663710683109326,
      "grad_norm": 11.372772216796875,
      "learning_rate": 2.774116668976029e-05,
      "loss": 2.0706,
      "step": 22130
    },
    {
      "epoch": 3.0677566856034364,
      "grad_norm": 10.184660911560059,
      "learning_rate": 2.7735624220590275e-05,
      "loss": 2.2007,
      "step": 22140
    },
    {
      "epoch": 3.06914230289594,
      "grad_norm": 11.539005279541016,
      "learning_rate": 2.773008175142026e-05,
      "loss": 2.2064,
      "step": 22150
    },
    {
      "epoch": 3.070527920188444,
      "grad_norm": 12.437414169311523,
      "learning_rate": 2.772453928225025e-05,
      "loss": 2.2908,
      "step": 22160
    },
    {
      "epoch": 3.0719135374809476,
      "grad_norm": 9.623082160949707,
      "learning_rate": 2.771899681308023e-05,
      "loss": 2.2177,
      "step": 22170
    },
    {
      "epoch": 3.0732991547734514,
      "grad_norm": 10.116608619689941,
      "learning_rate": 2.7713454343910212e-05,
      "loss": 2.4004,
      "step": 22180
    },
    {
      "epoch": 3.0746847720659556,
      "grad_norm": 12.762177467346191,
      "learning_rate": 2.77079118747402e-05,
      "loss": 2.3572,
      "step": 22190
    },
    {
      "epoch": 3.0760703893584593,
      "grad_norm": 12.822975158691406,
      "learning_rate": 2.7702369405570183e-05,
      "loss": 2.9409,
      "step": 22200
    },
    {
      "epoch": 3.077456006650963,
      "grad_norm": 12.681197166442871,
      "learning_rate": 2.7696826936400168e-05,
      "loss": 2.4921,
      "step": 22210
    },
    {
      "epoch": 3.078841623943467,
      "grad_norm": 13.318233489990234,
      "learning_rate": 2.7691284467230157e-05,
      "loss": 2.1406,
      "step": 22220
    },
    {
      "epoch": 3.0802272412359706,
      "grad_norm": 8.920731544494629,
      "learning_rate": 2.768574199806014e-05,
      "loss": 2.0294,
      "step": 22230
    },
    {
      "epoch": 3.0816128585284743,
      "grad_norm": 12.51604175567627,
      "learning_rate": 2.768019952889012e-05,
      "loss": 2.6537,
      "step": 22240
    },
    {
      "epoch": 3.082998475820978,
      "grad_norm": 11.045999526977539,
      "learning_rate": 2.767465705972011e-05,
      "loss": 1.8604,
      "step": 22250
    },
    {
      "epoch": 3.0843840931134823,
      "grad_norm": 11.242464065551758,
      "learning_rate": 2.766911459055009e-05,
      "loss": 2.359,
      "step": 22260
    },
    {
      "epoch": 3.085769710405986,
      "grad_norm": 10.30418586730957,
      "learning_rate": 2.7663572121380076e-05,
      "loss": 2.122,
      "step": 22270
    },
    {
      "epoch": 3.0871553276984898,
      "grad_norm": 14.244346618652344,
      "learning_rate": 2.7658029652210065e-05,
      "loss": 2.0236,
      "step": 22280
    },
    {
      "epoch": 3.0885409449909935,
      "grad_norm": 12.088788986206055,
      "learning_rate": 2.7652487183040047e-05,
      "loss": 2.4871,
      "step": 22290
    },
    {
      "epoch": 3.0899265622834973,
      "grad_norm": 8.32356071472168,
      "learning_rate": 2.764694471387003e-05,
      "loss": 1.9944,
      "step": 22300
    },
    {
      "epoch": 3.091312179576001,
      "grad_norm": 10.00747013092041,
      "learning_rate": 2.7641402244700017e-05,
      "loss": 2.3494,
      "step": 22310
    },
    {
      "epoch": 3.0926977968685048,
      "grad_norm": 10.714200973510742,
      "learning_rate": 2.7635859775530002e-05,
      "loss": 1.7905,
      "step": 22320
    },
    {
      "epoch": 3.0940834141610085,
      "grad_norm": 11.530455589294434,
      "learning_rate": 2.7630317306359984e-05,
      "loss": 2.1501,
      "step": 22330
    },
    {
      "epoch": 3.0954690314535127,
      "grad_norm": 10.711857795715332,
      "learning_rate": 2.7624774837189973e-05,
      "loss": 2.1578,
      "step": 22340
    },
    {
      "epoch": 3.0968546487460165,
      "grad_norm": 11.28238582611084,
      "learning_rate": 2.7619232368019955e-05,
      "loss": 2.2787,
      "step": 22350
    },
    {
      "epoch": 3.09824026603852,
      "grad_norm": 18.53041648864746,
      "learning_rate": 2.7613689898849937e-05,
      "loss": 2.0285,
      "step": 22360
    },
    {
      "epoch": 3.099625883331024,
      "grad_norm": 17.19281768798828,
      "learning_rate": 2.7608147429679925e-05,
      "loss": 2.2114,
      "step": 22370
    },
    {
      "epoch": 3.1010115006235277,
      "grad_norm": 10.111530303955078,
      "learning_rate": 2.760260496050991e-05,
      "loss": 2.3108,
      "step": 22380
    },
    {
      "epoch": 3.1023971179160315,
      "grad_norm": 9.809270858764648,
      "learning_rate": 2.7597062491339892e-05,
      "loss": 2.4076,
      "step": 22390
    },
    {
      "epoch": 3.103782735208535,
      "grad_norm": 6.016821384429932,
      "learning_rate": 2.759152002216988e-05,
      "loss": 2.5964,
      "step": 22400
    },
    {
      "epoch": 3.1051683525010394,
      "grad_norm": 8.757434844970703,
      "learning_rate": 2.7585977552999863e-05,
      "loss": 2.1047,
      "step": 22410
    },
    {
      "epoch": 3.106553969793543,
      "grad_norm": 12.04153823852539,
      "learning_rate": 2.7580435083829845e-05,
      "loss": 1.9054,
      "step": 22420
    },
    {
      "epoch": 3.107939587086047,
      "grad_norm": 8.169800758361816,
      "learning_rate": 2.7574892614659833e-05,
      "loss": 2.7172,
      "step": 22430
    },
    {
      "epoch": 3.1093252043785506,
      "grad_norm": 10.273082733154297,
      "learning_rate": 2.756935014548982e-05,
      "loss": 2.3595,
      "step": 22440
    },
    {
      "epoch": 3.1107108216710544,
      "grad_norm": 5.508977890014648,
      "learning_rate": 2.7563807676319804e-05,
      "loss": 2.3361,
      "step": 22450
    },
    {
      "epoch": 3.112096438963558,
      "grad_norm": 19.841856002807617,
      "learning_rate": 2.755826520714979e-05,
      "loss": 2.0348,
      "step": 22460
    },
    {
      "epoch": 3.113482056256062,
      "grad_norm": 13.352099418640137,
      "learning_rate": 2.755272273797977e-05,
      "loss": 1.8972,
      "step": 22470
    },
    {
      "epoch": 3.114867673548566,
      "grad_norm": 12.38248348236084,
      "learning_rate": 2.754718026880976e-05,
      "loss": 2.1665,
      "step": 22480
    },
    {
      "epoch": 3.11625329084107,
      "grad_norm": 7.335716724395752,
      "learning_rate": 2.754163779963974e-05,
      "loss": 1.8517,
      "step": 22490
    },
    {
      "epoch": 3.1176389081335736,
      "grad_norm": 11.700170516967773,
      "learning_rate": 2.7536095330469727e-05,
      "loss": 1.7943,
      "step": 22500
    },
    {
      "epoch": 3.1190245254260773,
      "grad_norm": 9.416410446166992,
      "learning_rate": 2.7530552861299712e-05,
      "loss": 2.2985,
      "step": 22510
    },
    {
      "epoch": 3.120410142718581,
      "grad_norm": 11.071197509765625,
      "learning_rate": 2.7525010392129697e-05,
      "loss": 2.0643,
      "step": 22520
    },
    {
      "epoch": 3.121795760011085,
      "grad_norm": 16.690868377685547,
      "learning_rate": 2.751946792295968e-05,
      "loss": 2.5603,
      "step": 22530
    },
    {
      "epoch": 3.1231813773035886,
      "grad_norm": 10.292584419250488,
      "learning_rate": 2.7513925453789668e-05,
      "loss": 2.4363,
      "step": 22540
    },
    {
      "epoch": 3.1245669945960928,
      "grad_norm": 12.955313682556152,
      "learning_rate": 2.750838298461965e-05,
      "loss": 2.5689,
      "step": 22550
    },
    {
      "epoch": 3.1259526118885965,
      "grad_norm": 12.33625316619873,
      "learning_rate": 2.7502840515449635e-05,
      "loss": 2.3501,
      "step": 22560
    },
    {
      "epoch": 3.1273382291811003,
      "grad_norm": 7.360189437866211,
      "learning_rate": 2.749729804627962e-05,
      "loss": 1.7768,
      "step": 22570
    },
    {
      "epoch": 3.128723846473604,
      "grad_norm": 11.50712776184082,
      "learning_rate": 2.7491755577109605e-05,
      "loss": 2.478,
      "step": 22580
    },
    {
      "epoch": 3.1301094637661078,
      "grad_norm": 11.736900329589844,
      "learning_rate": 2.7486213107939587e-05,
      "loss": 2.094,
      "step": 22590
    },
    {
      "epoch": 3.1314950810586115,
      "grad_norm": 8.018350601196289,
      "learning_rate": 2.7480670638769576e-05,
      "loss": 2.6134,
      "step": 22600
    },
    {
      "epoch": 3.1328806983511153,
      "grad_norm": 12.017282485961914,
      "learning_rate": 2.7475128169599558e-05,
      "loss": 2.2279,
      "step": 22610
    },
    {
      "epoch": 3.134266315643619,
      "grad_norm": 13.652262687683105,
      "learning_rate": 2.7469585700429543e-05,
      "loss": 2.3191,
      "step": 22620
    },
    {
      "epoch": 3.135651932936123,
      "grad_norm": 10.079066276550293,
      "learning_rate": 2.7464043231259528e-05,
      "loss": 2.9582,
      "step": 22630
    },
    {
      "epoch": 3.137037550228627,
      "grad_norm": 14.8137845993042,
      "learning_rate": 2.7458500762089513e-05,
      "loss": 1.7215,
      "step": 22640
    },
    {
      "epoch": 3.1384231675211307,
      "grad_norm": 9.308086395263672,
      "learning_rate": 2.7452958292919495e-05,
      "loss": 2.6422,
      "step": 22650
    },
    {
      "epoch": 3.1398087848136345,
      "grad_norm": 11.41407585144043,
      "learning_rate": 2.7447415823749484e-05,
      "loss": 1.9977,
      "step": 22660
    },
    {
      "epoch": 3.141194402106138,
      "grad_norm": 6.900907039642334,
      "learning_rate": 2.7441873354579466e-05,
      "loss": 2.3433,
      "step": 22670
    },
    {
      "epoch": 3.142580019398642,
      "grad_norm": 13.358016967773438,
      "learning_rate": 2.743633088540945e-05,
      "loss": 2.1011,
      "step": 22680
    },
    {
      "epoch": 3.1439656366911457,
      "grad_norm": 8.429804801940918,
      "learning_rate": 2.7430788416239436e-05,
      "loss": 2.1823,
      "step": 22690
    },
    {
      "epoch": 3.14535125398365,
      "grad_norm": 6.071671009063721,
      "learning_rate": 2.742524594706942e-05,
      "loss": 2.3022,
      "step": 22700
    },
    {
      "epoch": 3.1467368712761536,
      "grad_norm": 16.041715621948242,
      "learning_rate": 2.7419703477899403e-05,
      "loss": 2.1247,
      "step": 22710
    },
    {
      "epoch": 3.1481224885686574,
      "grad_norm": 13.236714363098145,
      "learning_rate": 2.7414161008729392e-05,
      "loss": 1.8529,
      "step": 22720
    },
    {
      "epoch": 3.149508105861161,
      "grad_norm": 7.982240200042725,
      "learning_rate": 2.7408618539559374e-05,
      "loss": 2.0967,
      "step": 22730
    },
    {
      "epoch": 3.150893723153665,
      "grad_norm": 13.922347068786621,
      "learning_rate": 2.7403076070389362e-05,
      "loss": 2.2133,
      "step": 22740
    },
    {
      "epoch": 3.1522793404461686,
      "grad_norm": 16.027345657348633,
      "learning_rate": 2.7397533601219344e-05,
      "loss": 2.2624,
      "step": 22750
    },
    {
      "epoch": 3.1536649577386724,
      "grad_norm": 12.323780059814453,
      "learning_rate": 2.739199113204933e-05,
      "loss": 2.465,
      "step": 22760
    },
    {
      "epoch": 3.1550505750311766,
      "grad_norm": 11.64874267578125,
      "learning_rate": 2.7386448662879318e-05,
      "loss": 2.2156,
      "step": 22770
    },
    {
      "epoch": 3.1564361923236803,
      "grad_norm": 9.19467544555664,
      "learning_rate": 2.73809061937093e-05,
      "loss": 2.4186,
      "step": 22780
    },
    {
      "epoch": 3.157821809616184,
      "grad_norm": 10.588968276977539,
      "learning_rate": 2.7375363724539282e-05,
      "loss": 2.3478,
      "step": 22790
    },
    {
      "epoch": 3.159207426908688,
      "grad_norm": 9.143816947937012,
      "learning_rate": 2.736982125536927e-05,
      "loss": 2.5605,
      "step": 22800
    },
    {
      "epoch": 3.1605930442011916,
      "grad_norm": 16.693113327026367,
      "learning_rate": 2.7364278786199256e-05,
      "loss": 2.8392,
      "step": 22810
    },
    {
      "epoch": 3.1619786614936953,
      "grad_norm": 11.816773414611816,
      "learning_rate": 2.7358736317029238e-05,
      "loss": 2.4885,
      "step": 22820
    },
    {
      "epoch": 3.163364278786199,
      "grad_norm": 9.108141899108887,
      "learning_rate": 2.7353193847859226e-05,
      "loss": 2.3946,
      "step": 22830
    },
    {
      "epoch": 3.1647498960787033,
      "grad_norm": 10.421744346618652,
      "learning_rate": 2.7347651378689208e-05,
      "loss": 2.5599,
      "step": 22840
    },
    {
      "epoch": 3.166135513371207,
      "grad_norm": 10.225351333618164,
      "learning_rate": 2.734210890951919e-05,
      "loss": 2.6509,
      "step": 22850
    },
    {
      "epoch": 3.1675211306637108,
      "grad_norm": 12.166058540344238,
      "learning_rate": 2.733656644034918e-05,
      "loss": 2.7663,
      "step": 22860
    },
    {
      "epoch": 3.1689067479562145,
      "grad_norm": 7.0918498039245605,
      "learning_rate": 2.7331023971179164e-05,
      "loss": 2.0981,
      "step": 22870
    },
    {
      "epoch": 3.1702923652487183,
      "grad_norm": 11.90328598022461,
      "learning_rate": 2.7325481502009146e-05,
      "loss": 1.9498,
      "step": 22880
    },
    {
      "epoch": 3.171677982541222,
      "grad_norm": 7.303672790527344,
      "learning_rate": 2.7319939032839134e-05,
      "loss": 2.1006,
      "step": 22890
    },
    {
      "epoch": 3.1730635998337258,
      "grad_norm": 15.003376007080078,
      "learning_rate": 2.7314396563669116e-05,
      "loss": 2.3096,
      "step": 22900
    },
    {
      "epoch": 3.17444921712623,
      "grad_norm": 10.492608070373535,
      "learning_rate": 2.7308854094499098e-05,
      "loss": 2.2198,
      "step": 22910
    },
    {
      "epoch": 3.1758348344187337,
      "grad_norm": 10.039331436157227,
      "learning_rate": 2.7303311625329087e-05,
      "loss": 2.219,
      "step": 22920
    },
    {
      "epoch": 3.1772204517112375,
      "grad_norm": 8.441222190856934,
      "learning_rate": 2.7297769156159072e-05,
      "loss": 2.2922,
      "step": 22930
    },
    {
      "epoch": 3.178606069003741,
      "grad_norm": 5.70198392868042,
      "learning_rate": 2.7292226686989054e-05,
      "loss": 2.33,
      "step": 22940
    },
    {
      "epoch": 3.179991686296245,
      "grad_norm": 13.42943286895752,
      "learning_rate": 2.7286684217819043e-05,
      "loss": 2.2886,
      "step": 22950
    },
    {
      "epoch": 3.1813773035887487,
      "grad_norm": 9.762811660766602,
      "learning_rate": 2.7281141748649024e-05,
      "loss": 1.8809,
      "step": 22960
    },
    {
      "epoch": 3.1827629208812525,
      "grad_norm": 13.392477035522461,
      "learning_rate": 2.727559927947901e-05,
      "loss": 2.068,
      "step": 22970
    },
    {
      "epoch": 3.184148538173756,
      "grad_norm": 8.6746244430542,
      "learning_rate": 2.7270056810308995e-05,
      "loss": 2.2417,
      "step": 22980
    },
    {
      "epoch": 3.1855341554662604,
      "grad_norm": 11.151670455932617,
      "learning_rate": 2.726451434113898e-05,
      "loss": 2.6322,
      "step": 22990
    },
    {
      "epoch": 3.186919772758764,
      "grad_norm": 13.368942260742188,
      "learning_rate": 2.7258971871968962e-05,
      "loss": 2.6119,
      "step": 23000
    },
    {
      "epoch": 3.188305390051268,
      "grad_norm": 9.313796043395996,
      "learning_rate": 2.725342940279895e-05,
      "loss": 1.9059,
      "step": 23010
    },
    {
      "epoch": 3.1896910073437716,
      "grad_norm": 11.1231050491333,
      "learning_rate": 2.7247886933628933e-05,
      "loss": 2.2862,
      "step": 23020
    },
    {
      "epoch": 3.1910766246362754,
      "grad_norm": 14.003867149353027,
      "learning_rate": 2.724234446445892e-05,
      "loss": 2.3054,
      "step": 23030
    },
    {
      "epoch": 3.192462241928779,
      "grad_norm": 15.368695259094238,
      "learning_rate": 2.7236801995288903e-05,
      "loss": 2.5838,
      "step": 23040
    },
    {
      "epoch": 3.193847859221283,
      "grad_norm": 14.79352855682373,
      "learning_rate": 2.7231259526118888e-05,
      "loss": 2.2918,
      "step": 23050
    },
    {
      "epoch": 3.195233476513787,
      "grad_norm": 14.062578201293945,
      "learning_rate": 2.7225717056948874e-05,
      "loss": 2.3624,
      "step": 23060
    },
    {
      "epoch": 3.196619093806291,
      "grad_norm": 17.80996322631836,
      "learning_rate": 2.722017458777886e-05,
      "loss": 2.3984,
      "step": 23070
    },
    {
      "epoch": 3.1980047110987946,
      "grad_norm": 10.62255573272705,
      "learning_rate": 2.721463211860884e-05,
      "loss": 3.1298,
      "step": 23080
    },
    {
      "epoch": 3.1993903283912983,
      "grad_norm": 13.700455665588379,
      "learning_rate": 2.720908964943883e-05,
      "loss": 2.2295,
      "step": 23090
    },
    {
      "epoch": 3.200775945683802,
      "grad_norm": 14.660219192504883,
      "learning_rate": 2.720354718026881e-05,
      "loss": 2.2544,
      "step": 23100
    },
    {
      "epoch": 3.202161562976306,
      "grad_norm": 16.002300262451172,
      "learning_rate": 2.7198004711098796e-05,
      "loss": 1.966,
      "step": 23110
    },
    {
      "epoch": 3.2035471802688096,
      "grad_norm": 9.957427024841309,
      "learning_rate": 2.719246224192878e-05,
      "loss": 1.9881,
      "step": 23120
    },
    {
      "epoch": 3.2049327975613138,
      "grad_norm": 18.484886169433594,
      "learning_rate": 2.7186919772758767e-05,
      "loss": 1.9864,
      "step": 23130
    },
    {
      "epoch": 3.2063184148538175,
      "grad_norm": 13.485727310180664,
      "learning_rate": 2.718137730358875e-05,
      "loss": 1.9709,
      "step": 23140
    },
    {
      "epoch": 3.2077040321463213,
      "grad_norm": 7.773565292358398,
      "learning_rate": 2.7175834834418737e-05,
      "loss": 2.4591,
      "step": 23150
    },
    {
      "epoch": 3.209089649438825,
      "grad_norm": 10.288116455078125,
      "learning_rate": 2.717029236524872e-05,
      "loss": 2.0833,
      "step": 23160
    },
    {
      "epoch": 3.2104752667313288,
      "grad_norm": 10.930059432983398,
      "learning_rate": 2.7164749896078704e-05,
      "loss": 2.2096,
      "step": 23170
    },
    {
      "epoch": 3.2118608840238325,
      "grad_norm": 15.3438138961792,
      "learning_rate": 2.715920742690869e-05,
      "loss": 2.5373,
      "step": 23180
    },
    {
      "epoch": 3.2132465013163363,
      "grad_norm": 11.406759262084961,
      "learning_rate": 2.7153664957738675e-05,
      "loss": 2.1769,
      "step": 23190
    },
    {
      "epoch": 3.21463211860884,
      "grad_norm": 9.928675651550293,
      "learning_rate": 2.7148122488568657e-05,
      "loss": 2.1588,
      "step": 23200
    },
    {
      "epoch": 3.216017735901344,
      "grad_norm": 22.45112419128418,
      "learning_rate": 2.7142580019398645e-05,
      "loss": 2.2841,
      "step": 23210
    },
    {
      "epoch": 3.217403353193848,
      "grad_norm": 15.72614860534668,
      "learning_rate": 2.7137037550228627e-05,
      "loss": 2.472,
      "step": 23220
    },
    {
      "epoch": 3.2187889704863517,
      "grad_norm": 16.627727508544922,
      "learning_rate": 2.7131495081058613e-05,
      "loss": 2.3098,
      "step": 23230
    },
    {
      "epoch": 3.2201745877788555,
      "grad_norm": 9.474202156066895,
      "learning_rate": 2.71259526118886e-05,
      "loss": 1.7117,
      "step": 23240
    },
    {
      "epoch": 3.221560205071359,
      "grad_norm": 10.140809059143066,
      "learning_rate": 2.7120410142718583e-05,
      "loss": 1.8217,
      "step": 23250
    },
    {
      "epoch": 3.222945822363863,
      "grad_norm": 8.325997352600098,
      "learning_rate": 2.7114867673548565e-05,
      "loss": 1.6528,
      "step": 23260
    },
    {
      "epoch": 3.2243314396563667,
      "grad_norm": 15.729673385620117,
      "learning_rate": 2.7109325204378554e-05,
      "loss": 2.5269,
      "step": 23270
    },
    {
      "epoch": 3.225717056948871,
      "grad_norm": 5.460456848144531,
      "learning_rate": 2.7103782735208535e-05,
      "loss": 1.8029,
      "step": 23280
    },
    {
      "epoch": 3.2271026742413746,
      "grad_norm": 17.812530517578125,
      "learning_rate": 2.709824026603852e-05,
      "loss": 3.0861,
      "step": 23290
    },
    {
      "epoch": 3.2284882915338784,
      "grad_norm": 16.107229232788086,
      "learning_rate": 2.709269779686851e-05,
      "loss": 2.4887,
      "step": 23300
    },
    {
      "epoch": 3.229873908826382,
      "grad_norm": 13.152801513671875,
      "learning_rate": 2.708715532769849e-05,
      "loss": 2.0904,
      "step": 23310
    },
    {
      "epoch": 3.231259526118886,
      "grad_norm": 10.2827787399292,
      "learning_rate": 2.708161285852848e-05,
      "loss": 1.9598,
      "step": 23320
    },
    {
      "epoch": 3.2326451434113896,
      "grad_norm": 5.460334777832031,
      "learning_rate": 2.707607038935846e-05,
      "loss": 2.3633,
      "step": 23330
    },
    {
      "epoch": 3.2340307607038934,
      "grad_norm": 10.977447509765625,
      "learning_rate": 2.7070527920188444e-05,
      "loss": 2.2395,
      "step": 23340
    },
    {
      "epoch": 3.2354163779963976,
      "grad_norm": 12.152626037597656,
      "learning_rate": 2.7064985451018432e-05,
      "loss": 2.914,
      "step": 23350
    },
    {
      "epoch": 3.2368019952889013,
      "grad_norm": 17.360410690307617,
      "learning_rate": 2.7059442981848417e-05,
      "loss": 2.3808,
      "step": 23360
    },
    {
      "epoch": 3.238187612581405,
      "grad_norm": 11.187582015991211,
      "learning_rate": 2.70539005126784e-05,
      "loss": 1.8143,
      "step": 23370
    },
    {
      "epoch": 3.239573229873909,
      "grad_norm": 8.849003791809082,
      "learning_rate": 2.7048358043508388e-05,
      "loss": 2.2816,
      "step": 23380
    },
    {
      "epoch": 3.2409588471664126,
      "grad_norm": 18.73025894165039,
      "learning_rate": 2.704281557433837e-05,
      "loss": 2.4579,
      "step": 23390
    },
    {
      "epoch": 3.2423444644589163,
      "grad_norm": 11.191319465637207,
      "learning_rate": 2.7037273105168355e-05,
      "loss": 2.3471,
      "step": 23400
    },
    {
      "epoch": 3.24373008175142,
      "grad_norm": 9.570734977722168,
      "learning_rate": 2.703173063599834e-05,
      "loss": 1.9945,
      "step": 23410
    },
    {
      "epoch": 3.2451156990439243,
      "grad_norm": 10.895598411560059,
      "learning_rate": 2.7026188166828326e-05,
      "loss": 2.3404,
      "step": 23420
    },
    {
      "epoch": 3.246501316336428,
      "grad_norm": 13.967999458312988,
      "learning_rate": 2.7020645697658307e-05,
      "loss": 2.1977,
      "step": 23430
    },
    {
      "epoch": 3.2478869336289318,
      "grad_norm": 13.465444564819336,
      "learning_rate": 2.7015103228488296e-05,
      "loss": 2.2528,
      "step": 23440
    },
    {
      "epoch": 3.2492725509214355,
      "grad_norm": 10.584470748901367,
      "learning_rate": 2.7009560759318278e-05,
      "loss": 2.7221,
      "step": 23450
    },
    {
      "epoch": 3.2506581682139393,
      "grad_norm": 11.588122367858887,
      "learning_rate": 2.7004018290148263e-05,
      "loss": 2.212,
      "step": 23460
    },
    {
      "epoch": 3.252043785506443,
      "grad_norm": 6.76398229598999,
      "learning_rate": 2.699847582097825e-05,
      "loss": 2.4693,
      "step": 23470
    },
    {
      "epoch": 3.2534294027989468,
      "grad_norm": 6.716969966888428,
      "learning_rate": 2.6992933351808234e-05,
      "loss": 2.2897,
      "step": 23480
    },
    {
      "epoch": 3.254815020091451,
      "grad_norm": 8.921539306640625,
      "learning_rate": 2.6987390882638216e-05,
      "loss": 2.507,
      "step": 23490
    },
    {
      "epoch": 3.2562006373839547,
      "grad_norm": 10.886080741882324,
      "learning_rate": 2.6981848413468204e-05,
      "loss": 2.0631,
      "step": 23500
    },
    {
      "epoch": 3.2575862546764585,
      "grad_norm": 8.08070182800293,
      "learning_rate": 2.6976305944298186e-05,
      "loss": 2.8941,
      "step": 23510
    },
    {
      "epoch": 3.258971871968962,
      "grad_norm": 10.54957389831543,
      "learning_rate": 2.697076347512817e-05,
      "loss": 2.52,
      "step": 23520
    },
    {
      "epoch": 3.260357489261466,
      "grad_norm": 16.467327117919922,
      "learning_rate": 2.6965221005958157e-05,
      "loss": 2.0157,
      "step": 23530
    },
    {
      "epoch": 3.2617431065539697,
      "grad_norm": 19.411102294921875,
      "learning_rate": 2.6959678536788142e-05,
      "loss": 2.2735,
      "step": 23540
    },
    {
      "epoch": 3.2631287238464735,
      "grad_norm": 9.280940055847168,
      "learning_rate": 2.6954136067618124e-05,
      "loss": 2.3229,
      "step": 23550
    },
    {
      "epoch": 3.2645143411389776,
      "grad_norm": 9.941913604736328,
      "learning_rate": 2.6948593598448112e-05,
      "loss": 1.8614,
      "step": 23560
    },
    {
      "epoch": 3.2658999584314814,
      "grad_norm": 11.90729808807373,
      "learning_rate": 2.6943051129278094e-05,
      "loss": 2.0546,
      "step": 23570
    },
    {
      "epoch": 3.267285575723985,
      "grad_norm": 9.480724334716797,
      "learning_rate": 2.693750866010808e-05,
      "loss": 2.1411,
      "step": 23580
    },
    {
      "epoch": 3.268671193016489,
      "grad_norm": 12.361067771911621,
      "learning_rate": 2.6931966190938065e-05,
      "loss": 1.9128,
      "step": 23590
    },
    {
      "epoch": 3.2700568103089926,
      "grad_norm": 13.966791152954102,
      "learning_rate": 2.692642372176805e-05,
      "loss": 2.478,
      "step": 23600
    },
    {
      "epoch": 3.2714424276014964,
      "grad_norm": 5.768478870391846,
      "learning_rate": 2.6921435499515033e-05,
      "loss": 2.0474,
      "step": 23610
    },
    {
      "epoch": 3.272828044894,
      "grad_norm": 13.444214820861816,
      "learning_rate": 2.691589303034502e-05,
      "loss": 2.1744,
      "step": 23620
    },
    {
      "epoch": 3.2742136621865043,
      "grad_norm": 9.246100425720215,
      "learning_rate": 2.6910350561175003e-05,
      "loss": 2.06,
      "step": 23630
    },
    {
      "epoch": 3.275599279479008,
      "grad_norm": 6.078278541564941,
      "learning_rate": 2.6904808092004992e-05,
      "loss": 2.1404,
      "step": 23640
    },
    {
      "epoch": 3.276984896771512,
      "grad_norm": 22.969276428222656,
      "learning_rate": 2.6899265622834977e-05,
      "loss": 2.3897,
      "step": 23650
    },
    {
      "epoch": 3.2783705140640156,
      "grad_norm": 8.538939476013184,
      "learning_rate": 2.689372315366496e-05,
      "loss": 2.2677,
      "step": 23660
    },
    {
      "epoch": 3.2797561313565193,
      "grad_norm": 16.448747634887695,
      "learning_rate": 2.6888180684494948e-05,
      "loss": 2.3481,
      "step": 23670
    },
    {
      "epoch": 3.281141748649023,
      "grad_norm": 11.8499116897583,
      "learning_rate": 2.688263821532493e-05,
      "loss": 2.2997,
      "step": 23680
    },
    {
      "epoch": 3.282527365941527,
      "grad_norm": 11.793905258178711,
      "learning_rate": 2.687709574615491e-05,
      "loss": 2.6377,
      "step": 23690
    },
    {
      "epoch": 3.2839129832340306,
      "grad_norm": 7.715320587158203,
      "learning_rate": 2.68715532769849e-05,
      "loss": 2.6973,
      "step": 23700
    },
    {
      "epoch": 3.2852986005265343,
      "grad_norm": 10.763051986694336,
      "learning_rate": 2.6866010807814885e-05,
      "loss": 2.4456,
      "step": 23710
    },
    {
      "epoch": 3.2866842178190385,
      "grad_norm": 11.511984825134277,
      "learning_rate": 2.6860468338644867e-05,
      "loss": 2.1719,
      "step": 23720
    },
    {
      "epoch": 3.2880698351115423,
      "grad_norm": 8.3026123046875,
      "learning_rate": 2.6854925869474856e-05,
      "loss": 1.9341,
      "step": 23730
    },
    {
      "epoch": 3.289455452404046,
      "grad_norm": 9.382244110107422,
      "learning_rate": 2.6849383400304838e-05,
      "loss": 2.8014,
      "step": 23740
    },
    {
      "epoch": 3.2908410696965498,
      "grad_norm": 16.21232032775879,
      "learning_rate": 2.6843840931134823e-05,
      "loss": 2.5135,
      "step": 23750
    },
    {
      "epoch": 3.2922266869890535,
      "grad_norm": 16.303876876831055,
      "learning_rate": 2.6838298461964808e-05,
      "loss": 2.3406,
      "step": 23760
    },
    {
      "epoch": 3.2936123042815573,
      "grad_norm": 13.026613235473633,
      "learning_rate": 2.6832755992794793e-05,
      "loss": 1.8509,
      "step": 23770
    },
    {
      "epoch": 3.294997921574061,
      "grad_norm": 11.307597160339355,
      "learning_rate": 2.6827213523624775e-05,
      "loss": 2.1229,
      "step": 23780
    },
    {
      "epoch": 3.296383538866565,
      "grad_norm": 11.062877655029297,
      "learning_rate": 2.6821671054454764e-05,
      "loss": 2.3836,
      "step": 23790
    },
    {
      "epoch": 3.297769156159069,
      "grad_norm": 11.384706497192383,
      "learning_rate": 2.6816128585284746e-05,
      "loss": 1.9379,
      "step": 23800
    },
    {
      "epoch": 3.2991547734515727,
      "grad_norm": 15.068496704101562,
      "learning_rate": 2.681058611611473e-05,
      "loss": 2.1128,
      "step": 23810
    },
    {
      "epoch": 3.3005403907440765,
      "grad_norm": 10.966035842895508,
      "learning_rate": 2.6805043646944716e-05,
      "loss": 2.4163,
      "step": 23820
    },
    {
      "epoch": 3.30192600803658,
      "grad_norm": 8.482686042785645,
      "learning_rate": 2.67995011777747e-05,
      "loss": 2.2914,
      "step": 23830
    },
    {
      "epoch": 3.303311625329084,
      "grad_norm": 19.51467514038086,
      "learning_rate": 2.6793958708604683e-05,
      "loss": 2.1944,
      "step": 23840
    },
    {
      "epoch": 3.3046972426215877,
      "grad_norm": 10.120482444763184,
      "learning_rate": 2.6788416239434672e-05,
      "loss": 1.7253,
      "step": 23850
    },
    {
      "epoch": 3.306082859914092,
      "grad_norm": 13.862007141113281,
      "learning_rate": 2.6782873770264654e-05,
      "loss": 2.5522,
      "step": 23860
    },
    {
      "epoch": 3.3074684772065956,
      "grad_norm": 15.771137237548828,
      "learning_rate": 2.677733130109464e-05,
      "loss": 2.3104,
      "step": 23870
    },
    {
      "epoch": 3.3088540944990994,
      "grad_norm": 18.69815444946289,
      "learning_rate": 2.6771788831924624e-05,
      "loss": 2.4384,
      "step": 23880
    },
    {
      "epoch": 3.310239711791603,
      "grad_norm": 9.492372512817383,
      "learning_rate": 2.676624636275461e-05,
      "loss": 2.2794,
      "step": 23890
    },
    {
      "epoch": 3.311625329084107,
      "grad_norm": 6.914906978607178,
      "learning_rate": 2.676070389358459e-05,
      "loss": 2.6084,
      "step": 23900
    },
    {
      "epoch": 3.3130109463766106,
      "grad_norm": 13.605351448059082,
      "learning_rate": 2.675516142441458e-05,
      "loss": 2.3015,
      "step": 23910
    },
    {
      "epoch": 3.3143965636691144,
      "grad_norm": 12.295372009277344,
      "learning_rate": 2.6749618955244562e-05,
      "loss": 2.6138,
      "step": 23920
    },
    {
      "epoch": 3.3157821809616186,
      "grad_norm": 19.372915267944336,
      "learning_rate": 2.674407648607455e-05,
      "loss": 2.2801,
      "step": 23930
    },
    {
      "epoch": 3.3171677982541223,
      "grad_norm": 10.223747253417969,
      "learning_rate": 2.6738534016904533e-05,
      "loss": 2.7381,
      "step": 23940
    },
    {
      "epoch": 3.318553415546626,
      "grad_norm": 11.054737091064453,
      "learning_rate": 2.6732991547734518e-05,
      "loss": 1.9291,
      "step": 23950
    },
    {
      "epoch": 3.31993903283913,
      "grad_norm": 14.568059921264648,
      "learning_rate": 2.6727449078564503e-05,
      "loss": 1.959,
      "step": 23960
    },
    {
      "epoch": 3.3213246501316336,
      "grad_norm": 8.7224760055542,
      "learning_rate": 2.6721906609394488e-05,
      "loss": 2.1311,
      "step": 23970
    },
    {
      "epoch": 3.3227102674241373,
      "grad_norm": 8.693578720092773,
      "learning_rate": 2.671636414022447e-05,
      "loss": 1.796,
      "step": 23980
    },
    {
      "epoch": 3.324095884716641,
      "grad_norm": 9.823234558105469,
      "learning_rate": 2.671082167105446e-05,
      "loss": 1.8376,
      "step": 23990
    },
    {
      "epoch": 3.3254815020091453,
      "grad_norm": 17.11298942565918,
      "learning_rate": 2.670527920188444e-05,
      "loss": 2.7898,
      "step": 24000
    },
    {
      "epoch": 3.326867119301649,
      "grad_norm": 10.889015197753906,
      "learning_rate": 2.6699736732714426e-05,
      "loss": 2.5457,
      "step": 24010
    },
    {
      "epoch": 3.3282527365941528,
      "grad_norm": 16.208017349243164,
      "learning_rate": 2.6694194263544415e-05,
      "loss": 2.1409,
      "step": 24020
    },
    {
      "epoch": 3.3296383538866565,
      "grad_norm": 10.995667457580566,
      "learning_rate": 2.6688651794374396e-05,
      "loss": 2.2747,
      "step": 24030
    },
    {
      "epoch": 3.3310239711791603,
      "grad_norm": 14.552783966064453,
      "learning_rate": 2.6683109325204378e-05,
      "loss": 2.436,
      "step": 24040
    },
    {
      "epoch": 3.332409588471664,
      "grad_norm": 12.26704216003418,
      "learning_rate": 2.6677566856034367e-05,
      "loss": 2.3112,
      "step": 24050
    },
    {
      "epoch": 3.3337952057641678,
      "grad_norm": 9.849265098571777,
      "learning_rate": 2.667202438686435e-05,
      "loss": 2.6326,
      "step": 24060
    },
    {
      "epoch": 3.335180823056672,
      "grad_norm": 7.534388542175293,
      "learning_rate": 2.6666481917694334e-05,
      "loss": 2.56,
      "step": 24070
    },
    {
      "epoch": 3.3365664403491757,
      "grad_norm": 12.299989700317383,
      "learning_rate": 2.6660939448524323e-05,
      "loss": 2.3146,
      "step": 24080
    },
    {
      "epoch": 3.3379520576416795,
      "grad_norm": 12.799025535583496,
      "learning_rate": 2.6655396979354305e-05,
      "loss": 2.5105,
      "step": 24090
    },
    {
      "epoch": 3.339337674934183,
      "grad_norm": 11.727411270141602,
      "learning_rate": 2.6649854510184286e-05,
      "loss": 2.6468,
      "step": 24100
    },
    {
      "epoch": 3.340723292226687,
      "grad_norm": 13.337535858154297,
      "learning_rate": 2.6644312041014275e-05,
      "loss": 2.4742,
      "step": 24110
    },
    {
      "epoch": 3.3421089095191907,
      "grad_norm": 7.530970096588135,
      "learning_rate": 2.6638769571844257e-05,
      "loss": 2.3774,
      "step": 24120
    },
    {
      "epoch": 3.3434945268116945,
      "grad_norm": 10.153142929077148,
      "learning_rate": 2.6633227102674242e-05,
      "loss": 2.1948,
      "step": 24130
    },
    {
      "epoch": 3.3448801441041986,
      "grad_norm": 10.066786766052246,
      "learning_rate": 2.662768463350423e-05,
      "loss": 2.1496,
      "step": 24140
    },
    {
      "epoch": 3.3462657613967024,
      "grad_norm": 15.311223030090332,
      "learning_rate": 2.6622142164334213e-05,
      "loss": 1.8246,
      "step": 24150
    },
    {
      "epoch": 3.347651378689206,
      "grad_norm": 12.200874328613281,
      "learning_rate": 2.6616599695164194e-05,
      "loss": 2.1793,
      "step": 24160
    },
    {
      "epoch": 3.34903699598171,
      "grad_norm": 14.697593688964844,
      "learning_rate": 2.6611057225994183e-05,
      "loss": 2.1493,
      "step": 24170
    },
    {
      "epoch": 3.3504226132742136,
      "grad_norm": 9.790630340576172,
      "learning_rate": 2.660551475682417e-05,
      "loss": 1.6892,
      "step": 24180
    },
    {
      "epoch": 3.3518082305667174,
      "grad_norm": 10.369073867797852,
      "learning_rate": 2.659997228765415e-05,
      "loss": 2.2045,
      "step": 24190
    },
    {
      "epoch": 3.353193847859221,
      "grad_norm": 13.611540794372559,
      "learning_rate": 2.659442981848414e-05,
      "loss": 2.3837,
      "step": 24200
    },
    {
      "epoch": 3.3545794651517253,
      "grad_norm": 14.888151168823242,
      "learning_rate": 2.658888734931412e-05,
      "loss": 2.108,
      "step": 24210
    },
    {
      "epoch": 3.355965082444229,
      "grad_norm": 9.406014442443848,
      "learning_rate": 2.658334488014411e-05,
      "loss": 2.2153,
      "step": 24220
    },
    {
      "epoch": 3.357350699736733,
      "grad_norm": 10.023831367492676,
      "learning_rate": 2.657780241097409e-05,
      "loss": 2.1093,
      "step": 24230
    },
    {
      "epoch": 3.3587363170292366,
      "grad_norm": 8.395734786987305,
      "learning_rate": 2.6572259941804076e-05,
      "loss": 2.2628,
      "step": 24240
    },
    {
      "epoch": 3.3601219343217403,
      "grad_norm": 19.64039421081543,
      "learning_rate": 2.6566717472634062e-05,
      "loss": 2.5247,
      "step": 24250
    },
    {
      "epoch": 3.361507551614244,
      "grad_norm": 12.161748886108398,
      "learning_rate": 2.6561175003464047e-05,
      "loss": 2.1151,
      "step": 24260
    },
    {
      "epoch": 3.362893168906748,
      "grad_norm": 12.623615264892578,
      "learning_rate": 2.655563253429403e-05,
      "loss": 2.3127,
      "step": 24270
    },
    {
      "epoch": 3.3642787861992516,
      "grad_norm": 12.40656566619873,
      "learning_rate": 2.6550090065124017e-05,
      "loss": 2.5422,
      "step": 24280
    },
    {
      "epoch": 3.3656644034917553,
      "grad_norm": 8.106850624084473,
      "learning_rate": 2.6544547595954e-05,
      "loss": 2.0754,
      "step": 24290
    },
    {
      "epoch": 3.3670500207842595,
      "grad_norm": 13.456271171569824,
      "learning_rate": 2.6539005126783985e-05,
      "loss": 2.0888,
      "step": 24300
    },
    {
      "epoch": 3.3684356380767633,
      "grad_norm": 19.489547729492188,
      "learning_rate": 2.653346265761397e-05,
      "loss": 1.8808,
      "step": 24310
    },
    {
      "epoch": 3.369821255369267,
      "grad_norm": 11.070097923278809,
      "learning_rate": 2.6527920188443955e-05,
      "loss": 2.3323,
      "step": 24320
    },
    {
      "epoch": 3.3712068726617708,
      "grad_norm": 11.977303504943848,
      "learning_rate": 2.6522377719273937e-05,
      "loss": 2.2263,
      "step": 24330
    },
    {
      "epoch": 3.3725924899542745,
      "grad_norm": 13.229072570800781,
      "learning_rate": 2.6516835250103926e-05,
      "loss": 2.3063,
      "step": 24340
    },
    {
      "epoch": 3.3739781072467783,
      "grad_norm": 13.111992835998535,
      "learning_rate": 2.6511292780933907e-05,
      "loss": 2.197,
      "step": 24350
    },
    {
      "epoch": 3.375363724539282,
      "grad_norm": 16.104795455932617,
      "learning_rate": 2.6505750311763893e-05,
      "loss": 2.3822,
      "step": 24360
    },
    {
      "epoch": 3.376749341831786,
      "grad_norm": 10.71567153930664,
      "learning_rate": 2.6500207842593878e-05,
      "loss": 2.3096,
      "step": 24370
    },
    {
      "epoch": 3.37813495912429,
      "grad_norm": 9.366057395935059,
      "learning_rate": 2.6494665373423863e-05,
      "loss": 2.2714,
      "step": 24380
    },
    {
      "epoch": 3.3795205764167937,
      "grad_norm": 16.026567459106445,
      "learning_rate": 2.6489122904253845e-05,
      "loss": 2.563,
      "step": 24390
    },
    {
      "epoch": 3.3809061937092975,
      "grad_norm": 7.8872222900390625,
      "learning_rate": 2.6483580435083834e-05,
      "loss": 2.1273,
      "step": 24400
    },
    {
      "epoch": 3.382291811001801,
      "grad_norm": 11.080194473266602,
      "learning_rate": 2.6478037965913816e-05,
      "loss": 2.7424,
      "step": 24410
    },
    {
      "epoch": 3.383677428294305,
      "grad_norm": 15.447726249694824,
      "learning_rate": 2.64724954967438e-05,
      "loss": 2.1981,
      "step": 24420
    },
    {
      "epoch": 3.3850630455868087,
      "grad_norm": 15.863603591918945,
      "learning_rate": 2.6466953027573786e-05,
      "loss": 2.3615,
      "step": 24430
    },
    {
      "epoch": 3.386448662879313,
      "grad_norm": 8.095836639404297,
      "learning_rate": 2.646141055840377e-05,
      "loss": 2.1941,
      "step": 24440
    },
    {
      "epoch": 3.3878342801718166,
      "grad_norm": 8.232274055480957,
      "learning_rate": 2.6455868089233753e-05,
      "loss": 1.9729,
      "step": 24450
    },
    {
      "epoch": 3.3892198974643204,
      "grad_norm": 5.3981170654296875,
      "learning_rate": 2.6450325620063742e-05,
      "loss": 1.6824,
      "step": 24460
    },
    {
      "epoch": 3.390605514756824,
      "grad_norm": 7.791531562805176,
      "learning_rate": 2.6444783150893724e-05,
      "loss": 2.0464,
      "step": 24470
    },
    {
      "epoch": 3.391991132049328,
      "grad_norm": 13.831395149230957,
      "learning_rate": 2.643924068172371e-05,
      "loss": 1.9628,
      "step": 24480
    },
    {
      "epoch": 3.3933767493418316,
      "grad_norm": 8.770391464233398,
      "learning_rate": 2.6433698212553694e-05,
      "loss": 2.5164,
      "step": 24490
    },
    {
      "epoch": 3.3947623666343354,
      "grad_norm": 16.624597549438477,
      "learning_rate": 2.642815574338368e-05,
      "loss": 2.2183,
      "step": 24500
    },
    {
      "epoch": 3.3961479839268396,
      "grad_norm": 17.029178619384766,
      "learning_rate": 2.6422613274213668e-05,
      "loss": 2.1631,
      "step": 24510
    },
    {
      "epoch": 3.3975336012193433,
      "grad_norm": 15.625740051269531,
      "learning_rate": 2.641707080504365e-05,
      "loss": 2.4981,
      "step": 24520
    },
    {
      "epoch": 3.398919218511847,
      "grad_norm": 7.7865214347839355,
      "learning_rate": 2.6411528335873632e-05,
      "loss": 1.9069,
      "step": 24530
    },
    {
      "epoch": 3.400304835804351,
      "grad_norm": 8.615549087524414,
      "learning_rate": 2.640598586670362e-05,
      "loss": 2.1428,
      "step": 24540
    },
    {
      "epoch": 3.4016904530968546,
      "grad_norm": 12.085102081298828,
      "learning_rate": 2.6400443397533602e-05,
      "loss": 2.3495,
      "step": 24550
    },
    {
      "epoch": 3.4030760703893583,
      "grad_norm": 8.390912055969238,
      "learning_rate": 2.6394900928363588e-05,
      "loss": 2.5319,
      "step": 24560
    },
    {
      "epoch": 3.404461687681862,
      "grad_norm": 14.128704071044922,
      "learning_rate": 2.6389358459193576e-05,
      "loss": 2.6871,
      "step": 24570
    },
    {
      "epoch": 3.4058473049743663,
      "grad_norm": 8.101208686828613,
      "learning_rate": 2.6383815990023558e-05,
      "loss": 2.185,
      "step": 24580
    },
    {
      "epoch": 3.40723292226687,
      "grad_norm": 12.45716381072998,
      "learning_rate": 2.637827352085354e-05,
      "loss": 1.8428,
      "step": 24590
    },
    {
      "epoch": 3.4086185395593738,
      "grad_norm": 13.139328956604004,
      "learning_rate": 2.637273105168353e-05,
      "loss": 1.9859,
      "step": 24600
    },
    {
      "epoch": 3.4100041568518775,
      "grad_norm": 9.125627517700195,
      "learning_rate": 2.6367188582513514e-05,
      "loss": 2.1682,
      "step": 24610
    },
    {
      "epoch": 3.4113897741443813,
      "grad_norm": 15.300470352172852,
      "learning_rate": 2.6361646113343496e-05,
      "loss": 2.2364,
      "step": 24620
    },
    {
      "epoch": 3.412775391436885,
      "grad_norm": 9.209656715393066,
      "learning_rate": 2.6356103644173484e-05,
      "loss": 2.6913,
      "step": 24630
    },
    {
      "epoch": 3.4141610087293888,
      "grad_norm": 10.112022399902344,
      "learning_rate": 2.6350561175003466e-05,
      "loss": 1.826,
      "step": 24640
    },
    {
      "epoch": 3.415546626021893,
      "grad_norm": 7.483942031860352,
      "learning_rate": 2.6345018705833448e-05,
      "loss": 1.8854,
      "step": 24650
    },
    {
      "epoch": 3.4169322433143967,
      "grad_norm": 9.011178970336914,
      "learning_rate": 2.6339476236663437e-05,
      "loss": 2.4033,
      "step": 24660
    },
    {
      "epoch": 3.4183178606069005,
      "grad_norm": 6.192559719085693,
      "learning_rate": 2.6333933767493422e-05,
      "loss": 2.5506,
      "step": 24670
    },
    {
      "epoch": 3.419703477899404,
      "grad_norm": 8.220314025878906,
      "learning_rate": 2.6328391298323404e-05,
      "loss": 2.3561,
      "step": 24680
    },
    {
      "epoch": 3.421089095191908,
      "grad_norm": 9.562234878540039,
      "learning_rate": 2.6322848829153392e-05,
      "loss": 2.2647,
      "step": 24690
    },
    {
      "epoch": 3.4224747124844117,
      "grad_norm": 14.621411323547363,
      "learning_rate": 2.6317306359983374e-05,
      "loss": 2.0431,
      "step": 24700
    },
    {
      "epoch": 3.4238603297769155,
      "grad_norm": 13.608356475830078,
      "learning_rate": 2.6311763890813356e-05,
      "loss": 2.1756,
      "step": 24710
    },
    {
      "epoch": 3.4252459470694196,
      "grad_norm": 11.31930923461914,
      "learning_rate": 2.6306221421643345e-05,
      "loss": 2.2096,
      "step": 24720
    },
    {
      "epoch": 3.4266315643619234,
      "grad_norm": 12.528091430664062,
      "learning_rate": 2.630067895247333e-05,
      "loss": 1.9658,
      "step": 24730
    },
    {
      "epoch": 3.428017181654427,
      "grad_norm": 7.990588665008545,
      "learning_rate": 2.6295136483303312e-05,
      "loss": 2.1404,
      "step": 24740
    },
    {
      "epoch": 3.429402798946931,
      "grad_norm": 15.760961532592773,
      "learning_rate": 2.62895940141333e-05,
      "loss": 1.9069,
      "step": 24750
    },
    {
      "epoch": 3.4307884162394346,
      "grad_norm": 5.532217979431152,
      "learning_rate": 2.6284051544963282e-05,
      "loss": 2.4672,
      "step": 24760
    },
    {
      "epoch": 3.4321740335319384,
      "grad_norm": 9.4578218460083,
      "learning_rate": 2.6278509075793268e-05,
      "loss": 2.2435,
      "step": 24770
    },
    {
      "epoch": 3.433559650824442,
      "grad_norm": 5.990697860717773,
      "learning_rate": 2.6272966606623253e-05,
      "loss": 2.2241,
      "step": 24780
    },
    {
      "epoch": 3.4349452681169463,
      "grad_norm": 5.5648040771484375,
      "learning_rate": 2.6267424137453238e-05,
      "loss": 1.6456,
      "step": 24790
    },
    {
      "epoch": 3.43633088540945,
      "grad_norm": 13.631044387817383,
      "learning_rate": 2.6261881668283223e-05,
      "loss": 1.9851,
      "step": 24800
    },
    {
      "epoch": 3.437716502701954,
      "grad_norm": 14.774276733398438,
      "learning_rate": 2.625633919911321e-05,
      "loss": 2.3137,
      "step": 24810
    },
    {
      "epoch": 3.4391021199944576,
      "grad_norm": 10.645277976989746,
      "learning_rate": 2.625079672994319e-05,
      "loss": 2.1147,
      "step": 24820
    },
    {
      "epoch": 3.4404877372869613,
      "grad_norm": 13.107551574707031,
      "learning_rate": 2.624525426077318e-05,
      "loss": 2.635,
      "step": 24830
    },
    {
      "epoch": 3.441873354579465,
      "grad_norm": 12.037287712097168,
      "learning_rate": 2.623971179160316e-05,
      "loss": 1.9732,
      "step": 24840
    },
    {
      "epoch": 3.443258971871969,
      "grad_norm": 8.856553077697754,
      "learning_rate": 2.6234169322433146e-05,
      "loss": 1.9615,
      "step": 24850
    },
    {
      "epoch": 3.4446445891644726,
      "grad_norm": 10.321398735046387,
      "learning_rate": 2.622862685326313e-05,
      "loss": 2.64,
      "step": 24860
    },
    {
      "epoch": 3.4460302064569768,
      "grad_norm": 11.369898796081543,
      "learning_rate": 2.6223084384093117e-05,
      "loss": 2.6492,
      "step": 24870
    },
    {
      "epoch": 3.4474158237494805,
      "grad_norm": 12.811119079589844,
      "learning_rate": 2.62175419149231e-05,
      "loss": 2.1226,
      "step": 24880
    },
    {
      "epoch": 3.4488014410419843,
      "grad_norm": 12.92216682434082,
      "learning_rate": 2.6211999445753087e-05,
      "loss": 2.1114,
      "step": 24890
    },
    {
      "epoch": 3.450187058334488,
      "grad_norm": 10.287108421325684,
      "learning_rate": 2.620645697658307e-05,
      "loss": 2.2283,
      "step": 24900
    },
    {
      "epoch": 3.4515726756269918,
      "grad_norm": 8.586713790893555,
      "learning_rate": 2.6200914507413054e-05,
      "loss": 2.1509,
      "step": 24910
    },
    {
      "epoch": 3.4529582929194955,
      "grad_norm": 13.045488357543945,
      "learning_rate": 2.619537203824304e-05,
      "loss": 2.1363,
      "step": 24920
    },
    {
      "epoch": 3.4543439102119993,
      "grad_norm": 8.233275413513184,
      "learning_rate": 2.6189829569073025e-05,
      "loss": 2.2222,
      "step": 24930
    },
    {
      "epoch": 3.455729527504503,
      "grad_norm": 9.59367847442627,
      "learning_rate": 2.6184287099903007e-05,
      "loss": 2.4131,
      "step": 24940
    },
    {
      "epoch": 3.457115144797007,
      "grad_norm": 12.576251029968262,
      "learning_rate": 2.6178744630732995e-05,
      "loss": 2.3405,
      "step": 24950
    },
    {
      "epoch": 3.458500762089511,
      "grad_norm": 11.201340675354004,
      "learning_rate": 2.6173202161562977e-05,
      "loss": 2.1139,
      "step": 24960
    },
    {
      "epoch": 3.4598863793820147,
      "grad_norm": 12.704569816589355,
      "learning_rate": 2.6167659692392962e-05,
      "loss": 1.7608,
      "step": 24970
    },
    {
      "epoch": 3.4612719966745185,
      "grad_norm": 11.824637413024902,
      "learning_rate": 2.6162117223222948e-05,
      "loss": 2.5509,
      "step": 24980
    },
    {
      "epoch": 3.462657613967022,
      "grad_norm": 8.824477195739746,
      "learning_rate": 2.6156574754052933e-05,
      "loss": 2.101,
      "step": 24990
    },
    {
      "epoch": 3.464043231259526,
      "grad_norm": 13.232518196105957,
      "learning_rate": 2.6151032284882915e-05,
      "loss": 1.7387,
      "step": 25000
    },
    {
      "epoch": 3.4654288485520297,
      "grad_norm": 12.613554954528809,
      "learning_rate": 2.6145489815712903e-05,
      "loss": 2.1414,
      "step": 25010
    },
    {
      "epoch": 3.466814465844534,
      "grad_norm": 16.85919189453125,
      "learning_rate": 2.6139947346542885e-05,
      "loss": 2.139,
      "step": 25020
    },
    {
      "epoch": 3.4682000831370376,
      "grad_norm": 11.415860176086426,
      "learning_rate": 2.613440487737287e-05,
      "loss": 2.3447,
      "step": 25030
    },
    {
      "epoch": 3.4695857004295414,
      "grad_norm": 14.396272659301758,
      "learning_rate": 2.612886240820286e-05,
      "loss": 2.3884,
      "step": 25040
    },
    {
      "epoch": 3.470971317722045,
      "grad_norm": 10.97027587890625,
      "learning_rate": 2.612331993903284e-05,
      "loss": 2.3561,
      "step": 25050
    },
    {
      "epoch": 3.472356935014549,
      "grad_norm": 15.23435115814209,
      "learning_rate": 2.6117777469862823e-05,
      "loss": 2.5266,
      "step": 25060
    },
    {
      "epoch": 3.4737425523070526,
      "grad_norm": 10.531839370727539,
      "learning_rate": 2.611223500069281e-05,
      "loss": 2.4714,
      "step": 25070
    },
    {
      "epoch": 3.4751281695995564,
      "grad_norm": 9.227566719055176,
      "learning_rate": 2.6106692531522793e-05,
      "loss": 2.1496,
      "step": 25080
    },
    {
      "epoch": 3.4765137868920606,
      "grad_norm": 8.107879638671875,
      "learning_rate": 2.6101150062352782e-05,
      "loss": 2.1199,
      "step": 25090
    },
    {
      "epoch": 3.4778994041845643,
      "grad_norm": 9.02291488647461,
      "learning_rate": 2.6095607593182767e-05,
      "loss": 2.5792,
      "step": 25100
    },
    {
      "epoch": 3.479285021477068,
      "grad_norm": 10.689933776855469,
      "learning_rate": 2.609006512401275e-05,
      "loss": 1.9467,
      "step": 25110
    },
    {
      "epoch": 3.480670638769572,
      "grad_norm": 10.735749244689941,
      "learning_rate": 2.6084522654842738e-05,
      "loss": 2.1281,
      "step": 25120
    },
    {
      "epoch": 3.4820562560620756,
      "grad_norm": 15.075339317321777,
      "learning_rate": 2.607898018567272e-05,
      "loss": 2.1234,
      "step": 25130
    },
    {
      "epoch": 3.4834418733545793,
      "grad_norm": 9.909876823425293,
      "learning_rate": 2.60734377165027e-05,
      "loss": 2.0576,
      "step": 25140
    },
    {
      "epoch": 3.484827490647083,
      "grad_norm": 13.323701858520508,
      "learning_rate": 2.606789524733269e-05,
      "loss": 2.1915,
      "step": 25150
    },
    {
      "epoch": 3.4862131079395873,
      "grad_norm": 10.1321439743042,
      "learning_rate": 2.6062352778162675e-05,
      "loss": 2.1651,
      "step": 25160
    },
    {
      "epoch": 3.487598725232091,
      "grad_norm": 14.51296615600586,
      "learning_rate": 2.6056810308992657e-05,
      "loss": 2.3482,
      "step": 25170
    },
    {
      "epoch": 3.4889843425245948,
      "grad_norm": 10.202452659606934,
      "learning_rate": 2.6051267839822646e-05,
      "loss": 1.9391,
      "step": 25180
    },
    {
      "epoch": 3.4903699598170985,
      "grad_norm": 12.27255630493164,
      "learning_rate": 2.6045725370652628e-05,
      "loss": 2.4873,
      "step": 25190
    },
    {
      "epoch": 3.4917555771096023,
      "grad_norm": 7.053696632385254,
      "learning_rate": 2.6040182901482613e-05,
      "loss": 1.948,
      "step": 25200
    },
    {
      "epoch": 3.493141194402106,
      "grad_norm": 13.643346786499023,
      "learning_rate": 2.6034640432312598e-05,
      "loss": 1.8159,
      "step": 25210
    },
    {
      "epoch": 3.4945268116946098,
      "grad_norm": 8.643291473388672,
      "learning_rate": 2.6029097963142583e-05,
      "loss": 2.1919,
      "step": 25220
    },
    {
      "epoch": 3.495912428987114,
      "grad_norm": 8.434326171875,
      "learning_rate": 2.6023555493972565e-05,
      "loss": 2.2852,
      "step": 25230
    },
    {
      "epoch": 3.4972980462796177,
      "grad_norm": 10.64412784576416,
      "learning_rate": 2.6018013024802554e-05,
      "loss": 2.125,
      "step": 25240
    },
    {
      "epoch": 3.4986836635721215,
      "grad_norm": 15.610028266906738,
      "learning_rate": 2.6012470555632536e-05,
      "loss": 1.9397,
      "step": 25250
    },
    {
      "epoch": 3.500069280864625,
      "grad_norm": 17.665645599365234,
      "learning_rate": 2.600692808646252e-05,
      "loss": 2.6974,
      "step": 25260
    },
    {
      "epoch": 3.501454898157129,
      "grad_norm": 13.046060562133789,
      "learning_rate": 2.6001385617292506e-05,
      "loss": 2.5473,
      "step": 25270
    },
    {
      "epoch": 3.5028405154496327,
      "grad_norm": 13.607665061950684,
      "learning_rate": 2.599584314812249e-05,
      "loss": 2.339,
      "step": 25280
    },
    {
      "epoch": 3.5042261327421365,
      "grad_norm": 11.3447265625,
      "learning_rate": 2.5990300678952473e-05,
      "loss": 2.4186,
      "step": 25290
    },
    {
      "epoch": 3.5056117500346407,
      "grad_norm": 12.867144584655762,
      "learning_rate": 2.5984758209782462e-05,
      "loss": 2.1136,
      "step": 25300
    },
    {
      "epoch": 3.5069973673271444,
      "grad_norm": 11.869894027709961,
      "learning_rate": 2.5979215740612444e-05,
      "loss": 2.6327,
      "step": 25310
    },
    {
      "epoch": 3.508382984619648,
      "grad_norm": 17.587888717651367,
      "learning_rate": 2.597367327144243e-05,
      "loss": 3.0766,
      "step": 25320
    },
    {
      "epoch": 3.509768601912152,
      "grad_norm": 11.320342063903809,
      "learning_rate": 2.5968130802272414e-05,
      "loss": 2.5453,
      "step": 25330
    },
    {
      "epoch": 3.5111542192046556,
      "grad_norm": 13.60000228881836,
      "learning_rate": 2.59625883331024e-05,
      "loss": 2.1128,
      "step": 25340
    },
    {
      "epoch": 3.5125398364971594,
      "grad_norm": 4.9908599853515625,
      "learning_rate": 2.595704586393238e-05,
      "loss": 2.115,
      "step": 25350
    },
    {
      "epoch": 3.513925453789663,
      "grad_norm": 12.250699043273926,
      "learning_rate": 2.595150339476237e-05,
      "loss": 2.5624,
      "step": 25360
    },
    {
      "epoch": 3.5153110710821673,
      "grad_norm": 8.54996109008789,
      "learning_rate": 2.5945960925592352e-05,
      "loss": 1.9535,
      "step": 25370
    },
    {
      "epoch": 3.5166966883746706,
      "grad_norm": 6.536498546600342,
      "learning_rate": 2.594041845642234e-05,
      "loss": 2.3901,
      "step": 25380
    },
    {
      "epoch": 3.518082305667175,
      "grad_norm": 14.385785102844238,
      "learning_rate": 2.5934875987252323e-05,
      "loss": 2.8423,
      "step": 25390
    },
    {
      "epoch": 3.5194679229596786,
      "grad_norm": 14.784457206726074,
      "learning_rate": 2.5929333518082308e-05,
      "loss": 2.3807,
      "step": 25400
    },
    {
      "epoch": 3.5208535402521823,
      "grad_norm": 7.078869342803955,
      "learning_rate": 2.5923791048912293e-05,
      "loss": 1.9419,
      "step": 25410
    },
    {
      "epoch": 3.522239157544686,
      "grad_norm": 10.849215507507324,
      "learning_rate": 2.5918248579742278e-05,
      "loss": 2.456,
      "step": 25420
    },
    {
      "epoch": 3.52362477483719,
      "grad_norm": 11.339488983154297,
      "learning_rate": 2.591270611057226e-05,
      "loss": 2.2849,
      "step": 25430
    },
    {
      "epoch": 3.525010392129694,
      "grad_norm": 14.47187614440918,
      "learning_rate": 2.590716364140225e-05,
      "loss": 2.555,
      "step": 25440
    },
    {
      "epoch": 3.5263960094221973,
      "grad_norm": 10.612250328063965,
      "learning_rate": 2.590162117223223e-05,
      "loss": 2.4738,
      "step": 25450
    },
    {
      "epoch": 3.5277816267147015,
      "grad_norm": 12.347678184509277,
      "learning_rate": 2.5896078703062216e-05,
      "loss": 2.0485,
      "step": 25460
    },
    {
      "epoch": 3.5291672440072053,
      "grad_norm": 13.313693046569824,
      "learning_rate": 2.58905362338922e-05,
      "loss": 2.4873,
      "step": 25470
    },
    {
      "epoch": 3.530552861299709,
      "grad_norm": 16.451568603515625,
      "learning_rate": 2.5884993764722186e-05,
      "loss": 2.5239,
      "step": 25480
    },
    {
      "epoch": 3.5319384785922128,
      "grad_norm": 10.036116600036621,
      "learning_rate": 2.5879451295552168e-05,
      "loss": 1.9317,
      "step": 25490
    },
    {
      "epoch": 3.5333240958847165,
      "grad_norm": 16.778308868408203,
      "learning_rate": 2.5873908826382157e-05,
      "loss": 2.2061,
      "step": 25500
    },
    {
      "epoch": 3.5347097131772207,
      "grad_norm": 10.905715942382812,
      "learning_rate": 2.586836635721214e-05,
      "loss": 2.2856,
      "step": 25510
    },
    {
      "epoch": 3.536095330469724,
      "grad_norm": 10.47582721710205,
      "learning_rate": 2.5862823888042124e-05,
      "loss": 2.8802,
      "step": 25520
    },
    {
      "epoch": 3.537480947762228,
      "grad_norm": 13.012140274047852,
      "learning_rate": 2.5857281418872113e-05,
      "loss": 2.0091,
      "step": 25530
    },
    {
      "epoch": 3.538866565054732,
      "grad_norm": 16.91731834411621,
      "learning_rate": 2.5851738949702095e-05,
      "loss": 1.9923,
      "step": 25540
    },
    {
      "epoch": 3.5402521823472357,
      "grad_norm": 12.998452186584473,
      "learning_rate": 2.5846196480532076e-05,
      "loss": 2.6797,
      "step": 25550
    },
    {
      "epoch": 3.5416377996397395,
      "grad_norm": 17.947935104370117,
      "learning_rate": 2.5840654011362065e-05,
      "loss": 2.5448,
      "step": 25560
    },
    {
      "epoch": 3.543023416932243,
      "grad_norm": 8.871061325073242,
      "learning_rate": 2.5835111542192047e-05,
      "loss": 2.0776,
      "step": 25570
    },
    {
      "epoch": 3.5444090342247474,
      "grad_norm": 8.49813461303711,
      "learning_rate": 2.5829569073022032e-05,
      "loss": 1.3886,
      "step": 25580
    },
    {
      "epoch": 3.5457946515172507,
      "grad_norm": 8.73059368133545,
      "learning_rate": 2.582402660385202e-05,
      "loss": 1.8593,
      "step": 25590
    },
    {
      "epoch": 3.547180268809755,
      "grad_norm": 6.950554847717285,
      "learning_rate": 2.5818484134682003e-05,
      "loss": 1.9874,
      "step": 25600
    },
    {
      "epoch": 3.5485658861022586,
      "grad_norm": 9.286229133605957,
      "learning_rate": 2.5812941665511984e-05,
      "loss": 2.2635,
      "step": 25610
    },
    {
      "epoch": 3.5499515033947624,
      "grad_norm": 15.747862815856934,
      "learning_rate": 2.5807399196341973e-05,
      "loss": 2.2411,
      "step": 25620
    },
    {
      "epoch": 3.551337120687266,
      "grad_norm": 12.695563316345215,
      "learning_rate": 2.580185672717196e-05,
      "loss": 1.9282,
      "step": 25630
    },
    {
      "epoch": 3.55272273797977,
      "grad_norm": 8.711986541748047,
      "learning_rate": 2.579631425800194e-05,
      "loss": 2.1384,
      "step": 25640
    },
    {
      "epoch": 3.5541083552722736,
      "grad_norm": 16.157546997070312,
      "learning_rate": 2.579077178883193e-05,
      "loss": 2.2797,
      "step": 25650
    },
    {
      "epoch": 3.5554939725647774,
      "grad_norm": 16.563337326049805,
      "learning_rate": 2.578522931966191e-05,
      "loss": 2.1822,
      "step": 25660
    },
    {
      "epoch": 3.5568795898572816,
      "grad_norm": 13.978148460388184,
      "learning_rate": 2.57796868504919e-05,
      "loss": 2.1127,
      "step": 25670
    },
    {
      "epoch": 3.5582652071497853,
      "grad_norm": 9.25126838684082,
      "learning_rate": 2.5774698628238882e-05,
      "loss": 2.8149,
      "step": 25680
    },
    {
      "epoch": 3.559650824442289,
      "grad_norm": 7.545522689819336,
      "learning_rate": 2.5769156159068868e-05,
      "loss": 2.5188,
      "step": 25690
    },
    {
      "epoch": 3.561036441734793,
      "grad_norm": 5.63252592086792,
      "learning_rate": 2.5763613689898853e-05,
      "loss": 1.9011,
      "step": 25700
    },
    {
      "epoch": 3.5624220590272966,
      "grad_norm": 10.149559020996094,
      "learning_rate": 2.5758071220728838e-05,
      "loss": 1.8798,
      "step": 25710
    },
    {
      "epoch": 3.5638076763198003,
      "grad_norm": 12.76091480255127,
      "learning_rate": 2.575252875155882e-05,
      "loss": 2.6137,
      "step": 25720
    },
    {
      "epoch": 3.565193293612304,
      "grad_norm": 9.147540092468262,
      "learning_rate": 2.574698628238881e-05,
      "loss": 2.4011,
      "step": 25730
    },
    {
      "epoch": 3.5665789109048083,
      "grad_norm": 7.360151290893555,
      "learning_rate": 2.574144381321879e-05,
      "loss": 2.1474,
      "step": 25740
    },
    {
      "epoch": 3.567964528197312,
      "grad_norm": 10.884910583496094,
      "learning_rate": 2.5735901344048776e-05,
      "loss": 2.0734,
      "step": 25750
    },
    {
      "epoch": 3.5693501454898158,
      "grad_norm": 14.940260887145996,
      "learning_rate": 2.573035887487876e-05,
      "loss": 1.8063,
      "step": 25760
    },
    {
      "epoch": 3.5707357627823195,
      "grad_norm": 15.852640151977539,
      "learning_rate": 2.5724816405708746e-05,
      "loss": 2.1132,
      "step": 25770
    },
    {
      "epoch": 3.5721213800748233,
      "grad_norm": 13.750964164733887,
      "learning_rate": 2.5719273936538728e-05,
      "loss": 2.1901,
      "step": 25780
    },
    {
      "epoch": 3.573506997367327,
      "grad_norm": 9.965787887573242,
      "learning_rate": 2.5713731467368717e-05,
      "loss": 2.0965,
      "step": 25790
    },
    {
      "epoch": 3.5748926146598308,
      "grad_norm": 9.273914337158203,
      "learning_rate": 2.57081889981987e-05,
      "loss": 2.4762,
      "step": 25800
    },
    {
      "epoch": 3.576278231952335,
      "grad_norm": 10.042799949645996,
      "learning_rate": 2.5702646529028684e-05,
      "loss": 2.0665,
      "step": 25810
    },
    {
      "epoch": 3.5776638492448387,
      "grad_norm": 17.04026985168457,
      "learning_rate": 2.569710405985867e-05,
      "loss": 2.0085,
      "step": 25820
    },
    {
      "epoch": 3.5790494665373425,
      "grad_norm": 9.885078430175781,
      "learning_rate": 2.5691561590688654e-05,
      "loss": 2.2559,
      "step": 25830
    },
    {
      "epoch": 3.580435083829846,
      "grad_norm": 9.090843200683594,
      "learning_rate": 2.5686019121518636e-05,
      "loss": 2.1524,
      "step": 25840
    },
    {
      "epoch": 3.58182070112235,
      "grad_norm": 14.243635177612305,
      "learning_rate": 2.5680476652348625e-05,
      "loss": 2.0107,
      "step": 25850
    },
    {
      "epoch": 3.5832063184148537,
      "grad_norm": 9.80628490447998,
      "learning_rate": 2.5674934183178607e-05,
      "loss": 2.2314,
      "step": 25860
    },
    {
      "epoch": 3.5845919357073575,
      "grad_norm": 8.97481632232666,
      "learning_rate": 2.5669391714008592e-05,
      "loss": 2.2969,
      "step": 25870
    },
    {
      "epoch": 3.5859775529998617,
      "grad_norm": 13.875848770141602,
      "learning_rate": 2.566384924483858e-05,
      "loss": 2.4997,
      "step": 25880
    },
    {
      "epoch": 3.5873631702923654,
      "grad_norm": 13.154411315917969,
      "learning_rate": 2.5658306775668562e-05,
      "loss": 2.0649,
      "step": 25890
    },
    {
      "epoch": 3.588748787584869,
      "grad_norm": 14.26586627960205,
      "learning_rate": 2.5652764306498544e-05,
      "loss": 2.2356,
      "step": 25900
    },
    {
      "epoch": 3.590134404877373,
      "grad_norm": 12.278298377990723,
      "learning_rate": 2.5647221837328533e-05,
      "loss": 2.4117,
      "step": 25910
    },
    {
      "epoch": 3.5915200221698766,
      "grad_norm": 15.962363243103027,
      "learning_rate": 2.5641679368158515e-05,
      "loss": 1.7605,
      "step": 25920
    },
    {
      "epoch": 3.5929056394623804,
      "grad_norm": 9.848987579345703,
      "learning_rate": 2.56361368989885e-05,
      "loss": 1.9307,
      "step": 25930
    },
    {
      "epoch": 3.594291256754884,
      "grad_norm": 8.081972122192383,
      "learning_rate": 2.563059442981849e-05,
      "loss": 2.1602,
      "step": 25940
    },
    {
      "epoch": 3.5956768740473883,
      "grad_norm": 8.77184009552002,
      "learning_rate": 2.562505196064847e-05,
      "loss": 2.316,
      "step": 25950
    },
    {
      "epoch": 3.597062491339892,
      "grad_norm": 20.0908260345459,
      "learning_rate": 2.5619509491478452e-05,
      "loss": 2.0464,
      "step": 25960
    },
    {
      "epoch": 3.598448108632396,
      "grad_norm": 14.8848295211792,
      "learning_rate": 2.561396702230844e-05,
      "loss": 2.274,
      "step": 25970
    },
    {
      "epoch": 3.5998337259248996,
      "grad_norm": 12.818035125732422,
      "learning_rate": 2.5608424553138423e-05,
      "loss": 2.4921,
      "step": 25980
    },
    {
      "epoch": 3.6012193432174033,
      "grad_norm": 10.480123519897461,
      "learning_rate": 2.560288208396841e-05,
      "loss": 2.068,
      "step": 25990
    },
    {
      "epoch": 3.602604960509907,
      "grad_norm": 16.210905075073242,
      "learning_rate": 2.5597339614798397e-05,
      "loss": 2.296,
      "step": 26000
    },
    {
      "epoch": 3.603990577802411,
      "grad_norm": 14.61007308959961,
      "learning_rate": 2.559179714562838e-05,
      "loss": 2.1609,
      "step": 26010
    },
    {
      "epoch": 3.605376195094915,
      "grad_norm": 6.228133201599121,
      "learning_rate": 2.5586254676458367e-05,
      "loss": 1.9823,
      "step": 26020
    },
    {
      "epoch": 3.6067618123874183,
      "grad_norm": 13.704460144042969,
      "learning_rate": 2.558071220728835e-05,
      "loss": 2.514,
      "step": 26030
    },
    {
      "epoch": 3.6081474296799225,
      "grad_norm": 7.988935947418213,
      "learning_rate": 2.5575169738118334e-05,
      "loss": 2.5007,
      "step": 26040
    },
    {
      "epoch": 3.6095330469724263,
      "grad_norm": 10.875155448913574,
      "learning_rate": 2.556962726894832e-05,
      "loss": 1.9651,
      "step": 26050
    },
    {
      "epoch": 3.61091866426493,
      "grad_norm": 13.303410530090332,
      "learning_rate": 2.5564084799778305e-05,
      "loss": 2.1153,
      "step": 26060
    },
    {
      "epoch": 3.6123042815574338,
      "grad_norm": 11.130239486694336,
      "learning_rate": 2.5558542330608287e-05,
      "loss": 2.973,
      "step": 26070
    },
    {
      "epoch": 3.6136898988499375,
      "grad_norm": 12.545540809631348,
      "learning_rate": 2.5552999861438275e-05,
      "loss": 2.511,
      "step": 26080
    },
    {
      "epoch": 3.6150755161424417,
      "grad_norm": 17.690582275390625,
      "learning_rate": 2.5547457392268257e-05,
      "loss": 2.2913,
      "step": 26090
    },
    {
      "epoch": 3.616461133434945,
      "grad_norm": 12.943746566772461,
      "learning_rate": 2.5541914923098242e-05,
      "loss": 2.2522,
      "step": 26100
    },
    {
      "epoch": 3.617846750727449,
      "grad_norm": 13.906237602233887,
      "learning_rate": 2.5536372453928228e-05,
      "loss": 2.6141,
      "step": 26110
    },
    {
      "epoch": 3.619232368019953,
      "grad_norm": 10.648656845092773,
      "learning_rate": 2.5530829984758213e-05,
      "loss": 1.9579,
      "step": 26120
    },
    {
      "epoch": 3.6206179853124567,
      "grad_norm": 11.150115013122559,
      "learning_rate": 2.5525287515588195e-05,
      "loss": 2.4994,
      "step": 26130
    },
    {
      "epoch": 3.6220036026049605,
      "grad_norm": 20.038267135620117,
      "learning_rate": 2.5519745046418183e-05,
      "loss": 2.0495,
      "step": 26140
    },
    {
      "epoch": 3.623389219897464,
      "grad_norm": 11.940194129943848,
      "learning_rate": 2.5514202577248165e-05,
      "loss": 2.3398,
      "step": 26150
    },
    {
      "epoch": 3.6247748371899684,
      "grad_norm": 6.627270221710205,
      "learning_rate": 2.550866010807815e-05,
      "loss": 2.3824,
      "step": 26160
    },
    {
      "epoch": 3.6261604544824717,
      "grad_norm": 13.788797378540039,
      "learning_rate": 2.5503117638908136e-05,
      "loss": 2.1433,
      "step": 26170
    },
    {
      "epoch": 3.627546071774976,
      "grad_norm": 16.126514434814453,
      "learning_rate": 2.549757516973812e-05,
      "loss": 1.483,
      "step": 26180
    },
    {
      "epoch": 3.6289316890674796,
      "grad_norm": 17.023773193359375,
      "learning_rate": 2.5492032700568103e-05,
      "loss": 1.976,
      "step": 26190
    },
    {
      "epoch": 3.6303173063599834,
      "grad_norm": 9.808987617492676,
      "learning_rate": 2.548649023139809e-05,
      "loss": 2.144,
      "step": 26200
    },
    {
      "epoch": 3.631702923652487,
      "grad_norm": 14.358009338378906,
      "learning_rate": 2.5480947762228073e-05,
      "loss": 2.4444,
      "step": 26210
    },
    {
      "epoch": 3.633088540944991,
      "grad_norm": 12.294520378112793,
      "learning_rate": 2.547540529305806e-05,
      "loss": 2.4498,
      "step": 26220
    },
    {
      "epoch": 3.6344741582374946,
      "grad_norm": 12.605379104614258,
      "learning_rate": 2.5469862823888044e-05,
      "loss": 2.6499,
      "step": 26230
    },
    {
      "epoch": 3.6358597755299984,
      "grad_norm": 9.257476806640625,
      "learning_rate": 2.546432035471803e-05,
      "loss": 2.2491,
      "step": 26240
    },
    {
      "epoch": 3.6372453928225026,
      "grad_norm": 13.442513465881348,
      "learning_rate": 2.545877788554801e-05,
      "loss": 2.2785,
      "step": 26250
    },
    {
      "epoch": 3.6386310101150063,
      "grad_norm": 12.373769760131836,
      "learning_rate": 2.5453235416378e-05,
      "loss": 2.0775,
      "step": 26260
    },
    {
      "epoch": 3.64001662740751,
      "grad_norm": 9.821732521057129,
      "learning_rate": 2.544769294720798e-05,
      "loss": 2.1583,
      "step": 26270
    },
    {
      "epoch": 3.641402244700014,
      "grad_norm": 16.72320556640625,
      "learning_rate": 2.5442150478037967e-05,
      "loss": 2.4889,
      "step": 26280
    },
    {
      "epoch": 3.6427878619925176,
      "grad_norm": 8.250027656555176,
      "learning_rate": 2.5436608008867952e-05,
      "loss": 2.4377,
      "step": 26290
    },
    {
      "epoch": 3.6441734792850213,
      "grad_norm": 6.555159091949463,
      "learning_rate": 2.5431065539697937e-05,
      "loss": 2.3843,
      "step": 26300
    },
    {
      "epoch": 3.645559096577525,
      "grad_norm": 7.919610023498535,
      "learning_rate": 2.5425523070527926e-05,
      "loss": 2.367,
      "step": 26310
    },
    {
      "epoch": 3.6469447138700293,
      "grad_norm": 14.566654205322266,
      "learning_rate": 2.5419980601357908e-05,
      "loss": 2.0359,
      "step": 26320
    },
    {
      "epoch": 3.648330331162533,
      "grad_norm": 11.745908737182617,
      "learning_rate": 2.541443813218789e-05,
      "loss": 2.1488,
      "step": 26330
    },
    {
      "epoch": 3.6497159484550368,
      "grad_norm": 11.915908813476562,
      "learning_rate": 2.540889566301788e-05,
      "loss": 2.4755,
      "step": 26340
    },
    {
      "epoch": 3.6511015657475405,
      "grad_norm": 16.66105842590332,
      "learning_rate": 2.540335319384786e-05,
      "loss": 2.0461,
      "step": 26350
    },
    {
      "epoch": 3.6524871830400443,
      "grad_norm": 10.147417068481445,
      "learning_rate": 2.5397810724677845e-05,
      "loss": 2.6017,
      "step": 26360
    },
    {
      "epoch": 3.653872800332548,
      "grad_norm": 15.596556663513184,
      "learning_rate": 2.5392268255507834e-05,
      "loss": 2.3881,
      "step": 26370
    },
    {
      "epoch": 3.6552584176250518,
      "grad_norm": 11.243624687194824,
      "learning_rate": 2.5386725786337816e-05,
      "loss": 2.6647,
      "step": 26380
    },
    {
      "epoch": 3.656644034917556,
      "grad_norm": 11.743786811828613,
      "learning_rate": 2.5381183317167798e-05,
      "loss": 2.2956,
      "step": 26390
    },
    {
      "epoch": 3.6580296522100597,
      "grad_norm": 17.901174545288086,
      "learning_rate": 2.5375640847997786e-05,
      "loss": 2.6747,
      "step": 26400
    },
    {
      "epoch": 3.6594152695025635,
      "grad_norm": 6.973747730255127,
      "learning_rate": 2.5370098378827768e-05,
      "loss": 2.1452,
      "step": 26410
    },
    {
      "epoch": 3.660800886795067,
      "grad_norm": 8.210975646972656,
      "learning_rate": 2.5364555909657754e-05,
      "loss": 1.9882,
      "step": 26420
    },
    {
      "epoch": 3.662186504087571,
      "grad_norm": 14.59443187713623,
      "learning_rate": 2.5359013440487742e-05,
      "loss": 2.1081,
      "step": 26430
    },
    {
      "epoch": 3.6635721213800747,
      "grad_norm": 20.24854278564453,
      "learning_rate": 2.5353470971317724e-05,
      "loss": 2.0907,
      "step": 26440
    },
    {
      "epoch": 3.6649577386725785,
      "grad_norm": 16.085186004638672,
      "learning_rate": 2.5347928502147706e-05,
      "loss": 2.2718,
      "step": 26450
    },
    {
      "epoch": 3.6663433559650827,
      "grad_norm": 13.378207206726074,
      "learning_rate": 2.5342386032977695e-05,
      "loss": 2.1151,
      "step": 26460
    },
    {
      "epoch": 3.6677289732575864,
      "grad_norm": 13.126585006713867,
      "learning_rate": 2.533684356380768e-05,
      "loss": 2.5489,
      "step": 26470
    },
    {
      "epoch": 3.66911459055009,
      "grad_norm": 12.347573280334473,
      "learning_rate": 2.533130109463766e-05,
      "loss": 2.3373,
      "step": 26480
    },
    {
      "epoch": 3.670500207842594,
      "grad_norm": 28.168445587158203,
      "learning_rate": 2.532575862546765e-05,
      "loss": 1.8437,
      "step": 26490
    },
    {
      "epoch": 3.6718858251350976,
      "grad_norm": 8.822393417358398,
      "learning_rate": 2.5320216156297632e-05,
      "loss": 2.4956,
      "step": 26500
    },
    {
      "epoch": 3.6732714424276014,
      "grad_norm": 8.794726371765137,
      "learning_rate": 2.5314673687127614e-05,
      "loss": 1.8722,
      "step": 26510
    },
    {
      "epoch": 3.674657059720105,
      "grad_norm": 11.927789688110352,
      "learning_rate": 2.5309131217957603e-05,
      "loss": 1.9332,
      "step": 26520
    },
    {
      "epoch": 3.6760426770126093,
      "grad_norm": 9.72514533996582,
      "learning_rate": 2.5303588748787588e-05,
      "loss": 2.2704,
      "step": 26530
    },
    {
      "epoch": 3.677428294305113,
      "grad_norm": 17.551246643066406,
      "learning_rate": 2.529804627961757e-05,
      "loss": 2.5204,
      "step": 26540
    },
    {
      "epoch": 3.678813911597617,
      "grad_norm": 8.69459342956543,
      "learning_rate": 2.529250381044756e-05,
      "loss": 2.3296,
      "step": 26550
    },
    {
      "epoch": 3.6801995288901206,
      "grad_norm": 14.266083717346191,
      "learning_rate": 2.528696134127754e-05,
      "loss": 2.4608,
      "step": 26560
    },
    {
      "epoch": 3.6815851461826243,
      "grad_norm": 8.041658401489258,
      "learning_rate": 2.5281418872107522e-05,
      "loss": 2.6396,
      "step": 26570
    },
    {
      "epoch": 3.682970763475128,
      "grad_norm": 12.922588348388672,
      "learning_rate": 2.527587640293751e-05,
      "loss": 2.1339,
      "step": 26580
    },
    {
      "epoch": 3.684356380767632,
      "grad_norm": 12.736984252929688,
      "learning_rate": 2.5270333933767496e-05,
      "loss": 2.5005,
      "step": 26590
    },
    {
      "epoch": 3.685741998060136,
      "grad_norm": 10.477567672729492,
      "learning_rate": 2.526479146459748e-05,
      "loss": 2.278,
      "step": 26600
    },
    {
      "epoch": 3.6871276153526393,
      "grad_norm": 12.205455780029297,
      "learning_rate": 2.5259248995427466e-05,
      "loss": 2.2629,
      "step": 26610
    },
    {
      "epoch": 3.6885132326451435,
      "grad_norm": 12.247517585754395,
      "learning_rate": 2.525370652625745e-05,
      "loss": 2.1574,
      "step": 26620
    },
    {
      "epoch": 3.6898988499376473,
      "grad_norm": 16.255046844482422,
      "learning_rate": 2.5248164057087437e-05,
      "loss": 2.3416,
      "step": 26630
    },
    {
      "epoch": 3.691284467230151,
      "grad_norm": 11.865462303161621,
      "learning_rate": 2.524262158791742e-05,
      "loss": 1.9099,
      "step": 26640
    },
    {
      "epoch": 3.6926700845226548,
      "grad_norm": 8.737666130065918,
      "learning_rate": 2.5237079118747404e-05,
      "loss": 2.0402,
      "step": 26650
    },
    {
      "epoch": 3.6940557018151585,
      "grad_norm": 8.685391426086426,
      "learning_rate": 2.523153664957739e-05,
      "loss": 1.9835,
      "step": 26660
    },
    {
      "epoch": 3.6954413191076627,
      "grad_norm": 11.746541023254395,
      "learning_rate": 2.5225994180407375e-05,
      "loss": 2.2925,
      "step": 26670
    },
    {
      "epoch": 3.696826936400166,
      "grad_norm": 9.221022605895996,
      "learning_rate": 2.5220451711237356e-05,
      "loss": 2.5282,
      "step": 26680
    },
    {
      "epoch": 3.69821255369267,
      "grad_norm": 10.707001686096191,
      "learning_rate": 2.5214909242067345e-05,
      "loss": 1.8447,
      "step": 26690
    },
    {
      "epoch": 3.699598170985174,
      "grad_norm": 10.938140869140625,
      "learning_rate": 2.5209366772897327e-05,
      "loss": 2.5142,
      "step": 26700
    },
    {
      "epoch": 3.7009837882776777,
      "grad_norm": 13.575199127197266,
      "learning_rate": 2.5203824303727312e-05,
      "loss": 2.0766,
      "step": 26710
    },
    {
      "epoch": 3.7023694055701815,
      "grad_norm": 11.167877197265625,
      "learning_rate": 2.5198281834557297e-05,
      "loss": 2.3484,
      "step": 26720
    },
    {
      "epoch": 3.703755022862685,
      "grad_norm": 18.72113800048828,
      "learning_rate": 2.5192739365387283e-05,
      "loss": 1.9425,
      "step": 26730
    },
    {
      "epoch": 3.7051406401551894,
      "grad_norm": 9.090076446533203,
      "learning_rate": 2.5187196896217265e-05,
      "loss": 1.7777,
      "step": 26740
    },
    {
      "epoch": 3.7065262574476927,
      "grad_norm": 9.926003456115723,
      "learning_rate": 2.5181654427047253e-05,
      "loss": 2.2677,
      "step": 26750
    },
    {
      "epoch": 3.707911874740197,
      "grad_norm": 21.029991149902344,
      "learning_rate": 2.5176111957877235e-05,
      "loss": 2.1781,
      "step": 26760
    },
    {
      "epoch": 3.7092974920327006,
      "grad_norm": 8.151333808898926,
      "learning_rate": 2.517056948870722e-05,
      "loss": 1.9113,
      "step": 26770
    },
    {
      "epoch": 3.7106831093252044,
      "grad_norm": 11.08022403717041,
      "learning_rate": 2.5165027019537206e-05,
      "loss": 2.6513,
      "step": 26780
    },
    {
      "epoch": 3.712068726617708,
      "grad_norm": 16.842357635498047,
      "learning_rate": 2.515948455036719e-05,
      "loss": 2.0758,
      "step": 26790
    },
    {
      "epoch": 3.713454343910212,
      "grad_norm": 9.512954711914062,
      "learning_rate": 2.5153942081197173e-05,
      "loss": 2.1107,
      "step": 26800
    },
    {
      "epoch": 3.7148399612027156,
      "grad_norm": 9.411221504211426,
      "learning_rate": 2.514839961202716e-05,
      "loss": 2.0984,
      "step": 26810
    },
    {
      "epoch": 3.7162255784952194,
      "grad_norm": 18.043148040771484,
      "learning_rate": 2.5142857142857143e-05,
      "loss": 2.3797,
      "step": 26820
    },
    {
      "epoch": 3.7176111957877236,
      "grad_norm": 14.134367942810059,
      "learning_rate": 2.513731467368713e-05,
      "loss": 2.6003,
      "step": 26830
    },
    {
      "epoch": 3.7189968130802273,
      "grad_norm": 16.591920852661133,
      "learning_rate": 2.5131772204517114e-05,
      "loss": 2.3416,
      "step": 26840
    },
    {
      "epoch": 3.720382430372731,
      "grad_norm": 9.669739723205566,
      "learning_rate": 2.51262297353471e-05,
      "loss": 1.91,
      "step": 26850
    },
    {
      "epoch": 3.721768047665235,
      "grad_norm": 8.017970085144043,
      "learning_rate": 2.512068726617708e-05,
      "loss": 2.1308,
      "step": 26860
    },
    {
      "epoch": 3.7231536649577386,
      "grad_norm": 13.93535327911377,
      "learning_rate": 2.511514479700707e-05,
      "loss": 2.6897,
      "step": 26870
    },
    {
      "epoch": 3.7245392822502423,
      "grad_norm": 31.758447647094727,
      "learning_rate": 2.510960232783705e-05,
      "loss": 2.061,
      "step": 26880
    },
    {
      "epoch": 3.725924899542746,
      "grad_norm": 9.526611328125,
      "learning_rate": 2.510405985866704e-05,
      "loss": 2.0403,
      "step": 26890
    },
    {
      "epoch": 3.7273105168352503,
      "grad_norm": 17.763351440429688,
      "learning_rate": 2.5098517389497025e-05,
      "loss": 2.465,
      "step": 26900
    },
    {
      "epoch": 3.728696134127754,
      "grad_norm": 12.142518043518066,
      "learning_rate": 2.5092974920327007e-05,
      "loss": 1.9354,
      "step": 26910
    },
    {
      "epoch": 3.7300817514202578,
      "grad_norm": 11.274772644042969,
      "learning_rate": 2.5087432451156996e-05,
      "loss": 2.1511,
      "step": 26920
    },
    {
      "epoch": 3.7314673687127615,
      "grad_norm": 9.034770011901855,
      "learning_rate": 2.5081889981986978e-05,
      "loss": 2.3839,
      "step": 26930
    },
    {
      "epoch": 3.7328529860052653,
      "grad_norm": 17.915889739990234,
      "learning_rate": 2.507634751281696e-05,
      "loss": 2.22,
      "step": 26940
    },
    {
      "epoch": 3.734238603297769,
      "grad_norm": 12.678110122680664,
      "learning_rate": 2.5070805043646948e-05,
      "loss": 1.9164,
      "step": 26950
    },
    {
      "epoch": 3.7356242205902728,
      "grad_norm": 10.396567344665527,
      "learning_rate": 2.5065262574476933e-05,
      "loss": 2.1304,
      "step": 26960
    },
    {
      "epoch": 3.737009837882777,
      "grad_norm": 11.197857856750488,
      "learning_rate": 2.5059720105306915e-05,
      "loss": 1.624,
      "step": 26970
    },
    {
      "epoch": 3.7383954551752807,
      "grad_norm": 8.627540588378906,
      "learning_rate": 2.5054177636136904e-05,
      "loss": 2.357,
      "step": 26980
    },
    {
      "epoch": 3.7397810724677845,
      "grad_norm": 9.364156723022461,
      "learning_rate": 2.5048635166966886e-05,
      "loss": 2.3093,
      "step": 26990
    },
    {
      "epoch": 3.741166689760288,
      "grad_norm": 12.606827735900879,
      "learning_rate": 2.5043092697796867e-05,
      "loss": 2.2528,
      "step": 27000
    },
    {
      "epoch": 3.742552307052792,
      "grad_norm": 9.068449974060059,
      "learning_rate": 2.5037550228626856e-05,
      "loss": 2.2734,
      "step": 27010
    },
    {
      "epoch": 3.7439379243452957,
      "grad_norm": 6.904048442840576,
      "learning_rate": 2.503200775945684e-05,
      "loss": 1.9073,
      "step": 27020
    },
    {
      "epoch": 3.7453235416377995,
      "grad_norm": 14.242501258850098,
      "learning_rate": 2.5026465290286823e-05,
      "loss": 2.3554,
      "step": 27030
    },
    {
      "epoch": 3.7467091589303037,
      "grad_norm": 16.014341354370117,
      "learning_rate": 2.5020922821116812e-05,
      "loss": 2.2271,
      "step": 27040
    },
    {
      "epoch": 3.7480947762228074,
      "grad_norm": 10.459025382995605,
      "learning_rate": 2.5015380351946794e-05,
      "loss": 2.2072,
      "step": 27050
    },
    {
      "epoch": 3.749480393515311,
      "grad_norm": 10.61817741394043,
      "learning_rate": 2.500983788277678e-05,
      "loss": 2.2463,
      "step": 27060
    },
    {
      "epoch": 3.750866010807815,
      "grad_norm": 8.212803840637207,
      "learning_rate": 2.5004295413606764e-05,
      "loss": 1.845,
      "step": 27070
    },
    {
      "epoch": 3.7522516281003186,
      "grad_norm": 5.7758660316467285,
      "learning_rate": 2.499875294443675e-05,
      "loss": 1.9535,
      "step": 27080
    },
    {
      "epoch": 3.7536372453928224,
      "grad_norm": 9.679203987121582,
      "learning_rate": 2.499321047526673e-05,
      "loss": 2.0654,
      "step": 27090
    },
    {
      "epoch": 3.755022862685326,
      "grad_norm": 25.048118591308594,
      "learning_rate": 2.498766800609672e-05,
      "loss": 2.3216,
      "step": 27100
    },
    {
      "epoch": 3.7564084799778303,
      "grad_norm": 6.273886203765869,
      "learning_rate": 2.4982125536926702e-05,
      "loss": 1.8755,
      "step": 27110
    },
    {
      "epoch": 3.757794097270334,
      "grad_norm": 6.036460876464844,
      "learning_rate": 2.4976583067756687e-05,
      "loss": 2.046,
      "step": 27120
    },
    {
      "epoch": 3.759179714562838,
      "grad_norm": 16.21613883972168,
      "learning_rate": 2.4971040598586672e-05,
      "loss": 2.3329,
      "step": 27130
    },
    {
      "epoch": 3.7605653318553416,
      "grad_norm": 13.704805374145508,
      "learning_rate": 2.4965498129416658e-05,
      "loss": 2.1615,
      "step": 27140
    },
    {
      "epoch": 3.7619509491478453,
      "grad_norm": 10.677263259887695,
      "learning_rate": 2.495995566024664e-05,
      "loss": 2.2834,
      "step": 27150
    },
    {
      "epoch": 3.763336566440349,
      "grad_norm": 11.761547088623047,
      "learning_rate": 2.4954413191076628e-05,
      "loss": 2.256,
      "step": 27160
    },
    {
      "epoch": 3.764722183732853,
      "grad_norm": 11.055970191955566,
      "learning_rate": 2.494887072190661e-05,
      "loss": 2.5072,
      "step": 27170
    },
    {
      "epoch": 3.766107801025357,
      "grad_norm": 13.614296913146973,
      "learning_rate": 2.49433282527366e-05,
      "loss": 2.1837,
      "step": 27180
    },
    {
      "epoch": 3.7674934183178603,
      "grad_norm": 19.463401794433594,
      "learning_rate": 2.493778578356658e-05,
      "loss": 2.332,
      "step": 27190
    },
    {
      "epoch": 3.7688790356103645,
      "grad_norm": 14.842366218566895,
      "learning_rate": 2.4932243314396566e-05,
      "loss": 2.1615,
      "step": 27200
    },
    {
      "epoch": 3.7702646529028683,
      "grad_norm": 8.61072063446045,
      "learning_rate": 2.492670084522655e-05,
      "loss": 2.1841,
      "step": 27210
    },
    {
      "epoch": 3.771650270195372,
      "grad_norm": 16.19557762145996,
      "learning_rate": 2.4921158376056536e-05,
      "loss": 2.476,
      "step": 27220
    },
    {
      "epoch": 3.7730358874878758,
      "grad_norm": 17.45790672302246,
      "learning_rate": 2.4915615906886518e-05,
      "loss": 2.2556,
      "step": 27230
    },
    {
      "epoch": 3.7744215047803795,
      "grad_norm": 9.60115909576416,
      "learning_rate": 2.4910073437716507e-05,
      "loss": 2.0375,
      "step": 27240
    },
    {
      "epoch": 3.7758071220728837,
      "grad_norm": 14.021077156066895,
      "learning_rate": 2.490453096854649e-05,
      "loss": 2.6806,
      "step": 27250
    },
    {
      "epoch": 3.777192739365387,
      "grad_norm": 13.7719087600708,
      "learning_rate": 2.4898988499376474e-05,
      "loss": 2.3973,
      "step": 27260
    },
    {
      "epoch": 3.778578356657891,
      "grad_norm": 14.23581314086914,
      "learning_rate": 2.489344603020646e-05,
      "loss": 2.1259,
      "step": 27270
    },
    {
      "epoch": 3.779963973950395,
      "grad_norm": 10.249778747558594,
      "learning_rate": 2.4887903561036444e-05,
      "loss": 2.4412,
      "step": 27280
    },
    {
      "epoch": 3.7813495912428987,
      "grad_norm": 14.993791580200195,
      "learning_rate": 2.4882361091866426e-05,
      "loss": 2.5258,
      "step": 27290
    },
    {
      "epoch": 3.7827352085354025,
      "grad_norm": 11.935647964477539,
      "learning_rate": 2.4876818622696415e-05,
      "loss": 2.1074,
      "step": 27300
    },
    {
      "epoch": 3.784120825827906,
      "grad_norm": 8.602935791015625,
      "learning_rate": 2.4871276153526397e-05,
      "loss": 2.0135,
      "step": 27310
    },
    {
      "epoch": 3.7855064431204104,
      "grad_norm": 9.608701705932617,
      "learning_rate": 2.4865733684356382e-05,
      "loss": 1.9507,
      "step": 27320
    },
    {
      "epoch": 3.7868920604129137,
      "grad_norm": 19.83203125,
      "learning_rate": 2.486019121518637e-05,
      "loss": 2.2913,
      "step": 27330
    },
    {
      "epoch": 3.788277677705418,
      "grad_norm": 11.114689826965332,
      "learning_rate": 2.4854648746016352e-05,
      "loss": 2.194,
      "step": 27340
    },
    {
      "epoch": 3.7896632949979216,
      "grad_norm": 7.970944881439209,
      "learning_rate": 2.4849106276846334e-05,
      "loss": 1.9388,
      "step": 27350
    },
    {
      "epoch": 3.7910489122904254,
      "grad_norm": 13.890183448791504,
      "learning_rate": 2.4843563807676323e-05,
      "loss": 2.2438,
      "step": 27360
    },
    {
      "epoch": 3.792434529582929,
      "grad_norm": 6.473328113555908,
      "learning_rate": 2.4838021338506305e-05,
      "loss": 2.0644,
      "step": 27370
    },
    {
      "epoch": 3.793820146875433,
      "grad_norm": 7.9516448974609375,
      "learning_rate": 2.483247886933629e-05,
      "loss": 2.1431,
      "step": 27380
    },
    {
      "epoch": 3.795205764167937,
      "grad_norm": 5.919312477111816,
      "learning_rate": 2.482693640016628e-05,
      "loss": 2.1151,
      "step": 27390
    },
    {
      "epoch": 3.7965913814604404,
      "grad_norm": 14.918831825256348,
      "learning_rate": 2.482139393099626e-05,
      "loss": 1.8198,
      "step": 27400
    },
    {
      "epoch": 3.7979769987529446,
      "grad_norm": 13.772994041442871,
      "learning_rate": 2.4815851461826242e-05,
      "loss": 2.4778,
      "step": 27410
    },
    {
      "epoch": 3.7993626160454483,
      "grad_norm": 10.764823913574219,
      "learning_rate": 2.481030899265623e-05,
      "loss": 1.8854,
      "step": 27420
    },
    {
      "epoch": 3.800748233337952,
      "grad_norm": 11.954777717590332,
      "learning_rate": 2.4804766523486213e-05,
      "loss": 2.1951,
      "step": 27430
    },
    {
      "epoch": 3.802133850630456,
      "grad_norm": 18.95777130126953,
      "learning_rate": 2.4799224054316198e-05,
      "loss": 2.4977,
      "step": 27440
    },
    {
      "epoch": 3.8035194679229596,
      "grad_norm": 9.747037887573242,
      "learning_rate": 2.4793681585146187e-05,
      "loss": 1.8422,
      "step": 27450
    },
    {
      "epoch": 3.8049050852154633,
      "grad_norm": 11.927885055541992,
      "learning_rate": 2.478813911597617e-05,
      "loss": 1.9999,
      "step": 27460
    },
    {
      "epoch": 3.806290702507967,
      "grad_norm": 10.367353439331055,
      "learning_rate": 2.4782596646806157e-05,
      "loss": 2.5416,
      "step": 27470
    },
    {
      "epoch": 3.8076763198004713,
      "grad_norm": 7.343422889709473,
      "learning_rate": 2.477705417763614e-05,
      "loss": 1.9107,
      "step": 27480
    },
    {
      "epoch": 3.809061937092975,
      "grad_norm": 12.474325180053711,
      "learning_rate": 2.4771511708466124e-05,
      "loss": 2.3875,
      "step": 27490
    },
    {
      "epoch": 3.8104475543854788,
      "grad_norm": 9.88290786743164,
      "learning_rate": 2.476596923929611e-05,
      "loss": 2.226,
      "step": 27500
    },
    {
      "epoch": 3.8118331716779825,
      "grad_norm": 14.899490356445312,
      "learning_rate": 2.4760426770126095e-05,
      "loss": 2.394,
      "step": 27510
    },
    {
      "epoch": 3.8132187889704863,
      "grad_norm": 12.260327339172363,
      "learning_rate": 2.4754884300956077e-05,
      "loss": 2.6075,
      "step": 27520
    },
    {
      "epoch": 3.81460440626299,
      "grad_norm": 7.932773590087891,
      "learning_rate": 2.4749341831786065e-05,
      "loss": 2.3548,
      "step": 27530
    },
    {
      "epoch": 3.8159900235554938,
      "grad_norm": 12.618242263793945,
      "learning_rate": 2.4743799362616047e-05,
      "loss": 2.2267,
      "step": 27540
    },
    {
      "epoch": 3.817375640847998,
      "grad_norm": 11.928631782531738,
      "learning_rate": 2.4738256893446032e-05,
      "loss": 2.5429,
      "step": 27550
    },
    {
      "epoch": 3.8187612581405017,
      "grad_norm": 14.119853973388672,
      "learning_rate": 2.4732714424276018e-05,
      "loss": 2.5571,
      "step": 27560
    },
    {
      "epoch": 3.8201468754330055,
      "grad_norm": 11.348190307617188,
      "learning_rate": 2.4727171955106003e-05,
      "loss": 2.3731,
      "step": 27570
    },
    {
      "epoch": 3.821532492725509,
      "grad_norm": 8.239410400390625,
      "learning_rate": 2.4721629485935985e-05,
      "loss": 2.0366,
      "step": 27580
    },
    {
      "epoch": 3.822918110018013,
      "grad_norm": 10.391788482666016,
      "learning_rate": 2.4716087016765973e-05,
      "loss": 2.6672,
      "step": 27590
    },
    {
      "epoch": 3.8243037273105167,
      "grad_norm": 12.662858009338379,
      "learning_rate": 2.4710544547595955e-05,
      "loss": 2.0976,
      "step": 27600
    },
    {
      "epoch": 3.8256893446030205,
      "grad_norm": 18.045330047607422,
      "learning_rate": 2.470500207842594e-05,
      "loss": 2.1086,
      "step": 27610
    },
    {
      "epoch": 3.8270749618955247,
      "grad_norm": 5.070356369018555,
      "learning_rate": 2.4699459609255926e-05,
      "loss": 2.2476,
      "step": 27620
    },
    {
      "epoch": 3.8284605791880284,
      "grad_norm": 13.1856050491333,
      "learning_rate": 2.469391714008591e-05,
      "loss": 2.378,
      "step": 27630
    },
    {
      "epoch": 3.829846196480532,
      "grad_norm": 8.556259155273438,
      "learning_rate": 2.4688374670915893e-05,
      "loss": 2.6426,
      "step": 27640
    },
    {
      "epoch": 3.831231813773036,
      "grad_norm": 8.528772354125977,
      "learning_rate": 2.468283220174588e-05,
      "loss": 2.078,
      "step": 27650
    },
    {
      "epoch": 3.8326174310655396,
      "grad_norm": 9.310280799865723,
      "learning_rate": 2.4677289732575863e-05,
      "loss": 2.1039,
      "step": 27660
    },
    {
      "epoch": 3.8340030483580434,
      "grad_norm": 17.168132781982422,
      "learning_rate": 2.467174726340585e-05,
      "loss": 2.0831,
      "step": 27670
    },
    {
      "epoch": 3.835388665650547,
      "grad_norm": 10.903776168823242,
      "learning_rate": 2.4666204794235834e-05,
      "loss": 2.6163,
      "step": 27680
    },
    {
      "epoch": 3.8367742829430513,
      "grad_norm": 12.957884788513184,
      "learning_rate": 2.466066232506582e-05,
      "loss": 2.108,
      "step": 27690
    },
    {
      "epoch": 3.838159900235555,
      "grad_norm": 16.4255428314209,
      "learning_rate": 2.46551198558958e-05,
      "loss": 1.772,
      "step": 27700
    },
    {
      "epoch": 3.839545517528059,
      "grad_norm": 12.395623207092285,
      "learning_rate": 2.464957738672579e-05,
      "loss": 1.6207,
      "step": 27710
    },
    {
      "epoch": 3.8409311348205626,
      "grad_norm": 11.583022117614746,
      "learning_rate": 2.464403491755577e-05,
      "loss": 2.3809,
      "step": 27720
    },
    {
      "epoch": 3.8423167521130663,
      "grad_norm": 12.491847038269043,
      "learning_rate": 2.4638492448385757e-05,
      "loss": 2.1058,
      "step": 27730
    },
    {
      "epoch": 3.84370236940557,
      "grad_norm": 8.734169006347656,
      "learning_rate": 2.4632949979215742e-05,
      "loss": 1.9185,
      "step": 27740
    },
    {
      "epoch": 3.845087986698074,
      "grad_norm": 24.799190521240234,
      "learning_rate": 2.4627407510045727e-05,
      "loss": 2.5373,
      "step": 27750
    },
    {
      "epoch": 3.846473603990578,
      "grad_norm": 8.593221664428711,
      "learning_rate": 2.4621865040875716e-05,
      "loss": 1.9073,
      "step": 27760
    },
    {
      "epoch": 3.8478592212830818,
      "grad_norm": 16.961732864379883,
      "learning_rate": 2.4616322571705698e-05,
      "loss": 2.6624,
      "step": 27770
    },
    {
      "epoch": 3.8492448385755855,
      "grad_norm": 9.269630432128906,
      "learning_rate": 2.461078010253568e-05,
      "loss": 1.9589,
      "step": 27780
    },
    {
      "epoch": 3.8506304558680893,
      "grad_norm": 12.501319885253906,
      "learning_rate": 2.460523763336567e-05,
      "loss": 2.2901,
      "step": 27790
    },
    {
      "epoch": 3.852016073160593,
      "grad_norm": 8.311490058898926,
      "learning_rate": 2.459969516419565e-05,
      "loss": 2.4555,
      "step": 27800
    },
    {
      "epoch": 3.8534016904530968,
      "grad_norm": 12.629776954650879,
      "learning_rate": 2.4594152695025635e-05,
      "loss": 2.3277,
      "step": 27810
    },
    {
      "epoch": 3.8547873077456005,
      "grad_norm": 9.488361358642578,
      "learning_rate": 2.4588610225855624e-05,
      "loss": 1.6026,
      "step": 27820
    },
    {
      "epoch": 3.8561729250381047,
      "grad_norm": 15.246074676513672,
      "learning_rate": 2.4583067756685606e-05,
      "loss": 2.5104,
      "step": 27830
    },
    {
      "epoch": 3.857558542330608,
      "grad_norm": 12.32628345489502,
      "learning_rate": 2.4577525287515588e-05,
      "loss": 2.1243,
      "step": 27840
    },
    {
      "epoch": 3.858944159623112,
      "grad_norm": 16.535545349121094,
      "learning_rate": 2.4571982818345576e-05,
      "loss": 2.5238,
      "step": 27850
    },
    {
      "epoch": 3.860329776915616,
      "grad_norm": 11.739276885986328,
      "learning_rate": 2.4566440349175558e-05,
      "loss": 2.2764,
      "step": 27860
    },
    {
      "epoch": 3.8617153942081197,
      "grad_norm": 16.730819702148438,
      "learning_rate": 2.4560897880005544e-05,
      "loss": 1.8589,
      "step": 27870
    },
    {
      "epoch": 3.8631010115006235,
      "grad_norm": 11.892823219299316,
      "learning_rate": 2.4555355410835532e-05,
      "loss": 2.2538,
      "step": 27880
    },
    {
      "epoch": 3.864486628793127,
      "grad_norm": 14.290207862854004,
      "learning_rate": 2.4549812941665514e-05,
      "loss": 2.3502,
      "step": 27890
    },
    {
      "epoch": 3.8658722460856314,
      "grad_norm": 12.334470748901367,
      "learning_rate": 2.4544270472495496e-05,
      "loss": 2.2597,
      "step": 27900
    },
    {
      "epoch": 3.8672578633781347,
      "grad_norm": 12.760709762573242,
      "learning_rate": 2.4538728003325485e-05,
      "loss": 2.7239,
      "step": 27910
    },
    {
      "epoch": 3.868643480670639,
      "grad_norm": 12.978815078735352,
      "learning_rate": 2.453318553415547e-05,
      "loss": 2.3937,
      "step": 27920
    },
    {
      "epoch": 3.8700290979631427,
      "grad_norm": 12.366994857788086,
      "learning_rate": 2.452764306498545e-05,
      "loss": 2.6368,
      "step": 27930
    },
    {
      "epoch": 3.8714147152556464,
      "grad_norm": 8.688912391662598,
      "learning_rate": 2.452210059581544e-05,
      "loss": 2.2632,
      "step": 27940
    },
    {
      "epoch": 3.87280033254815,
      "grad_norm": 11.052932739257812,
      "learning_rate": 2.4516558126645422e-05,
      "loss": 2.2125,
      "step": 27950
    },
    {
      "epoch": 3.874185949840654,
      "grad_norm": 13.35856819152832,
      "learning_rate": 2.4511015657475404e-05,
      "loss": 2.1611,
      "step": 27960
    },
    {
      "epoch": 3.875571567133158,
      "grad_norm": 15.322032928466797,
      "learning_rate": 2.4505473188305393e-05,
      "loss": 2.1195,
      "step": 27970
    },
    {
      "epoch": 3.8769571844256614,
      "grad_norm": 9.000789642333984,
      "learning_rate": 2.4499930719135378e-05,
      "loss": 2.0439,
      "step": 27980
    },
    {
      "epoch": 3.8783428017181656,
      "grad_norm": 25.217803955078125,
      "learning_rate": 2.449438824996536e-05,
      "loss": 2.1822,
      "step": 27990
    },
    {
      "epoch": 3.8797284190106693,
      "grad_norm": 14.834933280944824,
      "learning_rate": 2.448884578079535e-05,
      "loss": 2.0784,
      "step": 28000
    },
    {
      "epoch": 3.881114036303173,
      "grad_norm": 10.940117835998535,
      "learning_rate": 2.448330331162533e-05,
      "loss": 2.3135,
      "step": 28010
    },
    {
      "epoch": 3.882499653595677,
      "grad_norm": 5.125997066497803,
      "learning_rate": 2.4477760842455312e-05,
      "loss": 2.169,
      "step": 28020
    },
    {
      "epoch": 3.8838852708881806,
      "grad_norm": 7.319301128387451,
      "learning_rate": 2.44722183732853e-05,
      "loss": 1.9133,
      "step": 28030
    },
    {
      "epoch": 3.8852708881806843,
      "grad_norm": 14.730027198791504,
      "learning_rate": 2.4466675904115286e-05,
      "loss": 1.8121,
      "step": 28040
    },
    {
      "epoch": 3.886656505473188,
      "grad_norm": 11.353635787963867,
      "learning_rate": 2.446113343494527e-05,
      "loss": 2.2261,
      "step": 28050
    },
    {
      "epoch": 3.8880421227656923,
      "grad_norm": 14.843655586242676,
      "learning_rate": 2.4455590965775256e-05,
      "loss": 2.2459,
      "step": 28060
    },
    {
      "epoch": 3.889427740058196,
      "grad_norm": 9.856730461120605,
      "learning_rate": 2.445004849660524e-05,
      "loss": 1.7622,
      "step": 28070
    },
    {
      "epoch": 3.8908133573506998,
      "grad_norm": 7.091089248657227,
      "learning_rate": 2.4444506027435227e-05,
      "loss": 2.147,
      "step": 28080
    },
    {
      "epoch": 3.8921989746432035,
      "grad_norm": 10.757491111755371,
      "learning_rate": 2.443896355826521e-05,
      "loss": 2.3903,
      "step": 28090
    },
    {
      "epoch": 3.8935845919357073,
      "grad_norm": 14.85252571105957,
      "learning_rate": 2.4433421089095194e-05,
      "loss": 2.3358,
      "step": 28100
    },
    {
      "epoch": 3.894970209228211,
      "grad_norm": 10.183249473571777,
      "learning_rate": 2.442787861992518e-05,
      "loss": 2.6353,
      "step": 28110
    },
    {
      "epoch": 3.8963558265207148,
      "grad_norm": 13.025443077087402,
      "learning_rate": 2.4422336150755165e-05,
      "loss": 2.3025,
      "step": 28120
    },
    {
      "epoch": 3.897741443813219,
      "grad_norm": 11.904607772827148,
      "learning_rate": 2.4416793681585146e-05,
      "loss": 2.1956,
      "step": 28130
    },
    {
      "epoch": 3.8991270611057227,
      "grad_norm": 9.801634788513184,
      "learning_rate": 2.4411251212415135e-05,
      "loss": 2.1989,
      "step": 28140
    },
    {
      "epoch": 3.9005126783982265,
      "grad_norm": NaN,
      "learning_rate": 2.4405708743245117e-05,
      "loss": 1.7183,
      "step": 28150
    },
    {
      "epoch": 3.90189829569073,
      "grad_norm": 13.789005279541016,
      "learning_rate": 2.4400720520992103e-05,
      "loss": 2.1419,
      "step": 28160
    },
    {
      "epoch": 3.903283912983234,
      "grad_norm": 12.974720001220703,
      "learning_rate": 2.4395178051822092e-05,
      "loss": 2.1881,
      "step": 28170
    },
    {
      "epoch": 3.9046695302757377,
      "grad_norm": 11.164889335632324,
      "learning_rate": 2.4389635582652074e-05,
      "loss": 2.3968,
      "step": 28180
    },
    {
      "epoch": 3.9060551475682415,
      "grad_norm": 9.003698348999023,
      "learning_rate": 2.4384093113482056e-05,
      "loss": 2.2474,
      "step": 28190
    },
    {
      "epoch": 3.9074407648607457,
      "grad_norm": 13.003278732299805,
      "learning_rate": 2.4378550644312044e-05,
      "loss": 2.5441,
      "step": 28200
    },
    {
      "epoch": 3.9088263821532494,
      "grad_norm": 12.32109260559082,
      "learning_rate": 2.4373008175142026e-05,
      "loss": 2.1356,
      "step": 28210
    },
    {
      "epoch": 3.910211999445753,
      "grad_norm": 10.648177146911621,
      "learning_rate": 2.436746570597201e-05,
      "loss": 2.2591,
      "step": 28220
    },
    {
      "epoch": 3.911597616738257,
      "grad_norm": 12.506856918334961,
      "learning_rate": 2.4361923236802e-05,
      "loss": 2.3652,
      "step": 28230
    },
    {
      "epoch": 3.9129832340307606,
      "grad_norm": 9.388588905334473,
      "learning_rate": 2.4356380767631982e-05,
      "loss": 2.0651,
      "step": 28240
    },
    {
      "epoch": 3.9143688513232644,
      "grad_norm": 11.4279146194458,
      "learning_rate": 2.4350838298461964e-05,
      "loss": 2.0156,
      "step": 28250
    },
    {
      "epoch": 3.915754468615768,
      "grad_norm": 15.050318717956543,
      "learning_rate": 2.4345295829291952e-05,
      "loss": 1.9452,
      "step": 28260
    },
    {
      "epoch": 3.9171400859082723,
      "grad_norm": 10.700186729431152,
      "learning_rate": 2.4339753360121938e-05,
      "loss": 2.1233,
      "step": 28270
    },
    {
      "epoch": 3.918525703200776,
      "grad_norm": 7.717954635620117,
      "learning_rate": 2.433421089095192e-05,
      "loss": 2.4879,
      "step": 28280
    },
    {
      "epoch": 3.91991132049328,
      "grad_norm": 9.375244140625,
      "learning_rate": 2.4328668421781908e-05,
      "loss": 2.1186,
      "step": 28290
    },
    {
      "epoch": 3.9212969377857836,
      "grad_norm": 7.938546657562256,
      "learning_rate": 2.432312595261189e-05,
      "loss": 2.0855,
      "step": 28300
    },
    {
      "epoch": 3.9226825550782873,
      "grad_norm": 13.20028305053711,
      "learning_rate": 2.4317583483441872e-05,
      "loss": 1.9585,
      "step": 28310
    },
    {
      "epoch": 3.924068172370791,
      "grad_norm": 11.356114387512207,
      "learning_rate": 2.431204101427186e-05,
      "loss": 2.5468,
      "step": 28320
    },
    {
      "epoch": 3.925453789663295,
      "grad_norm": 9.932563781738281,
      "learning_rate": 2.4306498545101846e-05,
      "loss": 1.7291,
      "step": 28330
    },
    {
      "epoch": 3.926839406955799,
      "grad_norm": 10.880459785461426,
      "learning_rate": 2.4300956075931828e-05,
      "loss": 2.2121,
      "step": 28340
    },
    {
      "epoch": 3.9282250242483028,
      "grad_norm": 14.230173110961914,
      "learning_rate": 2.4295413606761816e-05,
      "loss": 2.0394,
      "step": 28350
    },
    {
      "epoch": 3.9296106415408065,
      "grad_norm": 11.631356239318848,
      "learning_rate": 2.4289871137591798e-05,
      "loss": 2.0824,
      "step": 28360
    },
    {
      "epoch": 3.9309962588333103,
      "grad_norm": 12.689624786376953,
      "learning_rate": 2.4284328668421787e-05,
      "loss": 2.2775,
      "step": 28370
    },
    {
      "epoch": 3.932381876125814,
      "grad_norm": 7.687203407287598,
      "learning_rate": 2.427878619925177e-05,
      "loss": 2.4856,
      "step": 28380
    },
    {
      "epoch": 3.9337674934183178,
      "grad_norm": 8.58085823059082,
      "learning_rate": 2.4273243730081754e-05,
      "loss": 1.8811,
      "step": 28390
    },
    {
      "epoch": 3.9351531107108215,
      "grad_norm": 8.10226058959961,
      "learning_rate": 2.426770126091174e-05,
      "loss": 2.1981,
      "step": 28400
    },
    {
      "epoch": 3.9365387280033257,
      "grad_norm": 8.326911926269531,
      "learning_rate": 2.4262158791741724e-05,
      "loss": 2.0428,
      "step": 28410
    },
    {
      "epoch": 3.937924345295829,
      "grad_norm": 15.928384780883789,
      "learning_rate": 2.4256616322571706e-05,
      "loss": 2.766,
      "step": 28420
    },
    {
      "epoch": 3.939309962588333,
      "grad_norm": 9.529226303100586,
      "learning_rate": 2.4251073853401695e-05,
      "loss": 2.3355,
      "step": 28430
    },
    {
      "epoch": 3.940695579880837,
      "grad_norm": 10.666899681091309,
      "learning_rate": 2.4245531384231677e-05,
      "loss": 2.1848,
      "step": 28440
    },
    {
      "epoch": 3.9420811971733407,
      "grad_norm": 12.848386764526367,
      "learning_rate": 2.4239988915061662e-05,
      "loss": 1.9176,
      "step": 28450
    },
    {
      "epoch": 3.9434668144658445,
      "grad_norm": 13.95583724975586,
      "learning_rate": 2.4234446445891647e-05,
      "loss": 2.4929,
      "step": 28460
    },
    {
      "epoch": 3.944852431758348,
      "grad_norm": 15.983339309692383,
      "learning_rate": 2.4228903976721633e-05,
      "loss": 2.1865,
      "step": 28470
    },
    {
      "epoch": 3.9462380490508524,
      "grad_norm": 21.790313720703125,
      "learning_rate": 2.4223361507551614e-05,
      "loss": 2.1529,
      "step": 28480
    },
    {
      "epoch": 3.9476236663433557,
      "grad_norm": 9.584985733032227,
      "learning_rate": 2.4217819038381603e-05,
      "loss": 2.6201,
      "step": 28490
    },
    {
      "epoch": 3.94900928363586,
      "grad_norm": 7.23419713973999,
      "learning_rate": 2.4212276569211585e-05,
      "loss": 1.8614,
      "step": 28500
    },
    {
      "epoch": 3.9503949009283637,
      "grad_norm": 7.269774436950684,
      "learning_rate": 2.420673410004157e-05,
      "loss": 1.8784,
      "step": 28510
    },
    {
      "epoch": 3.9517805182208674,
      "grad_norm": 12.997560501098633,
      "learning_rate": 2.4201191630871555e-05,
      "loss": 2.5604,
      "step": 28520
    },
    {
      "epoch": 3.953166135513371,
      "grad_norm": 13.126026153564453,
      "learning_rate": 2.419564916170154e-05,
      "loss": 2.7625,
      "step": 28530
    },
    {
      "epoch": 3.954551752805875,
      "grad_norm": 11.930181503295898,
      "learning_rate": 2.4190106692531522e-05,
      "loss": 2.1627,
      "step": 28540
    },
    {
      "epoch": 3.955937370098379,
      "grad_norm": 12.345017433166504,
      "learning_rate": 2.418456422336151e-05,
      "loss": 2.4198,
      "step": 28550
    },
    {
      "epoch": 3.9573229873908824,
      "grad_norm": 15.65147590637207,
      "learning_rate": 2.4179021754191493e-05,
      "loss": 2.0719,
      "step": 28560
    },
    {
      "epoch": 3.9587086046833866,
      "grad_norm": 11.562121391296387,
      "learning_rate": 2.4173479285021478e-05,
      "loss": 2.0865,
      "step": 28570
    },
    {
      "epoch": 3.9600942219758903,
      "grad_norm": 14.348467826843262,
      "learning_rate": 2.4167936815851463e-05,
      "loss": 2.3782,
      "step": 28580
    },
    {
      "epoch": 3.961479839268394,
      "grad_norm": 6.641359806060791,
      "learning_rate": 2.416239434668145e-05,
      "loss": 2.162,
      "step": 28590
    },
    {
      "epoch": 3.962865456560898,
      "grad_norm": 9.687429428100586,
      "learning_rate": 2.415685187751143e-05,
      "loss": 2.5286,
      "step": 28600
    },
    {
      "epoch": 3.9642510738534016,
      "grad_norm": 11.955780982971191,
      "learning_rate": 2.415130940834142e-05,
      "loss": 2.0456,
      "step": 28610
    },
    {
      "epoch": 3.9656366911459053,
      "grad_norm": 8.08855152130127,
      "learning_rate": 2.41457669391714e-05,
      "loss": 2.3678,
      "step": 28620
    },
    {
      "epoch": 3.967022308438409,
      "grad_norm": 16.840864181518555,
      "learning_rate": 2.4140224470001386e-05,
      "loss": 2.2362,
      "step": 28630
    },
    {
      "epoch": 3.9684079257309133,
      "grad_norm": 9.934568405151367,
      "learning_rate": 2.413468200083137e-05,
      "loss": 2.4489,
      "step": 28640
    },
    {
      "epoch": 3.969793543023417,
      "grad_norm": 11.029123306274414,
      "learning_rate": 2.4129139531661357e-05,
      "loss": 2.2071,
      "step": 28650
    },
    {
      "epoch": 3.9711791603159208,
      "grad_norm": 9.719766616821289,
      "learning_rate": 2.4123597062491345e-05,
      "loss": 2.185,
      "step": 28660
    },
    {
      "epoch": 3.9725647776084245,
      "grad_norm": 13.938937187194824,
      "learning_rate": 2.4118054593321327e-05,
      "loss": 1.6704,
      "step": 28670
    },
    {
      "epoch": 3.9739503949009283,
      "grad_norm": 11.154047966003418,
      "learning_rate": 2.411251212415131e-05,
      "loss": 2.5592,
      "step": 28680
    },
    {
      "epoch": 3.975336012193432,
      "grad_norm": 8.597691535949707,
      "learning_rate": 2.4106969654981298e-05,
      "loss": 2.0559,
      "step": 28690
    },
    {
      "epoch": 3.9767216294859358,
      "grad_norm": 19.8033447265625,
      "learning_rate": 2.410142718581128e-05,
      "loss": 2.3454,
      "step": 28700
    },
    {
      "epoch": 3.97810724677844,
      "grad_norm": 11.553455352783203,
      "learning_rate": 2.4095884716641265e-05,
      "loss": 2.4286,
      "step": 28710
    },
    {
      "epoch": 3.9794928640709437,
      "grad_norm": 17.762174606323242,
      "learning_rate": 2.4090342247471254e-05,
      "loss": 2.2427,
      "step": 28720
    },
    {
      "epoch": 3.9808784813634475,
      "grad_norm": 9.199591636657715,
      "learning_rate": 2.4084799778301235e-05,
      "loss": 2.1889,
      "step": 28730
    },
    {
      "epoch": 3.982264098655951,
      "grad_norm": 12.144219398498535,
      "learning_rate": 2.4079257309131217e-05,
      "loss": 2.0325,
      "step": 28740
    },
    {
      "epoch": 3.983649715948455,
      "grad_norm": 8.266799926757812,
      "learning_rate": 2.4073714839961206e-05,
      "loss": 2.1132,
      "step": 28750
    },
    {
      "epoch": 3.9850353332409587,
      "grad_norm": 6.641878604888916,
      "learning_rate": 2.406817237079119e-05,
      "loss": 1.9913,
      "step": 28760
    },
    {
      "epoch": 3.9864209505334625,
      "grad_norm": 12.436875343322754,
      "learning_rate": 2.4062629901621173e-05,
      "loss": 2.9174,
      "step": 28770
    },
    {
      "epoch": 3.9878065678259667,
      "grad_norm": 18.398405075073242,
      "learning_rate": 2.405708743245116e-05,
      "loss": 2.2241,
      "step": 28780
    },
    {
      "epoch": 3.9891921851184704,
      "grad_norm": 8.248376846313477,
      "learning_rate": 2.4051544963281144e-05,
      "loss": 1.9148,
      "step": 28790
    },
    {
      "epoch": 3.990577802410974,
      "grad_norm": 15.412243843078613,
      "learning_rate": 2.4046002494111125e-05,
      "loss": 2.0007,
      "step": 28800
    },
    {
      "epoch": 3.991963419703478,
      "grad_norm": 10.446484565734863,
      "learning_rate": 2.4040460024941114e-05,
      "loss": 2.7715,
      "step": 28810
    },
    {
      "epoch": 3.9933490369959816,
      "grad_norm": 12.705526351928711,
      "learning_rate": 2.40349175557711e-05,
      "loss": 2.0721,
      "step": 28820
    },
    {
      "epoch": 3.9947346542884854,
      "grad_norm": 9.880302429199219,
      "learning_rate": 2.402937508660108e-05,
      "loss": 2.1457,
      "step": 28830
    },
    {
      "epoch": 3.996120271580989,
      "grad_norm": 6.511822700500488,
      "learning_rate": 2.402383261743107e-05,
      "loss": 2.0648,
      "step": 28840
    },
    {
      "epoch": 3.9975058888734933,
      "grad_norm": 15.440174102783203,
      "learning_rate": 2.401829014826105e-05,
      "loss": 2.327,
      "step": 28850
    },
    {
      "epoch": 3.998891506165997,
      "grad_norm": 7.08736515045166,
      "learning_rate": 2.4012747679091037e-05,
      "loss": 2.1103,
      "step": 28860
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.49653499653499655,
      "eval_bert_f1": 0.9875496625900269,
      "eval_bert_precision": 0.989611804485321,
      "eval_bert_recall": 0.9859192371368408,
      "eval_f1": 0.05144514681672576,
      "eval_loss": 2.3219401836395264,
      "eval_runtime": 312.5844,
      "eval_samples_per_second": 46.164,
      "eval_steps_per_second": 5.771,
      "eval_synonym_accuracy": 0.5078309078309078,
      "step": 28868
    },
    {
      "epoch": 4.000277123458501,
      "grad_norm": 9.271230697631836,
      "learning_rate": 2.4007205209921022e-05,
      "loss": 2.2025,
      "step": 28870
    },
    {
      "epoch": 4.001662740751004,
      "grad_norm": 10.566929817199707,
      "learning_rate": 2.4001662740751007e-05,
      "loss": 1.8693,
      "step": 28880
    },
    {
      "epoch": 4.003048358043508,
      "grad_norm": 16.07021713256836,
      "learning_rate": 2.399612027158099e-05,
      "loss": 2.1368,
      "step": 28890
    },
    {
      "epoch": 4.0044339753360125,
      "grad_norm": 11.119982719421387,
      "learning_rate": 2.3990577802410978e-05,
      "loss": 2.0742,
      "step": 28900
    },
    {
      "epoch": 4.005819592628516,
      "grad_norm": 9.704389572143555,
      "learning_rate": 2.398503533324096e-05,
      "loss": 2.0106,
      "step": 28910
    },
    {
      "epoch": 4.00720520992102,
      "grad_norm": 11.861689567565918,
      "learning_rate": 2.3979492864070945e-05,
      "loss": 2.2549,
      "step": 28920
    },
    {
      "epoch": 4.008590827213523,
      "grad_norm": 13.039824485778809,
      "learning_rate": 2.397395039490093e-05,
      "loss": 2.0707,
      "step": 28930
    },
    {
      "epoch": 4.0099764445060275,
      "grad_norm": 8.55837345123291,
      "learning_rate": 2.3968407925730916e-05,
      "loss": 1.9589,
      "step": 28940
    },
    {
      "epoch": 4.011362061798531,
      "grad_norm": 11.786985397338867,
      "learning_rate": 2.39628654565609e-05,
      "loss": 2.5607,
      "step": 28950
    },
    {
      "epoch": 4.012747679091035,
      "grad_norm": 12.868440628051758,
      "learning_rate": 2.3957322987390886e-05,
      "loss": 2.0205,
      "step": 28960
    },
    {
      "epoch": 4.014133296383539,
      "grad_norm": 16.366758346557617,
      "learning_rate": 2.3951780518220868e-05,
      "loss": 1.9045,
      "step": 28970
    },
    {
      "epoch": 4.0155189136760425,
      "grad_norm": 12.104087829589844,
      "learning_rate": 2.3946238049050857e-05,
      "loss": 2.4775,
      "step": 28980
    },
    {
      "epoch": 4.016904530968547,
      "grad_norm": 22.696144104003906,
      "learning_rate": 2.394069557988084e-05,
      "loss": 2.2764,
      "step": 28990
    },
    {
      "epoch": 4.01829014826105,
      "grad_norm": 8.293866157531738,
      "learning_rate": 2.3935153110710824e-05,
      "loss": 2.3038,
      "step": 29000
    },
    {
      "epoch": 4.019675765553554,
      "grad_norm": 10.708715438842773,
      "learning_rate": 2.392961064154081e-05,
      "loss": 2.1417,
      "step": 29010
    },
    {
      "epoch": 4.0210613828460575,
      "grad_norm": 9.212672233581543,
      "learning_rate": 2.3924068172370794e-05,
      "loss": 1.9494,
      "step": 29020
    },
    {
      "epoch": 4.022447000138562,
      "grad_norm": 9.38281536102295,
      "learning_rate": 2.3918525703200776e-05,
      "loss": 1.8982,
      "step": 29030
    },
    {
      "epoch": 4.023832617431066,
      "grad_norm": 22.915027618408203,
      "learning_rate": 2.3912983234030765e-05,
      "loss": 2.1633,
      "step": 29040
    },
    {
      "epoch": 4.025218234723569,
      "grad_norm": 10.935049057006836,
      "learning_rate": 2.3907440764860746e-05,
      "loss": 2.0455,
      "step": 29050
    },
    {
      "epoch": 4.026603852016073,
      "grad_norm": 14.810812950134277,
      "learning_rate": 2.3901898295690732e-05,
      "loss": 2.2176,
      "step": 29060
    },
    {
      "epoch": 4.027989469308577,
      "grad_norm": 6.739584922790527,
      "learning_rate": 2.3896355826520717e-05,
      "loss": 2.2399,
      "step": 29070
    },
    {
      "epoch": 4.029375086601081,
      "grad_norm": 8.993257522583008,
      "learning_rate": 2.3890813357350702e-05,
      "loss": 2.4786,
      "step": 29080
    },
    {
      "epoch": 4.030760703893584,
      "grad_norm": 13.718321800231934,
      "learning_rate": 2.3885270888180684e-05,
      "loss": 2.3418,
      "step": 29090
    },
    {
      "epoch": 4.032146321186088,
      "grad_norm": 12.36447525024414,
      "learning_rate": 2.3879728419010673e-05,
      "loss": 2.3336,
      "step": 29100
    },
    {
      "epoch": 4.033531938478593,
      "grad_norm": 19.482128143310547,
      "learning_rate": 2.3874185949840655e-05,
      "loss": 2.1953,
      "step": 29110
    },
    {
      "epoch": 4.034917555771096,
      "grad_norm": 10.387101173400879,
      "learning_rate": 2.386864348067064e-05,
      "loss": 2.7192,
      "step": 29120
    },
    {
      "epoch": 4.0363031730636,
      "grad_norm": 10.944962501525879,
      "learning_rate": 2.3863101011500625e-05,
      "loss": 2.2364,
      "step": 29130
    },
    {
      "epoch": 4.037688790356103,
      "grad_norm": 9.347639083862305,
      "learning_rate": 2.385755854233061e-05,
      "loss": 2.2145,
      "step": 29140
    },
    {
      "epoch": 4.039074407648608,
      "grad_norm": 20.80185890197754,
      "learning_rate": 2.3852016073160592e-05,
      "loss": 1.8577,
      "step": 29150
    },
    {
      "epoch": 4.040460024941111,
      "grad_norm": 16.146713256835938,
      "learning_rate": 2.384647360399058e-05,
      "loss": 2.0502,
      "step": 29160
    },
    {
      "epoch": 4.041845642233615,
      "grad_norm": 18.799020767211914,
      "learning_rate": 2.3840931134820563e-05,
      "loss": 2.3345,
      "step": 29170
    },
    {
      "epoch": 4.043231259526119,
      "grad_norm": 10.97229290008545,
      "learning_rate": 2.3835388665650548e-05,
      "loss": 2.0601,
      "step": 29180
    },
    {
      "epoch": 4.044616876818623,
      "grad_norm": 11.957474708557129,
      "learning_rate": 2.3829846196480537e-05,
      "loss": 2.1149,
      "step": 29190
    },
    {
      "epoch": 4.046002494111127,
      "grad_norm": 12.926708221435547,
      "learning_rate": 2.382430372731052e-05,
      "loss": 2.3713,
      "step": 29200
    },
    {
      "epoch": 4.04738811140363,
      "grad_norm": 8.20329475402832,
      "learning_rate": 2.38187612581405e-05,
      "loss": 2.3279,
      "step": 29210
    },
    {
      "epoch": 4.048773728696134,
      "grad_norm": 15.407404899597168,
      "learning_rate": 2.381321878897049e-05,
      "loss": 2.0572,
      "step": 29220
    },
    {
      "epoch": 4.050159345988638,
      "grad_norm": 15.293643951416016,
      "learning_rate": 2.380767631980047e-05,
      "loss": 2.1792,
      "step": 29230
    },
    {
      "epoch": 4.051544963281142,
      "grad_norm": 11.990340232849121,
      "learning_rate": 2.380213385063046e-05,
      "loss": 2.3726,
      "step": 29240
    },
    {
      "epoch": 4.052930580573646,
      "grad_norm": 12.720694541931152,
      "learning_rate": 2.3796591381460445e-05,
      "loss": 2.2684,
      "step": 29250
    },
    {
      "epoch": 4.054316197866149,
      "grad_norm": 9.059471130371094,
      "learning_rate": 2.3791048912290427e-05,
      "loss": 1.8408,
      "step": 29260
    },
    {
      "epoch": 4.0557018151586535,
      "grad_norm": 14.022533416748047,
      "learning_rate": 2.3785506443120415e-05,
      "loss": 1.9577,
      "step": 29270
    },
    {
      "epoch": 4.057087432451157,
      "grad_norm": 8.257111549377441,
      "learning_rate": 2.3779963973950397e-05,
      "loss": 2.3093,
      "step": 29280
    },
    {
      "epoch": 4.058473049743661,
      "grad_norm": 11.784415245056152,
      "learning_rate": 2.377442150478038e-05,
      "loss": 1.9533,
      "step": 29290
    },
    {
      "epoch": 4.059858667036164,
      "grad_norm": 6.3102498054504395,
      "learning_rate": 2.3768879035610368e-05,
      "loss": 2.2258,
      "step": 29300
    },
    {
      "epoch": 4.0612442843286685,
      "grad_norm": 20.55382537841797,
      "learning_rate": 2.3763336566440353e-05,
      "loss": 2.6187,
      "step": 29310
    },
    {
      "epoch": 4.062629901621173,
      "grad_norm": 7.934423446655273,
      "learning_rate": 2.3757794097270335e-05,
      "loss": 1.9002,
      "step": 29320
    },
    {
      "epoch": 4.064015518913676,
      "grad_norm": 8.869349479675293,
      "learning_rate": 2.3752251628100323e-05,
      "loss": 2.076,
      "step": 29330
    },
    {
      "epoch": 4.06540113620618,
      "grad_norm": 17.32971954345703,
      "learning_rate": 2.3746709158930305e-05,
      "loss": 2.0351,
      "step": 29340
    },
    {
      "epoch": 4.0667867534986835,
      "grad_norm": 10.070353507995605,
      "learning_rate": 2.374116668976029e-05,
      "loss": 2.1062,
      "step": 29350
    },
    {
      "epoch": 4.068172370791188,
      "grad_norm": 5.809153079986572,
      "learning_rate": 2.3735624220590276e-05,
      "loss": 1.9934,
      "step": 29360
    },
    {
      "epoch": 4.069557988083691,
      "grad_norm": 9.986104965209961,
      "learning_rate": 2.373008175142026e-05,
      "loss": 1.9931,
      "step": 29370
    },
    {
      "epoch": 4.070943605376195,
      "grad_norm": 14.359070777893066,
      "learning_rate": 2.3724539282250243e-05,
      "loss": 1.6854,
      "step": 29380
    },
    {
      "epoch": 4.072329222668699,
      "grad_norm": 9.23745059967041,
      "learning_rate": 2.371899681308023e-05,
      "loss": 2.2403,
      "step": 29390
    },
    {
      "epoch": 4.073714839961203,
      "grad_norm": 8.694304466247559,
      "learning_rate": 2.3713454343910213e-05,
      "loss": 2.0674,
      "step": 29400
    },
    {
      "epoch": 4.075100457253707,
      "grad_norm": 23.88481903076172,
      "learning_rate": 2.37079118747402e-05,
      "loss": 2.3254,
      "step": 29410
    },
    {
      "epoch": 4.07648607454621,
      "grad_norm": 15.364545822143555,
      "learning_rate": 2.3702369405570184e-05,
      "loss": 2.0464,
      "step": 29420
    },
    {
      "epoch": 4.077871691838714,
      "grad_norm": 16.66879653930664,
      "learning_rate": 2.369682693640017e-05,
      "loss": 1.9073,
      "step": 29430
    },
    {
      "epoch": 4.079257309131218,
      "grad_norm": 9.615060806274414,
      "learning_rate": 2.369128446723015e-05,
      "loss": 2.1759,
      "step": 29440
    },
    {
      "epoch": 4.080642926423722,
      "grad_norm": 17.513671875,
      "learning_rate": 2.368574199806014e-05,
      "loss": 2.0773,
      "step": 29450
    },
    {
      "epoch": 4.082028543716225,
      "grad_norm": 12.951053619384766,
      "learning_rate": 2.368019952889012e-05,
      "loss": 2.127,
      "step": 29460
    },
    {
      "epoch": 4.083414161008729,
      "grad_norm": 16.663482666015625,
      "learning_rate": 2.3674657059720107e-05,
      "loss": 1.9913,
      "step": 29470
    },
    {
      "epoch": 4.0847997783012335,
      "grad_norm": 10.582769393920898,
      "learning_rate": 2.3669114590550092e-05,
      "loss": 2.0913,
      "step": 29480
    },
    {
      "epoch": 4.086185395593737,
      "grad_norm": 15.573539733886719,
      "learning_rate": 2.3663572121380077e-05,
      "loss": 1.941,
      "step": 29490
    },
    {
      "epoch": 4.087571012886241,
      "grad_norm": 6.476613521575928,
      "learning_rate": 2.365802965221006e-05,
      "loss": 1.6889,
      "step": 29500
    },
    {
      "epoch": 4.088956630178744,
      "grad_norm": 17.982866287231445,
      "learning_rate": 2.3652487183040048e-05,
      "loss": 1.6521,
      "step": 29510
    },
    {
      "epoch": 4.0903422474712485,
      "grad_norm": 11.887020111083984,
      "learning_rate": 2.364694471387003e-05,
      "loss": 2.0637,
      "step": 29520
    },
    {
      "epoch": 4.091727864763752,
      "grad_norm": 11.168020248413086,
      "learning_rate": 2.3641402244700018e-05,
      "loss": 1.8502,
      "step": 29530
    },
    {
      "epoch": 4.093113482056256,
      "grad_norm": 9.613301277160645,
      "learning_rate": 2.363585977553e-05,
      "loss": 1.834,
      "step": 29540
    },
    {
      "epoch": 4.09449909934876,
      "grad_norm": 17.755422592163086,
      "learning_rate": 2.3630317306359985e-05,
      "loss": 1.9752,
      "step": 29550
    },
    {
      "epoch": 4.0958847166412635,
      "grad_norm": 10.336352348327637,
      "learning_rate": 2.362477483718997e-05,
      "loss": 2.5118,
      "step": 29560
    },
    {
      "epoch": 4.097270333933768,
      "grad_norm": 12.589558601379395,
      "learning_rate": 2.3619232368019956e-05,
      "loss": 2.4789,
      "step": 29570
    },
    {
      "epoch": 4.098655951226271,
      "grad_norm": 14.542265892028809,
      "learning_rate": 2.3613689898849938e-05,
      "loss": 1.9205,
      "step": 29580
    },
    {
      "epoch": 4.100041568518775,
      "grad_norm": 9.198247909545898,
      "learning_rate": 2.3608147429679926e-05,
      "loss": 2.0426,
      "step": 29590
    },
    {
      "epoch": 4.1014271858112785,
      "grad_norm": 17.054418563842773,
      "learning_rate": 2.3602604960509908e-05,
      "loss": 1.927,
      "step": 29600
    },
    {
      "epoch": 4.102812803103783,
      "grad_norm": 21.093107223510742,
      "learning_rate": 2.3597062491339893e-05,
      "loss": 1.9952,
      "step": 29610
    },
    {
      "epoch": 4.104198420396287,
      "grad_norm": 10.732715606689453,
      "learning_rate": 2.3591520022169882e-05,
      "loss": 2.2443,
      "step": 29620
    },
    {
      "epoch": 4.10558403768879,
      "grad_norm": 12.307727813720703,
      "learning_rate": 2.3585977552999864e-05,
      "loss": 2.2048,
      "step": 29630
    },
    {
      "epoch": 4.106969654981294,
      "grad_norm": 4.0633673667907715,
      "learning_rate": 2.3580435083829846e-05,
      "loss": 1.9158,
      "step": 29640
    },
    {
      "epoch": 4.108355272273798,
      "grad_norm": 9.346271514892578,
      "learning_rate": 2.3574892614659834e-05,
      "loss": 2.1019,
      "step": 29650
    },
    {
      "epoch": 4.109740889566302,
      "grad_norm": 10.768617630004883,
      "learning_rate": 2.3569350145489816e-05,
      "loss": 1.9785,
      "step": 29660
    },
    {
      "epoch": 4.111126506858805,
      "grad_norm": 9.539986610412598,
      "learning_rate": 2.35638076763198e-05,
      "loss": 1.9122,
      "step": 29670
    },
    {
      "epoch": 4.112512124151309,
      "grad_norm": 13.119430541992188,
      "learning_rate": 2.355826520714979e-05,
      "loss": 2.3914,
      "step": 29680
    },
    {
      "epoch": 4.113897741443814,
      "grad_norm": 12.208017349243164,
      "learning_rate": 2.3552722737979772e-05,
      "loss": 1.7788,
      "step": 29690
    },
    {
      "epoch": 4.115283358736317,
      "grad_norm": 10.540446281433105,
      "learning_rate": 2.3547180268809754e-05,
      "loss": 2.3755,
      "step": 29700
    },
    {
      "epoch": 4.116668976028821,
      "grad_norm": 10.065244674682617,
      "learning_rate": 2.3541637799639742e-05,
      "loss": 1.9021,
      "step": 29710
    },
    {
      "epoch": 4.118054593321324,
      "grad_norm": 11.439318656921387,
      "learning_rate": 2.3536095330469724e-05,
      "loss": 2.3178,
      "step": 29720
    },
    {
      "epoch": 4.119440210613829,
      "grad_norm": 8.67749309539795,
      "learning_rate": 2.353055286129971e-05,
      "loss": 1.9052,
      "step": 29730
    },
    {
      "epoch": 4.120825827906332,
      "grad_norm": 9.605073928833008,
      "learning_rate": 2.3525010392129698e-05,
      "loss": 2.4378,
      "step": 29740
    },
    {
      "epoch": 4.122211445198836,
      "grad_norm": 14.545595169067383,
      "learning_rate": 2.351946792295968e-05,
      "loss": 1.4964,
      "step": 29750
    },
    {
      "epoch": 4.12359706249134,
      "grad_norm": 11.58385944366455,
      "learning_rate": 2.3513925453789662e-05,
      "loss": 2.2338,
      "step": 29760
    },
    {
      "epoch": 4.124982679783844,
      "grad_norm": 7.399021625518799,
      "learning_rate": 2.350838298461965e-05,
      "loss": 2.4135,
      "step": 29770
    },
    {
      "epoch": 4.126368297076348,
      "grad_norm": 17.48776626586914,
      "learning_rate": 2.3502840515449636e-05,
      "loss": 2.3173,
      "step": 29780
    },
    {
      "epoch": 4.127753914368851,
      "grad_norm": 13.18836498260498,
      "learning_rate": 2.3497298046279618e-05,
      "loss": 2.3476,
      "step": 29790
    },
    {
      "epoch": 4.129139531661355,
      "grad_norm": 15.379493713378906,
      "learning_rate": 2.3491755577109606e-05,
      "loss": 2.2262,
      "step": 29800
    },
    {
      "epoch": 4.130525148953859,
      "grad_norm": 12.088637351989746,
      "learning_rate": 2.3486213107939588e-05,
      "loss": 2.4961,
      "step": 29810
    },
    {
      "epoch": 4.131910766246363,
      "grad_norm": 12.161781311035156,
      "learning_rate": 2.3480670638769577e-05,
      "loss": 1.9936,
      "step": 29820
    },
    {
      "epoch": 4.133296383538867,
      "grad_norm": 9.765885353088379,
      "learning_rate": 2.347512816959956e-05,
      "loss": 1.9916,
      "step": 29830
    },
    {
      "epoch": 4.13468200083137,
      "grad_norm": 11.262981414794922,
      "learning_rate": 2.3469585700429544e-05,
      "loss": 2.005,
      "step": 29840
    },
    {
      "epoch": 4.1360676181238745,
      "grad_norm": 17.0517520904541,
      "learning_rate": 2.346404323125953e-05,
      "loss": 1.9721,
      "step": 29850
    },
    {
      "epoch": 4.137453235416378,
      "grad_norm": 10.614294052124023,
      "learning_rate": 2.3458500762089514e-05,
      "loss": 2.4974,
      "step": 29860
    },
    {
      "epoch": 4.138838852708882,
      "grad_norm": 8.749300003051758,
      "learning_rate": 2.3452958292919496e-05,
      "loss": 2.0135,
      "step": 29870
    },
    {
      "epoch": 4.140224470001385,
      "grad_norm": 8.946436882019043,
      "learning_rate": 2.3447415823749485e-05,
      "loss": 2.3418,
      "step": 29880
    },
    {
      "epoch": 4.1416100872938895,
      "grad_norm": 7.451663970947266,
      "learning_rate": 2.3441873354579467e-05,
      "loss": 1.8812,
      "step": 29890
    },
    {
      "epoch": 4.142995704586394,
      "grad_norm": 14.228642463684082,
      "learning_rate": 2.3436330885409452e-05,
      "loss": 1.9117,
      "step": 29900
    },
    {
      "epoch": 4.144381321878897,
      "grad_norm": 20.017261505126953,
      "learning_rate": 2.3430788416239437e-05,
      "loss": 2.2188,
      "step": 29910
    },
    {
      "epoch": 4.145766939171401,
      "grad_norm": 14.286930084228516,
      "learning_rate": 2.3425245947069423e-05,
      "loss": 2.1698,
      "step": 29920
    },
    {
      "epoch": 4.1471525564639045,
      "grad_norm": 16.545934677124023,
      "learning_rate": 2.3419703477899404e-05,
      "loss": 2.5596,
      "step": 29930
    },
    {
      "epoch": 4.148538173756409,
      "grad_norm": 8.54317569732666,
      "learning_rate": 2.3414161008729393e-05,
      "loss": 2.1891,
      "step": 29940
    },
    {
      "epoch": 4.149923791048912,
      "grad_norm": 8.895182609558105,
      "learning_rate": 2.3408618539559375e-05,
      "loss": 2.5047,
      "step": 29950
    },
    {
      "epoch": 4.151309408341416,
      "grad_norm": 9.267623901367188,
      "learning_rate": 2.340307607038936e-05,
      "loss": 2.4978,
      "step": 29960
    },
    {
      "epoch": 4.1526950256339195,
      "grad_norm": 15.747159957885742,
      "learning_rate": 2.3397533601219345e-05,
      "loss": 2.217,
      "step": 29970
    },
    {
      "epoch": 4.154080642926424,
      "grad_norm": 10.178616523742676,
      "learning_rate": 2.339199113204933e-05,
      "loss": 1.8359,
      "step": 29980
    },
    {
      "epoch": 4.155466260218928,
      "grad_norm": 8.378785133361816,
      "learning_rate": 2.3386448662879312e-05,
      "loss": 1.7366,
      "step": 29990
    },
    {
      "epoch": 4.156851877511431,
      "grad_norm": 10.315820693969727,
      "learning_rate": 2.33809061937093e-05,
      "loss": 1.7233,
      "step": 30000
    },
    {
      "epoch": 4.158237494803935,
      "grad_norm": 6.45681095123291,
      "learning_rate": 2.3375363724539283e-05,
      "loss": 2.2465,
      "step": 30010
    },
    {
      "epoch": 4.159623112096439,
      "grad_norm": 9.820961952209473,
      "learning_rate": 2.3369821255369268e-05,
      "loss": 2.0947,
      "step": 30020
    },
    {
      "epoch": 4.161008729388943,
      "grad_norm": 11.044437408447266,
      "learning_rate": 2.3364278786199253e-05,
      "loss": 2.58,
      "step": 30030
    },
    {
      "epoch": 4.162394346681447,
      "grad_norm": 9.177406311035156,
      "learning_rate": 2.335873631702924e-05,
      "loss": 1.9141,
      "step": 30040
    },
    {
      "epoch": 4.16377996397395,
      "grad_norm": 12.429465293884277,
      "learning_rate": 2.335319384785922e-05,
      "loss": 1.773,
      "step": 30050
    },
    {
      "epoch": 4.1651655812664545,
      "grad_norm": 12.837544441223145,
      "learning_rate": 2.334765137868921e-05,
      "loss": 1.9716,
      "step": 30060
    },
    {
      "epoch": 4.166551198558958,
      "grad_norm": 11.478675842285156,
      "learning_rate": 2.334210890951919e-05,
      "loss": 1.6556,
      "step": 30070
    },
    {
      "epoch": 4.167936815851462,
      "grad_norm": 18.171985626220703,
      "learning_rate": 2.3336566440349176e-05,
      "loss": 1.5475,
      "step": 30080
    },
    {
      "epoch": 4.169322433143965,
      "grad_norm": 11.4829683303833,
      "learning_rate": 2.333102397117916e-05,
      "loss": 1.8641,
      "step": 30090
    },
    {
      "epoch": 4.1707080504364695,
      "grad_norm": 15.55392837524414,
      "learning_rate": 2.3325481502009147e-05,
      "loss": 1.7582,
      "step": 30100
    },
    {
      "epoch": 4.172093667728973,
      "grad_norm": 10.843485832214355,
      "learning_rate": 2.331993903283913e-05,
      "loss": 1.7547,
      "step": 30110
    },
    {
      "epoch": 4.173479285021477,
      "grad_norm": 21.87079620361328,
      "learning_rate": 2.3314396563669117e-05,
      "loss": 1.918,
      "step": 30120
    },
    {
      "epoch": 4.174864902313981,
      "grad_norm": 10.19692611694336,
      "learning_rate": 2.33088540944991e-05,
      "loss": 2.0048,
      "step": 30130
    },
    {
      "epoch": 4.1762505196064845,
      "grad_norm": 9.800415992736816,
      "learning_rate": 2.3303311625329088e-05,
      "loss": 1.988,
      "step": 30140
    },
    {
      "epoch": 4.177636136898989,
      "grad_norm": 10.963951110839844,
      "learning_rate": 2.329776915615907e-05,
      "loss": 2.4272,
      "step": 30150
    },
    {
      "epoch": 4.179021754191492,
      "grad_norm": 8.188770294189453,
      "learning_rate": 2.3292226686989055e-05,
      "loss": 2.1699,
      "step": 30160
    },
    {
      "epoch": 4.180407371483996,
      "grad_norm": 20.303903579711914,
      "learning_rate": 2.3286684217819044e-05,
      "loss": 2.6957,
      "step": 30170
    },
    {
      "epoch": 4.1817929887764995,
      "grad_norm": 5.4837260246276855,
      "learning_rate": 2.3281141748649025e-05,
      "loss": 2.4097,
      "step": 30180
    },
    {
      "epoch": 4.183178606069004,
      "grad_norm": 11.611679077148438,
      "learning_rate": 2.3275599279479007e-05,
      "loss": 2.454,
      "step": 30190
    },
    {
      "epoch": 4.184564223361508,
      "grad_norm": 13.923229217529297,
      "learning_rate": 2.3270056810308996e-05,
      "loss": 2.1447,
      "step": 30200
    },
    {
      "epoch": 4.185949840654011,
      "grad_norm": 10.306876182556152,
      "learning_rate": 2.326451434113898e-05,
      "loss": 2.2477,
      "step": 30210
    },
    {
      "epoch": 4.187335457946515,
      "grad_norm": 7.553158760070801,
      "learning_rate": 2.3258971871968963e-05,
      "loss": 1.9963,
      "step": 30220
    },
    {
      "epoch": 4.188721075239019,
      "grad_norm": 5.780259609222412,
      "learning_rate": 2.325342940279895e-05,
      "loss": 2.4047,
      "step": 30230
    },
    {
      "epoch": 4.190106692531523,
      "grad_norm": 5.940270900726318,
      "learning_rate": 2.3247886933628934e-05,
      "loss": 1.9364,
      "step": 30240
    },
    {
      "epoch": 4.191492309824026,
      "grad_norm": 12.499075889587402,
      "learning_rate": 2.3242344464458915e-05,
      "loss": 2.1901,
      "step": 30250
    },
    {
      "epoch": 4.19287792711653,
      "grad_norm": 9.438823699951172,
      "learning_rate": 2.3236801995288904e-05,
      "loss": 2.1043,
      "step": 30260
    },
    {
      "epoch": 4.194263544409035,
      "grad_norm": 11.363000869750977,
      "learning_rate": 2.323125952611889e-05,
      "loss": 2.2292,
      "step": 30270
    },
    {
      "epoch": 4.195649161701538,
      "grad_norm": 9.622711181640625,
      "learning_rate": 2.322571705694887e-05,
      "loss": 1.8942,
      "step": 30280
    },
    {
      "epoch": 4.197034778994042,
      "grad_norm": 17.297395706176758,
      "learning_rate": 2.322017458777886e-05,
      "loss": 1.8789,
      "step": 30290
    },
    {
      "epoch": 4.198420396286545,
      "grad_norm": 10.761690139770508,
      "learning_rate": 2.321463211860884e-05,
      "loss": 2.3081,
      "step": 30300
    },
    {
      "epoch": 4.19980601357905,
      "grad_norm": 11.230083465576172,
      "learning_rate": 2.3209089649438824e-05,
      "loss": 1.9176,
      "step": 30310
    },
    {
      "epoch": 4.201191630871553,
      "grad_norm": 9.968461036682129,
      "learning_rate": 2.3203547180268812e-05,
      "loss": 2.5321,
      "step": 30320
    },
    {
      "epoch": 4.202577248164057,
      "grad_norm": 15.147144317626953,
      "learning_rate": 2.3198004711098797e-05,
      "loss": 2.0526,
      "step": 30330
    },
    {
      "epoch": 4.203962865456561,
      "grad_norm": 8.632039070129395,
      "learning_rate": 2.319246224192878e-05,
      "loss": 2.3251,
      "step": 30340
    },
    {
      "epoch": 4.205348482749065,
      "grad_norm": 16.092559814453125,
      "learning_rate": 2.3186919772758768e-05,
      "loss": 2.2908,
      "step": 30350
    },
    {
      "epoch": 4.206734100041569,
      "grad_norm": 10.88056755065918,
      "learning_rate": 2.318137730358875e-05,
      "loss": 1.835,
      "step": 30360
    },
    {
      "epoch": 4.208119717334072,
      "grad_norm": 13.218290328979492,
      "learning_rate": 2.3175834834418735e-05,
      "loss": 1.827,
      "step": 30370
    },
    {
      "epoch": 4.209505334626576,
      "grad_norm": 8.936305046081543,
      "learning_rate": 2.317029236524872e-05,
      "loss": 1.7223,
      "step": 30380
    },
    {
      "epoch": 4.21089095191908,
      "grad_norm": 12.87495231628418,
      "learning_rate": 2.3164749896078706e-05,
      "loss": 2.1435,
      "step": 30390
    },
    {
      "epoch": 4.212276569211584,
      "grad_norm": 12.217764854431152,
      "learning_rate": 2.3159207426908687e-05,
      "loss": 1.7558,
      "step": 30400
    },
    {
      "epoch": 4.213662186504088,
      "grad_norm": 15.419379234313965,
      "learning_rate": 2.3153664957738676e-05,
      "loss": 1.9137,
      "step": 30410
    },
    {
      "epoch": 4.215047803796591,
      "grad_norm": 9.400267601013184,
      "learning_rate": 2.3148122488568658e-05,
      "loss": 2.2726,
      "step": 30420
    },
    {
      "epoch": 4.2164334210890955,
      "grad_norm": 22.125423431396484,
      "learning_rate": 2.3142580019398647e-05,
      "loss": 2.1192,
      "step": 30430
    },
    {
      "epoch": 4.217819038381599,
      "grad_norm": 12.974364280700684,
      "learning_rate": 2.313703755022863e-05,
      "loss": 2.1933,
      "step": 30440
    },
    {
      "epoch": 4.219204655674103,
      "grad_norm": 13.694748878479004,
      "learning_rate": 2.3131495081058614e-05,
      "loss": 2.5541,
      "step": 30450
    },
    {
      "epoch": 4.220590272966606,
      "grad_norm": 19.946020126342773,
      "learning_rate": 2.31259526118886e-05,
      "loss": 2.2825,
      "step": 30460
    },
    {
      "epoch": 4.2219758902591105,
      "grad_norm": 11.719368934631348,
      "learning_rate": 2.3120410142718584e-05,
      "loss": 2.2752,
      "step": 30470
    },
    {
      "epoch": 4.223361507551615,
      "grad_norm": 10.989251136779785,
      "learning_rate": 2.3114867673548566e-05,
      "loss": 2.118,
      "step": 30480
    },
    {
      "epoch": 4.224747124844118,
      "grad_norm": 9.795047760009766,
      "learning_rate": 2.3109325204378555e-05,
      "loss": 2.3228,
      "step": 30490
    },
    {
      "epoch": 4.226132742136622,
      "grad_norm": 6.468178749084473,
      "learning_rate": 2.3103782735208536e-05,
      "loss": 2.2765,
      "step": 30500
    },
    {
      "epoch": 4.2275183594291255,
      "grad_norm": 11.1764497756958,
      "learning_rate": 2.3098240266038522e-05,
      "loss": 2.3331,
      "step": 30510
    },
    {
      "epoch": 4.22890397672163,
      "grad_norm": 11.431632041931152,
      "learning_rate": 2.3092697796868507e-05,
      "loss": 1.8279,
      "step": 30520
    },
    {
      "epoch": 4.230289594014133,
      "grad_norm": 9.837384223937988,
      "learning_rate": 2.3087155327698492e-05,
      "loss": 2.0746,
      "step": 30530
    },
    {
      "epoch": 4.231675211306637,
      "grad_norm": 12.569942474365234,
      "learning_rate": 2.3081612858528474e-05,
      "loss": 1.9913,
      "step": 30540
    },
    {
      "epoch": 4.233060828599141,
      "grad_norm": 6.914081573486328,
      "learning_rate": 2.3076070389358463e-05,
      "loss": 2.0258,
      "step": 30550
    },
    {
      "epoch": 4.234446445891645,
      "grad_norm": 7.737005233764648,
      "learning_rate": 2.3070527920188445e-05,
      "loss": 1.9607,
      "step": 30560
    },
    {
      "epoch": 4.235832063184149,
      "grad_norm": 10.809356689453125,
      "learning_rate": 2.306498545101843e-05,
      "loss": 1.5121,
      "step": 30570
    },
    {
      "epoch": 4.237217680476652,
      "grad_norm": 13.941633224487305,
      "learning_rate": 2.3059442981848415e-05,
      "loss": 1.8796,
      "step": 30580
    },
    {
      "epoch": 4.238603297769156,
      "grad_norm": 8.33572769165039,
      "learning_rate": 2.30539005126784e-05,
      "loss": 2.2589,
      "step": 30590
    },
    {
      "epoch": 4.23998891506166,
      "grad_norm": 9.623620986938477,
      "learning_rate": 2.3048358043508382e-05,
      "loss": 2.0646,
      "step": 30600
    },
    {
      "epoch": 4.241374532354164,
      "grad_norm": 12.075078010559082,
      "learning_rate": 2.304281557433837e-05,
      "loss": 1.7898,
      "step": 30610
    },
    {
      "epoch": 4.242760149646667,
      "grad_norm": 18.282360076904297,
      "learning_rate": 2.3037273105168353e-05,
      "loss": 2.4817,
      "step": 30620
    },
    {
      "epoch": 4.244145766939171,
      "grad_norm": 9.399771690368652,
      "learning_rate": 2.3031730635998338e-05,
      "loss": 2.2455,
      "step": 30630
    },
    {
      "epoch": 4.2455313842316755,
      "grad_norm": 11.685400009155273,
      "learning_rate": 2.3026188166828327e-05,
      "loss": 1.9756,
      "step": 30640
    },
    {
      "epoch": 4.246917001524179,
      "grad_norm": 14.993160247802734,
      "learning_rate": 2.302064569765831e-05,
      "loss": 2.028,
      "step": 30650
    },
    {
      "epoch": 4.248302618816683,
      "grad_norm": 14.357467651367188,
      "learning_rate": 2.301510322848829e-05,
      "loss": 1.9767,
      "step": 30660
    },
    {
      "epoch": 4.249688236109186,
      "grad_norm": 14.29373550415039,
      "learning_rate": 2.300956075931828e-05,
      "loss": 2.2953,
      "step": 30670
    },
    {
      "epoch": 4.2510738534016905,
      "grad_norm": 12.359636306762695,
      "learning_rate": 2.300401829014826e-05,
      "loss": 1.7007,
      "step": 30680
    },
    {
      "epoch": 4.252459470694195,
      "grad_norm": 8.000680923461914,
      "learning_rate": 2.2998475820978246e-05,
      "loss": 1.9944,
      "step": 30690
    },
    {
      "epoch": 4.253845087986698,
      "grad_norm": 17.17586326599121,
      "learning_rate": 2.2992933351808235e-05,
      "loss": 2.3286,
      "step": 30700
    },
    {
      "epoch": 4.255230705279202,
      "grad_norm": 16.030925750732422,
      "learning_rate": 2.2987390882638217e-05,
      "loss": 2.1517,
      "step": 30710
    },
    {
      "epoch": 4.2566163225717055,
      "grad_norm": 17.04145622253418,
      "learning_rate": 2.2981848413468205e-05,
      "loss": 1.7896,
      "step": 30720
    },
    {
      "epoch": 4.25800193986421,
      "grad_norm": 13.406445503234863,
      "learning_rate": 2.2976305944298187e-05,
      "loss": 2.4317,
      "step": 30730
    },
    {
      "epoch": 4.259387557156713,
      "grad_norm": 9.571308135986328,
      "learning_rate": 2.297076347512817e-05,
      "loss": 2.4212,
      "step": 30740
    },
    {
      "epoch": 4.260773174449217,
      "grad_norm": 11.57596492767334,
      "learning_rate": 2.2965221005958158e-05,
      "loss": 2.1824,
      "step": 30750
    },
    {
      "epoch": 4.2621587917417205,
      "grad_norm": 10.315425872802734,
      "learning_rate": 2.2959678536788143e-05,
      "loss": 2.1733,
      "step": 30760
    },
    {
      "epoch": 4.263544409034225,
      "grad_norm": 6.681501865386963,
      "learning_rate": 2.2954136067618125e-05,
      "loss": 2.0787,
      "step": 30770
    },
    {
      "epoch": 4.264930026326729,
      "grad_norm": 15.46933650970459,
      "learning_rate": 2.2948593598448113e-05,
      "loss": 2.0148,
      "step": 30780
    },
    {
      "epoch": 4.266315643619232,
      "grad_norm": 10.08151626586914,
      "learning_rate": 2.2943051129278095e-05,
      "loss": 1.9406,
      "step": 30790
    },
    {
      "epoch": 4.267701260911736,
      "grad_norm": 12.851520538330078,
      "learning_rate": 2.293750866010808e-05,
      "loss": 2.1512,
      "step": 30800
    },
    {
      "epoch": 4.26908687820424,
      "grad_norm": 7.580249786376953,
      "learning_rate": 2.2931966190938066e-05,
      "loss": 2.0726,
      "step": 30810
    },
    {
      "epoch": 4.270472495496744,
      "grad_norm": 10.10324478149414,
      "learning_rate": 2.292642372176805e-05,
      "loss": 1.9556,
      "step": 30820
    },
    {
      "epoch": 4.271858112789247,
      "grad_norm": 11.788089752197266,
      "learning_rate": 2.2920881252598033e-05,
      "loss": 1.5468,
      "step": 30830
    },
    {
      "epoch": 4.273243730081751,
      "grad_norm": 15.738505363464355,
      "learning_rate": 2.291533878342802e-05,
      "loss": 1.9285,
      "step": 30840
    },
    {
      "epoch": 4.274629347374256,
      "grad_norm": 14.459300994873047,
      "learning_rate": 2.2909796314258003e-05,
      "loss": 2.11,
      "step": 30850
    },
    {
      "epoch": 4.276014964666759,
      "grad_norm": 14.143965721130371,
      "learning_rate": 2.290425384508799e-05,
      "loss": 2.0822,
      "step": 30860
    },
    {
      "epoch": 4.277400581959263,
      "grad_norm": 15.883869171142578,
      "learning_rate": 2.2898711375917974e-05,
      "loss": 2.0813,
      "step": 30870
    },
    {
      "epoch": 4.278786199251766,
      "grad_norm": 13.900775909423828,
      "learning_rate": 2.289316890674796e-05,
      "loss": 2.0959,
      "step": 30880
    },
    {
      "epoch": 4.280171816544271,
      "grad_norm": 20.649044036865234,
      "learning_rate": 2.288762643757794e-05,
      "loss": 2.0039,
      "step": 30890
    },
    {
      "epoch": 4.281557433836774,
      "grad_norm": 15.193313598632812,
      "learning_rate": 2.288208396840793e-05,
      "loss": 2.3191,
      "step": 30900
    },
    {
      "epoch": 4.282943051129278,
      "grad_norm": 16.5052547454834,
      "learning_rate": 2.287654149923791e-05,
      "loss": 1.5324,
      "step": 30910
    },
    {
      "epoch": 4.284328668421782,
      "grad_norm": 12.8305025100708,
      "learning_rate": 2.2870999030067897e-05,
      "loss": 2.043,
      "step": 30920
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 13.261062622070312,
      "learning_rate": 2.2865456560897882e-05,
      "loss": 2.3357,
      "step": 30930
    },
    {
      "epoch": 4.28709990300679,
      "grad_norm": 16.901798248291016,
      "learning_rate": 2.2859914091727867e-05,
      "loss": 2.1668,
      "step": 30940
    },
    {
      "epoch": 4.288485520299293,
      "grad_norm": 11.787674903869629,
      "learning_rate": 2.285437162255785e-05,
      "loss": 2.0647,
      "step": 30950
    },
    {
      "epoch": 4.289871137591797,
      "grad_norm": 12.721632957458496,
      "learning_rate": 2.2848829153387838e-05,
      "loss": 1.9151,
      "step": 30960
    },
    {
      "epoch": 4.291256754884301,
      "grad_norm": 14.435722351074219,
      "learning_rate": 2.284328668421782e-05,
      "loss": 2.129,
      "step": 30970
    },
    {
      "epoch": 4.292642372176805,
      "grad_norm": 6.325794696807861,
      "learning_rate": 2.2837744215047805e-05,
      "loss": 2.0256,
      "step": 30980
    },
    {
      "epoch": 4.294027989469309,
      "grad_norm": 15.581879615783691,
      "learning_rate": 2.283220174587779e-05,
      "loss": 2.3886,
      "step": 30990
    },
    {
      "epoch": 4.295413606761812,
      "grad_norm": 9.496918678283691,
      "learning_rate": 2.2826659276707775e-05,
      "loss": 2.2367,
      "step": 31000
    },
    {
      "epoch": 4.2967992240543165,
      "grad_norm": 12.570114135742188,
      "learning_rate": 2.282111680753776e-05,
      "loss": 2.0288,
      "step": 31010
    },
    {
      "epoch": 4.29818484134682,
      "grad_norm": 13.525964736938477,
      "learning_rate": 2.2815574338367746e-05,
      "loss": 1.8167,
      "step": 31020
    },
    {
      "epoch": 4.299570458639324,
      "grad_norm": 13.055193901062012,
      "learning_rate": 2.2810031869197728e-05,
      "loss": 1.7367,
      "step": 31030
    },
    {
      "epoch": 4.300956075931827,
      "grad_norm": 11.749062538146973,
      "learning_rate": 2.2804489400027716e-05,
      "loss": 2.2925,
      "step": 31040
    },
    {
      "epoch": 4.3023416932243315,
      "grad_norm": 10.156188011169434,
      "learning_rate": 2.2798946930857698e-05,
      "loss": 2.1177,
      "step": 31050
    },
    {
      "epoch": 4.303727310516836,
      "grad_norm": 10.265910148620605,
      "learning_rate": 2.2793404461687683e-05,
      "loss": 1.9435,
      "step": 31060
    },
    {
      "epoch": 4.305112927809339,
      "grad_norm": 11.706050872802734,
      "learning_rate": 2.2787861992517672e-05,
      "loss": 2.2927,
      "step": 31070
    },
    {
      "epoch": 4.306498545101843,
      "grad_norm": 10.463712692260742,
      "learning_rate": 2.2782319523347654e-05,
      "loss": 1.8159,
      "step": 31080
    },
    {
      "epoch": 4.3078841623943465,
      "grad_norm": 13.912872314453125,
      "learning_rate": 2.2776777054177636e-05,
      "loss": 1.8419,
      "step": 31090
    },
    {
      "epoch": 4.309269779686851,
      "grad_norm": 11.783055305480957,
      "learning_rate": 2.2771234585007624e-05,
      "loss": 2.1405,
      "step": 31100
    },
    {
      "epoch": 4.310655396979354,
      "grad_norm": 10.323782920837402,
      "learning_rate": 2.2765692115837606e-05,
      "loss": 2.3011,
      "step": 31110
    },
    {
      "epoch": 4.312041014271858,
      "grad_norm": 8.832859992980957,
      "learning_rate": 2.276014964666759e-05,
      "loss": 2.1558,
      "step": 31120
    },
    {
      "epoch": 4.3134266315643615,
      "grad_norm": 13.159253120422363,
      "learning_rate": 2.275460717749758e-05,
      "loss": 2.1902,
      "step": 31130
    },
    {
      "epoch": 4.314812248856866,
      "grad_norm": 11.477103233337402,
      "learning_rate": 2.2749064708327562e-05,
      "loss": 2.0185,
      "step": 31140
    },
    {
      "epoch": 4.31619786614937,
      "grad_norm": 8.904674530029297,
      "learning_rate": 2.2743522239157544e-05,
      "loss": 1.8995,
      "step": 31150
    },
    {
      "epoch": 4.317583483441873,
      "grad_norm": 15.073685646057129,
      "learning_rate": 2.2737979769987532e-05,
      "loss": 1.9812,
      "step": 31160
    },
    {
      "epoch": 4.318969100734377,
      "grad_norm": 17.240440368652344,
      "learning_rate": 2.2732437300817514e-05,
      "loss": 2.0122,
      "step": 31170
    },
    {
      "epoch": 4.320354718026881,
      "grad_norm": 10.275089263916016,
      "learning_rate": 2.27268948316475e-05,
      "loss": 1.8391,
      "step": 31180
    },
    {
      "epoch": 4.321740335319385,
      "grad_norm": 14.744267463684082,
      "learning_rate": 2.2721352362477488e-05,
      "loss": 2.3229,
      "step": 31190
    },
    {
      "epoch": 4.323125952611889,
      "grad_norm": 15.465892791748047,
      "learning_rate": 2.271580989330747e-05,
      "loss": 2.0521,
      "step": 31200
    },
    {
      "epoch": 4.324511569904392,
      "grad_norm": 17.26145362854004,
      "learning_rate": 2.2710267424137452e-05,
      "loss": 2.7705,
      "step": 31210
    },
    {
      "epoch": 4.3258971871968965,
      "grad_norm": 14.716856002807617,
      "learning_rate": 2.270472495496744e-05,
      "loss": 1.9934,
      "step": 31220
    },
    {
      "epoch": 4.3272828044894,
      "grad_norm": 14.293458938598633,
      "learning_rate": 2.2699182485797426e-05,
      "loss": 2.2843,
      "step": 31230
    },
    {
      "epoch": 4.328668421781904,
      "grad_norm": 10.640483856201172,
      "learning_rate": 2.2693640016627408e-05,
      "loss": 2.4487,
      "step": 31240
    },
    {
      "epoch": 4.330054039074407,
      "grad_norm": 12.825921058654785,
      "learning_rate": 2.2688097547457396e-05,
      "loss": 2.0448,
      "step": 31250
    },
    {
      "epoch": 4.3314396563669115,
      "grad_norm": 10.646023750305176,
      "learning_rate": 2.2682555078287378e-05,
      "loss": 2.2244,
      "step": 31260
    },
    {
      "epoch": 4.332825273659415,
      "grad_norm": 12.067975044250488,
      "learning_rate": 2.267701260911736e-05,
      "loss": 2.112,
      "step": 31270
    },
    {
      "epoch": 4.334210890951919,
      "grad_norm": 14.66977310180664,
      "learning_rate": 2.267147013994735e-05,
      "loss": 2.0826,
      "step": 31280
    },
    {
      "epoch": 4.335596508244423,
      "grad_norm": 11.91969108581543,
      "learning_rate": 2.2665927670777334e-05,
      "loss": 2.2578,
      "step": 31290
    },
    {
      "epoch": 4.3369821255369265,
      "grad_norm": 5.563319206237793,
      "learning_rate": 2.266038520160732e-05,
      "loss": 2.0131,
      "step": 31300
    },
    {
      "epoch": 4.338367742829431,
      "grad_norm": 28.3500919342041,
      "learning_rate": 2.2654842732437304e-05,
      "loss": 2.1843,
      "step": 31310
    },
    {
      "epoch": 4.339753360121934,
      "grad_norm": 19.037919998168945,
      "learning_rate": 2.2649300263267286e-05,
      "loss": 1.8736,
      "step": 31320
    },
    {
      "epoch": 4.341138977414438,
      "grad_norm": 10.611419677734375,
      "learning_rate": 2.2643757794097275e-05,
      "loss": 1.8679,
      "step": 31330
    },
    {
      "epoch": 4.342524594706942,
      "grad_norm": 11.671030044555664,
      "learning_rate": 2.2638215324927257e-05,
      "loss": 1.9106,
      "step": 31340
    },
    {
      "epoch": 4.343910211999446,
      "grad_norm": 17.647491455078125,
      "learning_rate": 2.2632672855757242e-05,
      "loss": 1.8426,
      "step": 31350
    },
    {
      "epoch": 4.34529582929195,
      "grad_norm": 15.033090591430664,
      "learning_rate": 2.2627130386587227e-05,
      "loss": 2.1846,
      "step": 31360
    },
    {
      "epoch": 4.346681446584453,
      "grad_norm": 15.061714172363281,
      "learning_rate": 2.2621587917417213e-05,
      "loss": 2.1623,
      "step": 31370
    },
    {
      "epoch": 4.348067063876957,
      "grad_norm": 15.248435020446777,
      "learning_rate": 2.2616045448247194e-05,
      "loss": 2.1423,
      "step": 31380
    },
    {
      "epoch": 4.349452681169461,
      "grad_norm": 16.797569274902344,
      "learning_rate": 2.2610502979077183e-05,
      "loss": 2.2113,
      "step": 31390
    },
    {
      "epoch": 4.350838298461965,
      "grad_norm": 8.334651947021484,
      "learning_rate": 2.2604960509907165e-05,
      "loss": 2.0616,
      "step": 31400
    },
    {
      "epoch": 4.352223915754468,
      "grad_norm": 9.879894256591797,
      "learning_rate": 2.259941804073715e-05,
      "loss": 2.1747,
      "step": 31410
    },
    {
      "epoch": 4.353609533046972,
      "grad_norm": 9.335758209228516,
      "learning_rate": 2.2593875571567135e-05,
      "loss": 2.2514,
      "step": 31420
    },
    {
      "epoch": 4.354995150339477,
      "grad_norm": 15.17873477935791,
      "learning_rate": 2.258833310239712e-05,
      "loss": 2.0866,
      "step": 31430
    },
    {
      "epoch": 4.35638076763198,
      "grad_norm": 9.628748893737793,
      "learning_rate": 2.2582790633227102e-05,
      "loss": 2.12,
      "step": 31440
    },
    {
      "epoch": 4.357766384924484,
      "grad_norm": 7.384475231170654,
      "learning_rate": 2.257724816405709e-05,
      "loss": 2.1971,
      "step": 31450
    },
    {
      "epoch": 4.359152002216987,
      "grad_norm": 13.637336730957031,
      "learning_rate": 2.2571705694887073e-05,
      "loss": 2.2054,
      "step": 31460
    },
    {
      "epoch": 4.360537619509492,
      "grad_norm": 9.24127197265625,
      "learning_rate": 2.2566163225717058e-05,
      "loss": 2.0141,
      "step": 31470
    },
    {
      "epoch": 4.361923236801995,
      "grad_norm": 16.173566818237305,
      "learning_rate": 2.2560620756547043e-05,
      "loss": 1.762,
      "step": 31480
    },
    {
      "epoch": 4.363308854094499,
      "grad_norm": 6.6682305335998535,
      "learning_rate": 2.255507828737703e-05,
      "loss": 1.9344,
      "step": 31490
    },
    {
      "epoch": 4.364694471387003,
      "grad_norm": 8.220394134521484,
      "learning_rate": 2.254953581820701e-05,
      "loss": 1.8558,
      "step": 31500
    },
    {
      "epoch": 4.366080088679507,
      "grad_norm": 7.613106727600098,
      "learning_rate": 2.2543993349037e-05,
      "loss": 2.0889,
      "step": 31510
    },
    {
      "epoch": 4.367465705972011,
      "grad_norm": 12.936119079589844,
      "learning_rate": 2.253845087986698e-05,
      "loss": 1.8691,
      "step": 31520
    },
    {
      "epoch": 4.368851323264514,
      "grad_norm": 14.915345191955566,
      "learning_rate": 2.2532908410696966e-05,
      "loss": 1.9999,
      "step": 31530
    },
    {
      "epoch": 4.370236940557018,
      "grad_norm": 9.941875457763672,
      "learning_rate": 2.252736594152695e-05,
      "loss": 2.1225,
      "step": 31540
    },
    {
      "epoch": 4.371622557849522,
      "grad_norm": 18.30054473876953,
      "learning_rate": 2.2521823472356937e-05,
      "loss": 2.1949,
      "step": 31550
    },
    {
      "epoch": 4.373008175142026,
      "grad_norm": 7.4720940589904785,
      "learning_rate": 2.251628100318692e-05,
      "loss": 1.9644,
      "step": 31560
    },
    {
      "epoch": 4.37439379243453,
      "grad_norm": 12.63796329498291,
      "learning_rate": 2.2510738534016907e-05,
      "loss": 2.573,
      "step": 31570
    },
    {
      "epoch": 4.375779409727033,
      "grad_norm": 10.29544448852539,
      "learning_rate": 2.250519606484689e-05,
      "loss": 2.3214,
      "step": 31580
    },
    {
      "epoch": 4.3771650270195375,
      "grad_norm": 12.117264747619629,
      "learning_rate": 2.2499653595676878e-05,
      "loss": 2.6334,
      "step": 31590
    },
    {
      "epoch": 4.378550644312041,
      "grad_norm": 14.747936248779297,
      "learning_rate": 2.249411112650686e-05,
      "loss": 2.2059,
      "step": 31600
    },
    {
      "epoch": 4.379936261604545,
      "grad_norm": 11.202887535095215,
      "learning_rate": 2.2488568657336845e-05,
      "loss": 2.1229,
      "step": 31610
    },
    {
      "epoch": 4.381321878897048,
      "grad_norm": 13.020899772644043,
      "learning_rate": 2.2483026188166834e-05,
      "loss": 2.099,
      "step": 31620
    },
    {
      "epoch": 4.3827074961895525,
      "grad_norm": 11.407818794250488,
      "learning_rate": 2.2477483718996815e-05,
      "loss": 2.0467,
      "step": 31630
    },
    {
      "epoch": 4.384093113482057,
      "grad_norm": 14.741459846496582,
      "learning_rate": 2.2471941249826797e-05,
      "loss": 2.355,
      "step": 31640
    },
    {
      "epoch": 4.38547873077456,
      "grad_norm": 15.04489517211914,
      "learning_rate": 2.2466398780656786e-05,
      "loss": 1.8162,
      "step": 31650
    },
    {
      "epoch": 4.386864348067064,
      "grad_norm": 12.28560733795166,
      "learning_rate": 2.2460856311486768e-05,
      "loss": 2.3211,
      "step": 31660
    },
    {
      "epoch": 4.3882499653595675,
      "grad_norm": 8.46701431274414,
      "learning_rate": 2.2455313842316753e-05,
      "loss": 1.782,
      "step": 31670
    },
    {
      "epoch": 4.389635582652072,
      "grad_norm": 10.71056079864502,
      "learning_rate": 2.244977137314674e-05,
      "loss": 1.9881,
      "step": 31680
    },
    {
      "epoch": 4.391021199944575,
      "grad_norm": 18.07770347595215,
      "learning_rate": 2.2444228903976724e-05,
      "loss": 1.8389,
      "step": 31690
    },
    {
      "epoch": 4.392406817237079,
      "grad_norm": 12.908317565917969,
      "learning_rate": 2.2438686434806705e-05,
      "loss": 2.1929,
      "step": 31700
    },
    {
      "epoch": 4.393792434529583,
      "grad_norm": 8.740851402282715,
      "learning_rate": 2.2433143965636694e-05,
      "loss": 1.6847,
      "step": 31710
    },
    {
      "epoch": 4.395178051822087,
      "grad_norm": 9.39126968383789,
      "learning_rate": 2.242760149646668e-05,
      "loss": 2.2532,
      "step": 31720
    },
    {
      "epoch": 4.396563669114591,
      "grad_norm": 8.646984100341797,
      "learning_rate": 2.242205902729666e-05,
      "loss": 1.9546,
      "step": 31730
    },
    {
      "epoch": 4.397949286407094,
      "grad_norm": 11.857050895690918,
      "learning_rate": 2.241651655812665e-05,
      "loss": 1.7184,
      "step": 31740
    },
    {
      "epoch": 4.399334903699598,
      "grad_norm": 15.729316711425781,
      "learning_rate": 2.241097408895663e-05,
      "loss": 2.2087,
      "step": 31750
    },
    {
      "epoch": 4.400720520992102,
      "grad_norm": 10.66609001159668,
      "learning_rate": 2.2405431619786614e-05,
      "loss": 2.1918,
      "step": 31760
    },
    {
      "epoch": 4.402106138284606,
      "grad_norm": 10.200128555297852,
      "learning_rate": 2.2399889150616602e-05,
      "loss": 1.5153,
      "step": 31770
    },
    {
      "epoch": 4.403491755577109,
      "grad_norm": 15.608977317810059,
      "learning_rate": 2.2394346681446587e-05,
      "loss": 2.1508,
      "step": 31780
    },
    {
      "epoch": 4.404877372869613,
      "grad_norm": 10.89980697631836,
      "learning_rate": 2.238880421227657e-05,
      "loss": 1.8311,
      "step": 31790
    },
    {
      "epoch": 4.4062629901621175,
      "grad_norm": 21.93440818786621,
      "learning_rate": 2.2383261743106558e-05,
      "loss": 1.9414,
      "step": 31800
    },
    {
      "epoch": 4.407648607454621,
      "grad_norm": 10.421937942504883,
      "learning_rate": 2.237771927393654e-05,
      "loss": 1.9865,
      "step": 31810
    },
    {
      "epoch": 4.409034224747125,
      "grad_norm": 11.729233741760254,
      "learning_rate": 2.2372176804766525e-05,
      "loss": 2.2526,
      "step": 31820
    },
    {
      "epoch": 4.410419842039628,
      "grad_norm": 5.2867536544799805,
      "learning_rate": 2.236663433559651e-05,
      "loss": 2.2635,
      "step": 31830
    },
    {
      "epoch": 4.4118054593321325,
      "grad_norm": 15.355928421020508,
      "learning_rate": 2.2361091866426496e-05,
      "loss": 1.8136,
      "step": 31840
    },
    {
      "epoch": 4.413191076624637,
      "grad_norm": 10.532384872436523,
      "learning_rate": 2.2355549397256477e-05,
      "loss": 1.9646,
      "step": 31850
    },
    {
      "epoch": 4.41457669391714,
      "grad_norm": 15.8728666305542,
      "learning_rate": 2.2350006928086466e-05,
      "loss": 2.1215,
      "step": 31860
    },
    {
      "epoch": 4.415962311209644,
      "grad_norm": 7.198375701904297,
      "learning_rate": 2.234501870583345e-05,
      "loss": 1.788,
      "step": 31870
    },
    {
      "epoch": 4.4173479285021475,
      "grad_norm": 11.82763385772705,
      "learning_rate": 2.2339476236663434e-05,
      "loss": 2.1207,
      "step": 31880
    },
    {
      "epoch": 4.418733545794652,
      "grad_norm": 19.600818634033203,
      "learning_rate": 2.233393376749342e-05,
      "loss": 2.3435,
      "step": 31890
    },
    {
      "epoch": 4.420119163087155,
      "grad_norm": 13.569859504699707,
      "learning_rate": 2.2328391298323405e-05,
      "loss": 2.2156,
      "step": 31900
    },
    {
      "epoch": 4.421504780379659,
      "grad_norm": 8.830362319946289,
      "learning_rate": 2.2322848829153393e-05,
      "loss": 2.2108,
      "step": 31910
    },
    {
      "epoch": 4.4228903976721625,
      "grad_norm": 13.735158920288086,
      "learning_rate": 2.2317306359983375e-05,
      "loss": 1.903,
      "step": 31920
    },
    {
      "epoch": 4.424276014964667,
      "grad_norm": 13.570413589477539,
      "learning_rate": 2.2311763890813357e-05,
      "loss": 1.9539,
      "step": 31930
    },
    {
      "epoch": 4.425661632257171,
      "grad_norm": 10.36244010925293,
      "learning_rate": 2.2306221421643346e-05,
      "loss": 2.1125,
      "step": 31940
    },
    {
      "epoch": 4.427047249549674,
      "grad_norm": 9.56784725189209,
      "learning_rate": 2.2300678952473328e-05,
      "loss": 1.9438,
      "step": 31950
    },
    {
      "epoch": 4.428432866842178,
      "grad_norm": 11.488984107971191,
      "learning_rate": 2.2295136483303313e-05,
      "loss": 1.5603,
      "step": 31960
    },
    {
      "epoch": 4.429818484134682,
      "grad_norm": 10.067676544189453,
      "learning_rate": 2.22895940141333e-05,
      "loss": 2.0656,
      "step": 31970
    },
    {
      "epoch": 4.431204101427186,
      "grad_norm": 17.23484992980957,
      "learning_rate": 2.2284051544963283e-05,
      "loss": 2.3223,
      "step": 31980
    },
    {
      "epoch": 4.432589718719689,
      "grad_norm": 16.116939544677734,
      "learning_rate": 2.2278509075793265e-05,
      "loss": 1.9152,
      "step": 31990
    },
    {
      "epoch": 4.433975336012193,
      "grad_norm": 10.396890640258789,
      "learning_rate": 2.2272966606623254e-05,
      "loss": 2.398,
      "step": 32000
    },
    {
      "epoch": 4.435360953304698,
      "grad_norm": 18.052757263183594,
      "learning_rate": 2.2267424137453236e-05,
      "loss": 2.1197,
      "step": 32010
    },
    {
      "epoch": 4.436746570597201,
      "grad_norm": 7.528366565704346,
      "learning_rate": 2.226188166828322e-05,
      "loss": 2.3514,
      "step": 32020
    },
    {
      "epoch": 4.438132187889705,
      "grad_norm": 8.073514938354492,
      "learning_rate": 2.225633919911321e-05,
      "loss": 1.7355,
      "step": 32030
    },
    {
      "epoch": 4.439517805182208,
      "grad_norm": 8.854964256286621,
      "learning_rate": 2.225079672994319e-05,
      "loss": 2.3192,
      "step": 32040
    },
    {
      "epoch": 4.440903422474713,
      "grad_norm": 12.63347053527832,
      "learning_rate": 2.2245254260773173e-05,
      "loss": 2.0865,
      "step": 32050
    },
    {
      "epoch": 4.442289039767216,
      "grad_norm": 7.33467435836792,
      "learning_rate": 2.2239711791603162e-05,
      "loss": 1.8483,
      "step": 32060
    },
    {
      "epoch": 4.44367465705972,
      "grad_norm": 8.847786903381348,
      "learning_rate": 2.2234169322433147e-05,
      "loss": 1.8906,
      "step": 32070
    },
    {
      "epoch": 4.445060274352224,
      "grad_norm": 12.004493713378906,
      "learning_rate": 2.222862685326313e-05,
      "loss": 2.172,
      "step": 32080
    },
    {
      "epoch": 4.446445891644728,
      "grad_norm": 13.626193046569824,
      "learning_rate": 2.2223084384093118e-05,
      "loss": 2.0256,
      "step": 32090
    },
    {
      "epoch": 4.447831508937232,
      "grad_norm": 11.395079612731934,
      "learning_rate": 2.22175419149231e-05,
      "loss": 2.0422,
      "step": 32100
    },
    {
      "epoch": 4.449217126229735,
      "grad_norm": 10.725042343139648,
      "learning_rate": 2.221199944575308e-05,
      "loss": 2.4476,
      "step": 32110
    },
    {
      "epoch": 4.450602743522239,
      "grad_norm": 13.633959770202637,
      "learning_rate": 2.220645697658307e-05,
      "loss": 1.9427,
      "step": 32120
    },
    {
      "epoch": 4.451988360814743,
      "grad_norm": 10.922584533691406,
      "learning_rate": 2.2200914507413055e-05,
      "loss": 2.1941,
      "step": 32130
    },
    {
      "epoch": 4.453373978107247,
      "grad_norm": 6.789024353027344,
      "learning_rate": 2.2195372038243037e-05,
      "loss": 2.1133,
      "step": 32140
    },
    {
      "epoch": 4.454759595399751,
      "grad_norm": 6.634787559509277,
      "learning_rate": 2.2189829569073026e-05,
      "loss": 2.2191,
      "step": 32150
    },
    {
      "epoch": 4.456145212692254,
      "grad_norm": 17.611230850219727,
      "learning_rate": 2.2184287099903008e-05,
      "loss": 1.9137,
      "step": 32160
    },
    {
      "epoch": 4.4575308299847585,
      "grad_norm": 20.88373374938965,
      "learning_rate": 2.217874463073299e-05,
      "loss": 2.0325,
      "step": 32170
    },
    {
      "epoch": 4.458916447277262,
      "grad_norm": 12.567947387695312,
      "learning_rate": 2.2173202161562978e-05,
      "loss": 2.169,
      "step": 32180
    },
    {
      "epoch": 4.460302064569766,
      "grad_norm": 14.802544593811035,
      "learning_rate": 2.2167659692392963e-05,
      "loss": 2.1413,
      "step": 32190
    },
    {
      "epoch": 4.461687681862269,
      "grad_norm": 11.266236305236816,
      "learning_rate": 2.216211722322295e-05,
      "loss": 2.2796,
      "step": 32200
    },
    {
      "epoch": 4.4630732991547735,
      "grad_norm": 7.7852277755737305,
      "learning_rate": 2.2156574754052934e-05,
      "loss": 1.975,
      "step": 32210
    },
    {
      "epoch": 4.464458916447278,
      "grad_norm": 10.080381393432617,
      "learning_rate": 2.2151032284882916e-05,
      "loss": 2.0243,
      "step": 32220
    },
    {
      "epoch": 4.465844533739781,
      "grad_norm": 12.753832817077637,
      "learning_rate": 2.2145489815712904e-05,
      "loss": 1.9339,
      "step": 32230
    },
    {
      "epoch": 4.467230151032285,
      "grad_norm": 11.702648162841797,
      "learning_rate": 2.2139947346542886e-05,
      "loss": 2.7629,
      "step": 32240
    },
    {
      "epoch": 4.4686157683247885,
      "grad_norm": 12.452016830444336,
      "learning_rate": 2.213440487737287e-05,
      "loss": 2.0163,
      "step": 32250
    },
    {
      "epoch": 4.470001385617293,
      "grad_norm": 8.238471984863281,
      "learning_rate": 2.2128862408202857e-05,
      "loss": 1.7251,
      "step": 32260
    },
    {
      "epoch": 4.471387002909796,
      "grad_norm": 11.5634183883667,
      "learning_rate": 2.2123319939032842e-05,
      "loss": 1.9587,
      "step": 32270
    },
    {
      "epoch": 4.4727726202023,
      "grad_norm": 11.490621566772461,
      "learning_rate": 2.2117777469862824e-05,
      "loss": 1.3357,
      "step": 32280
    },
    {
      "epoch": 4.474158237494804,
      "grad_norm": 12.080445289611816,
      "learning_rate": 2.2112235000692813e-05,
      "loss": 2.1237,
      "step": 32290
    },
    {
      "epoch": 4.475543854787308,
      "grad_norm": 13.741223335266113,
      "learning_rate": 2.2106692531522794e-05,
      "loss": 2.4538,
      "step": 32300
    },
    {
      "epoch": 4.476929472079812,
      "grad_norm": 17.787424087524414,
      "learning_rate": 2.210115006235278e-05,
      "loss": 1.7652,
      "step": 32310
    },
    {
      "epoch": 4.478315089372315,
      "grad_norm": 12.653251647949219,
      "learning_rate": 2.2095607593182765e-05,
      "loss": 1.7605,
      "step": 32320
    },
    {
      "epoch": 4.479700706664819,
      "grad_norm": 7.846608638763428,
      "learning_rate": 2.209006512401275e-05,
      "loss": 1.7421,
      "step": 32330
    },
    {
      "epoch": 4.481086323957323,
      "grad_norm": 10.681755065917969,
      "learning_rate": 2.2084522654842732e-05,
      "loss": 2.1723,
      "step": 32340
    },
    {
      "epoch": 4.482471941249827,
      "grad_norm": 19.69915199279785,
      "learning_rate": 2.207898018567272e-05,
      "loss": 2.192,
      "step": 32350
    },
    {
      "epoch": 4.483857558542331,
      "grad_norm": 10.403703689575195,
      "learning_rate": 2.2073437716502702e-05,
      "loss": 1.9084,
      "step": 32360
    },
    {
      "epoch": 4.485243175834834,
      "grad_norm": 12.175708770751953,
      "learning_rate": 2.2067895247332688e-05,
      "loss": 2.1773,
      "step": 32370
    },
    {
      "epoch": 4.4866287931273385,
      "grad_norm": 8.313546180725098,
      "learning_rate": 2.2062352778162673e-05,
      "loss": 2.1505,
      "step": 32380
    },
    {
      "epoch": 4.488014410419842,
      "grad_norm": 15.605555534362793,
      "learning_rate": 2.2056810308992658e-05,
      "loss": 2.1424,
      "step": 32390
    },
    {
      "epoch": 4.489400027712346,
      "grad_norm": 16.409677505493164,
      "learning_rate": 2.205126783982264e-05,
      "loss": 1.8532,
      "step": 32400
    },
    {
      "epoch": 4.490785645004849,
      "grad_norm": 9.30517578125,
      "learning_rate": 2.204572537065263e-05,
      "loss": 2.4398,
      "step": 32410
    },
    {
      "epoch": 4.4921712622973535,
      "grad_norm": 14.66088581085205,
      "learning_rate": 2.204018290148261e-05,
      "loss": 1.8956,
      "step": 32420
    },
    {
      "epoch": 4.493556879589857,
      "grad_norm": 8.631490707397461,
      "learning_rate": 2.2034640432312596e-05,
      "loss": 2.3031,
      "step": 32430
    },
    {
      "epoch": 4.494942496882361,
      "grad_norm": 22.330711364746094,
      "learning_rate": 2.202909796314258e-05,
      "loss": 2.3688,
      "step": 32440
    },
    {
      "epoch": 4.496328114174865,
      "grad_norm": 21.278905868530273,
      "learning_rate": 2.2023555493972566e-05,
      "loss": 2.1066,
      "step": 32450
    },
    {
      "epoch": 4.4977137314673685,
      "grad_norm": 20.714645385742188,
      "learning_rate": 2.2018013024802548e-05,
      "loss": 2.444,
      "step": 32460
    },
    {
      "epoch": 4.499099348759873,
      "grad_norm": 14.818492889404297,
      "learning_rate": 2.2012470555632537e-05,
      "loss": 2.3223,
      "step": 32470
    },
    {
      "epoch": 4.500484966052376,
      "grad_norm": 10.65028190612793,
      "learning_rate": 2.200692808646252e-05,
      "loss": 2.467,
      "step": 32480
    },
    {
      "epoch": 4.50187058334488,
      "grad_norm": 12.27593994140625,
      "learning_rate": 2.2001385617292507e-05,
      "loss": 2.0128,
      "step": 32490
    },
    {
      "epoch": 4.503256200637384,
      "grad_norm": 10.31235408782959,
      "learning_rate": 2.1995843148122493e-05,
      "loss": 1.8819,
      "step": 32500
    },
    {
      "epoch": 4.504641817929888,
      "grad_norm": 13.72257137298584,
      "learning_rate": 2.1990300678952474e-05,
      "loss": 2.4659,
      "step": 32510
    },
    {
      "epoch": 4.506027435222392,
      "grad_norm": 14.229595184326172,
      "learning_rate": 2.1984758209782463e-05,
      "loss": 1.9137,
      "step": 32520
    },
    {
      "epoch": 4.507413052514895,
      "grad_norm": 10.314635276794434,
      "learning_rate": 2.1979215740612445e-05,
      "loss": 1.9642,
      "step": 32530
    },
    {
      "epoch": 4.508798669807399,
      "grad_norm": 22.808815002441406,
      "learning_rate": 2.1973673271442427e-05,
      "loss": 2.3862,
      "step": 32540
    },
    {
      "epoch": 4.510184287099903,
      "grad_norm": 8.509745597839355,
      "learning_rate": 2.1968130802272415e-05,
      "loss": 2.1375,
      "step": 32550
    },
    {
      "epoch": 4.511569904392407,
      "grad_norm": 12.105141639709473,
      "learning_rate": 2.19625883331024e-05,
      "loss": 1.7645,
      "step": 32560
    },
    {
      "epoch": 4.51295552168491,
      "grad_norm": 13.448307037353516,
      "learning_rate": 2.1957045863932383e-05,
      "loss": 1.97,
      "step": 32570
    },
    {
      "epoch": 4.514341138977414,
      "grad_norm": 12.597039222717285,
      "learning_rate": 2.195150339476237e-05,
      "loss": 1.8826,
      "step": 32580
    },
    {
      "epoch": 4.515726756269919,
      "grad_norm": 22.103748321533203,
      "learning_rate": 2.1945960925592353e-05,
      "loss": 2.5583,
      "step": 32590
    },
    {
      "epoch": 4.517112373562422,
      "grad_norm": 13.514870643615723,
      "learning_rate": 2.1940418456422335e-05,
      "loss": 2.3282,
      "step": 32600
    },
    {
      "epoch": 4.518497990854926,
      "grad_norm": 9.968995094299316,
      "learning_rate": 2.1934875987252324e-05,
      "loss": 1.9401,
      "step": 32610
    },
    {
      "epoch": 4.519883608147429,
      "grad_norm": 8.969327926635742,
      "learning_rate": 2.192933351808231e-05,
      "loss": 2.2521,
      "step": 32620
    },
    {
      "epoch": 4.521269225439934,
      "grad_norm": 12.135770797729492,
      "learning_rate": 2.192379104891229e-05,
      "loss": 1.9216,
      "step": 32630
    },
    {
      "epoch": 4.522654842732438,
      "grad_norm": 7.758633613586426,
      "learning_rate": 2.191824857974228e-05,
      "loss": 1.8923,
      "step": 32640
    },
    {
      "epoch": 4.524040460024941,
      "grad_norm": 7.947431564331055,
      "learning_rate": 2.191270611057226e-05,
      "loss": 2.2864,
      "step": 32650
    },
    {
      "epoch": 4.525426077317445,
      "grad_norm": 10.131869316101074,
      "learning_rate": 2.1907163641402246e-05,
      "loss": 1.8268,
      "step": 32660
    },
    {
      "epoch": 4.526811694609949,
      "grad_norm": 3.1513798236846924,
      "learning_rate": 2.190162117223223e-05,
      "loss": 1.5971,
      "step": 32670
    },
    {
      "epoch": 4.528197311902453,
      "grad_norm": 11.197129249572754,
      "learning_rate": 2.1896078703062217e-05,
      "loss": 2.3962,
      "step": 32680
    },
    {
      "epoch": 4.529582929194956,
      "grad_norm": 15.130987167358398,
      "learning_rate": 2.18905362338922e-05,
      "loss": 1.9752,
      "step": 32690
    },
    {
      "epoch": 4.53096854648746,
      "grad_norm": 11.535082817077637,
      "learning_rate": 2.1884993764722187e-05,
      "loss": 1.7924,
      "step": 32700
    },
    {
      "epoch": 4.532354163779964,
      "grad_norm": 18.870542526245117,
      "learning_rate": 2.187945129555217e-05,
      "loss": 2.0904,
      "step": 32710
    },
    {
      "epoch": 4.533739781072468,
      "grad_norm": 13.424302101135254,
      "learning_rate": 2.1873908826382155e-05,
      "loss": 2.0705,
      "step": 32720
    },
    {
      "epoch": 4.535125398364972,
      "grad_norm": 13.111306190490723,
      "learning_rate": 2.186836635721214e-05,
      "loss": 2.1546,
      "step": 32730
    },
    {
      "epoch": 4.536511015657475,
      "grad_norm": 7.592220783233643,
      "learning_rate": 2.1862823888042125e-05,
      "loss": 2.0159,
      "step": 32740
    },
    {
      "epoch": 4.5378966329499795,
      "grad_norm": 14.55757999420166,
      "learning_rate": 2.1857281418872107e-05,
      "loss": 2.1107,
      "step": 32750
    },
    {
      "epoch": 4.539282250242483,
      "grad_norm": 14.653191566467285,
      "learning_rate": 2.1851738949702096e-05,
      "loss": 2.2814,
      "step": 32760
    },
    {
      "epoch": 4.540667867534987,
      "grad_norm": 12.412991523742676,
      "learning_rate": 2.1846196480532077e-05,
      "loss": 2.1964,
      "step": 32770
    },
    {
      "epoch": 4.54205348482749,
      "grad_norm": 13.422039985656738,
      "learning_rate": 2.1840654011362066e-05,
      "loss": 2.2466,
      "step": 32780
    },
    {
      "epoch": 4.5434391021199945,
      "grad_norm": 7.082953453063965,
      "learning_rate": 2.1835111542192048e-05,
      "loss": 2.0048,
      "step": 32790
    },
    {
      "epoch": 4.544824719412498,
      "grad_norm": 11.690427780151367,
      "learning_rate": 2.1829569073022033e-05,
      "loss": 2.1938,
      "step": 32800
    },
    {
      "epoch": 4.546210336705002,
      "grad_norm": 9.31727409362793,
      "learning_rate": 2.182402660385202e-05,
      "loss": 1.8469,
      "step": 32810
    },
    {
      "epoch": 4.547595953997506,
      "grad_norm": 14.894124984741211,
      "learning_rate": 2.1818484134682004e-05,
      "loss": 2.4195,
      "step": 32820
    },
    {
      "epoch": 4.5489815712900095,
      "grad_norm": 15.506807327270508,
      "learning_rate": 2.1812941665511985e-05,
      "loss": 1.8868,
      "step": 32830
    },
    {
      "epoch": 4.550367188582514,
      "grad_norm": 14.23214340209961,
      "learning_rate": 2.1807399196341974e-05,
      "loss": 2.4952,
      "step": 32840
    },
    {
      "epoch": 4.551752805875017,
      "grad_norm": 7.688987731933594,
      "learning_rate": 2.1801856727171956e-05,
      "loss": 2.0265,
      "step": 32850
    },
    {
      "epoch": 4.553138423167521,
      "grad_norm": 16.535207748413086,
      "learning_rate": 2.179631425800194e-05,
      "loss": 2.4373,
      "step": 32860
    },
    {
      "epoch": 4.554524040460025,
      "grad_norm": 17.70904541015625,
      "learning_rate": 2.1790771788831926e-05,
      "loss": 2.4889,
      "step": 32870
    },
    {
      "epoch": 4.555909657752529,
      "grad_norm": 14.928869247436523,
      "learning_rate": 2.1785229319661912e-05,
      "loss": 1.7955,
      "step": 32880
    },
    {
      "epoch": 4.557295275045033,
      "grad_norm": 8.78774356842041,
      "learning_rate": 2.1779686850491894e-05,
      "loss": 2.0346,
      "step": 32890
    },
    {
      "epoch": 4.558680892337536,
      "grad_norm": 5.846127033233643,
      "learning_rate": 2.1774144381321882e-05,
      "loss": 2.1916,
      "step": 32900
    },
    {
      "epoch": 4.56006650963004,
      "grad_norm": 16.48427963256836,
      "learning_rate": 2.1768601912151864e-05,
      "loss": 1.8713,
      "step": 32910
    },
    {
      "epoch": 4.561452126922544,
      "grad_norm": 10.191493034362793,
      "learning_rate": 2.176305944298185e-05,
      "loss": 2.1203,
      "step": 32920
    },
    {
      "epoch": 4.562837744215048,
      "grad_norm": 8.40060043334961,
      "learning_rate": 2.1757516973811838e-05,
      "loss": 1.8474,
      "step": 32930
    },
    {
      "epoch": 4.564223361507551,
      "grad_norm": 8.333378791809082,
      "learning_rate": 2.175197450464182e-05,
      "loss": 1.7693,
      "step": 32940
    },
    {
      "epoch": 4.565608978800055,
      "grad_norm": 17.166902542114258,
      "learning_rate": 2.1746432035471802e-05,
      "loss": 2.4196,
      "step": 32950
    },
    {
      "epoch": 4.5669945960925595,
      "grad_norm": 11.831826210021973,
      "learning_rate": 2.174088956630179e-05,
      "loss": 2.0292,
      "step": 32960
    },
    {
      "epoch": 4.568380213385063,
      "grad_norm": 17.028526306152344,
      "learning_rate": 2.1735347097131772e-05,
      "loss": 1.826,
      "step": 32970
    },
    {
      "epoch": 4.569765830677567,
      "grad_norm": 10.963586807250977,
      "learning_rate": 2.1729804627961757e-05,
      "loss": 2.6802,
      "step": 32980
    },
    {
      "epoch": 4.57115144797007,
      "grad_norm": 13.493632316589355,
      "learning_rate": 2.1724262158791746e-05,
      "loss": 2.3422,
      "step": 32990
    },
    {
      "epoch": 4.5725370652625745,
      "grad_norm": 10.359260559082031,
      "learning_rate": 2.1718719689621728e-05,
      "loss": 2.5528,
      "step": 33000
    },
    {
      "epoch": 4.573922682555079,
      "grad_norm": 28.869949340820312,
      "learning_rate": 2.171317722045171e-05,
      "loss": 2.2766,
      "step": 33010
    },
    {
      "epoch": 4.575308299847582,
      "grad_norm": 10.83850383758545,
      "learning_rate": 2.17076347512817e-05,
      "loss": 2.3054,
      "step": 33020
    },
    {
      "epoch": 4.576693917140086,
      "grad_norm": 10.995728492736816,
      "learning_rate": 2.170209228211168e-05,
      "loss": 2.0697,
      "step": 33030
    },
    {
      "epoch": 4.5780795344325895,
      "grad_norm": 11.482871055603027,
      "learning_rate": 2.1696549812941666e-05,
      "loss": 1.9375,
      "step": 33040
    },
    {
      "epoch": 4.579465151725094,
      "grad_norm": 20.908205032348633,
      "learning_rate": 2.1691007343771654e-05,
      "loss": 1.8648,
      "step": 33050
    },
    {
      "epoch": 4.580850769017597,
      "grad_norm": 13.875654220581055,
      "learning_rate": 2.1685464874601636e-05,
      "loss": 2.2532,
      "step": 33060
    },
    {
      "epoch": 4.582236386310101,
      "grad_norm": 10.425880432128906,
      "learning_rate": 2.1679922405431625e-05,
      "loss": 2.2471,
      "step": 33070
    },
    {
      "epoch": 4.5836220036026045,
      "grad_norm": 12.66544246673584,
      "learning_rate": 2.1674379936261607e-05,
      "loss": 2.1708,
      "step": 33080
    },
    {
      "epoch": 4.585007620895109,
      "grad_norm": 13.270036697387695,
      "learning_rate": 2.1668837467091592e-05,
      "loss": 2.0309,
      "step": 33090
    },
    {
      "epoch": 4.586393238187613,
      "grad_norm": 11.799209594726562,
      "learning_rate": 2.1663294997921577e-05,
      "loss": 1.8066,
      "step": 33100
    },
    {
      "epoch": 4.587778855480116,
      "grad_norm": 6.006275653839111,
      "learning_rate": 2.1657752528751562e-05,
      "loss": 2.2918,
      "step": 33110
    },
    {
      "epoch": 4.58916447277262,
      "grad_norm": 6.224569320678711,
      "learning_rate": 2.1652210059581544e-05,
      "loss": 2.2123,
      "step": 33120
    },
    {
      "epoch": 4.590550090065124,
      "grad_norm": 12.229373931884766,
      "learning_rate": 2.1646667590411533e-05,
      "loss": 2.0392,
      "step": 33130
    },
    {
      "epoch": 4.591935707357628,
      "grad_norm": 15.609847068786621,
      "learning_rate": 2.1641125121241515e-05,
      "loss": 2.0743,
      "step": 33140
    },
    {
      "epoch": 4.593321324650132,
      "grad_norm": 20.929780960083008,
      "learning_rate": 2.16355826520715e-05,
      "loss": 1.9032,
      "step": 33150
    },
    {
      "epoch": 4.594706941942635,
      "grad_norm": 14.0275239944458,
      "learning_rate": 2.1630040182901485e-05,
      "loss": 1.9299,
      "step": 33160
    },
    {
      "epoch": 4.59609255923514,
      "grad_norm": 22.061752319335938,
      "learning_rate": 2.162449771373147e-05,
      "loss": 2.327,
      "step": 33170
    },
    {
      "epoch": 4.597478176527643,
      "grad_norm": 11.734825134277344,
      "learning_rate": 2.1618955244561452e-05,
      "loss": 2.3779,
      "step": 33180
    },
    {
      "epoch": 4.598863793820147,
      "grad_norm": 15.037692070007324,
      "learning_rate": 2.161341277539144e-05,
      "loss": 2.4382,
      "step": 33190
    },
    {
      "epoch": 4.60024941111265,
      "grad_norm": 13.56613540649414,
      "learning_rate": 2.1607870306221423e-05,
      "loss": 1.6969,
      "step": 33200
    },
    {
      "epoch": 4.601635028405155,
      "grad_norm": 8.563163757324219,
      "learning_rate": 2.1602327837051408e-05,
      "loss": 2.0619,
      "step": 33210
    },
    {
      "epoch": 4.603020645697658,
      "grad_norm": 15.650856018066406,
      "learning_rate": 2.1596785367881393e-05,
      "loss": 2.526,
      "step": 33220
    },
    {
      "epoch": 4.604406262990162,
      "grad_norm": 7.138677597045898,
      "learning_rate": 2.159124289871138e-05,
      "loss": 1.7433,
      "step": 33230
    },
    {
      "epoch": 4.605791880282666,
      "grad_norm": 12.6227445602417,
      "learning_rate": 2.158570042954136e-05,
      "loss": 2.2239,
      "step": 33240
    },
    {
      "epoch": 4.60717749757517,
      "grad_norm": 11.61308765411377,
      "learning_rate": 2.158015796037135e-05,
      "loss": 1.7105,
      "step": 33250
    },
    {
      "epoch": 4.608563114867674,
      "grad_norm": 9.945353507995605,
      "learning_rate": 2.157461549120133e-05,
      "loss": 2.1365,
      "step": 33260
    },
    {
      "epoch": 4.609948732160177,
      "grad_norm": 11.974400520324707,
      "learning_rate": 2.1569073022031316e-05,
      "loss": 2.1754,
      "step": 33270
    },
    {
      "epoch": 4.611334349452681,
      "grad_norm": 11.30422306060791,
      "learning_rate": 2.15635305528613e-05,
      "loss": 1.6947,
      "step": 33280
    },
    {
      "epoch": 4.6127199667451855,
      "grad_norm": 8.784456253051758,
      "learning_rate": 2.1557988083691287e-05,
      "loss": 2.3366,
      "step": 33290
    },
    {
      "epoch": 4.614105584037689,
      "grad_norm": 4.867526054382324,
      "learning_rate": 2.155244561452127e-05,
      "loss": 1.9907,
      "step": 33300
    },
    {
      "epoch": 4.615491201330193,
      "grad_norm": 11.791803359985352,
      "learning_rate": 2.1546903145351257e-05,
      "loss": 1.8268,
      "step": 33310
    },
    {
      "epoch": 4.616876818622696,
      "grad_norm": 9.670494079589844,
      "learning_rate": 2.154136067618124e-05,
      "loss": 2.3786,
      "step": 33320
    },
    {
      "epoch": 4.6182624359152005,
      "grad_norm": 7.406877040863037,
      "learning_rate": 2.1535818207011224e-05,
      "loss": 2.2129,
      "step": 33330
    },
    {
      "epoch": 4.619648053207704,
      "grad_norm": 9.34911823272705,
      "learning_rate": 2.153027573784121e-05,
      "loss": 2.3444,
      "step": 33340
    },
    {
      "epoch": 4.621033670500208,
      "grad_norm": 12.755675315856934,
      "learning_rate": 2.1524733268671195e-05,
      "loss": 2.0316,
      "step": 33350
    },
    {
      "epoch": 4.622419287792711,
      "grad_norm": 11.724194526672363,
      "learning_rate": 2.1519190799501183e-05,
      "loss": 2.2353,
      "step": 33360
    },
    {
      "epoch": 4.6238049050852155,
      "grad_norm": 12.899568557739258,
      "learning_rate": 2.1513648330331165e-05,
      "loss": 2.3293,
      "step": 33370
    },
    {
      "epoch": 4.62519052237772,
      "grad_norm": 17.692630767822266,
      "learning_rate": 2.1508105861161147e-05,
      "loss": 2.1642,
      "step": 33380
    },
    {
      "epoch": 4.626576139670223,
      "grad_norm": 6.389327049255371,
      "learning_rate": 2.1502563391991136e-05,
      "loss": 2.077,
      "step": 33390
    },
    {
      "epoch": 4.627961756962727,
      "grad_norm": 11.283197402954102,
      "learning_rate": 2.1497020922821118e-05,
      "loss": 2.0476,
      "step": 33400
    },
    {
      "epoch": 4.6293473742552305,
      "grad_norm": 8.475202560424805,
      "learning_rate": 2.1491478453651103e-05,
      "loss": 1.9266,
      "step": 33410
    },
    {
      "epoch": 4.630732991547735,
      "grad_norm": 20.249588012695312,
      "learning_rate": 2.148593598448109e-05,
      "loss": 2.0582,
      "step": 33420
    },
    {
      "epoch": 4.632118608840238,
      "grad_norm": 14.17760181427002,
      "learning_rate": 2.1480393515311073e-05,
      "loss": 1.9507,
      "step": 33430
    },
    {
      "epoch": 4.633504226132742,
      "grad_norm": 11.547796249389648,
      "learning_rate": 2.1474851046141055e-05,
      "loss": 2.2146,
      "step": 33440
    },
    {
      "epoch": 4.6348898434252455,
      "grad_norm": 8.64628791809082,
      "learning_rate": 2.1469308576971044e-05,
      "loss": 1.865,
      "step": 33450
    },
    {
      "epoch": 4.63627546071775,
      "grad_norm": 16.109668731689453,
      "learning_rate": 2.1463766107801026e-05,
      "loss": 1.8686,
      "step": 33460
    },
    {
      "epoch": 4.637661078010254,
      "grad_norm": 16.06528091430664,
      "learning_rate": 2.145822363863101e-05,
      "loss": 2.0935,
      "step": 33470
    },
    {
      "epoch": 4.639046695302757,
      "grad_norm": 14.30421257019043,
      "learning_rate": 2.1452681169461e-05,
      "loss": 1.8557,
      "step": 33480
    },
    {
      "epoch": 4.640432312595261,
      "grad_norm": 16.140913009643555,
      "learning_rate": 2.144713870029098e-05,
      "loss": 1.8429,
      "step": 33490
    },
    {
      "epoch": 4.641817929887765,
      "grad_norm": 16.223695755004883,
      "learning_rate": 2.1441596231120963e-05,
      "loss": 2.34,
      "step": 33500
    },
    {
      "epoch": 4.643203547180269,
      "grad_norm": 9.815791130065918,
      "learning_rate": 2.1436053761950952e-05,
      "loss": 2.1467,
      "step": 33510
    },
    {
      "epoch": 4.644589164472773,
      "grad_norm": 15.057657241821289,
      "learning_rate": 2.1430511292780937e-05,
      "loss": 2.0859,
      "step": 33520
    },
    {
      "epoch": 4.645974781765276,
      "grad_norm": 13.970686912536621,
      "learning_rate": 2.142496882361092e-05,
      "loss": 2.4294,
      "step": 33530
    },
    {
      "epoch": 4.6473603990577805,
      "grad_norm": 9.286369323730469,
      "learning_rate": 2.1419426354440908e-05,
      "loss": 2.149,
      "step": 33540
    },
    {
      "epoch": 4.648746016350284,
      "grad_norm": 10.321779251098633,
      "learning_rate": 2.141388388527089e-05,
      "loss": 1.7373,
      "step": 33550
    },
    {
      "epoch": 4.650131633642788,
      "grad_norm": 14.707612037658691,
      "learning_rate": 2.140834141610087e-05,
      "loss": 1.8213,
      "step": 33560
    },
    {
      "epoch": 4.651517250935291,
      "grad_norm": 17.107173919677734,
      "learning_rate": 2.140279894693086e-05,
      "loss": 2.0736,
      "step": 33570
    },
    {
      "epoch": 4.6529028682277955,
      "grad_norm": 15.764607429504395,
      "learning_rate": 2.1397256477760845e-05,
      "loss": 2.4369,
      "step": 33580
    },
    {
      "epoch": 4.654288485520299,
      "grad_norm": 12.239543914794922,
      "learning_rate": 2.1391714008590827e-05,
      "loss": 2.2936,
      "step": 33590
    },
    {
      "epoch": 4.655674102812803,
      "grad_norm": 15.176301956176758,
      "learning_rate": 2.1386171539420816e-05,
      "loss": 2.3204,
      "step": 33600
    },
    {
      "epoch": 4.657059720105307,
      "grad_norm": 14.140771865844727,
      "learning_rate": 2.1380629070250798e-05,
      "loss": 1.9784,
      "step": 33610
    },
    {
      "epoch": 4.6584453373978105,
      "grad_norm": 16.055696487426758,
      "learning_rate": 2.137508660108078e-05,
      "loss": 2.5765,
      "step": 33620
    },
    {
      "epoch": 4.659830954690315,
      "grad_norm": 16.11564826965332,
      "learning_rate": 2.1369544131910768e-05,
      "loss": 2.4865,
      "step": 33630
    },
    {
      "epoch": 4.661216571982818,
      "grad_norm": 9.312883377075195,
      "learning_rate": 2.1364001662740753e-05,
      "loss": 1.5209,
      "step": 33640
    },
    {
      "epoch": 4.662602189275322,
      "grad_norm": 17.857784271240234,
      "learning_rate": 2.135845919357074e-05,
      "loss": 2.1196,
      "step": 33650
    },
    {
      "epoch": 4.663987806567826,
      "grad_norm": 12.131009101867676,
      "learning_rate": 2.1352916724400724e-05,
      "loss": 2.2007,
      "step": 33660
    },
    {
      "epoch": 4.66537342386033,
      "grad_norm": 17.037221908569336,
      "learning_rate": 2.1347374255230706e-05,
      "loss": 2.0912,
      "step": 33670
    },
    {
      "epoch": 4.666759041152834,
      "grad_norm": 10.37865161895752,
      "learning_rate": 2.1341831786060694e-05,
      "loss": 1.8602,
      "step": 33680
    },
    {
      "epoch": 4.668144658445337,
      "grad_norm": 9.435430526733398,
      "learning_rate": 2.1336289316890676e-05,
      "loss": 1.6838,
      "step": 33690
    },
    {
      "epoch": 4.669530275737841,
      "grad_norm": 12.456724166870117,
      "learning_rate": 2.133074684772066e-05,
      "loss": 2.2026,
      "step": 33700
    },
    {
      "epoch": 4.670915893030345,
      "grad_norm": 16.552927017211914,
      "learning_rate": 2.1325204378550647e-05,
      "loss": 2.1347,
      "step": 33710
    },
    {
      "epoch": 4.672301510322849,
      "grad_norm": 11.66711711883545,
      "learning_rate": 2.1319661909380632e-05,
      "loss": 2.133,
      "step": 33720
    },
    {
      "epoch": 4.673687127615352,
      "grad_norm": 10.380389213562012,
      "learning_rate": 2.1314119440210614e-05,
      "loss": 2.3794,
      "step": 33730
    },
    {
      "epoch": 4.675072744907856,
      "grad_norm": 14.023277282714844,
      "learning_rate": 2.1308576971040603e-05,
      "loss": 2.3731,
      "step": 33740
    },
    {
      "epoch": 4.676458362200361,
      "grad_norm": 11.091958045959473,
      "learning_rate": 2.1303034501870584e-05,
      "loss": 1.6273,
      "step": 33750
    },
    {
      "epoch": 4.677843979492864,
      "grad_norm": 13.600597381591797,
      "learning_rate": 2.129749203270057e-05,
      "loss": 2.1649,
      "step": 33760
    },
    {
      "epoch": 4.679229596785368,
      "grad_norm": 10.870930671691895,
      "learning_rate": 2.1291949563530555e-05,
      "loss": 2.2306,
      "step": 33770
    },
    {
      "epoch": 4.680615214077871,
      "grad_norm": 13.428702354431152,
      "learning_rate": 2.128640709436054e-05,
      "loss": 2.5298,
      "step": 33780
    },
    {
      "epoch": 4.682000831370376,
      "grad_norm": 9.571764945983887,
      "learning_rate": 2.1280864625190522e-05,
      "loss": 2.0478,
      "step": 33790
    },
    {
      "epoch": 4.68338644866288,
      "grad_norm": 8.129868507385254,
      "learning_rate": 2.127532215602051e-05,
      "loss": 2.1759,
      "step": 33800
    },
    {
      "epoch": 4.684772065955383,
      "grad_norm": 20.6021728515625,
      "learning_rate": 2.1269779686850492e-05,
      "loss": 2.4544,
      "step": 33810
    },
    {
      "epoch": 4.686157683247887,
      "grad_norm": 11.846911430358887,
      "learning_rate": 2.126479146459748e-05,
      "loss": 2.5129,
      "step": 33820
    },
    {
      "epoch": 4.687543300540391,
      "grad_norm": 14.948751449584961,
      "learning_rate": 2.1259248995427468e-05,
      "loss": 2.0962,
      "step": 33830
    },
    {
      "epoch": 4.688928917832895,
      "grad_norm": 13.774238586425781,
      "learning_rate": 2.125370652625745e-05,
      "loss": 1.6596,
      "step": 33840
    },
    {
      "epoch": 4.690314535125398,
      "grad_norm": 17.761198043823242,
      "learning_rate": 2.124816405708743e-05,
      "loss": 2.4151,
      "step": 33850
    },
    {
      "epoch": 4.691700152417902,
      "grad_norm": 14.361842155456543,
      "learning_rate": 2.124262158791742e-05,
      "loss": 1.7657,
      "step": 33860
    },
    {
      "epoch": 4.693085769710406,
      "grad_norm": 15.976183891296387,
      "learning_rate": 2.1237079118747405e-05,
      "loss": 2.3495,
      "step": 33870
    },
    {
      "epoch": 4.69447138700291,
      "grad_norm": 8.808178901672363,
      "learning_rate": 2.1231536649577387e-05,
      "loss": 2.1184,
      "step": 33880
    },
    {
      "epoch": 4.695857004295414,
      "grad_norm": 14.242456436157227,
      "learning_rate": 2.1225994180407376e-05,
      "loss": 2.0207,
      "step": 33890
    },
    {
      "epoch": 4.697242621587917,
      "grad_norm": 15.015752792358398,
      "learning_rate": 2.1220451711237357e-05,
      "loss": 2.2042,
      "step": 33900
    },
    {
      "epoch": 4.6986282388804215,
      "grad_norm": 17.06403350830078,
      "learning_rate": 2.121490924206734e-05,
      "loss": 2.7468,
      "step": 33910
    },
    {
      "epoch": 4.700013856172925,
      "grad_norm": 8.47815990447998,
      "learning_rate": 2.1209366772897328e-05,
      "loss": 2.068,
      "step": 33920
    },
    {
      "epoch": 4.701399473465429,
      "grad_norm": 10.823744773864746,
      "learning_rate": 2.1203824303727313e-05,
      "loss": 2.1031,
      "step": 33930
    },
    {
      "epoch": 4.702785090757932,
      "grad_norm": 15.418895721435547,
      "learning_rate": 2.1198281834557295e-05,
      "loss": 2.1204,
      "step": 33940
    },
    {
      "epoch": 4.7041707080504365,
      "grad_norm": 11.735921859741211,
      "learning_rate": 2.1192739365387284e-05,
      "loss": 2.1523,
      "step": 33950
    },
    {
      "epoch": 4.705556325342941,
      "grad_norm": 14.22346019744873,
      "learning_rate": 2.1187196896217266e-05,
      "loss": 2.3097,
      "step": 33960
    },
    {
      "epoch": 4.706941942635444,
      "grad_norm": 10.079947471618652,
      "learning_rate": 2.1181654427047254e-05,
      "loss": 1.744,
      "step": 33970
    },
    {
      "epoch": 4.708327559927948,
      "grad_norm": 10.2655668258667,
      "learning_rate": 2.1176111957877236e-05,
      "loss": 1.7167,
      "step": 33980
    },
    {
      "epoch": 4.7097131772204515,
      "grad_norm": 9.002005577087402,
      "learning_rate": 2.117056948870722e-05,
      "loss": 2.3343,
      "step": 33990
    },
    {
      "epoch": 4.711098794512956,
      "grad_norm": 17.4941463470459,
      "learning_rate": 2.1165027019537207e-05,
      "loss": 2.1877,
      "step": 34000
    },
    {
      "epoch": 4.712484411805459,
      "grad_norm": 14.011110305786133,
      "learning_rate": 2.1159484550367192e-05,
      "loss": 1.9744,
      "step": 34010
    },
    {
      "epoch": 4.713870029097963,
      "grad_norm": 10.395162582397461,
      "learning_rate": 2.1153942081197174e-05,
      "loss": 1.9206,
      "step": 34020
    },
    {
      "epoch": 4.715255646390467,
      "grad_norm": 9.656397819519043,
      "learning_rate": 2.1148399612027162e-05,
      "loss": 2.1698,
      "step": 34030
    },
    {
      "epoch": 4.716641263682971,
      "grad_norm": 20.729978561401367,
      "learning_rate": 2.1142857142857144e-05,
      "loss": 2.0229,
      "step": 34040
    },
    {
      "epoch": 4.718026880975475,
      "grad_norm": 7.565013885498047,
      "learning_rate": 2.113731467368713e-05,
      "loss": 1.8607,
      "step": 34050
    },
    {
      "epoch": 4.719412498267978,
      "grad_norm": 10.878544807434082,
      "learning_rate": 2.1131772204517115e-05,
      "loss": 1.7609,
      "step": 34060
    },
    {
      "epoch": 4.720798115560482,
      "grad_norm": 11.641266822814941,
      "learning_rate": 2.11262297353471e-05,
      "loss": 2.0451,
      "step": 34070
    },
    {
      "epoch": 4.722183732852986,
      "grad_norm": 11.813570976257324,
      "learning_rate": 2.1120687266177082e-05,
      "loss": 1.6085,
      "step": 34080
    },
    {
      "epoch": 4.72356935014549,
      "grad_norm": 9.682536125183105,
      "learning_rate": 2.111514479700707e-05,
      "loss": 2.2244,
      "step": 34090
    },
    {
      "epoch": 4.724954967437993,
      "grad_norm": 18.383882522583008,
      "learning_rate": 2.1109602327837052e-05,
      "loss": 1.7514,
      "step": 34100
    },
    {
      "epoch": 4.726340584730497,
      "grad_norm": 14.13222599029541,
      "learning_rate": 2.1104059858667038e-05,
      "loss": 2.4239,
      "step": 34110
    },
    {
      "epoch": 4.7277262020230015,
      "grad_norm": 12.330802917480469,
      "learning_rate": 2.1098517389497023e-05,
      "loss": 2.0697,
      "step": 34120
    },
    {
      "epoch": 4.729111819315505,
      "grad_norm": 11.531386375427246,
      "learning_rate": 2.1092974920327008e-05,
      "loss": 1.9727,
      "step": 34130
    },
    {
      "epoch": 4.730497436608009,
      "grad_norm": 10.293310165405273,
      "learning_rate": 2.108743245115699e-05,
      "loss": 2.2314,
      "step": 34140
    },
    {
      "epoch": 4.731883053900512,
      "grad_norm": 9.845412254333496,
      "learning_rate": 2.108188998198698e-05,
      "loss": 2.3609,
      "step": 34150
    },
    {
      "epoch": 4.7332686711930165,
      "grad_norm": 8.720499992370605,
      "learning_rate": 2.107634751281696e-05,
      "loss": 2.137,
      "step": 34160
    },
    {
      "epoch": 4.734654288485521,
      "grad_norm": 9.830716133117676,
      "learning_rate": 2.1070805043646946e-05,
      "loss": 2.0955,
      "step": 34170
    },
    {
      "epoch": 4.736039905778024,
      "grad_norm": 9.79621696472168,
      "learning_rate": 2.106526257447693e-05,
      "loss": 1.9796,
      "step": 34180
    },
    {
      "epoch": 4.737425523070528,
      "grad_norm": 9.777058601379395,
      "learning_rate": 2.1059720105306916e-05,
      "loss": 2.0032,
      "step": 34190
    },
    {
      "epoch": 4.7388111403630315,
      "grad_norm": 8.972611427307129,
      "learning_rate": 2.1054177636136898e-05,
      "loss": 1.9209,
      "step": 34200
    },
    {
      "epoch": 4.740196757655536,
      "grad_norm": 14.487724304199219,
      "learning_rate": 2.1048635166966887e-05,
      "loss": 2.1986,
      "step": 34210
    },
    {
      "epoch": 4.741582374948039,
      "grad_norm": 11.739076614379883,
      "learning_rate": 2.104309269779687e-05,
      "loss": 2.2421,
      "step": 34220
    },
    {
      "epoch": 4.742967992240543,
      "grad_norm": 13.856285095214844,
      "learning_rate": 2.1037550228626854e-05,
      "loss": 1.9462,
      "step": 34230
    },
    {
      "epoch": 4.7443536095330465,
      "grad_norm": 17.355377197265625,
      "learning_rate": 2.103200775945684e-05,
      "loss": 1.9538,
      "step": 34240
    },
    {
      "epoch": 4.745739226825551,
      "grad_norm": 17.4962158203125,
      "learning_rate": 2.1026465290286824e-05,
      "loss": 2.0397,
      "step": 34250
    },
    {
      "epoch": 4.747124844118055,
      "grad_norm": 12.933034896850586,
      "learning_rate": 2.1020922821116806e-05,
      "loss": 2.0821,
      "step": 34260
    },
    {
      "epoch": 4.748510461410558,
      "grad_norm": 9.068804740905762,
      "learning_rate": 2.1015380351946795e-05,
      "loss": 1.6342,
      "step": 34270
    },
    {
      "epoch": 4.749896078703062,
      "grad_norm": 10.840487480163574,
      "learning_rate": 2.1009837882776777e-05,
      "loss": 2.1985,
      "step": 34280
    },
    {
      "epoch": 4.751281695995566,
      "grad_norm": 16.05670928955078,
      "learning_rate": 2.1004295413606765e-05,
      "loss": 2.2633,
      "step": 34290
    },
    {
      "epoch": 4.75266731328807,
      "grad_norm": 12.031904220581055,
      "learning_rate": 2.099875294443675e-05,
      "loss": 2.0587,
      "step": 34300
    },
    {
      "epoch": 4.754052930580574,
      "grad_norm": 14.19218635559082,
      "learning_rate": 2.0993210475266732e-05,
      "loss": 1.9538,
      "step": 34310
    },
    {
      "epoch": 4.755438547873077,
      "grad_norm": 14.435592651367188,
      "learning_rate": 2.098766800609672e-05,
      "loss": 2.4237,
      "step": 34320
    },
    {
      "epoch": 4.756824165165582,
      "grad_norm": 14.503072738647461,
      "learning_rate": 2.0982125536926703e-05,
      "loss": 2.4289,
      "step": 34330
    },
    {
      "epoch": 4.758209782458085,
      "grad_norm": 11.984572410583496,
      "learning_rate": 2.0976583067756685e-05,
      "loss": 1.7845,
      "step": 34340
    },
    {
      "epoch": 4.759595399750589,
      "grad_norm": 8.001714706420898,
      "learning_rate": 2.0971040598586673e-05,
      "loss": 2.0241,
      "step": 34350
    },
    {
      "epoch": 4.760981017043092,
      "grad_norm": 12.308292388916016,
      "learning_rate": 2.096549812941666e-05,
      "loss": 1.991,
      "step": 34360
    },
    {
      "epoch": 4.762366634335597,
      "grad_norm": 7.858513832092285,
      "learning_rate": 2.095995566024664e-05,
      "loss": 2.0276,
      "step": 34370
    },
    {
      "epoch": 4.7637522516281,
      "grad_norm": 13.157083511352539,
      "learning_rate": 2.095441319107663e-05,
      "loss": 2.5846,
      "step": 34380
    },
    {
      "epoch": 4.765137868920604,
      "grad_norm": 18.545827865600586,
      "learning_rate": 2.094887072190661e-05,
      "loss": 2.2179,
      "step": 34390
    },
    {
      "epoch": 4.766523486213108,
      "grad_norm": 13.556615829467773,
      "learning_rate": 2.0943328252736593e-05,
      "loss": 2.2507,
      "step": 34400
    },
    {
      "epoch": 4.767909103505612,
      "grad_norm": 13.336040496826172,
      "learning_rate": 2.093778578356658e-05,
      "loss": 2.0407,
      "step": 34410
    },
    {
      "epoch": 4.769294720798116,
      "grad_norm": 21.03276824951172,
      "learning_rate": 2.0932243314396567e-05,
      "loss": 1.8965,
      "step": 34420
    },
    {
      "epoch": 4.770680338090619,
      "grad_norm": 11.124555587768555,
      "learning_rate": 2.092670084522655e-05,
      "loss": 2.1851,
      "step": 34430
    },
    {
      "epoch": 4.772065955383123,
      "grad_norm": 13.99073600769043,
      "learning_rate": 2.0921158376056537e-05,
      "loss": 1.8893,
      "step": 34440
    },
    {
      "epoch": 4.7734515726756275,
      "grad_norm": 16.703659057617188,
      "learning_rate": 2.091561590688652e-05,
      "loss": 1.8067,
      "step": 34450
    },
    {
      "epoch": 4.774837189968131,
      "grad_norm": 14.459090232849121,
      "learning_rate": 2.0910073437716504e-05,
      "loss": 2.2167,
      "step": 34460
    },
    {
      "epoch": 4.776222807260635,
      "grad_norm": 18.954965591430664,
      "learning_rate": 2.090453096854649e-05,
      "loss": 2.1637,
      "step": 34470
    },
    {
      "epoch": 4.777608424553138,
      "grad_norm": 13.216103553771973,
      "learning_rate": 2.0898988499376475e-05,
      "loss": 1.8722,
      "step": 34480
    },
    {
      "epoch": 4.7789940418456425,
      "grad_norm": 14.850269317626953,
      "learning_rate": 2.0893446030206457e-05,
      "loss": 2.8737,
      "step": 34490
    },
    {
      "epoch": 4.780379659138146,
      "grad_norm": 10.813203811645508,
      "learning_rate": 2.0887903561036445e-05,
      "loss": 2.0278,
      "step": 34500
    },
    {
      "epoch": 4.78176527643065,
      "grad_norm": 11.808865547180176,
      "learning_rate": 2.0882361091866427e-05,
      "loss": 2.1122,
      "step": 34510
    },
    {
      "epoch": 4.783150893723153,
      "grad_norm": 16.865097045898438,
      "learning_rate": 2.0876818622696412e-05,
      "loss": 2.5985,
      "step": 34520
    },
    {
      "epoch": 4.7845365110156575,
      "grad_norm": 15.562108039855957,
      "learning_rate": 2.0871276153526398e-05,
      "loss": 2.0387,
      "step": 34530
    },
    {
      "epoch": 4.785922128308162,
      "grad_norm": 9.050314903259277,
      "learning_rate": 2.0865733684356383e-05,
      "loss": 2.0454,
      "step": 34540
    },
    {
      "epoch": 4.787307745600665,
      "grad_norm": 17.753429412841797,
      "learning_rate": 2.0860191215186365e-05,
      "loss": 1.9893,
      "step": 34550
    },
    {
      "epoch": 4.788693362893169,
      "grad_norm": 13.811930656433105,
      "learning_rate": 2.0854648746016353e-05,
      "loss": 2.5823,
      "step": 34560
    },
    {
      "epoch": 4.7900789801856725,
      "grad_norm": 10.387434005737305,
      "learning_rate": 2.0849106276846335e-05,
      "loss": 2.0646,
      "step": 34570
    },
    {
      "epoch": 4.791464597478177,
      "grad_norm": 10.698051452636719,
      "learning_rate": 2.0843563807676324e-05,
      "loss": 1.7841,
      "step": 34580
    },
    {
      "epoch": 4.79285021477068,
      "grad_norm": 9.069242477416992,
      "learning_rate": 2.0838021338506306e-05,
      "loss": 1.8522,
      "step": 34590
    },
    {
      "epoch": 4.794235832063184,
      "grad_norm": 17.182235717773438,
      "learning_rate": 2.083247886933629e-05,
      "loss": 2.2544,
      "step": 34600
    },
    {
      "epoch": 4.7956214493556875,
      "grad_norm": 8.001666069030762,
      "learning_rate": 2.0826936400166276e-05,
      "loss": 1.9648,
      "step": 34610
    },
    {
      "epoch": 4.797007066648192,
      "grad_norm": 14.391597747802734,
      "learning_rate": 2.082139393099626e-05,
      "loss": 2.1953,
      "step": 34620
    },
    {
      "epoch": 4.798392683940696,
      "grad_norm": 6.2494587898254395,
      "learning_rate": 2.0815851461826243e-05,
      "loss": 2.0899,
      "step": 34630
    },
    {
      "epoch": 4.799778301233199,
      "grad_norm": 4.900784492492676,
      "learning_rate": 2.0810308992656232e-05,
      "loss": 1.949,
      "step": 34640
    },
    {
      "epoch": 4.801163918525703,
      "grad_norm": 10.3499755859375,
      "learning_rate": 2.0804766523486214e-05,
      "loss": 1.7736,
      "step": 34650
    },
    {
      "epoch": 4.802549535818207,
      "grad_norm": 11.343828201293945,
      "learning_rate": 2.07992240543162e-05,
      "loss": 1.6077,
      "step": 34660
    },
    {
      "epoch": 4.803935153110711,
      "grad_norm": 12.18687629699707,
      "learning_rate": 2.0793681585146184e-05,
      "loss": 2.1904,
      "step": 34670
    },
    {
      "epoch": 4.805320770403215,
      "grad_norm": 10.261489868164062,
      "learning_rate": 2.078813911597617e-05,
      "loss": 2.0538,
      "step": 34680
    },
    {
      "epoch": 4.806706387695718,
      "grad_norm": 9.62186050415039,
      "learning_rate": 2.078259664680615e-05,
      "loss": 2.2555,
      "step": 34690
    },
    {
      "epoch": 4.8080920049882225,
      "grad_norm": 14.39951229095459,
      "learning_rate": 2.077705417763614e-05,
      "loss": 1.8072,
      "step": 34700
    },
    {
      "epoch": 4.809477622280726,
      "grad_norm": 13.155887603759766,
      "learning_rate": 2.0771511708466122e-05,
      "loss": 2.0253,
      "step": 34710
    },
    {
      "epoch": 4.81086323957323,
      "grad_norm": 18.639888763427734,
      "learning_rate": 2.0765969239296107e-05,
      "loss": 2.3628,
      "step": 34720
    },
    {
      "epoch": 4.812248856865733,
      "grad_norm": 12.468392372131348,
      "learning_rate": 2.0760426770126093e-05,
      "loss": 2.2192,
      "step": 34730
    },
    {
      "epoch": 4.8136344741582375,
      "grad_norm": 10.140617370605469,
      "learning_rate": 2.0754884300956078e-05,
      "loss": 2.2865,
      "step": 34740
    },
    {
      "epoch": 4.815020091450741,
      "grad_norm": 15.263703346252441,
      "learning_rate": 2.074934183178606e-05,
      "loss": 1.8265,
      "step": 34750
    },
    {
      "epoch": 4.816405708743245,
      "grad_norm": 9.452406883239746,
      "learning_rate": 2.0743799362616048e-05,
      "loss": 2.1703,
      "step": 34760
    },
    {
      "epoch": 4.817791326035749,
      "grad_norm": 10.253913879394531,
      "learning_rate": 2.073825689344603e-05,
      "loss": 2.2379,
      "step": 34770
    },
    {
      "epoch": 4.8191769433282525,
      "grad_norm": 17.834482192993164,
      "learning_rate": 2.0732714424276015e-05,
      "loss": 2.1381,
      "step": 34780
    },
    {
      "epoch": 4.820562560620757,
      "grad_norm": 28.269882202148438,
      "learning_rate": 2.0727171955106004e-05,
      "loss": 2.1996,
      "step": 34790
    },
    {
      "epoch": 4.82194817791326,
      "grad_norm": 19.5421142578125,
      "learning_rate": 2.0721629485935986e-05,
      "loss": 2.0371,
      "step": 34800
    },
    {
      "epoch": 4.823333795205764,
      "grad_norm": 11.672228813171387,
      "learning_rate": 2.0716087016765968e-05,
      "loss": 1.975,
      "step": 34810
    },
    {
      "epoch": 4.824719412498268,
      "grad_norm": 12.504047393798828,
      "learning_rate": 2.0710544547595956e-05,
      "loss": 1.8834,
      "step": 34820
    },
    {
      "epoch": 4.826105029790772,
      "grad_norm": 26.012767791748047,
      "learning_rate": 2.0705002078425938e-05,
      "loss": 2.1132,
      "step": 34830
    },
    {
      "epoch": 4.827490647083276,
      "grad_norm": 14.55679988861084,
      "learning_rate": 2.0699459609255923e-05,
      "loss": 1.7025,
      "step": 34840
    },
    {
      "epoch": 4.828876264375779,
      "grad_norm": 13.487690925598145,
      "learning_rate": 2.0693917140085912e-05,
      "loss": 2.0978,
      "step": 34850
    },
    {
      "epoch": 4.830261881668283,
      "grad_norm": 16.58094596862793,
      "learning_rate": 2.0688374670915894e-05,
      "loss": 1.8333,
      "step": 34860
    },
    {
      "epoch": 4.831647498960787,
      "grad_norm": 12.446135520935059,
      "learning_rate": 2.0682832201745883e-05,
      "loss": 2.2398,
      "step": 34870
    },
    {
      "epoch": 4.833033116253291,
      "grad_norm": 11.478375434875488,
      "learning_rate": 2.0677289732575864e-05,
      "loss": 2.3452,
      "step": 34880
    },
    {
      "epoch": 4.834418733545794,
      "grad_norm": 9.018484115600586,
      "learning_rate": 2.0671747263405846e-05,
      "loss": 1.9972,
      "step": 34890
    },
    {
      "epoch": 4.835804350838298,
      "grad_norm": 8.158686637878418,
      "learning_rate": 2.0666204794235835e-05,
      "loss": 2.3568,
      "step": 34900
    },
    {
      "epoch": 4.837189968130803,
      "grad_norm": 9.706045150756836,
      "learning_rate": 2.066066232506582e-05,
      "loss": 2.2076,
      "step": 34910
    },
    {
      "epoch": 4.838575585423306,
      "grad_norm": 7.439884185791016,
      "learning_rate": 2.0655119855895802e-05,
      "loss": 2.2219,
      "step": 34920
    },
    {
      "epoch": 4.83996120271581,
      "grad_norm": 7.0281877517700195,
      "learning_rate": 2.064957738672579e-05,
      "loss": 1.8224,
      "step": 34930
    },
    {
      "epoch": 4.841346820008313,
      "grad_norm": 13.068962097167969,
      "learning_rate": 2.0644034917555773e-05,
      "loss": 1.8759,
      "step": 34940
    },
    {
      "epoch": 4.842732437300818,
      "grad_norm": 12.516233444213867,
      "learning_rate": 2.0638492448385758e-05,
      "loss": 1.7212,
      "step": 34950
    },
    {
      "epoch": 4.844118054593322,
      "grad_norm": 16.46194839477539,
      "learning_rate": 2.0632949979215743e-05,
      "loss": 2.0083,
      "step": 34960
    },
    {
      "epoch": 4.845503671885825,
      "grad_norm": 14.326183319091797,
      "learning_rate": 2.062740751004573e-05,
      "loss": 2.2501,
      "step": 34970
    },
    {
      "epoch": 4.846889289178329,
      "grad_norm": 9.424280166625977,
      "learning_rate": 2.062186504087571e-05,
      "loss": 1.9893,
      "step": 34980
    },
    {
      "epoch": 4.848274906470833,
      "grad_norm": 8.887413024902344,
      "learning_rate": 2.06163225717057e-05,
      "loss": 1.9678,
      "step": 34990
    },
    {
      "epoch": 4.849660523763337,
      "grad_norm": 14.80635929107666,
      "learning_rate": 2.061078010253568e-05,
      "loss": 2.6971,
      "step": 35000
    },
    {
      "epoch": 4.85104614105584,
      "grad_norm": 11.908611297607422,
      "learning_rate": 2.0605237633365666e-05,
      "loss": 2.2477,
      "step": 35010
    },
    {
      "epoch": 4.852431758348344,
      "grad_norm": 20.382551193237305,
      "learning_rate": 2.059969516419565e-05,
      "loss": 2.7242,
      "step": 35020
    },
    {
      "epoch": 4.853817375640848,
      "grad_norm": 12.248726844787598,
      "learning_rate": 2.0594152695025636e-05,
      "loss": 2.0532,
      "step": 35030
    },
    {
      "epoch": 4.855202992933352,
      "grad_norm": 14.575339317321777,
      "learning_rate": 2.058861022585562e-05,
      "loss": 1.9603,
      "step": 35040
    },
    {
      "epoch": 4.856588610225856,
      "grad_norm": 12.27731990814209,
      "learning_rate": 2.0583067756685607e-05,
      "loss": 1.9808,
      "step": 35050
    },
    {
      "epoch": 4.857974227518359,
      "grad_norm": 14.326064109802246,
      "learning_rate": 2.057752528751559e-05,
      "loss": 2.2092,
      "step": 35060
    },
    {
      "epoch": 4.8593598448108635,
      "grad_norm": 10.474867820739746,
      "learning_rate": 2.0571982818345574e-05,
      "loss": 2.2391,
      "step": 35070
    },
    {
      "epoch": 4.860745462103367,
      "grad_norm": 12.2155179977417,
      "learning_rate": 2.056644034917556e-05,
      "loss": 2.2026,
      "step": 35080
    },
    {
      "epoch": 4.862131079395871,
      "grad_norm": 12.387412071228027,
      "learning_rate": 2.0560897880005545e-05,
      "loss": 1.9261,
      "step": 35090
    },
    {
      "epoch": 4.863516696688375,
      "grad_norm": 11.090023040771484,
      "learning_rate": 2.0555355410835526e-05,
      "loss": 1.9836,
      "step": 35100
    },
    {
      "epoch": 4.8649023139808785,
      "grad_norm": 9.306297302246094,
      "learning_rate": 2.0549812941665515e-05,
      "loss": 1.8261,
      "step": 35110
    },
    {
      "epoch": 4.866287931273383,
      "grad_norm": 11.559199333190918,
      "learning_rate": 2.0544270472495497e-05,
      "loss": 2.1309,
      "step": 35120
    },
    {
      "epoch": 4.867673548565886,
      "grad_norm": 13.460030555725098,
      "learning_rate": 2.0538728003325482e-05,
      "loss": 2.1074,
      "step": 35130
    },
    {
      "epoch": 4.86905916585839,
      "grad_norm": 16.13676643371582,
      "learning_rate": 2.0533185534155467e-05,
      "loss": 1.856,
      "step": 35140
    },
    {
      "epoch": 4.8704447831508935,
      "grad_norm": 11.864787101745605,
      "learning_rate": 2.0527643064985453e-05,
      "loss": 1.9987,
      "step": 35150
    },
    {
      "epoch": 4.871830400443398,
      "grad_norm": 9.273487091064453,
      "learning_rate": 2.0522100595815438e-05,
      "loss": 1.8633,
      "step": 35160
    },
    {
      "epoch": 4.873216017735901,
      "grad_norm": 9.147530555725098,
      "learning_rate": 2.0516558126645423e-05,
      "loss": 1.8916,
      "step": 35170
    },
    {
      "epoch": 4.874601635028405,
      "grad_norm": 17.394493103027344,
      "learning_rate": 2.0511015657475405e-05,
      "loss": 2.1018,
      "step": 35180
    },
    {
      "epoch": 4.875987252320909,
      "grad_norm": 11.526544570922852,
      "learning_rate": 2.0505473188305394e-05,
      "loss": 1.8309,
      "step": 35190
    },
    {
      "epoch": 4.877372869613413,
      "grad_norm": 13.987927436828613,
      "learning_rate": 2.0499930719135376e-05,
      "loss": 2.2966,
      "step": 35200
    },
    {
      "epoch": 4.878758486905917,
      "grad_norm": 12.3931884765625,
      "learning_rate": 2.049438824996536e-05,
      "loss": 2.3934,
      "step": 35210
    },
    {
      "epoch": 4.88014410419842,
      "grad_norm": 18.322284698486328,
      "learning_rate": 2.048884578079535e-05,
      "loss": 2.2662,
      "step": 35220
    },
    {
      "epoch": 4.881529721490924,
      "grad_norm": 11.181297302246094,
      "learning_rate": 2.048330331162533e-05,
      "loss": 1.7997,
      "step": 35230
    },
    {
      "epoch": 4.882915338783428,
      "grad_norm": 20.006378173828125,
      "learning_rate": 2.0477760842455313e-05,
      "loss": 2.2578,
      "step": 35240
    },
    {
      "epoch": 4.884300956075932,
      "grad_norm": 12.76112174987793,
      "learning_rate": 2.0472218373285302e-05,
      "loss": 2.3338,
      "step": 35250
    },
    {
      "epoch": 4.885686573368435,
      "grad_norm": 12.700214385986328,
      "learning_rate": 2.0466675904115284e-05,
      "loss": 2.1835,
      "step": 35260
    },
    {
      "epoch": 4.887072190660939,
      "grad_norm": 9.263349533081055,
      "learning_rate": 2.046113343494527e-05,
      "loss": 1.9094,
      "step": 35270
    },
    {
      "epoch": 4.8884578079534435,
      "grad_norm": 10.472986221313477,
      "learning_rate": 2.0455590965775258e-05,
      "loss": 2.334,
      "step": 35280
    },
    {
      "epoch": 4.889843425245947,
      "grad_norm": 14.961786270141602,
      "learning_rate": 2.045004849660524e-05,
      "loss": 2.5258,
      "step": 35290
    },
    {
      "epoch": 4.891229042538451,
      "grad_norm": 12.346595764160156,
      "learning_rate": 2.044450602743522e-05,
      "loss": 2.1037,
      "step": 35300
    },
    {
      "epoch": 4.892614659830954,
      "grad_norm": 21.815645217895508,
      "learning_rate": 2.043896355826521e-05,
      "loss": 2.3223,
      "step": 35310
    },
    {
      "epoch": 4.8940002771234585,
      "grad_norm": 20.728193283081055,
      "learning_rate": 2.0433421089095192e-05,
      "loss": 2.5481,
      "step": 35320
    },
    {
      "epoch": 4.895385894415963,
      "grad_norm": 15.968517303466797,
      "learning_rate": 2.0427878619925177e-05,
      "loss": 1.8921,
      "step": 35330
    },
    {
      "epoch": 4.896771511708466,
      "grad_norm": 11.208062171936035,
      "learning_rate": 2.0422336150755166e-05,
      "loss": 2.2693,
      "step": 35340
    },
    {
      "epoch": 4.89815712900097,
      "grad_norm": 12.385871887207031,
      "learning_rate": 2.0416793681585147e-05,
      "loss": 2.2094,
      "step": 35350
    },
    {
      "epoch": 4.8995427462934735,
      "grad_norm": 12.05933666229248,
      "learning_rate": 2.041125121241513e-05,
      "loss": 1.9165,
      "step": 35360
    },
    {
      "epoch": 4.900928363585978,
      "grad_norm": 16.82380485534668,
      "learning_rate": 2.0405708743245118e-05,
      "loss": 2.0661,
      "step": 35370
    },
    {
      "epoch": 4.902313980878481,
      "grad_norm": 14.777467727661133,
      "learning_rate": 2.0400166274075103e-05,
      "loss": 2.4426,
      "step": 35380
    },
    {
      "epoch": 4.903699598170985,
      "grad_norm": 18.520950317382812,
      "learning_rate": 2.0394623804905085e-05,
      "loss": 2.089,
      "step": 35390
    },
    {
      "epoch": 4.9050852154634885,
      "grad_norm": 11.85455322265625,
      "learning_rate": 2.0389081335735074e-05,
      "loss": 2.0619,
      "step": 35400
    },
    {
      "epoch": 4.906470832755993,
      "grad_norm": 8.579550743103027,
      "learning_rate": 2.0383538866565056e-05,
      "loss": 2.1054,
      "step": 35410
    },
    {
      "epoch": 4.907856450048497,
      "grad_norm": 16.87782096862793,
      "learning_rate": 2.0377996397395037e-05,
      "loss": 2.0736,
      "step": 35420
    },
    {
      "epoch": 4.909242067341,
      "grad_norm": 12.153243064880371,
      "learning_rate": 2.0372453928225026e-05,
      "loss": 2.5011,
      "step": 35430
    },
    {
      "epoch": 4.910627684633504,
      "grad_norm": 10.204225540161133,
      "learning_rate": 2.036691145905501e-05,
      "loss": 1.7406,
      "step": 35440
    },
    {
      "epoch": 4.912013301926008,
      "grad_norm": 15.18394947052002,
      "learning_rate": 2.0361368989884997e-05,
      "loss": 2.1765,
      "step": 35450
    },
    {
      "epoch": 4.913398919218512,
      "grad_norm": 7.334325790405273,
      "learning_rate": 2.0355826520714982e-05,
      "loss": 2.5638,
      "step": 35460
    },
    {
      "epoch": 4.914784536511016,
      "grad_norm": 12.494087219238281,
      "learning_rate": 2.0350284051544964e-05,
      "loss": 1.8456,
      "step": 35470
    },
    {
      "epoch": 4.916170153803519,
      "grad_norm": 13.307594299316406,
      "learning_rate": 2.0344741582374952e-05,
      "loss": 2.4113,
      "step": 35480
    },
    {
      "epoch": 4.917555771096024,
      "grad_norm": 8.369804382324219,
      "learning_rate": 2.0339199113204934e-05,
      "loss": 1.7424,
      "step": 35490
    },
    {
      "epoch": 4.918941388388527,
      "grad_norm": 15.351396560668945,
      "learning_rate": 2.033365664403492e-05,
      "loss": 2.2616,
      "step": 35500
    },
    {
      "epoch": 4.920327005681031,
      "grad_norm": 13.588515281677246,
      "learning_rate": 2.0328114174864905e-05,
      "loss": 1.6417,
      "step": 35510
    },
    {
      "epoch": 4.921712622973534,
      "grad_norm": 13.462296485900879,
      "learning_rate": 2.032257170569489e-05,
      "loss": 1.8977,
      "step": 35520
    },
    {
      "epoch": 4.923098240266039,
      "grad_norm": 24.19554901123047,
      "learning_rate": 2.0317029236524872e-05,
      "loss": 2.1641,
      "step": 35530
    },
    {
      "epoch": 4.924483857558542,
      "grad_norm": 12.89748477935791,
      "learning_rate": 2.031148676735486e-05,
      "loss": 2.0731,
      "step": 35540
    },
    {
      "epoch": 4.925869474851046,
      "grad_norm": 15.17142105102539,
      "learning_rate": 2.0305944298184842e-05,
      "loss": 1.9702,
      "step": 35550
    },
    {
      "epoch": 4.92725509214355,
      "grad_norm": 12.13355827331543,
      "learning_rate": 2.0300401829014828e-05,
      "loss": 1.9926,
      "step": 35560
    },
    {
      "epoch": 4.928640709436054,
      "grad_norm": 8.846552848815918,
      "learning_rate": 2.0294859359844813e-05,
      "loss": 2.0998,
      "step": 35570
    },
    {
      "epoch": 4.930026326728558,
      "grad_norm": 15.42783260345459,
      "learning_rate": 2.0289316890674798e-05,
      "loss": 2.0166,
      "step": 35580
    },
    {
      "epoch": 4.931411944021061,
      "grad_norm": 19.112342834472656,
      "learning_rate": 2.028377442150478e-05,
      "loss": 2.1368,
      "step": 35590
    },
    {
      "epoch": 4.932797561313565,
      "grad_norm": 16.729692459106445,
      "learning_rate": 2.027823195233477e-05,
      "loss": 1.8361,
      "step": 35600
    },
    {
      "epoch": 4.9341831786060695,
      "grad_norm": 15.301895141601562,
      "learning_rate": 2.027268948316475e-05,
      "loss": 2.0645,
      "step": 35610
    },
    {
      "epoch": 4.935568795898573,
      "grad_norm": 15.587373733520508,
      "learning_rate": 2.0267147013994736e-05,
      "loss": 1.9095,
      "step": 35620
    },
    {
      "epoch": 4.936954413191077,
      "grad_norm": 17.428375244140625,
      "learning_rate": 2.026160454482472e-05,
      "loss": 1.9347,
      "step": 35630
    },
    {
      "epoch": 4.93834003048358,
      "grad_norm": 15.083577156066895,
      "learning_rate": 2.0256062075654706e-05,
      "loss": 2.1015,
      "step": 35640
    },
    {
      "epoch": 4.9397256477760845,
      "grad_norm": 8.901138305664062,
      "learning_rate": 2.0250519606484688e-05,
      "loss": 2.2567,
      "step": 35650
    },
    {
      "epoch": 4.941111265068588,
      "grad_norm": 15.785171508789062,
      "learning_rate": 2.0244977137314677e-05,
      "loss": 2.5268,
      "step": 35660
    },
    {
      "epoch": 4.942496882361092,
      "grad_norm": 17.74376678466797,
      "learning_rate": 2.023943466814466e-05,
      "loss": 2.08,
      "step": 35670
    },
    {
      "epoch": 4.943882499653595,
      "grad_norm": 9.7687349319458,
      "learning_rate": 2.0233892198974644e-05,
      "loss": 2.126,
      "step": 35680
    },
    {
      "epoch": 4.9452681169460995,
      "grad_norm": 9.47976016998291,
      "learning_rate": 2.022834972980463e-05,
      "loss": 2.1238,
      "step": 35690
    },
    {
      "epoch": 4.946653734238604,
      "grad_norm": 13.466437339782715,
      "learning_rate": 2.0222807260634614e-05,
      "loss": 2.1296,
      "step": 35700
    },
    {
      "epoch": 4.948039351531107,
      "grad_norm": 14.542045593261719,
      "learning_rate": 2.0217264791464596e-05,
      "loss": 2.2193,
      "step": 35710
    },
    {
      "epoch": 4.949424968823611,
      "grad_norm": 12.533924102783203,
      "learning_rate": 2.0211722322294585e-05,
      "loss": 2.04,
      "step": 35720
    },
    {
      "epoch": 4.9508105861161145,
      "grad_norm": 8.246183395385742,
      "learning_rate": 2.0206179853124567e-05,
      "loss": 1.7244,
      "step": 35730
    },
    {
      "epoch": 4.952196203408619,
      "grad_norm": 9.763643264770508,
      "learning_rate": 2.0200637383954555e-05,
      "loss": 1.7782,
      "step": 35740
    },
    {
      "epoch": 4.953581820701122,
      "grad_norm": 13.491284370422363,
      "learning_rate": 2.0195094914784537e-05,
      "loss": 2.1135,
      "step": 35750
    },
    {
      "epoch": 4.954967437993626,
      "grad_norm": 10.75455379486084,
      "learning_rate": 2.0189552445614522e-05,
      "loss": 2.1584,
      "step": 35760
    },
    {
      "epoch": 4.95635305528613,
      "grad_norm": 14.161819458007812,
      "learning_rate": 2.018400997644451e-05,
      "loss": 1.9289,
      "step": 35770
    },
    {
      "epoch": 4.957738672578634,
      "grad_norm": 15.156091690063477,
      "learning_rate": 2.0178467507274493e-05,
      "loss": 2.3105,
      "step": 35780
    },
    {
      "epoch": 4.959124289871138,
      "grad_norm": 8.528963088989258,
      "learning_rate": 2.0172925038104475e-05,
      "loss": 1.7712,
      "step": 35790
    },
    {
      "epoch": 4.960509907163641,
      "grad_norm": 10.418142318725586,
      "learning_rate": 2.0167382568934463e-05,
      "loss": 2.4388,
      "step": 35800
    },
    {
      "epoch": 4.961895524456145,
      "grad_norm": 18.244884490966797,
      "learning_rate": 2.016184009976445e-05,
      "loss": 1.9501,
      "step": 35810
    },
    {
      "epoch": 4.963281141748649,
      "grad_norm": 23.27883529663086,
      "learning_rate": 2.015629763059443e-05,
      "loss": 1.8915,
      "step": 35820
    },
    {
      "epoch": 4.964666759041153,
      "grad_norm": 8.963309288024902,
      "learning_rate": 2.015075516142442e-05,
      "loss": 2.0793,
      "step": 35830
    },
    {
      "epoch": 4.966052376333657,
      "grad_norm": 12.75314712524414,
      "learning_rate": 2.01452126922544e-05,
      "loss": 1.9766,
      "step": 35840
    },
    {
      "epoch": 4.96743799362616,
      "grad_norm": 12.290244102478027,
      "learning_rate": 2.0139670223084383e-05,
      "loss": 2.2991,
      "step": 35850
    },
    {
      "epoch": 4.9688236109186645,
      "grad_norm": 12.84509563446045,
      "learning_rate": 2.013412775391437e-05,
      "loss": 1.9473,
      "step": 35860
    },
    {
      "epoch": 4.970209228211168,
      "grad_norm": 13.884170532226562,
      "learning_rate": 2.0128585284744357e-05,
      "loss": 2.1827,
      "step": 35870
    },
    {
      "epoch": 4.971594845503672,
      "grad_norm": 15.1784086227417,
      "learning_rate": 2.012304281557434e-05,
      "loss": 2.2949,
      "step": 35880
    },
    {
      "epoch": 4.972980462796175,
      "grad_norm": 12.79277229309082,
      "learning_rate": 2.0117500346404327e-05,
      "loss": 1.9086,
      "step": 35890
    },
    {
      "epoch": 4.9743660800886795,
      "grad_norm": 13.369576454162598,
      "learning_rate": 2.011195787723431e-05,
      "loss": 1.7439,
      "step": 35900
    },
    {
      "epoch": 4.975751697381183,
      "grad_norm": 10.996846199035645,
      "learning_rate": 2.010641540806429e-05,
      "loss": 2.475,
      "step": 35910
    },
    {
      "epoch": 4.977137314673687,
      "grad_norm": 6.355001449584961,
      "learning_rate": 2.010087293889428e-05,
      "loss": 2.0377,
      "step": 35920
    },
    {
      "epoch": 4.978522931966191,
      "grad_norm": 9.430768966674805,
      "learning_rate": 2.0095330469724265e-05,
      "loss": 2.3716,
      "step": 35930
    },
    {
      "epoch": 4.9799085492586945,
      "grad_norm": 15.158161163330078,
      "learning_rate": 2.0089788000554247e-05,
      "loss": 2.0852,
      "step": 35940
    },
    {
      "epoch": 4.981294166551199,
      "grad_norm": 14.289551734924316,
      "learning_rate": 2.0084245531384235e-05,
      "loss": 2.3752,
      "step": 35950
    },
    {
      "epoch": 4.982679783843702,
      "grad_norm": 14.090208053588867,
      "learning_rate": 2.0078703062214217e-05,
      "loss": 2.2131,
      "step": 35960
    },
    {
      "epoch": 4.984065401136206,
      "grad_norm": 17.579011917114258,
      "learning_rate": 2.0073160593044202e-05,
      "loss": 1.827,
      "step": 35970
    },
    {
      "epoch": 4.98545101842871,
      "grad_norm": 9.711260795593262,
      "learning_rate": 2.0067618123874188e-05,
      "loss": 2.4325,
      "step": 35980
    },
    {
      "epoch": 4.986836635721214,
      "grad_norm": 12.465675354003906,
      "learning_rate": 2.0062075654704173e-05,
      "loss": 2.1326,
      "step": 35990
    },
    {
      "epoch": 4.988222253013718,
      "grad_norm": 16.673233032226562,
      "learning_rate": 2.0056533185534155e-05,
      "loss": 1.849,
      "step": 36000
    },
    {
      "epoch": 4.989607870306221,
      "grad_norm": 24.97939109802246,
      "learning_rate": 2.0050990716364143e-05,
      "loss": 1.9712,
      "step": 36010
    },
    {
      "epoch": 4.990993487598725,
      "grad_norm": 25.234159469604492,
      "learning_rate": 2.0045448247194125e-05,
      "loss": 1.9758,
      "step": 36020
    },
    {
      "epoch": 4.992379104891229,
      "grad_norm": 13.624463081359863,
      "learning_rate": 2.0039905778024114e-05,
      "loss": 2.1289,
      "step": 36030
    },
    {
      "epoch": 4.993764722183733,
      "grad_norm": 11.24411392211914,
      "learning_rate": 2.0034363308854096e-05,
      "loss": 2.2913,
      "step": 36040
    },
    {
      "epoch": 4.995150339476236,
      "grad_norm": 10.555948257446289,
      "learning_rate": 2.0029375086601082e-05,
      "loss": 2.3188,
      "step": 36050
    },
    {
      "epoch": 4.99653595676874,
      "grad_norm": 13.756852149963379,
      "learning_rate": 2.002383261743107e-05,
      "loss": 2.2441,
      "step": 36060
    },
    {
      "epoch": 4.997921574061245,
      "grad_norm": 11.153112411499023,
      "learning_rate": 2.0018290148261053e-05,
      "loss": 2.5646,
      "step": 36070
    },
    {
      "epoch": 4.999307191353748,
      "grad_norm": 15.574777603149414,
      "learning_rate": 2.0012747679091035e-05,
      "loss": 2.3794,
      "step": 36080
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.5163548163548164,
      "eval_bert_f1": 0.9876772165298462,
      "eval_bert_precision": 0.9895336031913757,
      "eval_bert_recall": 0.986247181892395,
      "eval_f1": 0.05958448991359601,
      "eval_loss": 2.2317066192626953,
      "eval_runtime": 288.9347,
      "eval_samples_per_second": 49.942,
      "eval_steps_per_second": 6.244,
      "eval_synonym_accuracy": 0.5288981288981289,
      "step": 36085
    },
    {
      "epoch": 5.000692808646252,
      "grad_norm": 9.252237319946289,
      "learning_rate": 2.0007205209921023e-05,
      "loss": 1.7926,
      "step": 36090
    },
    {
      "epoch": 5.002078425938755,
      "grad_norm": 15.022377014160156,
      "learning_rate": 2.0001662740751005e-05,
      "loss": 2.2945,
      "step": 36100
    },
    {
      "epoch": 5.00346404323126,
      "grad_norm": 8.117547988891602,
      "learning_rate": 1.999612027158099e-05,
      "loss": 1.8265,
      "step": 36110
    },
    {
      "epoch": 5.004849660523763,
      "grad_norm": 11.652387619018555,
      "learning_rate": 1.9990577802410976e-05,
      "loss": 2.065,
      "step": 36120
    },
    {
      "epoch": 5.006235277816267,
      "grad_norm": 6.233953475952148,
      "learning_rate": 1.998503533324096e-05,
      "loss": 1.4275,
      "step": 36130
    },
    {
      "epoch": 5.007620895108771,
      "grad_norm": 12.362353324890137,
      "learning_rate": 1.9979492864070946e-05,
      "loss": 1.887,
      "step": 36140
    },
    {
      "epoch": 5.009006512401275,
      "grad_norm": 13.35562801361084,
      "learning_rate": 1.9973950394900928e-05,
      "loss": 1.8319,
      "step": 36150
    },
    {
      "epoch": 5.010392129693779,
      "grad_norm": 11.312438011169434,
      "learning_rate": 1.9968407925730917e-05,
      "loss": 1.875,
      "step": 36160
    },
    {
      "epoch": 5.011777746986282,
      "grad_norm": 14.089001655578613,
      "learning_rate": 1.9962865456560902e-05,
      "loss": 1.4583,
      "step": 36170
    },
    {
      "epoch": 5.013163364278786,
      "grad_norm": 14.53900146484375,
      "learning_rate": 1.9957322987390884e-05,
      "loss": 2.2171,
      "step": 36180
    },
    {
      "epoch": 5.01454898157129,
      "grad_norm": 12.167682647705078,
      "learning_rate": 1.995178051822087e-05,
      "loss": 2.0763,
      "step": 36190
    },
    {
      "epoch": 5.015934598863794,
      "grad_norm": 11.23750114440918,
      "learning_rate": 1.9946238049050854e-05,
      "loss": 2.137,
      "step": 36200
    },
    {
      "epoch": 5.017320216156298,
      "grad_norm": 7.944331169128418,
      "learning_rate": 1.994069557988084e-05,
      "loss": 1.7964,
      "step": 36210
    },
    {
      "epoch": 5.018705833448801,
      "grad_norm": 11.792342185974121,
      "learning_rate": 1.9935153110710825e-05,
      "loss": 1.5891,
      "step": 36220
    },
    {
      "epoch": 5.0200914507413055,
      "grad_norm": 12.551788330078125,
      "learning_rate": 1.992961064154081e-05,
      "loss": 2.2112,
      "step": 36230
    },
    {
      "epoch": 5.021477068033809,
      "grad_norm": 19.839378356933594,
      "learning_rate": 1.9924068172370792e-05,
      "loss": 1.636,
      "step": 36240
    },
    {
      "epoch": 5.022862685326313,
      "grad_norm": 12.25126838684082,
      "learning_rate": 1.9918525703200777e-05,
      "loss": 1.7584,
      "step": 36250
    },
    {
      "epoch": 5.024248302618816,
      "grad_norm": 11.479752540588379,
      "learning_rate": 1.9912983234030762e-05,
      "loss": 1.8377,
      "step": 36260
    },
    {
      "epoch": 5.0256339199113205,
      "grad_norm": 11.484543800354004,
      "learning_rate": 1.9907440764860748e-05,
      "loss": 1.8099,
      "step": 36270
    },
    {
      "epoch": 5.027019537203825,
      "grad_norm": 7.870195388793945,
      "learning_rate": 1.9901898295690733e-05,
      "loss": 2.5614,
      "step": 36280
    },
    {
      "epoch": 5.028405154496328,
      "grad_norm": 12.764447212219238,
      "learning_rate": 1.9896355826520718e-05,
      "loss": 2.2354,
      "step": 36290
    },
    {
      "epoch": 5.029790771788832,
      "grad_norm": 8.7567777633667,
      "learning_rate": 1.9890813357350703e-05,
      "loss": 1.8487,
      "step": 36300
    },
    {
      "epoch": 5.0311763890813355,
      "grad_norm": 11.359033584594727,
      "learning_rate": 1.9885270888180685e-05,
      "loss": 1.8024,
      "step": 36310
    },
    {
      "epoch": 5.03256200637384,
      "grad_norm": 9.674500465393066,
      "learning_rate": 1.987972841901067e-05,
      "loss": 2.1165,
      "step": 36320
    },
    {
      "epoch": 5.033947623666343,
      "grad_norm": 9.81844425201416,
      "learning_rate": 1.9874185949840656e-05,
      "loss": 2.1985,
      "step": 36330
    },
    {
      "epoch": 5.035333240958847,
      "grad_norm": 7.101305961608887,
      "learning_rate": 1.986864348067064e-05,
      "loss": 1.8022,
      "step": 36340
    },
    {
      "epoch": 5.036718858251351,
      "grad_norm": 12.31857967376709,
      "learning_rate": 1.9863101011500626e-05,
      "loss": 2.1648,
      "step": 36350
    },
    {
      "epoch": 5.038104475543855,
      "grad_norm": 12.525625228881836,
      "learning_rate": 1.985755854233061e-05,
      "loss": 1.9234,
      "step": 36360
    },
    {
      "epoch": 5.039490092836359,
      "grad_norm": 12.67758846282959,
      "learning_rate": 1.9852016073160593e-05,
      "loss": 1.8936,
      "step": 36370
    },
    {
      "epoch": 5.040875710128862,
      "grad_norm": 10.170133590698242,
      "learning_rate": 1.984647360399058e-05,
      "loss": 2.1161,
      "step": 36380
    },
    {
      "epoch": 5.042261327421366,
      "grad_norm": 7.834043025970459,
      "learning_rate": 1.9840931134820564e-05,
      "loss": 1.7023,
      "step": 36390
    },
    {
      "epoch": 5.04364694471387,
      "grad_norm": 10.204833030700684,
      "learning_rate": 1.983538866565055e-05,
      "loss": 1.5123,
      "step": 36400
    },
    {
      "epoch": 5.045032562006374,
      "grad_norm": 11.757341384887695,
      "learning_rate": 1.9829846196480534e-05,
      "loss": 1.9648,
      "step": 36410
    },
    {
      "epoch": 5.046418179298878,
      "grad_norm": 7.426819801330566,
      "learning_rate": 1.982430372731052e-05,
      "loss": 2.1361,
      "step": 36420
    },
    {
      "epoch": 5.047803796591381,
      "grad_norm": 14.997679710388184,
      "learning_rate": 1.9818761258140505e-05,
      "loss": 1.8632,
      "step": 36430
    },
    {
      "epoch": 5.0491894138838855,
      "grad_norm": 11.649012565612793,
      "learning_rate": 1.9813218788970487e-05,
      "loss": 1.8602,
      "step": 36440
    },
    {
      "epoch": 5.050575031176389,
      "grad_norm": 11.802674293518066,
      "learning_rate": 1.9807676319800472e-05,
      "loss": 1.8769,
      "step": 36450
    },
    {
      "epoch": 5.051960648468893,
      "grad_norm": 15.69691276550293,
      "learning_rate": 1.9802133850630457e-05,
      "loss": 2.0474,
      "step": 36460
    },
    {
      "epoch": 5.053346265761396,
      "grad_norm": 7.77912712097168,
      "learning_rate": 1.9796591381460442e-05,
      "loss": 2.0096,
      "step": 36470
    },
    {
      "epoch": 5.0547318830539005,
      "grad_norm": 11.050058364868164,
      "learning_rate": 1.9791048912290428e-05,
      "loss": 1.6858,
      "step": 36480
    },
    {
      "epoch": 5.056117500346405,
      "grad_norm": 11.595385551452637,
      "learning_rate": 1.9785506443120413e-05,
      "loss": 1.9484,
      "step": 36490
    },
    {
      "epoch": 5.057503117638908,
      "grad_norm": 12.327845573425293,
      "learning_rate": 1.9779963973950395e-05,
      "loss": 1.9723,
      "step": 36500
    },
    {
      "epoch": 5.058888734931412,
      "grad_norm": 10.88401985168457,
      "learning_rate": 1.977442150478038e-05,
      "loss": 1.979,
      "step": 36510
    },
    {
      "epoch": 5.0602743522239155,
      "grad_norm": 15.590225219726562,
      "learning_rate": 1.9768879035610365e-05,
      "loss": 1.7081,
      "step": 36520
    },
    {
      "epoch": 5.06165996951642,
      "grad_norm": 11.183364868164062,
      "learning_rate": 1.976333656644035e-05,
      "loss": 1.8249,
      "step": 36530
    },
    {
      "epoch": 5.063045586808923,
      "grad_norm": 10.592193603515625,
      "learning_rate": 1.9757794097270336e-05,
      "loss": 2.2172,
      "step": 36540
    },
    {
      "epoch": 5.064431204101427,
      "grad_norm": 12.140656471252441,
      "learning_rate": 1.975225162810032e-05,
      "loss": 1.8107,
      "step": 36550
    },
    {
      "epoch": 5.065816821393931,
      "grad_norm": 13.520646095275879,
      "learning_rate": 1.9746709158930306e-05,
      "loss": 2.2475,
      "step": 36560
    },
    {
      "epoch": 5.067202438686435,
      "grad_norm": 9.935547828674316,
      "learning_rate": 1.9741166689760288e-05,
      "loss": 1.8766,
      "step": 36570
    },
    {
      "epoch": 5.068588055978939,
      "grad_norm": 14.310531616210938,
      "learning_rate": 1.9735624220590273e-05,
      "loss": 1.8262,
      "step": 36580
    },
    {
      "epoch": 5.069973673271442,
      "grad_norm": 16.918546676635742,
      "learning_rate": 1.9730081751420262e-05,
      "loss": 2.1771,
      "step": 36590
    },
    {
      "epoch": 5.071359290563946,
      "grad_norm": 13.92708683013916,
      "learning_rate": 1.9724539282250244e-05,
      "loss": 2.1412,
      "step": 36600
    },
    {
      "epoch": 5.07274490785645,
      "grad_norm": 14.01539134979248,
      "learning_rate": 1.971899681308023e-05,
      "loss": 2.1057,
      "step": 36610
    },
    {
      "epoch": 5.074130525148954,
      "grad_norm": 17.43869972229004,
      "learning_rate": 1.9713454343910214e-05,
      "loss": 2.2356,
      "step": 36620
    },
    {
      "epoch": 5.075516142441458,
      "grad_norm": 13.065353393554688,
      "learning_rate": 1.9707911874740196e-05,
      "loss": 1.7299,
      "step": 36630
    },
    {
      "epoch": 5.076901759733961,
      "grad_norm": 10.460123062133789,
      "learning_rate": 1.970236940557018e-05,
      "loss": 1.8823,
      "step": 36640
    },
    {
      "epoch": 5.078287377026466,
      "grad_norm": 10.273608207702637,
      "learning_rate": 1.969682693640017e-05,
      "loss": 2.2872,
      "step": 36650
    },
    {
      "epoch": 5.079672994318969,
      "grad_norm": 10.073919296264648,
      "learning_rate": 1.9691284467230152e-05,
      "loss": 1.8031,
      "step": 36660
    },
    {
      "epoch": 5.081058611611473,
      "grad_norm": 6.9084858894348145,
      "learning_rate": 1.9685741998060137e-05,
      "loss": 1.7936,
      "step": 36670
    },
    {
      "epoch": 5.082444228903976,
      "grad_norm": 6.193079948425293,
      "learning_rate": 1.9680199528890122e-05,
      "loss": 1.1868,
      "step": 36680
    },
    {
      "epoch": 5.083829846196481,
      "grad_norm": 11.414311408996582,
      "learning_rate": 1.9674657059720104e-05,
      "loss": 1.9133,
      "step": 36690
    },
    {
      "epoch": 5.085215463488984,
      "grad_norm": 14.784662246704102,
      "learning_rate": 1.9669114590550093e-05,
      "loss": 1.75,
      "step": 36700
    },
    {
      "epoch": 5.086601080781488,
      "grad_norm": 12.378408432006836,
      "learning_rate": 1.9663572121380078e-05,
      "loss": 2.0392,
      "step": 36710
    },
    {
      "epoch": 5.087986698073992,
      "grad_norm": 12.047493934631348,
      "learning_rate": 1.9658029652210063e-05,
      "loss": 2.364,
      "step": 36720
    },
    {
      "epoch": 5.089372315366496,
      "grad_norm": 10.89617919921875,
      "learning_rate": 1.9652487183040045e-05,
      "loss": 2.1029,
      "step": 36730
    },
    {
      "epoch": 5.090757932659,
      "grad_norm": 16.06139373779297,
      "learning_rate": 1.964694471387003e-05,
      "loss": 1.7869,
      "step": 36740
    },
    {
      "epoch": 5.092143549951503,
      "grad_norm": 8.46076774597168,
      "learning_rate": 1.9641402244700016e-05,
      "loss": 1.8491,
      "step": 36750
    },
    {
      "epoch": 5.093529167244007,
      "grad_norm": 12.38687801361084,
      "learning_rate": 1.963585977553e-05,
      "loss": 1.9915,
      "step": 36760
    },
    {
      "epoch": 5.094914784536511,
      "grad_norm": 12.246155738830566,
      "learning_rate": 1.9630317306359986e-05,
      "loss": 2.0654,
      "step": 36770
    },
    {
      "epoch": 5.096300401829015,
      "grad_norm": 13.494563102722168,
      "learning_rate": 1.962477483718997e-05,
      "loss": 1.9971,
      "step": 36780
    },
    {
      "epoch": 5.097686019121519,
      "grad_norm": 16.887046813964844,
      "learning_rate": 1.9619232368019953e-05,
      "loss": 2.1131,
      "step": 36790
    },
    {
      "epoch": 5.099071636414022,
      "grad_norm": 6.058461666107178,
      "learning_rate": 1.961368989884994e-05,
      "loss": 1.837,
      "step": 36800
    },
    {
      "epoch": 5.1004572537065265,
      "grad_norm": 16.735395431518555,
      "learning_rate": 1.9608147429679924e-05,
      "loss": 1.9231,
      "step": 36810
    },
    {
      "epoch": 5.10184287099903,
      "grad_norm": 12.32925796508789,
      "learning_rate": 1.960260496050991e-05,
      "loss": 2.0221,
      "step": 36820
    },
    {
      "epoch": 5.103228488291534,
      "grad_norm": 12.328327178955078,
      "learning_rate": 1.9597062491339894e-05,
      "loss": 1.8983,
      "step": 36830
    },
    {
      "epoch": 5.104614105584037,
      "grad_norm": 16.831430435180664,
      "learning_rate": 1.959152002216988e-05,
      "loss": 1.8885,
      "step": 36840
    },
    {
      "epoch": 5.1059997228765415,
      "grad_norm": 4.494750499725342,
      "learning_rate": 1.9585977552999865e-05,
      "loss": 1.9024,
      "step": 36850
    },
    {
      "epoch": 5.107385340169046,
      "grad_norm": 16.506816864013672,
      "learning_rate": 1.9580435083829847e-05,
      "loss": 2.1981,
      "step": 36860
    },
    {
      "epoch": 5.108770957461549,
      "grad_norm": 7.89316463470459,
      "learning_rate": 1.9574892614659832e-05,
      "loss": 1.927,
      "step": 36870
    },
    {
      "epoch": 5.110156574754053,
      "grad_norm": 10.55215835571289,
      "learning_rate": 1.9569350145489817e-05,
      "loss": 2.5067,
      "step": 36880
    },
    {
      "epoch": 5.1115421920465565,
      "grad_norm": 8.675518035888672,
      "learning_rate": 1.9563807676319802e-05,
      "loss": 2.0126,
      "step": 36890
    },
    {
      "epoch": 5.112927809339061,
      "grad_norm": 13.136979103088379,
      "learning_rate": 1.9558265207149788e-05,
      "loss": 1.7854,
      "step": 36900
    },
    {
      "epoch": 5.114313426631564,
      "grad_norm": 15.166938781738281,
      "learning_rate": 1.9552722737979773e-05,
      "loss": 2.3795,
      "step": 36910
    },
    {
      "epoch": 5.115699043924068,
      "grad_norm": 13.296088218688965,
      "learning_rate": 1.9547180268809755e-05,
      "loss": 2.9145,
      "step": 36920
    },
    {
      "epoch": 5.117084661216572,
      "grad_norm": 10.136299133300781,
      "learning_rate": 1.954163779963974e-05,
      "loss": 2.0468,
      "step": 36930
    },
    {
      "epoch": 5.118470278509076,
      "grad_norm": 7.855406761169434,
      "learning_rate": 1.9536095330469725e-05,
      "loss": 2.0647,
      "step": 36940
    },
    {
      "epoch": 5.11985589580158,
      "grad_norm": 8.01663875579834,
      "learning_rate": 1.953055286129971e-05,
      "loss": 1.9269,
      "step": 36950
    },
    {
      "epoch": 5.121241513094083,
      "grad_norm": 8.897035598754883,
      "learning_rate": 1.9525010392129696e-05,
      "loss": 2.3535,
      "step": 36960
    },
    {
      "epoch": 5.122627130386587,
      "grad_norm": 18.68252182006836,
      "learning_rate": 1.951946792295968e-05,
      "loss": 1.9131,
      "step": 36970
    },
    {
      "epoch": 5.124012747679091,
      "grad_norm": 7.559847354888916,
      "learning_rate": 1.9513925453789663e-05,
      "loss": 1.8756,
      "step": 36980
    },
    {
      "epoch": 5.125398364971595,
      "grad_norm": 11.454380989074707,
      "learning_rate": 1.9508382984619648e-05,
      "loss": 2.2823,
      "step": 36990
    },
    {
      "epoch": 5.126783982264099,
      "grad_norm": 16.69207191467285,
      "learning_rate": 1.9502840515449633e-05,
      "loss": 2.0896,
      "step": 37000
    },
    {
      "epoch": 5.128169599556602,
      "grad_norm": 7.149132251739502,
      "learning_rate": 1.949729804627962e-05,
      "loss": 2.061,
      "step": 37010
    },
    {
      "epoch": 5.1295552168491065,
      "grad_norm": 15.409109115600586,
      "learning_rate": 1.9491755577109604e-05,
      "loss": 1.8442,
      "step": 37020
    },
    {
      "epoch": 5.13094083414161,
      "grad_norm": 12.535070419311523,
      "learning_rate": 1.948621310793959e-05,
      "loss": 1.9493,
      "step": 37030
    },
    {
      "epoch": 5.132326451434114,
      "grad_norm": 14.756375312805176,
      "learning_rate": 1.9480670638769574e-05,
      "loss": 2.2951,
      "step": 37040
    },
    {
      "epoch": 5.133712068726617,
      "grad_norm": 8.028265953063965,
      "learning_rate": 1.9475128169599556e-05,
      "loss": 1.7249,
      "step": 37050
    },
    {
      "epoch": 5.1350976860191215,
      "grad_norm": 6.28277063369751,
      "learning_rate": 1.946958570042954e-05,
      "loss": 1.653,
      "step": 37060
    },
    {
      "epoch": 5.136483303311626,
      "grad_norm": 18.31566619873047,
      "learning_rate": 1.9464043231259527e-05,
      "loss": 2.183,
      "step": 37070
    },
    {
      "epoch": 5.137868920604129,
      "grad_norm": 11.63811206817627,
      "learning_rate": 1.9458500762089512e-05,
      "loss": 2.2311,
      "step": 37080
    },
    {
      "epoch": 5.139254537896633,
      "grad_norm": 8.805532455444336,
      "learning_rate": 1.9452958292919497e-05,
      "loss": 2.1473,
      "step": 37090
    },
    {
      "epoch": 5.1406401551891365,
      "grad_norm": 5.8184814453125,
      "learning_rate": 1.9447415823749483e-05,
      "loss": 1.7638,
      "step": 37100
    },
    {
      "epoch": 5.142025772481641,
      "grad_norm": 10.143067359924316,
      "learning_rate": 1.9441873354579464e-05,
      "loss": 2.3435,
      "step": 37110
    },
    {
      "epoch": 5.143411389774144,
      "grad_norm": 11.71536636352539,
      "learning_rate": 1.943633088540945e-05,
      "loss": 2.1664,
      "step": 37120
    },
    {
      "epoch": 5.144797007066648,
      "grad_norm": 8.457106590270996,
      "learning_rate": 1.9430788416239438e-05,
      "loss": 2.0046,
      "step": 37130
    },
    {
      "epoch": 5.146182624359152,
      "grad_norm": 19.743879318237305,
      "learning_rate": 1.942524594706942e-05,
      "loss": 1.827,
      "step": 37140
    },
    {
      "epoch": 5.147568241651656,
      "grad_norm": 12.145804405212402,
      "learning_rate": 1.9419703477899405e-05,
      "loss": 2.1435,
      "step": 37150
    },
    {
      "epoch": 5.14895385894416,
      "grad_norm": 14.374320030212402,
      "learning_rate": 1.941416100872939e-05,
      "loss": 2.2148,
      "step": 37160
    },
    {
      "epoch": 5.150339476236663,
      "grad_norm": 11.678077697753906,
      "learning_rate": 1.9408618539559376e-05,
      "loss": 1.7281,
      "step": 37170
    },
    {
      "epoch": 5.151725093529167,
      "grad_norm": 7.86362886428833,
      "learning_rate": 1.940307607038936e-05,
      "loss": 1.9289,
      "step": 37180
    },
    {
      "epoch": 5.153110710821671,
      "grad_norm": 18.474842071533203,
      "learning_rate": 1.9397533601219346e-05,
      "loss": 2.0704,
      "step": 37190
    },
    {
      "epoch": 5.154496328114175,
      "grad_norm": 6.303546905517578,
      "learning_rate": 1.939199113204933e-05,
      "loss": 2.0316,
      "step": 37200
    },
    {
      "epoch": 5.155881945406679,
      "grad_norm": 13.845271110534668,
      "learning_rate": 1.9386448662879314e-05,
      "loss": 2.0331,
      "step": 37210
    },
    {
      "epoch": 5.157267562699182,
      "grad_norm": 14.960799217224121,
      "learning_rate": 1.93809061937093e-05,
      "loss": 1.8609,
      "step": 37220
    },
    {
      "epoch": 5.158653179991687,
      "grad_norm": 9.537895202636719,
      "learning_rate": 1.9375363724539284e-05,
      "loss": 1.7132,
      "step": 37230
    },
    {
      "epoch": 5.16003879728419,
      "grad_norm": 9.395350456237793,
      "learning_rate": 1.936982125536927e-05,
      "loss": 2.3336,
      "step": 37240
    },
    {
      "epoch": 5.161424414576694,
      "grad_norm": 9.796208381652832,
      "learning_rate": 1.9364278786199255e-05,
      "loss": 1.7735,
      "step": 37250
    },
    {
      "epoch": 5.162810031869197,
      "grad_norm": 8.434721946716309,
      "learning_rate": 1.935873631702924e-05,
      "loss": 2.1801,
      "step": 37260
    },
    {
      "epoch": 5.164195649161702,
      "grad_norm": 8.665738105773926,
      "learning_rate": 1.935319384785922e-05,
      "loss": 1.6416,
      "step": 37270
    },
    {
      "epoch": 5.165581266454206,
      "grad_norm": 7.068018913269043,
      "learning_rate": 1.9347651378689207e-05,
      "loss": 2.096,
      "step": 37280
    },
    {
      "epoch": 5.166966883746709,
      "grad_norm": 18.178529739379883,
      "learning_rate": 1.9342108909519192e-05,
      "loss": 2.291,
      "step": 37290
    },
    {
      "epoch": 5.168352501039213,
      "grad_norm": 8.17778205871582,
      "learning_rate": 1.9336566440349177e-05,
      "loss": 1.8234,
      "step": 37300
    },
    {
      "epoch": 5.169738118331717,
      "grad_norm": 19.080121994018555,
      "learning_rate": 1.9331023971179163e-05,
      "loss": 2.2127,
      "step": 37310
    },
    {
      "epoch": 5.171123735624221,
      "grad_norm": 14.744889259338379,
      "learning_rate": 1.9325481502009148e-05,
      "loss": 2.1036,
      "step": 37320
    },
    {
      "epoch": 5.172509352916724,
      "grad_norm": 16.419063568115234,
      "learning_rate": 1.9319939032839133e-05,
      "loss": 1.6724,
      "step": 37330
    },
    {
      "epoch": 5.173894970209228,
      "grad_norm": 11.232008934020996,
      "learning_rate": 1.9314396563669115e-05,
      "loss": 1.7659,
      "step": 37340
    },
    {
      "epoch": 5.175280587501732,
      "grad_norm": 10.97826099395752,
      "learning_rate": 1.93088540944991e-05,
      "loss": 1.637,
      "step": 37350
    },
    {
      "epoch": 5.176666204794236,
      "grad_norm": 13.524150848388672,
      "learning_rate": 1.9303311625329085e-05,
      "loss": 2.2017,
      "step": 37360
    },
    {
      "epoch": 5.17805182208674,
      "grad_norm": 20.419918060302734,
      "learning_rate": 1.929776915615907e-05,
      "loss": 1.8404,
      "step": 37370
    },
    {
      "epoch": 5.179437439379243,
      "grad_norm": 14.926559448242188,
      "learning_rate": 1.9292226686989056e-05,
      "loss": 1.7651,
      "step": 37380
    },
    {
      "epoch": 5.1808230566717475,
      "grad_norm": 23.463029861450195,
      "learning_rate": 1.928668421781904e-05,
      "loss": 2.2434,
      "step": 37390
    },
    {
      "epoch": 5.182208673964251,
      "grad_norm": 14.579734802246094,
      "learning_rate": 1.9281141748649023e-05,
      "loss": 2.1003,
      "step": 37400
    },
    {
      "epoch": 5.183594291256755,
      "grad_norm": 17.929704666137695,
      "learning_rate": 1.927559927947901e-05,
      "loss": 1.9206,
      "step": 37410
    },
    {
      "epoch": 5.184979908549258,
      "grad_norm": 15.076762199401855,
      "learning_rate": 1.9270056810308994e-05,
      "loss": 2.4415,
      "step": 37420
    },
    {
      "epoch": 5.1863655258417625,
      "grad_norm": 6.189752578735352,
      "learning_rate": 1.926451434113898e-05,
      "loss": 1.6103,
      "step": 37430
    },
    {
      "epoch": 5.187751143134267,
      "grad_norm": 11.717743873596191,
      "learning_rate": 1.9258971871968964e-05,
      "loss": 1.8233,
      "step": 37440
    },
    {
      "epoch": 5.18913676042677,
      "grad_norm": 8.667162895202637,
      "learning_rate": 1.925342940279895e-05,
      "loss": 1.8826,
      "step": 37450
    },
    {
      "epoch": 5.190522377719274,
      "grad_norm": 11.229294776916504,
      "learning_rate": 1.9247886933628935e-05,
      "loss": 2.0303,
      "step": 37460
    },
    {
      "epoch": 5.1919079950117775,
      "grad_norm": 11.633370399475098,
      "learning_rate": 1.9242344464458916e-05,
      "loss": 1.5691,
      "step": 37470
    },
    {
      "epoch": 5.193293612304282,
      "grad_norm": 17.98043441772461,
      "learning_rate": 1.92368019952889e-05,
      "loss": 2.1224,
      "step": 37480
    },
    {
      "epoch": 5.194679229596785,
      "grad_norm": 17.60734748840332,
      "learning_rate": 1.9231259526118887e-05,
      "loss": 2.2293,
      "step": 37490
    },
    {
      "epoch": 5.196064846889289,
      "grad_norm": 10.210406303405762,
      "learning_rate": 1.9225717056948872e-05,
      "loss": 1.9479,
      "step": 37500
    },
    {
      "epoch": 5.197450464181793,
      "grad_norm": 14.065165519714355,
      "learning_rate": 1.9220174587778857e-05,
      "loss": 2.2487,
      "step": 37510
    },
    {
      "epoch": 5.198836081474297,
      "grad_norm": 8.55102252960205,
      "learning_rate": 1.9214632118608843e-05,
      "loss": 1.8512,
      "step": 37520
    },
    {
      "epoch": 5.200221698766801,
      "grad_norm": 14.065845489501953,
      "learning_rate": 1.9209089649438825e-05,
      "loss": 2.0949,
      "step": 37530
    },
    {
      "epoch": 5.201607316059304,
      "grad_norm": 10.2001314163208,
      "learning_rate": 1.920354718026881e-05,
      "loss": 2.3202,
      "step": 37540
    },
    {
      "epoch": 5.202992933351808,
      "grad_norm": 10.984148979187012,
      "learning_rate": 1.9198004711098795e-05,
      "loss": 2.1926,
      "step": 37550
    },
    {
      "epoch": 5.204378550644312,
      "grad_norm": 13.590551376342773,
      "learning_rate": 1.919246224192878e-05,
      "loss": 1.9666,
      "step": 37560
    },
    {
      "epoch": 5.205764167936816,
      "grad_norm": 19.05424690246582,
      "learning_rate": 1.9186919772758766e-05,
      "loss": 1.6668,
      "step": 37570
    },
    {
      "epoch": 5.20714978522932,
      "grad_norm": 9.663233757019043,
      "learning_rate": 1.918137730358875e-05,
      "loss": 1.5452,
      "step": 37580
    },
    {
      "epoch": 5.208535402521823,
      "grad_norm": 21.81199836730957,
      "learning_rate": 1.9175834834418736e-05,
      "loss": 2.343,
      "step": 37590
    },
    {
      "epoch": 5.2099210198143275,
      "grad_norm": 16.251800537109375,
      "learning_rate": 1.9170292365248718e-05,
      "loss": 1.9184,
      "step": 37600
    },
    {
      "epoch": 5.211306637106831,
      "grad_norm": 14.142505645751953,
      "learning_rate": 1.9164749896078703e-05,
      "loss": 2.1945,
      "step": 37610
    },
    {
      "epoch": 5.212692254399335,
      "grad_norm": 13.454514503479004,
      "learning_rate": 1.9159207426908692e-05,
      "loss": 2.4193,
      "step": 37620
    },
    {
      "epoch": 5.214077871691838,
      "grad_norm": 14.03705883026123,
      "learning_rate": 1.9153664957738674e-05,
      "loss": 2.0184,
      "step": 37630
    },
    {
      "epoch": 5.2154634889843425,
      "grad_norm": 14.766840934753418,
      "learning_rate": 1.914812248856866e-05,
      "loss": 1.7785,
      "step": 37640
    },
    {
      "epoch": 5.216849106276847,
      "grad_norm": 16.109148025512695,
      "learning_rate": 1.9142580019398644e-05,
      "loss": 2.3956,
      "step": 37650
    },
    {
      "epoch": 5.21823472356935,
      "grad_norm": 5.955911636352539,
      "learning_rate": 1.9137037550228626e-05,
      "loss": 1.7613,
      "step": 37660
    },
    {
      "epoch": 5.219620340861854,
      "grad_norm": 9.342611312866211,
      "learning_rate": 1.9131495081058615e-05,
      "loss": 2.4379,
      "step": 37670
    },
    {
      "epoch": 5.2210059581543575,
      "grad_norm": 8.129448890686035,
      "learning_rate": 1.91259526118886e-05,
      "loss": 1.9047,
      "step": 37680
    },
    {
      "epoch": 5.222391575446862,
      "grad_norm": 9.972373008728027,
      "learning_rate": 1.9120410142718582e-05,
      "loss": 2.0672,
      "step": 37690
    },
    {
      "epoch": 5.223777192739365,
      "grad_norm": 19.128925323486328,
      "learning_rate": 1.9114867673548567e-05,
      "loss": 2.4319,
      "step": 37700
    },
    {
      "epoch": 5.225162810031869,
      "grad_norm": 18.96475601196289,
      "learning_rate": 1.9109325204378552e-05,
      "loss": 1.8856,
      "step": 37710
    },
    {
      "epoch": 5.226548427324373,
      "grad_norm": 13.203413963317871,
      "learning_rate": 1.9103782735208538e-05,
      "loss": 2.0136,
      "step": 37720
    },
    {
      "epoch": 5.227934044616877,
      "grad_norm": 10.731132507324219,
      "learning_rate": 1.9098240266038523e-05,
      "loss": 2.2969,
      "step": 37730
    },
    {
      "epoch": 5.229319661909381,
      "grad_norm": 11.008134841918945,
      "learning_rate": 1.9092697796868508e-05,
      "loss": 1.9379,
      "step": 37740
    },
    {
      "epoch": 5.230705279201884,
      "grad_norm": 11.306922912597656,
      "learning_rate": 1.9087155327698493e-05,
      "loss": 2.1169,
      "step": 37750
    },
    {
      "epoch": 5.232090896494388,
      "grad_norm": 8.074495315551758,
      "learning_rate": 1.9081612858528475e-05,
      "loss": 1.5865,
      "step": 37760
    },
    {
      "epoch": 5.233476513786892,
      "grad_norm": 20.53139877319336,
      "learning_rate": 1.907607038935846e-05,
      "loss": 1.7401,
      "step": 37770
    },
    {
      "epoch": 5.234862131079396,
      "grad_norm": 24.551279067993164,
      "learning_rate": 1.9070527920188446e-05,
      "loss": 1.877,
      "step": 37780
    },
    {
      "epoch": 5.2362477483719,
      "grad_norm": 10.406293869018555,
      "learning_rate": 1.906498545101843e-05,
      "loss": 2.4268,
      "step": 37790
    },
    {
      "epoch": 5.237633365664403,
      "grad_norm": 12.788969993591309,
      "learning_rate": 1.9059442981848416e-05,
      "loss": 2.0838,
      "step": 37800
    },
    {
      "epoch": 5.239018982956908,
      "grad_norm": 11.312615394592285,
      "learning_rate": 1.90539005126784e-05,
      "loss": 1.8879,
      "step": 37810
    },
    {
      "epoch": 5.240404600249411,
      "grad_norm": 8.798255920410156,
      "learning_rate": 1.9048358043508383e-05,
      "loss": 1.872,
      "step": 37820
    },
    {
      "epoch": 5.241790217541915,
      "grad_norm": 16.152585983276367,
      "learning_rate": 1.904281557433837e-05,
      "loss": 1.7465,
      "step": 37830
    },
    {
      "epoch": 5.243175834834418,
      "grad_norm": 9.57911491394043,
      "learning_rate": 1.9037273105168354e-05,
      "loss": 2.0633,
      "step": 37840
    },
    {
      "epoch": 5.244561452126923,
      "grad_norm": 8.31596851348877,
      "learning_rate": 1.903173063599834e-05,
      "loss": 1.8707,
      "step": 37850
    },
    {
      "epoch": 5.245947069419426,
      "grad_norm": 9.445453643798828,
      "learning_rate": 1.9026188166828324e-05,
      "loss": 2.074,
      "step": 37860
    },
    {
      "epoch": 5.24733268671193,
      "grad_norm": 13.944107055664062,
      "learning_rate": 1.902064569765831e-05,
      "loss": 1.8609,
      "step": 37870
    },
    {
      "epoch": 5.248718304004434,
      "grad_norm": 11.434246063232422,
      "learning_rate": 1.9015103228488295e-05,
      "loss": 1.7721,
      "step": 37880
    },
    {
      "epoch": 5.250103921296938,
      "grad_norm": 14.756336212158203,
      "learning_rate": 1.9009560759318277e-05,
      "loss": 2.2572,
      "step": 37890
    },
    {
      "epoch": 5.251489538589442,
      "grad_norm": 9.365700721740723,
      "learning_rate": 1.9004018290148262e-05,
      "loss": 1.5395,
      "step": 37900
    },
    {
      "epoch": 5.252875155881945,
      "grad_norm": 15.158693313598633,
      "learning_rate": 1.8998475820978247e-05,
      "loss": 1.8492,
      "step": 37910
    },
    {
      "epoch": 5.254260773174449,
      "grad_norm": 12.018024444580078,
      "learning_rate": 1.8992933351808232e-05,
      "loss": 2.3947,
      "step": 37920
    },
    {
      "epoch": 5.2556463904669535,
      "grad_norm": 13.333576202392578,
      "learning_rate": 1.8987390882638218e-05,
      "loss": 2.0486,
      "step": 37930
    },
    {
      "epoch": 5.257032007759457,
      "grad_norm": 15.204305648803711,
      "learning_rate": 1.8981848413468203e-05,
      "loss": 1.8868,
      "step": 37940
    },
    {
      "epoch": 5.258417625051961,
      "grad_norm": 18.591001510620117,
      "learning_rate": 1.8976305944298185e-05,
      "loss": 1.7283,
      "step": 37950
    },
    {
      "epoch": 5.259803242344464,
      "grad_norm": 10.383411407470703,
      "learning_rate": 1.897076347512817e-05,
      "loss": 1.8807,
      "step": 37960
    },
    {
      "epoch": 5.2611888596369685,
      "grad_norm": 16.990793228149414,
      "learning_rate": 1.8965221005958155e-05,
      "loss": 2.2298,
      "step": 37970
    },
    {
      "epoch": 5.262574476929472,
      "grad_norm": 12.545577049255371,
      "learning_rate": 1.895967853678814e-05,
      "loss": 2.0324,
      "step": 37980
    },
    {
      "epoch": 5.263960094221976,
      "grad_norm": 7.471199035644531,
      "learning_rate": 1.8954136067618126e-05,
      "loss": 2.138,
      "step": 37990
    },
    {
      "epoch": 5.265345711514479,
      "grad_norm": 15.823275566101074,
      "learning_rate": 1.894859359844811e-05,
      "loss": 1.8062,
      "step": 38000
    },
    {
      "epoch": 5.2667313288069835,
      "grad_norm": 11.625103950500488,
      "learning_rate": 1.8943051129278093e-05,
      "loss": 1.3611,
      "step": 38010
    },
    {
      "epoch": 5.268116946099488,
      "grad_norm": 11.561930656433105,
      "learning_rate": 1.8937508660108078e-05,
      "loss": 1.9507,
      "step": 38020
    },
    {
      "epoch": 5.269502563391991,
      "grad_norm": 13.293280601501465,
      "learning_rate": 1.8931966190938063e-05,
      "loss": 2.1305,
      "step": 38030
    },
    {
      "epoch": 5.270888180684495,
      "grad_norm": 13.162732124328613,
      "learning_rate": 1.892642372176805e-05,
      "loss": 1.9709,
      "step": 38040
    },
    {
      "epoch": 5.2722737979769985,
      "grad_norm": 14.07027816772461,
      "learning_rate": 1.8920881252598034e-05,
      "loss": 1.6206,
      "step": 38050
    },
    {
      "epoch": 5.273659415269503,
      "grad_norm": 10.396127700805664,
      "learning_rate": 1.891533878342802e-05,
      "loss": 1.9246,
      "step": 38060
    },
    {
      "epoch": 5.275045032562006,
      "grad_norm": 9.458515167236328,
      "learning_rate": 1.8909796314258004e-05,
      "loss": 2.2629,
      "step": 38070
    },
    {
      "epoch": 5.27643064985451,
      "grad_norm": 14.022491455078125,
      "learning_rate": 1.8904253845087986e-05,
      "loss": 2.1631,
      "step": 38080
    },
    {
      "epoch": 5.277816267147014,
      "grad_norm": 19.582651138305664,
      "learning_rate": 1.889871137591797e-05,
      "loss": 2.4105,
      "step": 38090
    },
    {
      "epoch": 5.279201884439518,
      "grad_norm": 13.180492401123047,
      "learning_rate": 1.889316890674796e-05,
      "loss": 2.2705,
      "step": 38100
    },
    {
      "epoch": 5.280587501732022,
      "grad_norm": 8.254496574401855,
      "learning_rate": 1.8887626437577942e-05,
      "loss": 1.7623,
      "step": 38110
    },
    {
      "epoch": 5.281973119024525,
      "grad_norm": 9.33385181427002,
      "learning_rate": 1.8882083968407927e-05,
      "loss": 1.8175,
      "step": 38120
    },
    {
      "epoch": 5.283358736317029,
      "grad_norm": 13.185625076293945,
      "learning_rate": 1.8876541499237912e-05,
      "loss": 2.2018,
      "step": 38130
    },
    {
      "epoch": 5.284744353609533,
      "grad_norm": 12.313494682312012,
      "learning_rate": 1.8870999030067894e-05,
      "loss": 1.7466,
      "step": 38140
    },
    {
      "epoch": 5.286129970902037,
      "grad_norm": 10.4797945022583,
      "learning_rate": 1.8865456560897883e-05,
      "loss": 1.7788,
      "step": 38150
    },
    {
      "epoch": 5.287515588194541,
      "grad_norm": 6.097790241241455,
      "learning_rate": 1.8859914091727868e-05,
      "loss": 1.5139,
      "step": 38160
    },
    {
      "epoch": 5.288901205487044,
      "grad_norm": 11.210539817810059,
      "learning_rate": 1.8854371622557853e-05,
      "loss": 2.2186,
      "step": 38170
    },
    {
      "epoch": 5.2902868227795485,
      "grad_norm": 18.055971145629883,
      "learning_rate": 1.8848829153387835e-05,
      "loss": 1.8486,
      "step": 38180
    },
    {
      "epoch": 5.291672440072052,
      "grad_norm": 20.827302932739258,
      "learning_rate": 1.884328668421782e-05,
      "loss": 1.773,
      "step": 38190
    },
    {
      "epoch": 5.293058057364556,
      "grad_norm": 15.178960800170898,
      "learning_rate": 1.8837744215047806e-05,
      "loss": 2.2938,
      "step": 38200
    },
    {
      "epoch": 5.294443674657059,
      "grad_norm": 9.20780086517334,
      "learning_rate": 1.883220174587779e-05,
      "loss": 2.2374,
      "step": 38210
    },
    {
      "epoch": 5.2958292919495635,
      "grad_norm": 10.987449645996094,
      "learning_rate": 1.8826659276707776e-05,
      "loss": 2.2726,
      "step": 38220
    },
    {
      "epoch": 5.297214909242068,
      "grad_norm": 13.350323677062988,
      "learning_rate": 1.882111680753776e-05,
      "loss": 2.1999,
      "step": 38230
    },
    {
      "epoch": 5.298600526534571,
      "grad_norm": 15.722618103027344,
      "learning_rate": 1.8815574338367743e-05,
      "loss": 1.8707,
      "step": 38240
    },
    {
      "epoch": 5.299986143827075,
      "grad_norm": 14.0242919921875,
      "learning_rate": 1.881003186919773e-05,
      "loss": 2.1231,
      "step": 38250
    },
    {
      "epoch": 5.3013717611195785,
      "grad_norm": 13.453991889953613,
      "learning_rate": 1.8804489400027714e-05,
      "loss": 1.7863,
      "step": 38260
    },
    {
      "epoch": 5.302757378412083,
      "grad_norm": 12.011960983276367,
      "learning_rate": 1.87989469308577e-05,
      "loss": 2.1591,
      "step": 38270
    },
    {
      "epoch": 5.304142995704586,
      "grad_norm": 14.593427658081055,
      "learning_rate": 1.8793404461687684e-05,
      "loss": 2.7637,
      "step": 38280
    },
    {
      "epoch": 5.30552861299709,
      "grad_norm": 11.2474365234375,
      "learning_rate": 1.878786199251767e-05,
      "loss": 1.6068,
      "step": 38290
    },
    {
      "epoch": 5.306914230289594,
      "grad_norm": 20.3233699798584,
      "learning_rate": 1.878231952334765e-05,
      "loss": 2.2938,
      "step": 38300
    },
    {
      "epoch": 5.308299847582098,
      "grad_norm": 25.912071228027344,
      "learning_rate": 1.8776777054177637e-05,
      "loss": 1.968,
      "step": 38310
    },
    {
      "epoch": 5.309685464874602,
      "grad_norm": 12.980987548828125,
      "learning_rate": 1.8771234585007622e-05,
      "loss": 1.9519,
      "step": 38320
    },
    {
      "epoch": 5.311071082167105,
      "grad_norm": 9.989997863769531,
      "learning_rate": 1.8765692115837607e-05,
      "loss": 2.4625,
      "step": 38330
    },
    {
      "epoch": 5.312456699459609,
      "grad_norm": 22.337732315063477,
      "learning_rate": 1.8760149646667592e-05,
      "loss": 2.0206,
      "step": 38340
    },
    {
      "epoch": 5.313842316752113,
      "grad_norm": 16.885454177856445,
      "learning_rate": 1.8754607177497578e-05,
      "loss": 1.812,
      "step": 38350
    },
    {
      "epoch": 5.315227934044617,
      "grad_norm": 12.591182708740234,
      "learning_rate": 1.8749064708327563e-05,
      "loss": 2.0173,
      "step": 38360
    },
    {
      "epoch": 5.316613551337121,
      "grad_norm": 12.708826065063477,
      "learning_rate": 1.8743522239157545e-05,
      "loss": 2.1369,
      "step": 38370
    },
    {
      "epoch": 5.317999168629624,
      "grad_norm": 10.413412094116211,
      "learning_rate": 1.873797976998753e-05,
      "loss": 2.0138,
      "step": 38380
    },
    {
      "epoch": 5.319384785922129,
      "grad_norm": 13.424654006958008,
      "learning_rate": 1.8732437300817515e-05,
      "loss": 2.444,
      "step": 38390
    },
    {
      "epoch": 5.320770403214632,
      "grad_norm": 10.77574348449707,
      "learning_rate": 1.87268948316475e-05,
      "loss": 1.7239,
      "step": 38400
    },
    {
      "epoch": 5.322156020507136,
      "grad_norm": 16.037195205688477,
      "learning_rate": 1.8721352362477486e-05,
      "loss": 2.0773,
      "step": 38410
    },
    {
      "epoch": 5.323541637799639,
      "grad_norm": 15.230818748474121,
      "learning_rate": 1.871580989330747e-05,
      "loss": 1.7902,
      "step": 38420
    },
    {
      "epoch": 5.324927255092144,
      "grad_norm": 7.204118251800537,
      "learning_rate": 1.8710267424137453e-05,
      "loss": 1.8491,
      "step": 38430
    },
    {
      "epoch": 5.326312872384648,
      "grad_norm": 12.063908576965332,
      "learning_rate": 1.8704724954967438e-05,
      "loss": 2.1999,
      "step": 38440
    },
    {
      "epoch": 5.327698489677151,
      "grad_norm": 12.727645874023438,
      "learning_rate": 1.8699182485797423e-05,
      "loss": 2.2044,
      "step": 38450
    },
    {
      "epoch": 5.329084106969655,
      "grad_norm": 13.466582298278809,
      "learning_rate": 1.869364001662741e-05,
      "loss": 1.6083,
      "step": 38460
    },
    {
      "epoch": 5.330469724262159,
      "grad_norm": 14.440790176391602,
      "learning_rate": 1.8688097547457394e-05,
      "loss": 2.0882,
      "step": 38470
    },
    {
      "epoch": 5.331855341554663,
      "grad_norm": 13.253246307373047,
      "learning_rate": 1.868310932520438e-05,
      "loss": 2.1268,
      "step": 38480
    },
    {
      "epoch": 5.333240958847166,
      "grad_norm": 5.248053550720215,
      "learning_rate": 1.8677566856034366e-05,
      "loss": 1.914,
      "step": 38490
    },
    {
      "epoch": 5.33462657613967,
      "grad_norm": 12.951661109924316,
      "learning_rate": 1.867202438686435e-05,
      "loss": 1.4414,
      "step": 38500
    },
    {
      "epoch": 5.336012193432174,
      "grad_norm": 15.902310371398926,
      "learning_rate": 1.8666481917694336e-05,
      "loss": 1.5484,
      "step": 38510
    },
    {
      "epoch": 5.337397810724678,
      "grad_norm": 9.487837791442871,
      "learning_rate": 1.866093944852432e-05,
      "loss": 2.0302,
      "step": 38520
    },
    {
      "epoch": 5.338783428017182,
      "grad_norm": 16.326889038085938,
      "learning_rate": 1.8655396979354303e-05,
      "loss": 1.8362,
      "step": 38530
    },
    {
      "epoch": 5.340169045309685,
      "grad_norm": 12.060867309570312,
      "learning_rate": 1.864985451018429e-05,
      "loss": 1.7538,
      "step": 38540
    },
    {
      "epoch": 5.3415546626021895,
      "grad_norm": 10.37801742553711,
      "learning_rate": 1.8644312041014274e-05,
      "loss": 1.937,
      "step": 38550
    },
    {
      "epoch": 5.342940279894693,
      "grad_norm": 14.096158981323242,
      "learning_rate": 1.863876957184426e-05,
      "loss": 2.3499,
      "step": 38560
    },
    {
      "epoch": 5.344325897187197,
      "grad_norm": 15.359527587890625,
      "learning_rate": 1.8633227102674244e-05,
      "loss": 2.0944,
      "step": 38570
    },
    {
      "epoch": 5.345711514479701,
      "grad_norm": 10.39476203918457,
      "learning_rate": 1.862768463350423e-05,
      "loss": 1.7622,
      "step": 38580
    },
    {
      "epoch": 5.3470971317722045,
      "grad_norm": 12.297741889953613,
      "learning_rate": 1.862214216433421e-05,
      "loss": 1.9003,
      "step": 38590
    },
    {
      "epoch": 5.348482749064709,
      "grad_norm": 8.20840835571289,
      "learning_rate": 1.8616599695164197e-05,
      "loss": 2.2266,
      "step": 38600
    },
    {
      "epoch": 5.349868366357212,
      "grad_norm": 16.72168731689453,
      "learning_rate": 1.8611057225994182e-05,
      "loss": 1.7586,
      "step": 38610
    },
    {
      "epoch": 5.351253983649716,
      "grad_norm": 17.746540069580078,
      "learning_rate": 1.8605514756824167e-05,
      "loss": 1.9273,
      "step": 38620
    },
    {
      "epoch": 5.3526396009422195,
      "grad_norm": 8.089315414428711,
      "learning_rate": 1.8599972287654152e-05,
      "loss": 2.208,
      "step": 38630
    },
    {
      "epoch": 5.354025218234724,
      "grad_norm": 14.112203598022461,
      "learning_rate": 1.8594429818484138e-05,
      "loss": 2.0611,
      "step": 38640
    },
    {
      "epoch": 5.355410835527227,
      "grad_norm": 15.382944107055664,
      "learning_rate": 1.8588887349314123e-05,
      "loss": 1.7462,
      "step": 38650
    },
    {
      "epoch": 5.356796452819731,
      "grad_norm": 13.83362865447998,
      "learning_rate": 1.8583344880144105e-05,
      "loss": 1.869,
      "step": 38660
    },
    {
      "epoch": 5.358182070112235,
      "grad_norm": 14.830461502075195,
      "learning_rate": 1.857780241097409e-05,
      "loss": 2.1518,
      "step": 38670
    },
    {
      "epoch": 5.359567687404739,
      "grad_norm": 11.819975852966309,
      "learning_rate": 1.8572259941804075e-05,
      "loss": 2.1284,
      "step": 38680
    },
    {
      "epoch": 5.360953304697243,
      "grad_norm": 7.850958824157715,
      "learning_rate": 1.856671747263406e-05,
      "loss": 2.2593,
      "step": 38690
    },
    {
      "epoch": 5.362338921989746,
      "grad_norm": 13.574095726013184,
      "learning_rate": 1.8561175003464046e-05,
      "loss": 2.1227,
      "step": 38700
    },
    {
      "epoch": 5.36372453928225,
      "grad_norm": 7.207884311676025,
      "learning_rate": 1.855563253429403e-05,
      "loss": 2.0692,
      "step": 38710
    },
    {
      "epoch": 5.365110156574754,
      "grad_norm": 23.002769470214844,
      "learning_rate": 1.8550090065124013e-05,
      "loss": 1.9964,
      "step": 38720
    },
    {
      "epoch": 5.366495773867258,
      "grad_norm": 9.586334228515625,
      "learning_rate": 1.8544547595953998e-05,
      "loss": 1.9017,
      "step": 38730
    },
    {
      "epoch": 5.367881391159762,
      "grad_norm": 11.549446105957031,
      "learning_rate": 1.8539005126783983e-05,
      "loss": 2.0044,
      "step": 38740
    },
    {
      "epoch": 5.369267008452265,
      "grad_norm": 18.624113082885742,
      "learning_rate": 1.853346265761397e-05,
      "loss": 1.8186,
      "step": 38750
    },
    {
      "epoch": 5.3706526257447695,
      "grad_norm": 8.002826690673828,
      "learning_rate": 1.8527920188443954e-05,
      "loss": 1.8522,
      "step": 38760
    },
    {
      "epoch": 5.372038243037273,
      "grad_norm": 14.089446067810059,
      "learning_rate": 1.852237771927394e-05,
      "loss": 1.9995,
      "step": 38770
    },
    {
      "epoch": 5.373423860329777,
      "grad_norm": 16.191246032714844,
      "learning_rate": 1.8516835250103924e-05,
      "loss": 1.8106,
      "step": 38780
    },
    {
      "epoch": 5.37480947762228,
      "grad_norm": 12.152914047241211,
      "learning_rate": 1.8511292780933906e-05,
      "loss": 2.1272,
      "step": 38790
    },
    {
      "epoch": 5.3761950949147845,
      "grad_norm": 12.550357818603516,
      "learning_rate": 1.850575031176389e-05,
      "loss": 1.665,
      "step": 38800
    },
    {
      "epoch": 5.377580712207289,
      "grad_norm": 13.525447845458984,
      "learning_rate": 1.8500207842593877e-05,
      "loss": 1.886,
      "step": 38810
    },
    {
      "epoch": 5.378966329499792,
      "grad_norm": 13.29626178741455,
      "learning_rate": 1.8494665373423862e-05,
      "loss": 2.0871,
      "step": 38820
    },
    {
      "epoch": 5.380351946792296,
      "grad_norm": 20.63917350769043,
      "learning_rate": 1.8489122904253847e-05,
      "loss": 2.0978,
      "step": 38830
    },
    {
      "epoch": 5.3817375640847995,
      "grad_norm": 10.590506553649902,
      "learning_rate": 1.8483580435083832e-05,
      "loss": 2.1112,
      "step": 38840
    },
    {
      "epoch": 5.383123181377304,
      "grad_norm": 17.7248477935791,
      "learning_rate": 1.8478037965913814e-05,
      "loss": 1.5994,
      "step": 38850
    },
    {
      "epoch": 5.384508798669807,
      "grad_norm": 10.905513763427734,
      "learning_rate": 1.84724954967438e-05,
      "loss": 1.6142,
      "step": 38860
    },
    {
      "epoch": 5.385894415962311,
      "grad_norm": 8.056899070739746,
      "learning_rate": 1.8466953027573785e-05,
      "loss": 1.8722,
      "step": 38870
    },
    {
      "epoch": 5.387280033254815,
      "grad_norm": 14.068692207336426,
      "learning_rate": 1.846141055840377e-05,
      "loss": 2.0468,
      "step": 38880
    },
    {
      "epoch": 5.388665650547319,
      "grad_norm": 10.835184097290039,
      "learning_rate": 1.8455868089233755e-05,
      "loss": 1.9189,
      "step": 38890
    },
    {
      "epoch": 5.390051267839823,
      "grad_norm": 7.899975299835205,
      "learning_rate": 1.845032562006374e-05,
      "loss": 1.9451,
      "step": 38900
    },
    {
      "epoch": 5.391436885132326,
      "grad_norm": 10.249040603637695,
      "learning_rate": 1.8444783150893722e-05,
      "loss": 2.05,
      "step": 38910
    },
    {
      "epoch": 5.39282250242483,
      "grad_norm": 14.031807899475098,
      "learning_rate": 1.8439240681723708e-05,
      "loss": 2.2809,
      "step": 38920
    },
    {
      "epoch": 5.394208119717334,
      "grad_norm": 12.13465690612793,
      "learning_rate": 1.8433698212553696e-05,
      "loss": 2.3091,
      "step": 38930
    },
    {
      "epoch": 5.395593737009838,
      "grad_norm": 21.579025268554688,
      "learning_rate": 1.842815574338368e-05,
      "loss": 1.7966,
      "step": 38940
    },
    {
      "epoch": 5.396979354302342,
      "grad_norm": 9.51370906829834,
      "learning_rate": 1.8422613274213663e-05,
      "loss": 1.7028,
      "step": 38950
    },
    {
      "epoch": 5.398364971594845,
      "grad_norm": 8.048747062683105,
      "learning_rate": 1.841707080504365e-05,
      "loss": 1.7815,
      "step": 38960
    },
    {
      "epoch": 5.39975058888735,
      "grad_norm": 8.15406322479248,
      "learning_rate": 1.8411528335873634e-05,
      "loss": 1.9575,
      "step": 38970
    },
    {
      "epoch": 5.401136206179853,
      "grad_norm": 19.57425308227539,
      "learning_rate": 1.8405985866703616e-05,
      "loss": 2.3551,
      "step": 38980
    },
    {
      "epoch": 5.402521823472357,
      "grad_norm": 9.684027671813965,
      "learning_rate": 1.8400443397533604e-05,
      "loss": 1.9053,
      "step": 38990
    },
    {
      "epoch": 5.40390744076486,
      "grad_norm": 8.675026893615723,
      "learning_rate": 1.839490092836359e-05,
      "loss": 1.9668,
      "step": 39000
    },
    {
      "epoch": 5.405293058057365,
      "grad_norm": 9.379688262939453,
      "learning_rate": 1.838935845919357e-05,
      "loss": 2.1223,
      "step": 39010
    },
    {
      "epoch": 5.406678675349868,
      "grad_norm": 13.408716201782227,
      "learning_rate": 1.8383815990023557e-05,
      "loss": 2.0625,
      "step": 39020
    },
    {
      "epoch": 5.408064292642372,
      "grad_norm": 11.792187690734863,
      "learning_rate": 1.8378827767770543e-05,
      "loss": 1.7665,
      "step": 39030
    },
    {
      "epoch": 5.409449909934876,
      "grad_norm": 10.194378852844238,
      "learning_rate": 1.837328529860053e-05,
      "loss": 1.9975,
      "step": 39040
    },
    {
      "epoch": 5.41083552722738,
      "grad_norm": 18.501771926879883,
      "learning_rate": 1.8367742829430514e-05,
      "loss": 2.0936,
      "step": 39050
    },
    {
      "epoch": 5.412221144519884,
      "grad_norm": 20.18404769897461,
      "learning_rate": 1.83622003602605e-05,
      "loss": 1.7183,
      "step": 39060
    },
    {
      "epoch": 5.413606761812387,
      "grad_norm": 17.90876579284668,
      "learning_rate": 1.835665789109048e-05,
      "loss": 2.0126,
      "step": 39070
    },
    {
      "epoch": 5.414992379104891,
      "grad_norm": 14.505170822143555,
      "learning_rate": 1.8351115421920466e-05,
      "loss": 2.1698,
      "step": 39080
    },
    {
      "epoch": 5.4163779963973955,
      "grad_norm": 13.409921646118164,
      "learning_rate": 1.834557295275045e-05,
      "loss": 2.5565,
      "step": 39090
    },
    {
      "epoch": 5.417763613689899,
      "grad_norm": 13.968563079833984,
      "learning_rate": 1.8340030483580436e-05,
      "loss": 1.8239,
      "step": 39100
    },
    {
      "epoch": 5.419149230982403,
      "grad_norm": 11.108744621276855,
      "learning_rate": 1.833448801441042e-05,
      "loss": 2.5229,
      "step": 39110
    },
    {
      "epoch": 5.420534848274906,
      "grad_norm": 12.188756942749023,
      "learning_rate": 1.8328945545240407e-05,
      "loss": 2.1568,
      "step": 39120
    },
    {
      "epoch": 5.4219204655674105,
      "grad_norm": 18.587617874145508,
      "learning_rate": 1.8323403076070392e-05,
      "loss": 1.8545,
      "step": 39130
    },
    {
      "epoch": 5.423306082859914,
      "grad_norm": 7.374874591827393,
      "learning_rate": 1.8317860606900374e-05,
      "loss": 1.9821,
      "step": 39140
    },
    {
      "epoch": 5.424691700152418,
      "grad_norm": 11.693741798400879,
      "learning_rate": 1.831231813773036e-05,
      "loss": 2.0417,
      "step": 39150
    },
    {
      "epoch": 5.426077317444921,
      "grad_norm": 15.04794979095459,
      "learning_rate": 1.8306775668560345e-05,
      "loss": 2.3036,
      "step": 39160
    },
    {
      "epoch": 5.4274629347374255,
      "grad_norm": 14.366527557373047,
      "learning_rate": 1.830123319939033e-05,
      "loss": 1.8228,
      "step": 39170
    },
    {
      "epoch": 5.42884855202993,
      "grad_norm": 10.166998863220215,
      "learning_rate": 1.8295690730220315e-05,
      "loss": 1.7564,
      "step": 39180
    },
    {
      "epoch": 5.430234169322433,
      "grad_norm": 10.475305557250977,
      "learning_rate": 1.82901482610503e-05,
      "loss": 1.6399,
      "step": 39190
    },
    {
      "epoch": 5.431619786614937,
      "grad_norm": 8.16795539855957,
      "learning_rate": 1.8284605791880282e-05,
      "loss": 1.8737,
      "step": 39200
    },
    {
      "epoch": 5.4330054039074405,
      "grad_norm": 10.113653182983398,
      "learning_rate": 1.8279063322710267e-05,
      "loss": 1.7527,
      "step": 39210
    },
    {
      "epoch": 5.434391021199945,
      "grad_norm": 10.582216262817383,
      "learning_rate": 1.8273520853540253e-05,
      "loss": 1.3483,
      "step": 39220
    },
    {
      "epoch": 5.435776638492449,
      "grad_norm": 4.122889995574951,
      "learning_rate": 1.8267978384370238e-05,
      "loss": 1.855,
      "step": 39230
    },
    {
      "epoch": 5.437162255784952,
      "grad_norm": 8.78541374206543,
      "learning_rate": 1.8262435915200223e-05,
      "loss": 1.8805,
      "step": 39240
    },
    {
      "epoch": 5.438547873077456,
      "grad_norm": 13.916864395141602,
      "learning_rate": 1.825689344603021e-05,
      "loss": 2.2336,
      "step": 39250
    },
    {
      "epoch": 5.43993349036996,
      "grad_norm": 7.341579914093018,
      "learning_rate": 1.8251350976860194e-05,
      "loss": 1.6719,
      "step": 39260
    },
    {
      "epoch": 5.441319107662464,
      "grad_norm": 6.527288436889648,
      "learning_rate": 1.8245808507690175e-05,
      "loss": 1.9073,
      "step": 39270
    },
    {
      "epoch": 5.442704724954967,
      "grad_norm": 13.243074417114258,
      "learning_rate": 1.824026603852016e-05,
      "loss": 1.7914,
      "step": 39280
    },
    {
      "epoch": 5.444090342247471,
      "grad_norm": 23.003938674926758,
      "learning_rate": 1.823472356935015e-05,
      "loss": 1.9192,
      "step": 39290
    },
    {
      "epoch": 5.445475959539975,
      "grad_norm": 11.751503944396973,
      "learning_rate": 1.822918110018013e-05,
      "loss": 1.7589,
      "step": 39300
    },
    {
      "epoch": 5.446861576832479,
      "grad_norm": 15.483162879943848,
      "learning_rate": 1.8223638631010116e-05,
      "loss": 1.8715,
      "step": 39310
    },
    {
      "epoch": 5.448247194124983,
      "grad_norm": 7.502552509307861,
      "learning_rate": 1.8218096161840102e-05,
      "loss": 1.9685,
      "step": 39320
    },
    {
      "epoch": 5.449632811417486,
      "grad_norm": 11.48889446258545,
      "learning_rate": 1.8212553692670084e-05,
      "loss": 2.1886,
      "step": 39330
    },
    {
      "epoch": 5.4510184287099905,
      "grad_norm": 12.013524055480957,
      "learning_rate": 1.8207011223500072e-05,
      "loss": 2.1188,
      "step": 39340
    },
    {
      "epoch": 5.452404046002494,
      "grad_norm": 14.803979873657227,
      "learning_rate": 1.8201468754330057e-05,
      "loss": 1.8895,
      "step": 39350
    },
    {
      "epoch": 5.453789663294998,
      "grad_norm": 9.060125350952148,
      "learning_rate": 1.819592628516004e-05,
      "loss": 1.9955,
      "step": 39360
    },
    {
      "epoch": 5.455175280587501,
      "grad_norm": 16.015625,
      "learning_rate": 1.8190383815990025e-05,
      "loss": 2.2692,
      "step": 39370
    },
    {
      "epoch": 5.4565608978800055,
      "grad_norm": 9.788537979125977,
      "learning_rate": 1.818484134682001e-05,
      "loss": 1.9374,
      "step": 39380
    },
    {
      "epoch": 5.45794651517251,
      "grad_norm": 16.627235412597656,
      "learning_rate": 1.8179298877649995e-05,
      "loss": 1.9576,
      "step": 39390
    },
    {
      "epoch": 5.459332132465013,
      "grad_norm": 5.747241497039795,
      "learning_rate": 1.817375640847998e-05,
      "loss": 1.9997,
      "step": 39400
    },
    {
      "epoch": 5.460717749757517,
      "grad_norm": 13.052977561950684,
      "learning_rate": 1.8168213939309966e-05,
      "loss": 1.875,
      "step": 39410
    },
    {
      "epoch": 5.4621033670500205,
      "grad_norm": 8.898635864257812,
      "learning_rate": 1.816267147013995e-05,
      "loss": 2.2825,
      "step": 39420
    },
    {
      "epoch": 5.463488984342525,
      "grad_norm": 13.977038383483887,
      "learning_rate": 1.8157129000969933e-05,
      "loss": 1.6241,
      "step": 39430
    },
    {
      "epoch": 5.464874601635028,
      "grad_norm": 12.615649223327637,
      "learning_rate": 1.8151586531799918e-05,
      "loss": 1.9336,
      "step": 39440
    },
    {
      "epoch": 5.466260218927532,
      "grad_norm": 14.549769401550293,
      "learning_rate": 1.8146044062629903e-05,
      "loss": 2.1109,
      "step": 39450
    },
    {
      "epoch": 5.467645836220036,
      "grad_norm": 7.947686672210693,
      "learning_rate": 1.814050159345989e-05,
      "loss": 1.8618,
      "step": 39460
    },
    {
      "epoch": 5.46903145351254,
      "grad_norm": 11.243148803710938,
      "learning_rate": 1.8134959124289874e-05,
      "loss": 2.0939,
      "step": 39470
    },
    {
      "epoch": 5.470417070805044,
      "grad_norm": 16.891773223876953,
      "learning_rate": 1.812941665511986e-05,
      "loss": 2.0609,
      "step": 39480
    },
    {
      "epoch": 5.471802688097547,
      "grad_norm": 13.020264625549316,
      "learning_rate": 1.812387418594984e-05,
      "loss": 1.9441,
      "step": 39490
    },
    {
      "epoch": 5.473188305390051,
      "grad_norm": 11.937796592712402,
      "learning_rate": 1.8118331716779826e-05,
      "loss": 2.4513,
      "step": 39500
    },
    {
      "epoch": 5.474573922682555,
      "grad_norm": 14.146074295043945,
      "learning_rate": 1.811278924760981e-05,
      "loss": 1.8987,
      "step": 39510
    },
    {
      "epoch": 5.475959539975059,
      "grad_norm": 16.84547233581543,
      "learning_rate": 1.8107246778439797e-05,
      "loss": 2.1134,
      "step": 39520
    },
    {
      "epoch": 5.477345157267563,
      "grad_norm": 14.199714660644531,
      "learning_rate": 1.8101704309269782e-05,
      "loss": 2.0708,
      "step": 39530
    },
    {
      "epoch": 5.478730774560066,
      "grad_norm": 12.304996490478516,
      "learning_rate": 1.8096161840099767e-05,
      "loss": 1.8544,
      "step": 39540
    },
    {
      "epoch": 5.480116391852571,
      "grad_norm": 8.501302719116211,
      "learning_rate": 1.8090619370929752e-05,
      "loss": 2.0736,
      "step": 39550
    },
    {
      "epoch": 5.481502009145074,
      "grad_norm": 9.804855346679688,
      "learning_rate": 1.8085076901759734e-05,
      "loss": 2.18,
      "step": 39560
    },
    {
      "epoch": 5.482887626437578,
      "grad_norm": 18.943857192993164,
      "learning_rate": 1.807953443258972e-05,
      "loss": 2.1806,
      "step": 39570
    },
    {
      "epoch": 5.484273243730081,
      "grad_norm": 9.993718147277832,
      "learning_rate": 1.8073991963419705e-05,
      "loss": 2.039,
      "step": 39580
    },
    {
      "epoch": 5.485658861022586,
      "grad_norm": 10.779104232788086,
      "learning_rate": 1.806844949424969e-05,
      "loss": 1.7899,
      "step": 39590
    },
    {
      "epoch": 5.48704447831509,
      "grad_norm": 10.060744285583496,
      "learning_rate": 1.8062907025079675e-05,
      "loss": 1.6787,
      "step": 39600
    },
    {
      "epoch": 5.488430095607593,
      "grad_norm": 13.435760498046875,
      "learning_rate": 1.805736455590966e-05,
      "loss": 1.9896,
      "step": 39610
    },
    {
      "epoch": 5.489815712900097,
      "grad_norm": 16.326255798339844,
      "learning_rate": 1.8051822086739642e-05,
      "loss": 2.2313,
      "step": 39620
    },
    {
      "epoch": 5.491201330192601,
      "grad_norm": 13.803886413574219,
      "learning_rate": 1.8046279617569628e-05,
      "loss": 1.5521,
      "step": 39630
    },
    {
      "epoch": 5.492586947485105,
      "grad_norm": 8.486157417297363,
      "learning_rate": 1.8040737148399613e-05,
      "loss": 1.8991,
      "step": 39640
    },
    {
      "epoch": 5.493972564777608,
      "grad_norm": 14.363081932067871,
      "learning_rate": 1.8035194679229598e-05,
      "loss": 1.3972,
      "step": 39650
    },
    {
      "epoch": 5.495358182070112,
      "grad_norm": 12.576515197753906,
      "learning_rate": 1.8029652210059583e-05,
      "loss": 1.5255,
      "step": 39660
    },
    {
      "epoch": 5.496743799362616,
      "grad_norm": 16.402830123901367,
      "learning_rate": 1.802410974088957e-05,
      "loss": 1.9979,
      "step": 39670
    },
    {
      "epoch": 5.49812941665512,
      "grad_norm": 10.7949857711792,
      "learning_rate": 1.801856727171955e-05,
      "loss": 1.6853,
      "step": 39680
    },
    {
      "epoch": 5.499515033947624,
      "grad_norm": 15.81334114074707,
      "learning_rate": 1.8013024802549536e-05,
      "loss": 1.7528,
      "step": 39690
    },
    {
      "epoch": 5.500900651240127,
      "grad_norm": 12.226935386657715,
      "learning_rate": 1.800748233337952e-05,
      "loss": 2.0337,
      "step": 39700
    },
    {
      "epoch": 5.5022862685326315,
      "grad_norm": 10.319592475891113,
      "learning_rate": 1.8001939864209506e-05,
      "loss": 2.1295,
      "step": 39710
    },
    {
      "epoch": 5.503671885825135,
      "grad_norm": 22.491287231445312,
      "learning_rate": 1.799639739503949e-05,
      "loss": 1.7221,
      "step": 39720
    },
    {
      "epoch": 5.505057503117639,
      "grad_norm": 13.29658031463623,
      "learning_rate": 1.7990854925869477e-05,
      "loss": 2.0564,
      "step": 39730
    },
    {
      "epoch": 5.506443120410143,
      "grad_norm": 10.004170417785645,
      "learning_rate": 1.7985312456699462e-05,
      "loss": 1.8714,
      "step": 39740
    },
    {
      "epoch": 5.5078287377026465,
      "grad_norm": 10.223401069641113,
      "learning_rate": 1.7979769987529444e-05,
      "loss": 1.7711,
      "step": 39750
    },
    {
      "epoch": 5.509214354995151,
      "grad_norm": 13.944083213806152,
      "learning_rate": 1.797422751835943e-05,
      "loss": 2.0746,
      "step": 39760
    },
    {
      "epoch": 5.510599972287654,
      "grad_norm": 17.219484329223633,
      "learning_rate": 1.7968685049189418e-05,
      "loss": 2.1182,
      "step": 39770
    },
    {
      "epoch": 5.511985589580158,
      "grad_norm": 9.744941711425781,
      "learning_rate": 1.79631425800194e-05,
      "loss": 1.5897,
      "step": 39780
    },
    {
      "epoch": 5.5133712068726615,
      "grad_norm": 22.577404022216797,
      "learning_rate": 1.7957600110849385e-05,
      "loss": 2.0647,
      "step": 39790
    },
    {
      "epoch": 5.514756824165166,
      "grad_norm": 11.627582550048828,
      "learning_rate": 1.795205764167937e-05,
      "loss": 2.065,
      "step": 39800
    },
    {
      "epoch": 5.516142441457669,
      "grad_norm": 9.253589630126953,
      "learning_rate": 1.7946515172509352e-05,
      "loss": 2.0421,
      "step": 39810
    },
    {
      "epoch": 5.517528058750173,
      "grad_norm": 18.204259872436523,
      "learning_rate": 1.794097270333934e-05,
      "loss": 2.239,
      "step": 39820
    },
    {
      "epoch": 5.518913676042677,
      "grad_norm": 14.511768341064453,
      "learning_rate": 1.7935430234169326e-05,
      "loss": 1.7895,
      "step": 39830
    },
    {
      "epoch": 5.520299293335181,
      "grad_norm": 22.712112426757812,
      "learning_rate": 1.792988776499931e-05,
      "loss": 2.2652,
      "step": 39840
    },
    {
      "epoch": 5.521684910627685,
      "grad_norm": 12.348528861999512,
      "learning_rate": 1.7924345295829293e-05,
      "loss": 1.84,
      "step": 39850
    },
    {
      "epoch": 5.523070527920188,
      "grad_norm": 11.286969184875488,
      "learning_rate": 1.7918802826659278e-05,
      "loss": 2.0173,
      "step": 39860
    },
    {
      "epoch": 5.524456145212692,
      "grad_norm": 14.175888061523438,
      "learning_rate": 1.7913260357489263e-05,
      "loss": 2.1539,
      "step": 39870
    },
    {
      "epoch": 5.5258417625051965,
      "grad_norm": 15.39599323272705,
      "learning_rate": 1.790771788831925e-05,
      "loss": 2.5024,
      "step": 39880
    },
    {
      "epoch": 5.5272273797977,
      "grad_norm": 14.021663665771484,
      "learning_rate": 1.7902175419149234e-05,
      "loss": 1.8102,
      "step": 39890
    },
    {
      "epoch": 5.528612997090204,
      "grad_norm": 12.086057662963867,
      "learning_rate": 1.789663294997922e-05,
      "loss": 1.7629,
      "step": 39900
    },
    {
      "epoch": 5.529998614382707,
      "grad_norm": 8.999529838562012,
      "learning_rate": 1.78910904808092e-05,
      "loss": 1.7006,
      "step": 39910
    },
    {
      "epoch": 5.5313842316752115,
      "grad_norm": 23.07309341430664,
      "learning_rate": 1.7885548011639186e-05,
      "loss": 1.7491,
      "step": 39920
    },
    {
      "epoch": 5.532769848967715,
      "grad_norm": 12.18710708618164,
      "learning_rate": 1.788000554246917e-05,
      "loss": 2.2899,
      "step": 39930
    },
    {
      "epoch": 5.534155466260219,
      "grad_norm": 13.102611541748047,
      "learning_rate": 1.7874463073299157e-05,
      "loss": 1.9265,
      "step": 39940
    },
    {
      "epoch": 5.535541083552722,
      "grad_norm": 17.415151596069336,
      "learning_rate": 1.7868920604129142e-05,
      "loss": 2.1703,
      "step": 39950
    },
    {
      "epoch": 5.5369267008452265,
      "grad_norm": 16.778202056884766,
      "learning_rate": 1.7863378134959127e-05,
      "loss": 1.9587,
      "step": 39960
    },
    {
      "epoch": 5.538312318137731,
      "grad_norm": 17.89896583557129,
      "learning_rate": 1.785783566578911e-05,
      "loss": 2.1549,
      "step": 39970
    },
    {
      "epoch": 5.539697935430234,
      "grad_norm": 10.166406631469727,
      "learning_rate": 1.7852293196619094e-05,
      "loss": 1.9004,
      "step": 39980
    },
    {
      "epoch": 5.541083552722738,
      "grad_norm": 13.463860511779785,
      "learning_rate": 1.784675072744908e-05,
      "loss": 1.6876,
      "step": 39990
    },
    {
      "epoch": 5.5424691700152415,
      "grad_norm": 11.405385971069336,
      "learning_rate": 1.7841208258279065e-05,
      "loss": 1.971,
      "step": 40000
    },
    {
      "epoch": 5.543854787307746,
      "grad_norm": 15.306243896484375,
      "learning_rate": 1.783566578910905e-05,
      "loss": 2.3032,
      "step": 40010
    },
    {
      "epoch": 5.545240404600249,
      "grad_norm": 10.286144256591797,
      "learning_rate": 1.7830123319939035e-05,
      "loss": 1.8094,
      "step": 40020
    },
    {
      "epoch": 5.546626021892753,
      "grad_norm": 9.860997200012207,
      "learning_rate": 1.782458085076902e-05,
      "loss": 1.8265,
      "step": 40030
    },
    {
      "epoch": 5.548011639185257,
      "grad_norm": 10.71257209777832,
      "learning_rate": 1.7819038381599002e-05,
      "loss": 1.9293,
      "step": 40040
    },
    {
      "epoch": 5.549397256477761,
      "grad_norm": 21.77892303466797,
      "learning_rate": 1.7813495912428988e-05,
      "loss": 2.0717,
      "step": 40050
    },
    {
      "epoch": 5.550782873770265,
      "grad_norm": 16.81888198852539,
      "learning_rate": 1.7807953443258973e-05,
      "loss": 1.8716,
      "step": 40060
    },
    {
      "epoch": 5.552168491062768,
      "grad_norm": 12.291183471679688,
      "learning_rate": 1.7802410974088958e-05,
      "loss": 2.1045,
      "step": 40070
    },
    {
      "epoch": 5.553554108355272,
      "grad_norm": 25.244014739990234,
      "learning_rate": 1.7796868504918943e-05,
      "loss": 2.36,
      "step": 40080
    },
    {
      "epoch": 5.554939725647776,
      "grad_norm": 13.071319580078125,
      "learning_rate": 1.779132603574893e-05,
      "loss": 2.0392,
      "step": 40090
    },
    {
      "epoch": 5.55632534294028,
      "grad_norm": 15.484086990356445,
      "learning_rate": 1.778578356657891e-05,
      "loss": 1.8958,
      "step": 40100
    },
    {
      "epoch": 5.557710960232784,
      "grad_norm": 10.986255645751953,
      "learning_rate": 1.7780241097408896e-05,
      "loss": 1.825,
      "step": 40110
    },
    {
      "epoch": 5.559096577525287,
      "grad_norm": 11.653645515441895,
      "learning_rate": 1.777469862823888e-05,
      "loss": 1.8394,
      "step": 40120
    },
    {
      "epoch": 5.560482194817792,
      "grad_norm": 11.738969802856445,
      "learning_rate": 1.7769156159068866e-05,
      "loss": 2.4406,
      "step": 40130
    },
    {
      "epoch": 5.561867812110295,
      "grad_norm": 10.655721664428711,
      "learning_rate": 1.776361368989885e-05,
      "loss": 2.2047,
      "step": 40140
    },
    {
      "epoch": 5.563253429402799,
      "grad_norm": 19.36565589904785,
      "learning_rate": 1.7758071220728837e-05,
      "loss": 1.8571,
      "step": 40150
    },
    {
      "epoch": 5.564639046695302,
      "grad_norm": 14.844136238098145,
      "learning_rate": 1.7752528751558822e-05,
      "loss": 1.9852,
      "step": 40160
    },
    {
      "epoch": 5.566024663987807,
      "grad_norm": 10.81285285949707,
      "learning_rate": 1.7746986282388804e-05,
      "loss": 2.195,
      "step": 40170
    },
    {
      "epoch": 5.56741028128031,
      "grad_norm": 9.732172966003418,
      "learning_rate": 1.774144381321879e-05,
      "loss": 2.293,
      "step": 40180
    },
    {
      "epoch": 5.568795898572814,
      "grad_norm": 13.596015930175781,
      "learning_rate": 1.7735901344048774e-05,
      "loss": 2.1256,
      "step": 40190
    },
    {
      "epoch": 5.570181515865318,
      "grad_norm": 7.0514421463012695,
      "learning_rate": 1.773035887487876e-05,
      "loss": 1.9315,
      "step": 40200
    },
    {
      "epoch": 5.571567133157822,
      "grad_norm": 14.315813064575195,
      "learning_rate": 1.7724816405708745e-05,
      "loss": 1.8407,
      "step": 40210
    },
    {
      "epoch": 5.572952750450326,
      "grad_norm": 11.637760162353516,
      "learning_rate": 1.771927393653873e-05,
      "loss": 2.143,
      "step": 40220
    },
    {
      "epoch": 5.574338367742829,
      "grad_norm": 15.731086730957031,
      "learning_rate": 1.7713731467368712e-05,
      "loss": 1.8861,
      "step": 40230
    },
    {
      "epoch": 5.575723985035333,
      "grad_norm": 14.734130859375,
      "learning_rate": 1.7708188998198697e-05,
      "loss": 1.6187,
      "step": 40240
    },
    {
      "epoch": 5.5771096023278375,
      "grad_norm": 21.847946166992188,
      "learning_rate": 1.7702646529028686e-05,
      "loss": 2.0354,
      "step": 40250
    },
    {
      "epoch": 5.578495219620341,
      "grad_norm": 10.19093132019043,
      "learning_rate": 1.7697104059858668e-05,
      "loss": 1.6954,
      "step": 40260
    },
    {
      "epoch": 5.579880836912845,
      "grad_norm": 25.94215202331543,
      "learning_rate": 1.7691561590688653e-05,
      "loss": 1.5848,
      "step": 40270
    },
    {
      "epoch": 5.581266454205348,
      "grad_norm": 12.67419719696045,
      "learning_rate": 1.7686019121518638e-05,
      "loss": 1.968,
      "step": 40280
    },
    {
      "epoch": 5.5826520714978525,
      "grad_norm": 7.945352077484131,
      "learning_rate": 1.7680476652348623e-05,
      "loss": 2.4214,
      "step": 40290
    },
    {
      "epoch": 5.584037688790356,
      "grad_norm": 12.335981369018555,
      "learning_rate": 1.7674934183178605e-05,
      "loss": 1.9683,
      "step": 40300
    },
    {
      "epoch": 5.58542330608286,
      "grad_norm": 10.219983100891113,
      "learning_rate": 1.7669391714008594e-05,
      "loss": 2.1861,
      "step": 40310
    },
    {
      "epoch": 5.586808923375363,
      "grad_norm": 17.182804107666016,
      "learning_rate": 1.766384924483858e-05,
      "loss": 2.439,
      "step": 40320
    },
    {
      "epoch": 5.5881945406678675,
      "grad_norm": 13.156100273132324,
      "learning_rate": 1.765830677566856e-05,
      "loss": 2.1765,
      "step": 40330
    },
    {
      "epoch": 5.589580157960372,
      "grad_norm": 13.175324440002441,
      "learning_rate": 1.7652764306498546e-05,
      "loss": 1.8198,
      "step": 40340
    },
    {
      "epoch": 5.590965775252875,
      "grad_norm": 11.444809913635254,
      "learning_rate": 1.764722183732853e-05,
      "loss": 1.53,
      "step": 40350
    },
    {
      "epoch": 5.592351392545379,
      "grad_norm": 15.278779029846191,
      "learning_rate": 1.7641679368158517e-05,
      "loss": 2.2373,
      "step": 40360
    },
    {
      "epoch": 5.5937370098378825,
      "grad_norm": 16.680648803710938,
      "learning_rate": 1.7636136898988502e-05,
      "loss": 1.4561,
      "step": 40370
    },
    {
      "epoch": 5.595122627130387,
      "grad_norm": 9.232651710510254,
      "learning_rate": 1.7630594429818487e-05,
      "loss": 2.1592,
      "step": 40380
    },
    {
      "epoch": 5.596508244422891,
      "grad_norm": 14.24609375,
      "learning_rate": 1.762505196064847e-05,
      "loss": 1.8647,
      "step": 40390
    },
    {
      "epoch": 5.597893861715394,
      "grad_norm": 15.096287727355957,
      "learning_rate": 1.7619509491478454e-05,
      "loss": 2.1909,
      "step": 40400
    },
    {
      "epoch": 5.599279479007898,
      "grad_norm": 18.256223678588867,
      "learning_rate": 1.761396702230844e-05,
      "loss": 1.7373,
      "step": 40410
    },
    {
      "epoch": 5.600665096300402,
      "grad_norm": 17.111431121826172,
      "learning_rate": 1.7608424553138425e-05,
      "loss": 2.1948,
      "step": 40420
    },
    {
      "epoch": 5.602050713592906,
      "grad_norm": 7.973109722137451,
      "learning_rate": 1.760288208396841e-05,
      "loss": 1.8789,
      "step": 40430
    },
    {
      "epoch": 5.603436330885409,
      "grad_norm": 11.884591102600098,
      "learning_rate": 1.7597339614798395e-05,
      "loss": 1.9435,
      "step": 40440
    },
    {
      "epoch": 5.604821948177913,
      "grad_norm": 10.023771286010742,
      "learning_rate": 1.759179714562838e-05,
      "loss": 2.1176,
      "step": 40450
    },
    {
      "epoch": 5.606207565470417,
      "grad_norm": 12.447503089904785,
      "learning_rate": 1.7586254676458363e-05,
      "loss": 1.7905,
      "step": 40460
    },
    {
      "epoch": 5.607593182762921,
      "grad_norm": 17.29431915283203,
      "learning_rate": 1.7580712207288348e-05,
      "loss": 2.2031,
      "step": 40470
    },
    {
      "epoch": 5.608978800055425,
      "grad_norm": 16.758195877075195,
      "learning_rate": 1.7575169738118333e-05,
      "loss": 2.0324,
      "step": 40480
    },
    {
      "epoch": 5.610364417347928,
      "grad_norm": 14.544862747192383,
      "learning_rate": 1.7569627268948318e-05,
      "loss": 2.3969,
      "step": 40490
    },
    {
      "epoch": 5.6117500346404325,
      "grad_norm": 7.969259262084961,
      "learning_rate": 1.7564084799778304e-05,
      "loss": 1.7919,
      "step": 40500
    },
    {
      "epoch": 5.613135651932936,
      "grad_norm": 22.516212463378906,
      "learning_rate": 1.755854233060829e-05,
      "loss": 2.557,
      "step": 40510
    },
    {
      "epoch": 5.61452126922544,
      "grad_norm": 14.547295570373535,
      "learning_rate": 1.755299986143827e-05,
      "loss": 2.0023,
      "step": 40520
    },
    {
      "epoch": 5.615906886517944,
      "grad_norm": 28.610429763793945,
      "learning_rate": 1.7547457392268256e-05,
      "loss": 2.2514,
      "step": 40530
    },
    {
      "epoch": 5.6172925038104475,
      "grad_norm": 13.530059814453125,
      "learning_rate": 1.754191492309824e-05,
      "loss": 2.0891,
      "step": 40540
    },
    {
      "epoch": 5.618678121102952,
      "grad_norm": 15.908907890319824,
      "learning_rate": 1.7536372453928226e-05,
      "loss": 1.8151,
      "step": 40550
    },
    {
      "epoch": 5.620063738395455,
      "grad_norm": 12.248796463012695,
      "learning_rate": 1.753082998475821e-05,
      "loss": 1.8025,
      "step": 40560
    },
    {
      "epoch": 5.621449355687959,
      "grad_norm": 10.223194122314453,
      "learning_rate": 1.7525287515588197e-05,
      "loss": 1.5402,
      "step": 40570
    },
    {
      "epoch": 5.6228349729804625,
      "grad_norm": 18.494375228881836,
      "learning_rate": 1.7519745046418182e-05,
      "loss": 1.8826,
      "step": 40580
    },
    {
      "epoch": 5.624220590272967,
      "grad_norm": 9.87460994720459,
      "learning_rate": 1.7514202577248164e-05,
      "loss": 2.1789,
      "step": 40590
    },
    {
      "epoch": 5.62560620756547,
      "grad_norm": 10.885663986206055,
      "learning_rate": 1.750866010807815e-05,
      "loss": 2.0825,
      "step": 40600
    },
    {
      "epoch": 5.626991824857974,
      "grad_norm": 16.978546142578125,
      "learning_rate": 1.7503117638908135e-05,
      "loss": 2.1245,
      "step": 40610
    },
    {
      "epoch": 5.628377442150478,
      "grad_norm": 13.039264678955078,
      "learning_rate": 1.749757516973812e-05,
      "loss": 2.6409,
      "step": 40620
    },
    {
      "epoch": 5.629763059442982,
      "grad_norm": 13.373034477233887,
      "learning_rate": 1.7492032700568105e-05,
      "loss": 2.2251,
      "step": 40630
    },
    {
      "epoch": 5.631148676735486,
      "grad_norm": 16.47736167907715,
      "learning_rate": 1.748649023139809e-05,
      "loss": 2.2147,
      "step": 40640
    },
    {
      "epoch": 5.632534294027989,
      "grad_norm": 13.20146369934082,
      "learning_rate": 1.7480947762228072e-05,
      "loss": 2.055,
      "step": 40650
    },
    {
      "epoch": 5.633919911320493,
      "grad_norm": 9.248903274536133,
      "learning_rate": 1.7475405293058057e-05,
      "loss": 1.5148,
      "step": 40660
    },
    {
      "epoch": 5.635305528612997,
      "grad_norm": 9.42162036895752,
      "learning_rate": 1.7469862823888043e-05,
      "loss": 2.2246,
      "step": 40670
    },
    {
      "epoch": 5.636691145905501,
      "grad_norm": 13.577292442321777,
      "learning_rate": 1.7464320354718028e-05,
      "loss": 1.9942,
      "step": 40680
    },
    {
      "epoch": 5.638076763198004,
      "grad_norm": 6.553655624389648,
      "learning_rate": 1.7458777885548013e-05,
      "loss": 2.0142,
      "step": 40690
    },
    {
      "epoch": 5.639462380490508,
      "grad_norm": 14.458117485046387,
      "learning_rate": 1.7453235416378e-05,
      "loss": 1.8669,
      "step": 40700
    },
    {
      "epoch": 5.640847997783013,
      "grad_norm": 21.661123275756836,
      "learning_rate": 1.7447692947207984e-05,
      "loss": 1.7326,
      "step": 40710
    },
    {
      "epoch": 5.642233615075516,
      "grad_norm": 20.86110496520996,
      "learning_rate": 1.7442150478037965e-05,
      "loss": 2.1975,
      "step": 40720
    },
    {
      "epoch": 5.64361923236802,
      "grad_norm": 11.026562690734863,
      "learning_rate": 1.743660800886795e-05,
      "loss": 1.6464,
      "step": 40730
    },
    {
      "epoch": 5.645004849660523,
      "grad_norm": 19.373870849609375,
      "learning_rate": 1.743106553969794e-05,
      "loss": 2.0008,
      "step": 40740
    },
    {
      "epoch": 5.646390466953028,
      "grad_norm": 14.362828254699707,
      "learning_rate": 1.742552307052792e-05,
      "loss": 1.8264,
      "step": 40750
    },
    {
      "epoch": 5.647776084245532,
      "grad_norm": 24.554725646972656,
      "learning_rate": 1.7419980601357906e-05,
      "loss": 2.4188,
      "step": 40760
    },
    {
      "epoch": 5.649161701538035,
      "grad_norm": 9.900285720825195,
      "learning_rate": 1.7414438132187892e-05,
      "loss": 2.2255,
      "step": 40770
    },
    {
      "epoch": 5.650547318830539,
      "grad_norm": 10.83510971069336,
      "learning_rate": 1.7408895663017874e-05,
      "loss": 2.0069,
      "step": 40780
    },
    {
      "epoch": 5.651932936123043,
      "grad_norm": 13.204216957092285,
      "learning_rate": 1.7403353193847862e-05,
      "loss": 2.2045,
      "step": 40790
    },
    {
      "epoch": 5.653318553415547,
      "grad_norm": 9.929312705993652,
      "learning_rate": 1.7397810724677847e-05,
      "loss": 2.1504,
      "step": 40800
    },
    {
      "epoch": 5.65470417070805,
      "grad_norm": 11.173946380615234,
      "learning_rate": 1.739226825550783e-05,
      "loss": 1.8049,
      "step": 40810
    },
    {
      "epoch": 5.656089788000554,
      "grad_norm": 14.008500099182129,
      "learning_rate": 1.7386725786337815e-05,
      "loss": 1.9865,
      "step": 40820
    },
    {
      "epoch": 5.657475405293058,
      "grad_norm": 11.756016731262207,
      "learning_rate": 1.73811833171678e-05,
      "loss": 2.2477,
      "step": 40830
    },
    {
      "epoch": 5.658861022585562,
      "grad_norm": 10.925467491149902,
      "learning_rate": 1.7375640847997782e-05,
      "loss": 2.2326,
      "step": 40840
    },
    {
      "epoch": 5.660246639878066,
      "grad_norm": 17.81702423095703,
      "learning_rate": 1.737009837882777e-05,
      "loss": 2.2433,
      "step": 40850
    },
    {
      "epoch": 5.661632257170569,
      "grad_norm": 17.802242279052734,
      "learning_rate": 1.7364555909657756e-05,
      "loss": 2.3177,
      "step": 40860
    },
    {
      "epoch": 5.6630178744630735,
      "grad_norm": 10.228678703308105,
      "learning_rate": 1.735901344048774e-05,
      "loss": 2.1895,
      "step": 40870
    },
    {
      "epoch": 5.664403491755577,
      "grad_norm": 6.66142463684082,
      "learning_rate": 1.7353470971317723e-05,
      "loss": 1.6443,
      "step": 40880
    },
    {
      "epoch": 5.665789109048081,
      "grad_norm": 12.22106647491455,
      "learning_rate": 1.7347928502147708e-05,
      "loss": 2.1678,
      "step": 40890
    },
    {
      "epoch": 5.667174726340585,
      "grad_norm": 14.877107620239258,
      "learning_rate": 1.7342386032977693e-05,
      "loss": 1.9757,
      "step": 40900
    },
    {
      "epoch": 5.6685603436330885,
      "grad_norm": 16.204648971557617,
      "learning_rate": 1.733684356380768e-05,
      "loss": 1.96,
      "step": 40910
    },
    {
      "epoch": 5.669945960925593,
      "grad_norm": 11.362683296203613,
      "learning_rate": 1.7331301094637664e-05,
      "loss": 2.0566,
      "step": 40920
    },
    {
      "epoch": 5.671331578218096,
      "grad_norm": 13.098380088806152,
      "learning_rate": 1.732575862546765e-05,
      "loss": 1.8699,
      "step": 40930
    },
    {
      "epoch": 5.6727171955106,
      "grad_norm": 8.794744491577148,
      "learning_rate": 1.732021615629763e-05,
      "loss": 2.0212,
      "step": 40940
    },
    {
      "epoch": 5.6741028128031035,
      "grad_norm": 22.70802879333496,
      "learning_rate": 1.7314673687127616e-05,
      "loss": 1.7518,
      "step": 40950
    },
    {
      "epoch": 5.675488430095608,
      "grad_norm": 13.644011497497559,
      "learning_rate": 1.73091312179576e-05,
      "loss": 2.432,
      "step": 40960
    },
    {
      "epoch": 5.676874047388111,
      "grad_norm": 9.426642417907715,
      "learning_rate": 1.7303588748787587e-05,
      "loss": 2.0996,
      "step": 40970
    },
    {
      "epoch": 5.678259664680615,
      "grad_norm": 12.108214378356934,
      "learning_rate": 1.7298046279617572e-05,
      "loss": 2.3045,
      "step": 40980
    },
    {
      "epoch": 5.679645281973119,
      "grad_norm": 8.52096939086914,
      "learning_rate": 1.7292503810447557e-05,
      "loss": 2.176,
      "step": 40990
    },
    {
      "epoch": 5.681030899265623,
      "grad_norm": 13.409271240234375,
      "learning_rate": 1.7286961341277542e-05,
      "loss": 2.279,
      "step": 41000
    },
    {
      "epoch": 5.682416516558127,
      "grad_norm": 12.863883972167969,
      "learning_rate": 1.7281418872107524e-05,
      "loss": 2.1382,
      "step": 41010
    },
    {
      "epoch": 5.68380213385063,
      "grad_norm": 12.621508598327637,
      "learning_rate": 1.727587640293751e-05,
      "loss": 2.3823,
      "step": 41020
    },
    {
      "epoch": 5.685187751143134,
      "grad_norm": 9.568635940551758,
      "learning_rate": 1.7270333933767495e-05,
      "loss": 1.465,
      "step": 41030
    },
    {
      "epoch": 5.6865733684356385,
      "grad_norm": 8.129302024841309,
      "learning_rate": 1.726479146459748e-05,
      "loss": 2.0689,
      "step": 41040
    },
    {
      "epoch": 5.687958985728142,
      "grad_norm": 13.224934577941895,
      "learning_rate": 1.7259248995427465e-05,
      "loss": 1.9588,
      "step": 41050
    },
    {
      "epoch": 5.689344603020646,
      "grad_norm": 11.001705169677734,
      "learning_rate": 1.725370652625745e-05,
      "loss": 1.7881,
      "step": 41060
    },
    {
      "epoch": 5.690730220313149,
      "grad_norm": 10.038691520690918,
      "learning_rate": 1.7248164057087432e-05,
      "loss": 1.5582,
      "step": 41070
    },
    {
      "epoch": 5.6921158376056535,
      "grad_norm": 12.996007919311523,
      "learning_rate": 1.7242621587917418e-05,
      "loss": 2.0719,
      "step": 41080
    },
    {
      "epoch": 5.693501454898157,
      "grad_norm": 10.86429214477539,
      "learning_rate": 1.7237079118747403e-05,
      "loss": 2.412,
      "step": 41090
    },
    {
      "epoch": 5.694887072190661,
      "grad_norm": 13.837105751037598,
      "learning_rate": 1.7231536649577388e-05,
      "loss": 1.8928,
      "step": 41100
    },
    {
      "epoch": 5.696272689483164,
      "grad_norm": 17.010011672973633,
      "learning_rate": 1.7225994180407373e-05,
      "loss": 1.8719,
      "step": 41110
    },
    {
      "epoch": 5.6976583067756685,
      "grad_norm": 10.631608009338379,
      "learning_rate": 1.722045171123736e-05,
      "loss": 1.8914,
      "step": 41120
    },
    {
      "epoch": 5.699043924068173,
      "grad_norm": 7.424211025238037,
      "learning_rate": 1.721490924206734e-05,
      "loss": 1.6698,
      "step": 41130
    },
    {
      "epoch": 5.700429541360676,
      "grad_norm": 9.454352378845215,
      "learning_rate": 1.7209366772897326e-05,
      "loss": 2.1729,
      "step": 41140
    },
    {
      "epoch": 5.70181515865318,
      "grad_norm": 10.557218551635742,
      "learning_rate": 1.720382430372731e-05,
      "loss": 1.729,
      "step": 41150
    },
    {
      "epoch": 5.7032007759456835,
      "grad_norm": 11.505443572998047,
      "learning_rate": 1.7198281834557296e-05,
      "loss": 1.6302,
      "step": 41160
    },
    {
      "epoch": 5.704586393238188,
      "grad_norm": 14.022303581237793,
      "learning_rate": 1.719273936538728e-05,
      "loss": 2.285,
      "step": 41170
    },
    {
      "epoch": 5.705972010530692,
      "grad_norm": 16.785701751708984,
      "learning_rate": 1.7187196896217267e-05,
      "loss": 1.552,
      "step": 41180
    },
    {
      "epoch": 5.707357627823195,
      "grad_norm": 7.888454914093018,
      "learning_rate": 1.7181654427047252e-05,
      "loss": 2.0377,
      "step": 41190
    },
    {
      "epoch": 5.708743245115699,
      "grad_norm": 6.585290908813477,
      "learning_rate": 1.7176111957877234e-05,
      "loss": 1.7523,
      "step": 41200
    },
    {
      "epoch": 5.710128862408203,
      "grad_norm": 9.674535751342773,
      "learning_rate": 1.717056948870722e-05,
      "loss": 2.2878,
      "step": 41210
    },
    {
      "epoch": 5.711514479700707,
      "grad_norm": 10.772745132446289,
      "learning_rate": 1.7165027019537208e-05,
      "loss": 2.4193,
      "step": 41220
    },
    {
      "epoch": 5.71290009699321,
      "grad_norm": 11.14169692993164,
      "learning_rate": 1.715948455036719e-05,
      "loss": 2.0304,
      "step": 41230
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 11.550554275512695,
      "learning_rate": 1.7153942081197175e-05,
      "loss": 2.1523,
      "step": 41240
    },
    {
      "epoch": 5.715671331578218,
      "grad_norm": 9.747785568237305,
      "learning_rate": 1.714839961202716e-05,
      "loss": 2.1187,
      "step": 41250
    },
    {
      "epoch": 5.717056948870722,
      "grad_norm": 11.59903621673584,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 1.6432,
      "step": 41260
    },
    {
      "epoch": 5.718442566163226,
      "grad_norm": 22.273591995239258,
      "learning_rate": 1.7137314673687127e-05,
      "loss": 1.8594,
      "step": 41270
    },
    {
      "epoch": 5.719828183455729,
      "grad_norm": 12.59985637664795,
      "learning_rate": 1.7131772204517116e-05,
      "loss": 1.9603,
      "step": 41280
    },
    {
      "epoch": 5.721213800748234,
      "grad_norm": 12.921630859375,
      "learning_rate": 1.7126229735347098e-05,
      "loss": 2.1691,
      "step": 41290
    },
    {
      "epoch": 5.722599418040737,
      "grad_norm": 8.512835502624512,
      "learning_rate": 1.7120687266177083e-05,
      "loss": 1.9187,
      "step": 41300
    },
    {
      "epoch": 5.723985035333241,
      "grad_norm": 8.508846282958984,
      "learning_rate": 1.7115144797007068e-05,
      "loss": 2.6383,
      "step": 41310
    },
    {
      "epoch": 5.725370652625744,
      "grad_norm": 9.772856712341309,
      "learning_rate": 1.7109602327837053e-05,
      "loss": 1.9637,
      "step": 41320
    },
    {
      "epoch": 5.726756269918249,
      "grad_norm": 12.660767555236816,
      "learning_rate": 1.710405985866704e-05,
      "loss": 2.1799,
      "step": 41330
    },
    {
      "epoch": 5.728141887210752,
      "grad_norm": 17.351892471313477,
      "learning_rate": 1.7098517389497024e-05,
      "loss": 1.9824,
      "step": 41340
    },
    {
      "epoch": 5.729527504503256,
      "grad_norm": 7.057352066040039,
      "learning_rate": 1.709297492032701e-05,
      "loss": 1.907,
      "step": 41350
    },
    {
      "epoch": 5.73091312179576,
      "grad_norm": 8.025781631469727,
      "learning_rate": 1.708743245115699e-05,
      "loss": 1.6921,
      "step": 41360
    },
    {
      "epoch": 5.732298739088264,
      "grad_norm": 13.828081130981445,
      "learning_rate": 1.7081889981986976e-05,
      "loss": 1.7008,
      "step": 41370
    },
    {
      "epoch": 5.733684356380768,
      "grad_norm": 11.203526496887207,
      "learning_rate": 1.707634751281696e-05,
      "loss": 2.6207,
      "step": 41380
    },
    {
      "epoch": 5.735069973673271,
      "grad_norm": 11.52048397064209,
      "learning_rate": 1.7070805043646947e-05,
      "loss": 2.4974,
      "step": 41390
    },
    {
      "epoch": 5.736455590965775,
      "grad_norm": 8.586326599121094,
      "learning_rate": 1.7065262574476932e-05,
      "loss": 1.994,
      "step": 41400
    },
    {
      "epoch": 5.7378412082582795,
      "grad_norm": 8.996407508850098,
      "learning_rate": 1.7059720105306917e-05,
      "loss": 2.4191,
      "step": 41410
    },
    {
      "epoch": 5.739226825550783,
      "grad_norm": 19.455467224121094,
      "learning_rate": 1.70541776361369e-05,
      "loss": 2.0118,
      "step": 41420
    },
    {
      "epoch": 5.740612442843287,
      "grad_norm": 10.635001182556152,
      "learning_rate": 1.7048635166966884e-05,
      "loss": 2.1224,
      "step": 41430
    },
    {
      "epoch": 5.74199806013579,
      "grad_norm": 12.639636039733887,
      "learning_rate": 1.704309269779687e-05,
      "loss": 2.2824,
      "step": 41440
    },
    {
      "epoch": 5.7433836774282945,
      "grad_norm": 7.602243900299072,
      "learning_rate": 1.7037550228626855e-05,
      "loss": 2.2239,
      "step": 41450
    },
    {
      "epoch": 5.744769294720798,
      "grad_norm": 9.494702339172363,
      "learning_rate": 1.703200775945684e-05,
      "loss": 1.6221,
      "step": 41460
    },
    {
      "epoch": 5.746154912013302,
      "grad_norm": 17.12324333190918,
      "learning_rate": 1.7026465290286825e-05,
      "loss": 2.1957,
      "step": 41470
    },
    {
      "epoch": 5.747540529305805,
      "grad_norm": 11.195005416870117,
      "learning_rate": 1.702092282111681e-05,
      "loss": 2.3519,
      "step": 41480
    },
    {
      "epoch": 5.7489261465983095,
      "grad_norm": 18.242595672607422,
      "learning_rate": 1.7015380351946792e-05,
      "loss": 1.9616,
      "step": 41490
    },
    {
      "epoch": 5.750311763890814,
      "grad_norm": 18.207061767578125,
      "learning_rate": 1.7009837882776778e-05,
      "loss": 2.1714,
      "step": 41500
    },
    {
      "epoch": 5.751697381183317,
      "grad_norm": 8.209075927734375,
      "learning_rate": 1.7004295413606763e-05,
      "loss": 1.9371,
      "step": 41510
    },
    {
      "epoch": 5.753082998475821,
      "grad_norm": 8.172318458557129,
      "learning_rate": 1.6998752944436748e-05,
      "loss": 1.4078,
      "step": 41520
    },
    {
      "epoch": 5.7544686157683245,
      "grad_norm": 15.622420310974121,
      "learning_rate": 1.6993210475266733e-05,
      "loss": 2.0238,
      "step": 41530
    },
    {
      "epoch": 5.755854233060829,
      "grad_norm": 17.023452758789062,
      "learning_rate": 1.698766800609672e-05,
      "loss": 1.8958,
      "step": 41540
    },
    {
      "epoch": 5.757239850353333,
      "grad_norm": 9.02597713470459,
      "learning_rate": 1.69821255369267e-05,
      "loss": 2.3734,
      "step": 41550
    },
    {
      "epoch": 5.758625467645836,
      "grad_norm": 15.094476699829102,
      "learning_rate": 1.6976583067756686e-05,
      "loss": 1.9723,
      "step": 41560
    },
    {
      "epoch": 5.76001108493834,
      "grad_norm": 12.177546501159668,
      "learning_rate": 1.697104059858667e-05,
      "loss": 2.4283,
      "step": 41570
    },
    {
      "epoch": 5.761396702230844,
      "grad_norm": 12.195781707763672,
      "learning_rate": 1.6965498129416656e-05,
      "loss": 1.8575,
      "step": 41580
    },
    {
      "epoch": 5.762782319523348,
      "grad_norm": 18.929285049438477,
      "learning_rate": 1.695995566024664e-05,
      "loss": 2.2488,
      "step": 41590
    },
    {
      "epoch": 5.764167936815851,
      "grad_norm": 11.51250171661377,
      "learning_rate": 1.6954413191076627e-05,
      "loss": 1.9556,
      "step": 41600
    },
    {
      "epoch": 5.765553554108355,
      "grad_norm": 8.640817642211914,
      "learning_rate": 1.6948870721906612e-05,
      "loss": 1.6655,
      "step": 41610
    },
    {
      "epoch": 5.766939171400859,
      "grad_norm": 16.992454528808594,
      "learning_rate": 1.6943328252736594e-05,
      "loss": 1.8413,
      "step": 41620
    },
    {
      "epoch": 5.768324788693363,
      "grad_norm": 13.967769622802734,
      "learning_rate": 1.693778578356658e-05,
      "loss": 1.9648,
      "step": 41630
    },
    {
      "epoch": 5.769710405985867,
      "grad_norm": 15.496443748474121,
      "learning_rate": 1.6932243314396564e-05,
      "loss": 1.9661,
      "step": 41640
    },
    {
      "epoch": 5.77109602327837,
      "grad_norm": 11.53294849395752,
      "learning_rate": 1.692670084522655e-05,
      "loss": 1.4546,
      "step": 41650
    },
    {
      "epoch": 5.7724816405708745,
      "grad_norm": 15.123662948608398,
      "learning_rate": 1.6921158376056535e-05,
      "loss": 2.0429,
      "step": 41660
    },
    {
      "epoch": 5.773867257863378,
      "grad_norm": 11.784662246704102,
      "learning_rate": 1.691561590688652e-05,
      "loss": 2.0044,
      "step": 41670
    },
    {
      "epoch": 5.775252875155882,
      "grad_norm": 12.427627563476562,
      "learning_rate": 1.6910073437716502e-05,
      "loss": 1.6713,
      "step": 41680
    },
    {
      "epoch": 5.776638492448386,
      "grad_norm": 23.956180572509766,
      "learning_rate": 1.6904530968546487e-05,
      "loss": 2.215,
      "step": 41690
    },
    {
      "epoch": 5.7780241097408895,
      "grad_norm": 15.548483848571777,
      "learning_rate": 1.6898988499376472e-05,
      "loss": 2.3567,
      "step": 41700
    },
    {
      "epoch": 5.779409727033394,
      "grad_norm": 9.44876766204834,
      "learning_rate": 1.6893446030206458e-05,
      "loss": 1.9941,
      "step": 41710
    },
    {
      "epoch": 5.780795344325897,
      "grad_norm": 10.960477828979492,
      "learning_rate": 1.6887903561036443e-05,
      "loss": 2.1308,
      "step": 41720
    },
    {
      "epoch": 5.782180961618401,
      "grad_norm": 7.088129997253418,
      "learning_rate": 1.6882361091866428e-05,
      "loss": 2.0125,
      "step": 41730
    },
    {
      "epoch": 5.7835665789109045,
      "grad_norm": 13.892492294311523,
      "learning_rate": 1.6876818622696413e-05,
      "loss": 1.7299,
      "step": 41740
    },
    {
      "epoch": 5.784952196203409,
      "grad_norm": 11.261417388916016,
      "learning_rate": 1.6871276153526395e-05,
      "loss": 1.7856,
      "step": 41750
    },
    {
      "epoch": 5.786337813495912,
      "grad_norm": 9.004107475280762,
      "learning_rate": 1.6865733684356384e-05,
      "loss": 2.04,
      "step": 41760
    },
    {
      "epoch": 5.787723430788416,
      "grad_norm": 16.684734344482422,
      "learning_rate": 1.686019121518637e-05,
      "loss": 2.277,
      "step": 41770
    },
    {
      "epoch": 5.78910904808092,
      "grad_norm": 15.66274642944336,
      "learning_rate": 1.685464874601635e-05,
      "loss": 1.9107,
      "step": 41780
    },
    {
      "epoch": 5.790494665373424,
      "grad_norm": 15.990243911743164,
      "learning_rate": 1.6849106276846336e-05,
      "loss": 1.7321,
      "step": 41790
    },
    {
      "epoch": 5.791880282665928,
      "grad_norm": 16.98090934753418,
      "learning_rate": 1.684356380767632e-05,
      "loss": 1.6222,
      "step": 41800
    },
    {
      "epoch": 5.793265899958431,
      "grad_norm": 10.459877014160156,
      "learning_rate": 1.6838021338506307e-05,
      "loss": 1.9792,
      "step": 41810
    },
    {
      "epoch": 5.794651517250935,
      "grad_norm": 12.611576080322266,
      "learning_rate": 1.6832478869336292e-05,
      "loss": 1.9525,
      "step": 41820
    },
    {
      "epoch": 5.796037134543439,
      "grad_norm": 7.7722554206848145,
      "learning_rate": 1.6826936400166277e-05,
      "loss": 1.7386,
      "step": 41830
    },
    {
      "epoch": 5.797422751835943,
      "grad_norm": 17.31060218811035,
      "learning_rate": 1.682139393099626e-05,
      "loss": 2.8239,
      "step": 41840
    },
    {
      "epoch": 5.798808369128447,
      "grad_norm": 12.998639106750488,
      "learning_rate": 1.6815851461826244e-05,
      "loss": 1.8261,
      "step": 41850
    },
    {
      "epoch": 5.80019398642095,
      "grad_norm": 15.040865898132324,
      "learning_rate": 1.681030899265623e-05,
      "loss": 2.2263,
      "step": 41860
    },
    {
      "epoch": 5.801579603713455,
      "grad_norm": 15.594223022460938,
      "learning_rate": 1.6804766523486215e-05,
      "loss": 1.6274,
      "step": 41870
    },
    {
      "epoch": 5.802965221005958,
      "grad_norm": 10.736922264099121,
      "learning_rate": 1.67992240543162e-05,
      "loss": 2.2696,
      "step": 41880
    },
    {
      "epoch": 5.804350838298462,
      "grad_norm": 13.921161651611328,
      "learning_rate": 1.6793681585146185e-05,
      "loss": 1.5331,
      "step": 41890
    },
    {
      "epoch": 5.805736455590965,
      "grad_norm": 12.936284065246582,
      "learning_rate": 1.678813911597617e-05,
      "loss": 1.6502,
      "step": 41900
    },
    {
      "epoch": 5.80712207288347,
      "grad_norm": 16.16368865966797,
      "learning_rate": 1.6782596646806153e-05,
      "loss": 2.0676,
      "step": 41910
    },
    {
      "epoch": 5.808507690175974,
      "grad_norm": 10.256636619567871,
      "learning_rate": 1.6777054177636138e-05,
      "loss": 1.6589,
      "step": 41920
    },
    {
      "epoch": 5.809893307468477,
      "grad_norm": 9.480171203613281,
      "learning_rate": 1.6771511708466123e-05,
      "loss": 1.9082,
      "step": 41930
    },
    {
      "epoch": 5.811278924760981,
      "grad_norm": 12.27091121673584,
      "learning_rate": 1.6765969239296108e-05,
      "loss": 1.7685,
      "step": 41940
    },
    {
      "epoch": 5.812664542053485,
      "grad_norm": 16.651840209960938,
      "learning_rate": 1.6760426770126094e-05,
      "loss": 1.8972,
      "step": 41950
    },
    {
      "epoch": 5.814050159345989,
      "grad_norm": 15.874239921569824,
      "learning_rate": 1.675488430095608e-05,
      "loss": 1.5392,
      "step": 41960
    },
    {
      "epoch": 5.815435776638492,
      "grad_norm": 9.595039367675781,
      "learning_rate": 1.674934183178606e-05,
      "loss": 1.8828,
      "step": 41970
    },
    {
      "epoch": 5.816821393930996,
      "grad_norm": 12.925086975097656,
      "learning_rate": 1.6743799362616046e-05,
      "loss": 1.9682,
      "step": 41980
    },
    {
      "epoch": 5.8182070112235,
      "grad_norm": 15.408790588378906,
      "learning_rate": 1.673825689344603e-05,
      "loss": 2.1791,
      "step": 41990
    },
    {
      "epoch": 5.819592628516004,
      "grad_norm": 10.013916969299316,
      "learning_rate": 1.6732714424276016e-05,
      "loss": 1.6437,
      "step": 42000
    },
    {
      "epoch": 5.820978245808508,
      "grad_norm": 13.301689147949219,
      "learning_rate": 1.6727171955106e-05,
      "loss": 2.1177,
      "step": 42010
    },
    {
      "epoch": 5.822363863101011,
      "grad_norm": 12.357418060302734,
      "learning_rate": 1.6721629485935987e-05,
      "loss": 2.1175,
      "step": 42020
    },
    {
      "epoch": 5.8237494803935155,
      "grad_norm": 17.073190689086914,
      "learning_rate": 1.6716087016765972e-05,
      "loss": 1.8179,
      "step": 42030
    },
    {
      "epoch": 5.825135097686019,
      "grad_norm": 5.428926944732666,
      "learning_rate": 1.6710544547595954e-05,
      "loss": 1.517,
      "step": 42040
    },
    {
      "epoch": 5.826520714978523,
      "grad_norm": 10.555259704589844,
      "learning_rate": 1.670500207842594e-05,
      "loss": 1.6884,
      "step": 42050
    },
    {
      "epoch": 5.827906332271027,
      "grad_norm": 16.029752731323242,
      "learning_rate": 1.6699459609255925e-05,
      "loss": 2.0786,
      "step": 42060
    },
    {
      "epoch": 5.8292919495635305,
      "grad_norm": 17.507671356201172,
      "learning_rate": 1.669391714008591e-05,
      "loss": 2.0081,
      "step": 42070
    },
    {
      "epoch": 5.830677566856035,
      "grad_norm": 10.565051078796387,
      "learning_rate": 1.6688374670915895e-05,
      "loss": 2.09,
      "step": 42080
    },
    {
      "epoch": 5.832063184148538,
      "grad_norm": 6.972193241119385,
      "learning_rate": 1.668283220174588e-05,
      "loss": 2.1669,
      "step": 42090
    },
    {
      "epoch": 5.833448801441042,
      "grad_norm": 16.90427589416504,
      "learning_rate": 1.6677289732575862e-05,
      "loss": 1.7248,
      "step": 42100
    },
    {
      "epoch": 5.8348344187335455,
      "grad_norm": 16.99327278137207,
      "learning_rate": 1.6671747263405847e-05,
      "loss": 2.0844,
      "step": 42110
    },
    {
      "epoch": 5.83622003602605,
      "grad_norm": 18.86879539489746,
      "learning_rate": 1.6666204794235833e-05,
      "loss": 1.8897,
      "step": 42120
    },
    {
      "epoch": 5.837605653318553,
      "grad_norm": 14.877490043640137,
      "learning_rate": 1.6660662325065818e-05,
      "loss": 2.0451,
      "step": 42130
    },
    {
      "epoch": 5.838991270611057,
      "grad_norm": 8.94156551361084,
      "learning_rate": 1.6655119855895803e-05,
      "loss": 1.8279,
      "step": 42140
    },
    {
      "epoch": 5.840376887903561,
      "grad_norm": 10.567120552062988,
      "learning_rate": 1.664957738672579e-05,
      "loss": 2.0592,
      "step": 42150
    },
    {
      "epoch": 5.841762505196065,
      "grad_norm": 12.797941207885742,
      "learning_rate": 1.664403491755577e-05,
      "loss": 1.6838,
      "step": 42160
    },
    {
      "epoch": 5.843148122488569,
      "grad_norm": 10.016289710998535,
      "learning_rate": 1.6638492448385755e-05,
      "loss": 2.1857,
      "step": 42170
    },
    {
      "epoch": 5.844533739781072,
      "grad_norm": 9.52393627166748,
      "learning_rate": 1.663294997921574e-05,
      "loss": 1.7937,
      "step": 42180
    },
    {
      "epoch": 5.845919357073576,
      "grad_norm": 8.290254592895508,
      "learning_rate": 1.662740751004573e-05,
      "loss": 2.0462,
      "step": 42190
    },
    {
      "epoch": 5.8473049743660805,
      "grad_norm": 12.56064224243164,
      "learning_rate": 1.662186504087571e-05,
      "loss": 2.2332,
      "step": 42200
    },
    {
      "epoch": 5.848690591658584,
      "grad_norm": 16.652067184448242,
      "learning_rate": 1.6616322571705696e-05,
      "loss": 2.1984,
      "step": 42210
    },
    {
      "epoch": 5.850076208951088,
      "grad_norm": 6.252962112426758,
      "learning_rate": 1.6610780102535682e-05,
      "loss": 2.0935,
      "step": 42220
    },
    {
      "epoch": 5.851461826243591,
      "grad_norm": 10.625846862792969,
      "learning_rate": 1.6605237633365664e-05,
      "loss": 1.6752,
      "step": 42230
    },
    {
      "epoch": 5.8528474435360955,
      "grad_norm": 11.930896759033203,
      "learning_rate": 1.659969516419565e-05,
      "loss": 2.2344,
      "step": 42240
    },
    {
      "epoch": 5.854233060828599,
      "grad_norm": 10.082484245300293,
      "learning_rate": 1.6594152695025637e-05,
      "loss": 2.1638,
      "step": 42250
    },
    {
      "epoch": 5.855618678121103,
      "grad_norm": 9.954780578613281,
      "learning_rate": 1.658861022585562e-05,
      "loss": 1.9657,
      "step": 42260
    },
    {
      "epoch": 5.857004295413606,
      "grad_norm": 13.718644142150879,
      "learning_rate": 1.6583067756685605e-05,
      "loss": 1.8557,
      "step": 42270
    },
    {
      "epoch": 5.8583899127061105,
      "grad_norm": 14.102338790893555,
      "learning_rate": 1.657752528751559e-05,
      "loss": 1.7183,
      "step": 42280
    },
    {
      "epoch": 5.859775529998615,
      "grad_norm": 15.270418167114258,
      "learning_rate": 1.657198281834557e-05,
      "loss": 2.1585,
      "step": 42290
    },
    {
      "epoch": 5.861161147291118,
      "grad_norm": 18.65170669555664,
      "learning_rate": 1.656644034917556e-05,
      "loss": 1.8496,
      "step": 42300
    },
    {
      "epoch": 5.862546764583622,
      "grad_norm": 9.99962329864502,
      "learning_rate": 1.6560897880005546e-05,
      "loss": 2.0326,
      "step": 42310
    },
    {
      "epoch": 5.8639323818761255,
      "grad_norm": 16.675613403320312,
      "learning_rate": 1.655535541083553e-05,
      "loss": 1.8991,
      "step": 42320
    },
    {
      "epoch": 5.86531799916863,
      "grad_norm": 14.50755500793457,
      "learning_rate": 1.6549812941665513e-05,
      "loss": 1.9253,
      "step": 42330
    },
    {
      "epoch": 5.866703616461134,
      "grad_norm": 13.765750885009766,
      "learning_rate": 1.6544270472495498e-05,
      "loss": 2.0988,
      "step": 42340
    },
    {
      "epoch": 5.868089233753637,
      "grad_norm": 18.504140853881836,
      "learning_rate": 1.6538728003325483e-05,
      "loss": 2.2175,
      "step": 42350
    },
    {
      "epoch": 5.869474851046141,
      "grad_norm": 16.0262393951416,
      "learning_rate": 1.653318553415547e-05,
      "loss": 1.8936,
      "step": 42360
    },
    {
      "epoch": 5.870860468338645,
      "grad_norm": 13.461934089660645,
      "learning_rate": 1.6527643064985454e-05,
      "loss": 1.6654,
      "step": 42370
    },
    {
      "epoch": 5.872246085631149,
      "grad_norm": 20.78443145751953,
      "learning_rate": 1.652210059581544e-05,
      "loss": 2.0242,
      "step": 42380
    },
    {
      "epoch": 5.873631702923652,
      "grad_norm": 5.6751532554626465,
      "learning_rate": 1.651655812664542e-05,
      "loss": 1.6831,
      "step": 42390
    },
    {
      "epoch": 5.875017320216156,
      "grad_norm": 8.471110343933105,
      "learning_rate": 1.6511015657475406e-05,
      "loss": 1.7166,
      "step": 42400
    },
    {
      "epoch": 5.87640293750866,
      "grad_norm": 10.457700729370117,
      "learning_rate": 1.650547318830539e-05,
      "loss": 1.8714,
      "step": 42410
    },
    {
      "epoch": 5.877788554801164,
      "grad_norm": 15.228474617004395,
      "learning_rate": 1.6499930719135377e-05,
      "loss": 1.9692,
      "step": 42420
    },
    {
      "epoch": 5.879174172093668,
      "grad_norm": 11.310997009277344,
      "learning_rate": 1.6494388249965362e-05,
      "loss": 2.0592,
      "step": 42430
    },
    {
      "epoch": 5.880559789386171,
      "grad_norm": 7.2386345863342285,
      "learning_rate": 1.6488845780795347e-05,
      "loss": 2.2828,
      "step": 42440
    },
    {
      "epoch": 5.881945406678676,
      "grad_norm": 17.6252498626709,
      "learning_rate": 1.648330331162533e-05,
      "loss": 1.7042,
      "step": 42450
    },
    {
      "epoch": 5.883331023971179,
      "grad_norm": 19.432777404785156,
      "learning_rate": 1.6477760842455314e-05,
      "loss": 1.7254,
      "step": 42460
    },
    {
      "epoch": 5.884716641263683,
      "grad_norm": 22.997499465942383,
      "learning_rate": 1.64722183732853e-05,
      "loss": 2.167,
      "step": 42470
    },
    {
      "epoch": 5.886102258556186,
      "grad_norm": 17.441856384277344,
      "learning_rate": 1.6466675904115285e-05,
      "loss": 2.071,
      "step": 42480
    },
    {
      "epoch": 5.887487875848691,
      "grad_norm": 12.654777526855469,
      "learning_rate": 1.646113343494527e-05,
      "loss": 2.0762,
      "step": 42490
    },
    {
      "epoch": 5.888873493141194,
      "grad_norm": 10.660597801208496,
      "learning_rate": 1.6455590965775255e-05,
      "loss": 2.0425,
      "step": 42500
    },
    {
      "epoch": 5.890259110433698,
      "grad_norm": 10.703743934631348,
      "learning_rate": 1.645004849660524e-05,
      "loss": 2.0051,
      "step": 42510
    },
    {
      "epoch": 5.891644727726202,
      "grad_norm": 10.023871421813965,
      "learning_rate": 1.6444506027435222e-05,
      "loss": 2.5493,
      "step": 42520
    },
    {
      "epoch": 5.893030345018706,
      "grad_norm": 7.855472087860107,
      "learning_rate": 1.6438963558265208e-05,
      "loss": 2.067,
      "step": 42530
    },
    {
      "epoch": 5.89441596231121,
      "grad_norm": 16.182464599609375,
      "learning_rate": 1.6433421089095193e-05,
      "loss": 2.1516,
      "step": 42540
    },
    {
      "epoch": 5.895801579603713,
      "grad_norm": 9.50790786743164,
      "learning_rate": 1.6427878619925178e-05,
      "loss": 2.2899,
      "step": 42550
    },
    {
      "epoch": 5.897187196896217,
      "grad_norm": 13.24429702758789,
      "learning_rate": 1.6422336150755163e-05,
      "loss": 2.2735,
      "step": 42560
    },
    {
      "epoch": 5.8985728141887215,
      "grad_norm": 14.170856475830078,
      "learning_rate": 1.641679368158515e-05,
      "loss": 1.959,
      "step": 42570
    },
    {
      "epoch": 5.899958431481225,
      "grad_norm": 11.8060884475708,
      "learning_rate": 1.641125121241513e-05,
      "loss": 1.7346,
      "step": 42580
    },
    {
      "epoch": 5.901344048773729,
      "grad_norm": 14.70506477355957,
      "learning_rate": 1.6405708743245116e-05,
      "loss": 2.1606,
      "step": 42590
    },
    {
      "epoch": 5.902729666066232,
      "grad_norm": 12.434271812438965,
      "learning_rate": 1.64001662740751e-05,
      "loss": 1.9179,
      "step": 42600
    },
    {
      "epoch": 5.9041152833587365,
      "grad_norm": 9.860410690307617,
      "learning_rate": 1.6394623804905086e-05,
      "loss": 1.6525,
      "step": 42610
    },
    {
      "epoch": 5.90550090065124,
      "grad_norm": 11.335225105285645,
      "learning_rate": 1.638908133573507e-05,
      "loss": 1.8521,
      "step": 42620
    },
    {
      "epoch": 5.906886517943744,
      "grad_norm": 13.720175743103027,
      "learning_rate": 1.6383538866565057e-05,
      "loss": 1.8303,
      "step": 42630
    },
    {
      "epoch": 5.908272135236247,
      "grad_norm": 12.546648979187012,
      "learning_rate": 1.6377996397395042e-05,
      "loss": 1.8618,
      "step": 42640
    },
    {
      "epoch": 5.9096577525287515,
      "grad_norm": 19.212890625,
      "learning_rate": 1.6372453928225024e-05,
      "loss": 2.3079,
      "step": 42650
    },
    {
      "epoch": 5.911043369821256,
      "grad_norm": 13.819929122924805,
      "learning_rate": 1.636691145905501e-05,
      "loss": 1.9854,
      "step": 42660
    },
    {
      "epoch": 5.912428987113759,
      "grad_norm": 16.539052963256836,
      "learning_rate": 1.6361368989884994e-05,
      "loss": 2.2207,
      "step": 42670
    },
    {
      "epoch": 5.913814604406263,
      "grad_norm": 9.065637588500977,
      "learning_rate": 1.635582652071498e-05,
      "loss": 1.62,
      "step": 42680
    },
    {
      "epoch": 5.9152002216987665,
      "grad_norm": 19.309715270996094,
      "learning_rate": 1.6350284051544965e-05,
      "loss": 1.8479,
      "step": 42690
    },
    {
      "epoch": 5.916585838991271,
      "grad_norm": 14.258035659790039,
      "learning_rate": 1.634474158237495e-05,
      "loss": 1.8105,
      "step": 42700
    },
    {
      "epoch": 5.917971456283775,
      "grad_norm": 12.331252098083496,
      "learning_rate": 1.6339199113204932e-05,
      "loss": 1.7857,
      "step": 42710
    },
    {
      "epoch": 5.919357073576278,
      "grad_norm": 7.62362003326416,
      "learning_rate": 1.6333656644034917e-05,
      "loss": 1.9141,
      "step": 42720
    },
    {
      "epoch": 5.920742690868782,
      "grad_norm": 11.331574440002441,
      "learning_rate": 1.6328114174864906e-05,
      "loss": 1.9304,
      "step": 42730
    },
    {
      "epoch": 5.922128308161286,
      "grad_norm": 21.76047706604004,
      "learning_rate": 1.6322571705694888e-05,
      "loss": 2.2798,
      "step": 42740
    },
    {
      "epoch": 5.92351392545379,
      "grad_norm": 19.13091468811035,
      "learning_rate": 1.6317029236524873e-05,
      "loss": 2.1213,
      "step": 42750
    },
    {
      "epoch": 5.924899542746293,
      "grad_norm": 14.6282320022583,
      "learning_rate": 1.6311486767354858e-05,
      "loss": 1.842,
      "step": 42760
    },
    {
      "epoch": 5.926285160038797,
      "grad_norm": 13.615949630737305,
      "learning_rate": 1.6305944298184843e-05,
      "loss": 1.9725,
      "step": 42770
    },
    {
      "epoch": 5.927670777331301,
      "grad_norm": 8.996301651000977,
      "learning_rate": 1.630040182901483e-05,
      "loss": 1.8044,
      "step": 42780
    },
    {
      "epoch": 5.929056394623805,
      "grad_norm": 10.421940803527832,
      "learning_rate": 1.6294859359844814e-05,
      "loss": 2.1881,
      "step": 42790
    },
    {
      "epoch": 5.930442011916309,
      "grad_norm": 14.433208465576172,
      "learning_rate": 1.62893168906748e-05,
      "loss": 2.0266,
      "step": 42800
    },
    {
      "epoch": 5.931827629208812,
      "grad_norm": 15.099103927612305,
      "learning_rate": 1.628377442150478e-05,
      "loss": 2.1668,
      "step": 42810
    },
    {
      "epoch": 5.9332132465013165,
      "grad_norm": 10.50938892364502,
      "learning_rate": 1.6278231952334766e-05,
      "loss": 1.6451,
      "step": 42820
    },
    {
      "epoch": 5.93459886379382,
      "grad_norm": 10.425190925598145,
      "learning_rate": 1.627268948316475e-05,
      "loss": 1.977,
      "step": 42830
    },
    {
      "epoch": 5.935984481086324,
      "grad_norm": 8.44011402130127,
      "learning_rate": 1.6267147013994737e-05,
      "loss": 2.278,
      "step": 42840
    },
    {
      "epoch": 5.937370098378828,
      "grad_norm": 18.967700958251953,
      "learning_rate": 1.6261604544824722e-05,
      "loss": 2.2774,
      "step": 42850
    },
    {
      "epoch": 5.9387557156713315,
      "grad_norm": 13.223426818847656,
      "learning_rate": 1.6256062075654707e-05,
      "loss": 2.4669,
      "step": 42860
    },
    {
      "epoch": 5.940141332963836,
      "grad_norm": 8.977177619934082,
      "learning_rate": 1.625051960648469e-05,
      "loss": 2.039,
      "step": 42870
    },
    {
      "epoch": 5.941526950256339,
      "grad_norm": 15.967411041259766,
      "learning_rate": 1.6244977137314674e-05,
      "loss": 2.1231,
      "step": 42880
    },
    {
      "epoch": 5.942912567548843,
      "grad_norm": 13.473808288574219,
      "learning_rate": 1.623943466814466e-05,
      "loss": 1.2878,
      "step": 42890
    },
    {
      "epoch": 5.9442981848413465,
      "grad_norm": 13.968268394470215,
      "learning_rate": 1.6233892198974645e-05,
      "loss": 2.0348,
      "step": 42900
    },
    {
      "epoch": 5.945683802133851,
      "grad_norm": 10.894078254699707,
      "learning_rate": 1.622834972980463e-05,
      "loss": 2.125,
      "step": 42910
    },
    {
      "epoch": 5.947069419426354,
      "grad_norm": 9.25223159790039,
      "learning_rate": 1.6222807260634615e-05,
      "loss": 1.9081,
      "step": 42920
    },
    {
      "epoch": 5.948455036718858,
      "grad_norm": 13.326652526855469,
      "learning_rate": 1.62172647914646e-05,
      "loss": 2.3391,
      "step": 42930
    },
    {
      "epoch": 5.949840654011362,
      "grad_norm": 6.901208877563477,
      "learning_rate": 1.6211722322294582e-05,
      "loss": 1.2618,
      "step": 42940
    },
    {
      "epoch": 5.951226271303866,
      "grad_norm": 9.68410587310791,
      "learning_rate": 1.6206179853124568e-05,
      "loss": 2.3116,
      "step": 42950
    },
    {
      "epoch": 5.95261188859637,
      "grad_norm": 20.634057998657227,
      "learning_rate": 1.6200637383954553e-05,
      "loss": 2.009,
      "step": 42960
    },
    {
      "epoch": 5.953997505888873,
      "grad_norm": 15.450910568237305,
      "learning_rate": 1.6195094914784538e-05,
      "loss": 1.7574,
      "step": 42970
    },
    {
      "epoch": 5.955383123181377,
      "grad_norm": 5.367196559906006,
      "learning_rate": 1.6189552445614523e-05,
      "loss": 1.5983,
      "step": 42980
    },
    {
      "epoch": 5.956768740473882,
      "grad_norm": 10.912994384765625,
      "learning_rate": 1.618400997644451e-05,
      "loss": 2.1187,
      "step": 42990
    },
    {
      "epoch": 5.958154357766385,
      "grad_norm": 13.14465045928955,
      "learning_rate": 1.617846750727449e-05,
      "loss": 2.2894,
      "step": 43000
    },
    {
      "epoch": 5.959539975058889,
      "grad_norm": 13.994880676269531,
      "learning_rate": 1.6172925038104476e-05,
      "loss": 1.9442,
      "step": 43010
    },
    {
      "epoch": 5.960925592351392,
      "grad_norm": 10.304291725158691,
      "learning_rate": 1.616738256893446e-05,
      "loss": 2.059,
      "step": 43020
    },
    {
      "epoch": 5.962311209643897,
      "grad_norm": 15.672002792358398,
      "learning_rate": 1.6161840099764446e-05,
      "loss": 1.9453,
      "step": 43030
    },
    {
      "epoch": 5.9636968269364,
      "grad_norm": 6.4005022048950195,
      "learning_rate": 1.615629763059443e-05,
      "loss": 2.1074,
      "step": 43040
    },
    {
      "epoch": 5.965082444228904,
      "grad_norm": 15.326454162597656,
      "learning_rate": 1.6150755161424417e-05,
      "loss": 1.8092,
      "step": 43050
    },
    {
      "epoch": 5.966468061521407,
      "grad_norm": 13.357521057128906,
      "learning_rate": 1.6145212692254402e-05,
      "loss": 2.3342,
      "step": 43060
    },
    {
      "epoch": 5.967853678813912,
      "grad_norm": 7.692866325378418,
      "learning_rate": 1.6139670223084384e-05,
      "loss": 1.8362,
      "step": 43070
    },
    {
      "epoch": 5.969239296106416,
      "grad_norm": 13.944062232971191,
      "learning_rate": 1.613412775391437e-05,
      "loss": 1.6756,
      "step": 43080
    },
    {
      "epoch": 5.970624913398919,
      "grad_norm": 7.9000935554504395,
      "learning_rate": 1.6128585284744354e-05,
      "loss": 1.7277,
      "step": 43090
    },
    {
      "epoch": 5.972010530691423,
      "grad_norm": 9.747977256774902,
      "learning_rate": 1.612304281557434e-05,
      "loss": 1.8464,
      "step": 43100
    },
    {
      "epoch": 5.973396147983927,
      "grad_norm": 11.633028984069824,
      "learning_rate": 1.6117500346404325e-05,
      "loss": 2.2663,
      "step": 43110
    },
    {
      "epoch": 5.974781765276431,
      "grad_norm": 15.13486099243164,
      "learning_rate": 1.611195787723431e-05,
      "loss": 1.8411,
      "step": 43120
    },
    {
      "epoch": 5.976167382568934,
      "grad_norm": 13.520275115966797,
      "learning_rate": 1.6106415408064292e-05,
      "loss": 2.1935,
      "step": 43130
    },
    {
      "epoch": 5.977552999861438,
      "grad_norm": 18.227981567382812,
      "learning_rate": 1.6100872938894277e-05,
      "loss": 2.2125,
      "step": 43140
    },
    {
      "epoch": 5.978938617153942,
      "grad_norm": 18.965442657470703,
      "learning_rate": 1.6095330469724262e-05,
      "loss": 1.8831,
      "step": 43150
    },
    {
      "epoch": 5.980324234446446,
      "grad_norm": 24.53362274169922,
      "learning_rate": 1.6089788000554248e-05,
      "loss": 2.2291,
      "step": 43160
    },
    {
      "epoch": 5.98170985173895,
      "grad_norm": 12.09611701965332,
      "learning_rate": 1.6084245531384233e-05,
      "loss": 1.9905,
      "step": 43170
    },
    {
      "epoch": 5.983095469031453,
      "grad_norm": 13.486199378967285,
      "learning_rate": 1.6078703062214218e-05,
      "loss": 1.6907,
      "step": 43180
    },
    {
      "epoch": 5.9844810863239575,
      "grad_norm": 22.881187438964844,
      "learning_rate": 1.60731605930442e-05,
      "loss": 2.3422,
      "step": 43190
    },
    {
      "epoch": 5.985866703616461,
      "grad_norm": 17.238937377929688,
      "learning_rate": 1.6067618123874185e-05,
      "loss": 1.8446,
      "step": 43200
    },
    {
      "epoch": 5.987252320908965,
      "grad_norm": 12.1956148147583,
      "learning_rate": 1.6062075654704174e-05,
      "loss": 2.0752,
      "step": 43210
    },
    {
      "epoch": 5.988637938201469,
      "grad_norm": 16.474254608154297,
      "learning_rate": 1.605653318553416e-05,
      "loss": 1.9319,
      "step": 43220
    },
    {
      "epoch": 5.9900235554939725,
      "grad_norm": 10.260359764099121,
      "learning_rate": 1.605099071636414e-05,
      "loss": 2.4232,
      "step": 43230
    },
    {
      "epoch": 5.991409172786477,
      "grad_norm": 16.727724075317383,
      "learning_rate": 1.6045448247194126e-05,
      "loss": 2.2461,
      "step": 43240
    },
    {
      "epoch": 5.99279479007898,
      "grad_norm": 12.042500495910645,
      "learning_rate": 1.603990577802411e-05,
      "loss": 1.907,
      "step": 43250
    },
    {
      "epoch": 5.994180407371484,
      "grad_norm": 19.80522918701172,
      "learning_rate": 1.6034363308854093e-05,
      "loss": 2.025,
      "step": 43260
    },
    {
      "epoch": 5.9955660246639875,
      "grad_norm": 14.75330638885498,
      "learning_rate": 1.6028820839684082e-05,
      "loss": 2.3008,
      "step": 43270
    },
    {
      "epoch": 5.996951641956492,
      "grad_norm": 10.476296424865723,
      "learning_rate": 1.6023278370514067e-05,
      "loss": 1.8291,
      "step": 43280
    },
    {
      "epoch": 5.998337259248995,
      "grad_norm": 17.60331916809082,
      "learning_rate": 1.601773590134405e-05,
      "loss": 1.928,
      "step": 43290
    },
    {
      "epoch": 5.999722876541499,
      "grad_norm": 13.014877319335938,
      "learning_rate": 1.6012193432174034e-05,
      "loss": 1.9272,
      "step": 43300
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.5180873180873181,
      "eval_bert_f1": 0.9875385761260986,
      "eval_bert_precision": 0.9891836643218994,
      "eval_bert_recall": 0.9863172769546509,
      "eval_f1": 0.06574609753703196,
      "eval_loss": 2.1756038665771484,
      "eval_runtime": 294.5771,
      "eval_samples_per_second": 48.985,
      "eval_steps_per_second": 6.124,
      "eval_synonym_accuracy": 0.530977130977131,
      "step": 43302
    },
    {
      "epoch": 6.001108493834003,
      "grad_norm": 9.75029468536377,
      "learning_rate": 1.600665096300402e-05,
      "loss": 1.8138,
      "step": 43310
    },
    {
      "epoch": 6.002494111126507,
      "grad_norm": 16.042312622070312,
      "learning_rate": 1.6001108493834005e-05,
      "loss": 1.3751,
      "step": 43320
    },
    {
      "epoch": 6.003879728419011,
      "grad_norm": 13.084243774414062,
      "learning_rate": 1.599556602466399e-05,
      "loss": 1.8448,
      "step": 43330
    },
    {
      "epoch": 6.005265345711514,
      "grad_norm": 20.397602081298828,
      "learning_rate": 1.5990023555493975e-05,
      "loss": 1.5877,
      "step": 43340
    },
    {
      "epoch": 6.006650963004018,
      "grad_norm": 19.908489227294922,
      "learning_rate": 1.598448108632396e-05,
      "loss": 1.5075,
      "step": 43350
    },
    {
      "epoch": 6.0080365802965225,
      "grad_norm": 8.718182563781738,
      "learning_rate": 1.5978938617153943e-05,
      "loss": 2.0172,
      "step": 43360
    },
    {
      "epoch": 6.009422197589026,
      "grad_norm": 14.913740158081055,
      "learning_rate": 1.5973396147983928e-05,
      "loss": 2.0795,
      "step": 43370
    },
    {
      "epoch": 6.01080781488153,
      "grad_norm": 12.474640846252441,
      "learning_rate": 1.5967853678813913e-05,
      "loss": 2.245,
      "step": 43380
    },
    {
      "epoch": 6.012193432174033,
      "grad_norm": 14.215129852294922,
      "learning_rate": 1.5962311209643898e-05,
      "loss": 2.0346,
      "step": 43390
    },
    {
      "epoch": 6.0135790494665375,
      "grad_norm": 15.959728240966797,
      "learning_rate": 1.5956768740473884e-05,
      "loss": 1.7863,
      "step": 43400
    },
    {
      "epoch": 6.014964666759041,
      "grad_norm": 11.122591972351074,
      "learning_rate": 1.595122627130387e-05,
      "loss": 2.1403,
      "step": 43410
    },
    {
      "epoch": 6.016350284051545,
      "grad_norm": 12.418394088745117,
      "learning_rate": 1.594568380213385e-05,
      "loss": 1.9258,
      "step": 43420
    },
    {
      "epoch": 6.017735901344048,
      "grad_norm": 7.8811516761779785,
      "learning_rate": 1.5940141332963836e-05,
      "loss": 2.0362,
      "step": 43430
    },
    {
      "epoch": 6.0191215186365525,
      "grad_norm": 12.241066932678223,
      "learning_rate": 1.593459886379382e-05,
      "loss": 1.5537,
      "step": 43440
    },
    {
      "epoch": 6.020507135929057,
      "grad_norm": 10.886483192443848,
      "learning_rate": 1.5929056394623806e-05,
      "loss": 1.7548,
      "step": 43450
    },
    {
      "epoch": 6.02189275322156,
      "grad_norm": 16.94669532775879,
      "learning_rate": 1.592351392545379e-05,
      "loss": 1.9786,
      "step": 43460
    },
    {
      "epoch": 6.023278370514064,
      "grad_norm": 13.368674278259277,
      "learning_rate": 1.5917971456283777e-05,
      "loss": 1.7903,
      "step": 43470
    },
    {
      "epoch": 6.0246639878065675,
      "grad_norm": 16.095182418823242,
      "learning_rate": 1.591242898711376e-05,
      "loss": 1.8956,
      "step": 43480
    },
    {
      "epoch": 6.026049605099072,
      "grad_norm": 8.959429740905762,
      "learning_rate": 1.5906886517943744e-05,
      "loss": 1.7979,
      "step": 43490
    },
    {
      "epoch": 6.027435222391575,
      "grad_norm": 8.699095726013184,
      "learning_rate": 1.590134404877373e-05,
      "loss": 1.9385,
      "step": 43500
    },
    {
      "epoch": 6.028820839684079,
      "grad_norm": 15.034253120422363,
      "learning_rate": 1.5895801579603715e-05,
      "loss": 1.9084,
      "step": 43510
    },
    {
      "epoch": 6.030206456976583,
      "grad_norm": 17.0070858001709,
      "learning_rate": 1.58902591104337e-05,
      "loss": 2.3015,
      "step": 43520
    },
    {
      "epoch": 6.031592074269087,
      "grad_norm": 19.565715789794922,
      "learning_rate": 1.5884716641263685e-05,
      "loss": 1.8845,
      "step": 43530
    },
    {
      "epoch": 6.032977691561591,
      "grad_norm": 20.823909759521484,
      "learning_rate": 1.587917417209367e-05,
      "loss": 1.7823,
      "step": 43540
    },
    {
      "epoch": 6.034363308854094,
      "grad_norm": 10.734270095825195,
      "learning_rate": 1.5873631702923652e-05,
      "loss": 1.7169,
      "step": 43550
    },
    {
      "epoch": 6.035748926146598,
      "grad_norm": 20.540122985839844,
      "learning_rate": 1.5868089233753637e-05,
      "loss": 2.0217,
      "step": 43560
    },
    {
      "epoch": 6.037134543439102,
      "grad_norm": 6.452181816101074,
      "learning_rate": 1.5862546764583623e-05,
      "loss": 1.8852,
      "step": 43570
    },
    {
      "epoch": 6.038520160731606,
      "grad_norm": 11.781533241271973,
      "learning_rate": 1.5857004295413608e-05,
      "loss": 1.6149,
      "step": 43580
    },
    {
      "epoch": 6.03990577802411,
      "grad_norm": 12.431546211242676,
      "learning_rate": 1.5851461826243593e-05,
      "loss": 1.8264,
      "step": 43590
    },
    {
      "epoch": 6.041291395316613,
      "grad_norm": 18.304828643798828,
      "learning_rate": 1.584591935707358e-05,
      "loss": 2.0502,
      "step": 43600
    },
    {
      "epoch": 6.042677012609118,
      "grad_norm": 13.29552173614502,
      "learning_rate": 1.584037688790356e-05,
      "loss": 1.9286,
      "step": 43610
    },
    {
      "epoch": 6.044062629901621,
      "grad_norm": 19.250080108642578,
      "learning_rate": 1.5834834418733545e-05,
      "loss": 2.2212,
      "step": 43620
    },
    {
      "epoch": 6.045448247194125,
      "grad_norm": 10.330629348754883,
      "learning_rate": 1.582929194956353e-05,
      "loss": 2.4358,
      "step": 43630
    },
    {
      "epoch": 6.046833864486628,
      "grad_norm": 8.579747200012207,
      "learning_rate": 1.5823749480393516e-05,
      "loss": 2.0115,
      "step": 43640
    },
    {
      "epoch": 6.048219481779133,
      "grad_norm": 12.469467163085938,
      "learning_rate": 1.58182070112235e-05,
      "loss": 1.6977,
      "step": 43650
    },
    {
      "epoch": 6.049605099071637,
      "grad_norm": 9.794124603271484,
      "learning_rate": 1.5812664542053486e-05,
      "loss": 1.8808,
      "step": 43660
    },
    {
      "epoch": 6.05099071636414,
      "grad_norm": 15.138477325439453,
      "learning_rate": 1.5807122072883472e-05,
      "loss": 2.0737,
      "step": 43670
    },
    {
      "epoch": 6.052376333656644,
      "grad_norm": 12.14423942565918,
      "learning_rate": 1.5801579603713454e-05,
      "loss": 1.7029,
      "step": 43680
    },
    {
      "epoch": 6.053761950949148,
      "grad_norm": 13.553635597229004,
      "learning_rate": 1.579603713454344e-05,
      "loss": 1.7774,
      "step": 43690
    },
    {
      "epoch": 6.055147568241652,
      "grad_norm": 13.9592924118042,
      "learning_rate": 1.5790494665373427e-05,
      "loss": 1.9827,
      "step": 43700
    },
    {
      "epoch": 6.056533185534155,
      "grad_norm": 10.402677536010742,
      "learning_rate": 1.578495219620341e-05,
      "loss": 1.9029,
      "step": 43710
    },
    {
      "epoch": 6.057918802826659,
      "grad_norm": 5.798059940338135,
      "learning_rate": 1.5779409727033395e-05,
      "loss": 1.595,
      "step": 43720
    },
    {
      "epoch": 6.0593044201191635,
      "grad_norm": 11.710317611694336,
      "learning_rate": 1.577386725786338e-05,
      "loss": 2.0804,
      "step": 43730
    },
    {
      "epoch": 6.060690037411667,
      "grad_norm": 13.023480415344238,
      "learning_rate": 1.576832478869336e-05,
      "loss": 1.8065,
      "step": 43740
    },
    {
      "epoch": 6.062075654704171,
      "grad_norm": 15.703720092773438,
      "learning_rate": 1.576278231952335e-05,
      "loss": 2.1397,
      "step": 43750
    },
    {
      "epoch": 6.063461271996674,
      "grad_norm": 19.802074432373047,
      "learning_rate": 1.5757239850353336e-05,
      "loss": 1.76,
      "step": 43760
    },
    {
      "epoch": 6.0648468892891785,
      "grad_norm": 10.877397537231445,
      "learning_rate": 1.5751697381183317e-05,
      "loss": 1.7778,
      "step": 43770
    },
    {
      "epoch": 6.066232506581682,
      "grad_norm": 21.6430721282959,
      "learning_rate": 1.5746154912013303e-05,
      "loss": 2.3003,
      "step": 43780
    },
    {
      "epoch": 6.067618123874186,
      "grad_norm": 13.712495803833008,
      "learning_rate": 1.5740612442843288e-05,
      "loss": 2.0202,
      "step": 43790
    },
    {
      "epoch": 6.06900374116669,
      "grad_norm": 11.083220481872559,
      "learning_rate": 1.5735069973673273e-05,
      "loss": 2.1025,
      "step": 43800
    },
    {
      "epoch": 6.0703893584591935,
      "grad_norm": 14.19765853881836,
      "learning_rate": 1.572952750450326e-05,
      "loss": 2.2577,
      "step": 43810
    },
    {
      "epoch": 6.071774975751698,
      "grad_norm": 10.716216087341309,
      "learning_rate": 1.5723985035333244e-05,
      "loss": 1.7164,
      "step": 43820
    },
    {
      "epoch": 6.073160593044201,
      "grad_norm": 19.5867862701416,
      "learning_rate": 1.571844256616323e-05,
      "loss": 1.9283,
      "step": 43830
    },
    {
      "epoch": 6.074546210336705,
      "grad_norm": 17.251020431518555,
      "learning_rate": 1.571290009699321e-05,
      "loss": 2.002,
      "step": 43840
    },
    {
      "epoch": 6.0759318276292085,
      "grad_norm": 20.519262313842773,
      "learning_rate": 1.5707357627823196e-05,
      "loss": 1.7467,
      "step": 43850
    },
    {
      "epoch": 6.077317444921713,
      "grad_norm": 12.000640869140625,
      "learning_rate": 1.570181515865318e-05,
      "loss": 2.391,
      "step": 43860
    },
    {
      "epoch": 6.078703062214217,
      "grad_norm": 7.433408260345459,
      "learning_rate": 1.5696272689483167e-05,
      "loss": 2.1976,
      "step": 43870
    },
    {
      "epoch": 6.08008867950672,
      "grad_norm": 11.689435005187988,
      "learning_rate": 1.5690730220313152e-05,
      "loss": 1.9679,
      "step": 43880
    },
    {
      "epoch": 6.081474296799224,
      "grad_norm": 12.711878776550293,
      "learning_rate": 1.5685187751143137e-05,
      "loss": 1.8308,
      "step": 43890
    },
    {
      "epoch": 6.082859914091728,
      "grad_norm": 10.967521667480469,
      "learning_rate": 1.567964528197312e-05,
      "loss": 1.8855,
      "step": 43900
    },
    {
      "epoch": 6.084245531384232,
      "grad_norm": 12.9161958694458,
      "learning_rate": 1.5674102812803104e-05,
      "loss": 1.8683,
      "step": 43910
    },
    {
      "epoch": 6.085631148676735,
      "grad_norm": 12.063838005065918,
      "learning_rate": 1.566856034363309e-05,
      "loss": 1.995,
      "step": 43920
    },
    {
      "epoch": 6.087016765969239,
      "grad_norm": 12.653952598571777,
      "learning_rate": 1.5663017874463075e-05,
      "loss": 2.0453,
      "step": 43930
    },
    {
      "epoch": 6.088402383261743,
      "grad_norm": 9.938785552978516,
      "learning_rate": 1.565747540529306e-05,
      "loss": 1.744,
      "step": 43940
    },
    {
      "epoch": 6.089788000554247,
      "grad_norm": 10.026992797851562,
      "learning_rate": 1.5651932936123045e-05,
      "loss": 2.4534,
      "step": 43950
    },
    {
      "epoch": 6.091173617846751,
      "grad_norm": 11.637492179870605,
      "learning_rate": 1.564639046695303e-05,
      "loss": 1.675,
      "step": 43960
    },
    {
      "epoch": 6.092559235139254,
      "grad_norm": 12.127350807189941,
      "learning_rate": 1.5640847997783012e-05,
      "loss": 1.6561,
      "step": 43970
    },
    {
      "epoch": 6.0939448524317585,
      "grad_norm": 14.649531364440918,
      "learning_rate": 1.5635305528612998e-05,
      "loss": 1.7507,
      "step": 43980
    },
    {
      "epoch": 6.095330469724262,
      "grad_norm": 10.215141296386719,
      "learning_rate": 1.5629763059442983e-05,
      "loss": 1.8595,
      "step": 43990
    },
    {
      "epoch": 6.096716087016766,
      "grad_norm": 12.100167274475098,
      "learning_rate": 1.5624220590272968e-05,
      "loss": 1.7347,
      "step": 44000
    },
    {
      "epoch": 6.098101704309269,
      "grad_norm": 17.681365966796875,
      "learning_rate": 1.5618678121102953e-05,
      "loss": 2.2106,
      "step": 44010
    },
    {
      "epoch": 6.0994873216017735,
      "grad_norm": 9.370978355407715,
      "learning_rate": 1.561313565193294e-05,
      "loss": 1.931,
      "step": 44020
    },
    {
      "epoch": 6.100872938894278,
      "grad_norm": 8.775398254394531,
      "learning_rate": 1.560759318276292e-05,
      "loss": 1.9106,
      "step": 44030
    },
    {
      "epoch": 6.102258556186781,
      "grad_norm": 13.482503890991211,
      "learning_rate": 1.5602050713592906e-05,
      "loss": 1.6869,
      "step": 44040
    },
    {
      "epoch": 6.103644173479285,
      "grad_norm": 10.488781929016113,
      "learning_rate": 1.559650824442289e-05,
      "loss": 1.6435,
      "step": 44050
    },
    {
      "epoch": 6.1050297907717885,
      "grad_norm": 7.026005744934082,
      "learning_rate": 1.5590965775252876e-05,
      "loss": 1.9051,
      "step": 44060
    },
    {
      "epoch": 6.106415408064293,
      "grad_norm": 19.71730613708496,
      "learning_rate": 1.558542330608286e-05,
      "loss": 1.5222,
      "step": 44070
    },
    {
      "epoch": 6.107801025356796,
      "grad_norm": 8.662930488586426,
      "learning_rate": 1.5579880836912847e-05,
      "loss": 1.8259,
      "step": 44080
    },
    {
      "epoch": 6.1091866426493,
      "grad_norm": 12.510644912719727,
      "learning_rate": 1.5574338367742832e-05,
      "loss": 1.593,
      "step": 44090
    },
    {
      "epoch": 6.110572259941804,
      "grad_norm": 9.033130645751953,
      "learning_rate": 1.5568795898572814e-05,
      "loss": 1.8733,
      "step": 44100
    },
    {
      "epoch": 6.111957877234308,
      "grad_norm": 10.936929702758789,
      "learning_rate": 1.55632534294028e-05,
      "loss": 1.7851,
      "step": 44110
    },
    {
      "epoch": 6.113343494526812,
      "grad_norm": 13.989118576049805,
      "learning_rate": 1.5557710960232784e-05,
      "loss": 2.1333,
      "step": 44120
    },
    {
      "epoch": 6.114729111819315,
      "grad_norm": 18.254207611083984,
      "learning_rate": 1.555216849106277e-05,
      "loss": 1.8695,
      "step": 44130
    },
    {
      "epoch": 6.116114729111819,
      "grad_norm": 6.078042984008789,
      "learning_rate": 1.5546626021892755e-05,
      "loss": 1.7259,
      "step": 44140
    },
    {
      "epoch": 6.117500346404323,
      "grad_norm": 11.14565372467041,
      "learning_rate": 1.554108355272274e-05,
      "loss": 2.4259,
      "step": 44150
    },
    {
      "epoch": 6.118885963696827,
      "grad_norm": 17.567617416381836,
      "learning_rate": 1.5535541083552722e-05,
      "loss": 2.3144,
      "step": 44160
    },
    {
      "epoch": 6.120271580989331,
      "grad_norm": 21.380632400512695,
      "learning_rate": 1.5529998614382707e-05,
      "loss": 1.8596,
      "step": 44170
    },
    {
      "epoch": 6.121657198281834,
      "grad_norm": 13.387639999389648,
      "learning_rate": 1.5524456145212696e-05,
      "loss": 2.032,
      "step": 44180
    },
    {
      "epoch": 6.123042815574339,
      "grad_norm": 10.540809631347656,
      "learning_rate": 1.5518913676042678e-05,
      "loss": 1.7969,
      "step": 44190
    },
    {
      "epoch": 6.124428432866842,
      "grad_norm": 19.38022804260254,
      "learning_rate": 1.5513371206872663e-05,
      "loss": 2.411,
      "step": 44200
    },
    {
      "epoch": 6.125814050159346,
      "grad_norm": 7.647383213043213,
      "learning_rate": 1.5507828737702648e-05,
      "loss": 1.9163,
      "step": 44210
    },
    {
      "epoch": 6.127199667451849,
      "grad_norm": 8.78010082244873,
      "learning_rate": 1.5502286268532633e-05,
      "loss": 1.7721,
      "step": 44220
    },
    {
      "epoch": 6.128585284744354,
      "grad_norm": 9.314850807189941,
      "learning_rate": 1.5496743799362615e-05,
      "loss": 2.1856,
      "step": 44230
    },
    {
      "epoch": 6.129970902036858,
      "grad_norm": 15.435073852539062,
      "learning_rate": 1.5491201330192604e-05,
      "loss": 1.7268,
      "step": 44240
    },
    {
      "epoch": 6.131356519329361,
      "grad_norm": 16.380460739135742,
      "learning_rate": 1.548565886102259e-05,
      "loss": 1.8133,
      "step": 44250
    },
    {
      "epoch": 6.132742136621865,
      "grad_norm": 11.670832633972168,
      "learning_rate": 1.548011639185257e-05,
      "loss": 1.7861,
      "step": 44260
    },
    {
      "epoch": 6.134127753914369,
      "grad_norm": 19.34691047668457,
      "learning_rate": 1.5474573922682556e-05,
      "loss": 2.1897,
      "step": 44270
    },
    {
      "epoch": 6.135513371206873,
      "grad_norm": 7.331147193908691,
      "learning_rate": 1.546903145351254e-05,
      "loss": 2.1125,
      "step": 44280
    },
    {
      "epoch": 6.136898988499376,
      "grad_norm": 11.004057884216309,
      "learning_rate": 1.5463488984342527e-05,
      "loss": 2.0063,
      "step": 44290
    },
    {
      "epoch": 6.13828460579188,
      "grad_norm": 9.928938865661621,
      "learning_rate": 1.5457946515172512e-05,
      "loss": 1.7461,
      "step": 44300
    },
    {
      "epoch": 6.1396702230843845,
      "grad_norm": 11.454744338989258,
      "learning_rate": 1.5452404046002497e-05,
      "loss": 1.8686,
      "step": 44310
    },
    {
      "epoch": 6.141055840376888,
      "grad_norm": 10.748054504394531,
      "learning_rate": 1.544686157683248e-05,
      "loss": 1.7078,
      "step": 44320
    },
    {
      "epoch": 6.142441457669392,
      "grad_norm": 12.51748275756836,
      "learning_rate": 1.5441319107662464e-05,
      "loss": 2.2665,
      "step": 44330
    },
    {
      "epoch": 6.143827074961895,
      "grad_norm": 10.285080909729004,
      "learning_rate": 1.543577663849245e-05,
      "loss": 1.6243,
      "step": 44340
    },
    {
      "epoch": 6.1452126922543995,
      "grad_norm": 9.415369987487793,
      "learning_rate": 1.5430234169322435e-05,
      "loss": 1.8428,
      "step": 44350
    },
    {
      "epoch": 6.146598309546903,
      "grad_norm": 13.46601676940918,
      "learning_rate": 1.542469170015242e-05,
      "loss": 2.0737,
      "step": 44360
    },
    {
      "epoch": 6.147983926839407,
      "grad_norm": 12.531160354614258,
      "learning_rate": 1.5419149230982405e-05,
      "loss": 1.7891,
      "step": 44370
    },
    {
      "epoch": 6.149369544131911,
      "grad_norm": 13.341703414916992,
      "learning_rate": 1.541360676181239e-05,
      "loss": 2.345,
      "step": 44380
    },
    {
      "epoch": 6.1507551614244145,
      "grad_norm": 7.759505271911621,
      "learning_rate": 1.5408064292642372e-05,
      "loss": 1.7008,
      "step": 44390
    },
    {
      "epoch": 6.152140778716919,
      "grad_norm": 16.82295036315918,
      "learning_rate": 1.5402521823472358e-05,
      "loss": 2.1241,
      "step": 44400
    },
    {
      "epoch": 6.153526396009422,
      "grad_norm": 12.378863334655762,
      "learning_rate": 1.5396979354302343e-05,
      "loss": 1.8591,
      "step": 44410
    },
    {
      "epoch": 6.154912013301926,
      "grad_norm": 9.297806739807129,
      "learning_rate": 1.5391436885132328e-05,
      "loss": 2.0698,
      "step": 44420
    },
    {
      "epoch": 6.1562976305944295,
      "grad_norm": 18.149349212646484,
      "learning_rate": 1.5385894415962313e-05,
      "loss": 1.806,
      "step": 44430
    },
    {
      "epoch": 6.157683247886934,
      "grad_norm": 10.501755714416504,
      "learning_rate": 1.53803519467923e-05,
      "loss": 1.5288,
      "step": 44440
    },
    {
      "epoch": 6.159068865179438,
      "grad_norm": 11.117867469787598,
      "learning_rate": 1.537480947762228e-05,
      "loss": 1.8807,
      "step": 44450
    },
    {
      "epoch": 6.160454482471941,
      "grad_norm": 23.59074592590332,
      "learning_rate": 1.5369267008452266e-05,
      "loss": 1.9928,
      "step": 44460
    },
    {
      "epoch": 6.161840099764445,
      "grad_norm": 11.444276809692383,
      "learning_rate": 1.536372453928225e-05,
      "loss": 1.7608,
      "step": 44470
    },
    {
      "epoch": 6.163225717056949,
      "grad_norm": 12.884247779846191,
      "learning_rate": 1.5358182070112236e-05,
      "loss": 1.8732,
      "step": 44480
    },
    {
      "epoch": 6.164611334349453,
      "grad_norm": 10.710620880126953,
      "learning_rate": 1.535263960094222e-05,
      "loss": 1.5203,
      "step": 44490
    },
    {
      "epoch": 6.165996951641956,
      "grad_norm": 13.503570556640625,
      "learning_rate": 1.5347097131772207e-05,
      "loss": 2.3312,
      "step": 44500
    },
    {
      "epoch": 6.16738256893446,
      "grad_norm": 11.88332462310791,
      "learning_rate": 1.5341554662602192e-05,
      "loss": 1.86,
      "step": 44510
    },
    {
      "epoch": 6.1687681862269645,
      "grad_norm": 17.920156478881836,
      "learning_rate": 1.5336012193432174e-05,
      "loss": 1.6383,
      "step": 44520
    },
    {
      "epoch": 6.170153803519468,
      "grad_norm": 12.451537132263184,
      "learning_rate": 1.533046972426216e-05,
      "loss": 1.8272,
      "step": 44530
    },
    {
      "epoch": 6.171539420811972,
      "grad_norm": 11.657038688659668,
      "learning_rate": 1.5324927255092144e-05,
      "loss": 1.6439,
      "step": 44540
    },
    {
      "epoch": 6.172925038104475,
      "grad_norm": 8.063464164733887,
      "learning_rate": 1.531938478592213e-05,
      "loss": 2.1687,
      "step": 44550
    },
    {
      "epoch": 6.1743106553969795,
      "grad_norm": 13.361616134643555,
      "learning_rate": 1.5313842316752115e-05,
      "loss": 1.9219,
      "step": 44560
    },
    {
      "epoch": 6.175696272689483,
      "grad_norm": 16.174095153808594,
      "learning_rate": 1.53082998475821e-05,
      "loss": 2.0523,
      "step": 44570
    },
    {
      "epoch": 6.177081889981987,
      "grad_norm": 15.653793334960938,
      "learning_rate": 1.5302757378412082e-05,
      "loss": 2.1478,
      "step": 44580
    },
    {
      "epoch": 6.17846750727449,
      "grad_norm": 9.639995574951172,
      "learning_rate": 1.5297214909242067e-05,
      "loss": 2.0081,
      "step": 44590
    },
    {
      "epoch": 6.1798531245669945,
      "grad_norm": 21.68549346923828,
      "learning_rate": 1.5291672440072052e-05,
      "loss": 2.146,
      "step": 44600
    },
    {
      "epoch": 6.181238741859499,
      "grad_norm": 12.889701843261719,
      "learning_rate": 1.5286129970902038e-05,
      "loss": 2.3573,
      "step": 44610
    },
    {
      "epoch": 6.182624359152002,
      "grad_norm": 12.893619537353516,
      "learning_rate": 1.5280587501732023e-05,
      "loss": 2.0595,
      "step": 44620
    },
    {
      "epoch": 6.184009976444506,
      "grad_norm": 13.255663871765137,
      "learning_rate": 1.5275045032562008e-05,
      "loss": 1.8029,
      "step": 44630
    },
    {
      "epoch": 6.1853955937370095,
      "grad_norm": 26.876401901245117,
      "learning_rate": 1.526950256339199e-05,
      "loss": 1.7685,
      "step": 44640
    },
    {
      "epoch": 6.186781211029514,
      "grad_norm": 12.931583404541016,
      "learning_rate": 1.5263960094221975e-05,
      "loss": 2.1419,
      "step": 44650
    },
    {
      "epoch": 6.188166828322017,
      "grad_norm": 11.065630912780762,
      "learning_rate": 1.5258417625051962e-05,
      "loss": 1.5522,
      "step": 44660
    },
    {
      "epoch": 6.189552445614521,
      "grad_norm": 11.887079238891602,
      "learning_rate": 1.5252875155881948e-05,
      "loss": 2.1223,
      "step": 44670
    },
    {
      "epoch": 6.190938062907025,
      "grad_norm": 14.974080085754395,
      "learning_rate": 1.5247332686711931e-05,
      "loss": 2.1729,
      "step": 44680
    },
    {
      "epoch": 6.192323680199529,
      "grad_norm": 11.392459869384766,
      "learning_rate": 1.5241790217541916e-05,
      "loss": 1.8906,
      "step": 44690
    },
    {
      "epoch": 6.193709297492033,
      "grad_norm": 15.407022476196289,
      "learning_rate": 1.5236247748371902e-05,
      "loss": 1.8526,
      "step": 44700
    },
    {
      "epoch": 6.195094914784536,
      "grad_norm": 12.791481971740723,
      "learning_rate": 1.5230705279201885e-05,
      "loss": 2.0501,
      "step": 44710
    },
    {
      "epoch": 6.19648053207704,
      "grad_norm": 20.595890045166016,
      "learning_rate": 1.522516281003187e-05,
      "loss": 2.2972,
      "step": 44720
    },
    {
      "epoch": 6.197866149369544,
      "grad_norm": 11.386852264404297,
      "learning_rate": 1.5219620340861856e-05,
      "loss": 1.7816,
      "step": 44730
    },
    {
      "epoch": 6.199251766662048,
      "grad_norm": 9.815330505371094,
      "learning_rate": 1.521407787169184e-05,
      "loss": 2.0162,
      "step": 44740
    },
    {
      "epoch": 6.200637383954552,
      "grad_norm": 8.519676208496094,
      "learning_rate": 1.5208535402521824e-05,
      "loss": 1.9513,
      "step": 44750
    },
    {
      "epoch": 6.202023001247055,
      "grad_norm": 17.610387802124023,
      "learning_rate": 1.520299293335181e-05,
      "loss": 2.0488,
      "step": 44760
    },
    {
      "epoch": 6.20340861853956,
      "grad_norm": 15.306533813476562,
      "learning_rate": 1.5197450464181793e-05,
      "loss": 2.0464,
      "step": 44770
    },
    {
      "epoch": 6.204794235832063,
      "grad_norm": 12.710601806640625,
      "learning_rate": 1.5191907995011778e-05,
      "loss": 1.6194,
      "step": 44780
    },
    {
      "epoch": 6.206179853124567,
      "grad_norm": 13.938272476196289,
      "learning_rate": 1.5186365525841764e-05,
      "loss": 1.8403,
      "step": 44790
    },
    {
      "epoch": 6.20756547041707,
      "grad_norm": 10.313678741455078,
      "learning_rate": 1.5180823056671749e-05,
      "loss": 2.0622,
      "step": 44800
    },
    {
      "epoch": 6.208951087709575,
      "grad_norm": 16.706039428710938,
      "learning_rate": 1.5175280587501733e-05,
      "loss": 2.12,
      "step": 44810
    },
    {
      "epoch": 6.210336705002079,
      "grad_norm": 21.230693817138672,
      "learning_rate": 1.5169738118331718e-05,
      "loss": 1.8852,
      "step": 44820
    },
    {
      "epoch": 6.211722322294582,
      "grad_norm": 12.424819946289062,
      "learning_rate": 1.5164195649161703e-05,
      "loss": 2.3134,
      "step": 44830
    },
    {
      "epoch": 6.213107939587086,
      "grad_norm": 15.247591018676758,
      "learning_rate": 1.5158653179991687e-05,
      "loss": 2.2737,
      "step": 44840
    },
    {
      "epoch": 6.21449355687959,
      "grad_norm": 14.444400787353516,
      "learning_rate": 1.5153110710821672e-05,
      "loss": 1.9693,
      "step": 44850
    },
    {
      "epoch": 6.215879174172094,
      "grad_norm": 9.078536033630371,
      "learning_rate": 1.5147568241651657e-05,
      "loss": 1.7944,
      "step": 44860
    },
    {
      "epoch": 6.217264791464597,
      "grad_norm": 9.36526870727539,
      "learning_rate": 1.514202577248164e-05,
      "loss": 1.7632,
      "step": 44870
    },
    {
      "epoch": 6.218650408757101,
      "grad_norm": 10.706449508666992,
      "learning_rate": 1.5136483303311626e-05,
      "loss": 1.9281,
      "step": 44880
    },
    {
      "epoch": 6.2200360260496055,
      "grad_norm": 12.135910987854004,
      "learning_rate": 1.5130940834141611e-05,
      "loss": 2.1575,
      "step": 44890
    },
    {
      "epoch": 6.221421643342109,
      "grad_norm": 8.070483207702637,
      "learning_rate": 1.5125398364971595e-05,
      "loss": 1.563,
      "step": 44900
    },
    {
      "epoch": 6.222807260634613,
      "grad_norm": 10.651285171508789,
      "learning_rate": 1.511985589580158e-05,
      "loss": 1.5894,
      "step": 44910
    },
    {
      "epoch": 6.224192877927116,
      "grad_norm": 17.040937423706055,
      "learning_rate": 1.5114313426631567e-05,
      "loss": 1.5937,
      "step": 44920
    },
    {
      "epoch": 6.2255784952196205,
      "grad_norm": 10.73879337310791,
      "learning_rate": 1.5108770957461549e-05,
      "loss": 1.666,
      "step": 44930
    },
    {
      "epoch": 6.226964112512124,
      "grad_norm": 9.099111557006836,
      "learning_rate": 1.5103228488291534e-05,
      "loss": 1.6576,
      "step": 44940
    },
    {
      "epoch": 6.228349729804628,
      "grad_norm": 13.78994369506836,
      "learning_rate": 1.5097686019121521e-05,
      "loss": 1.6736,
      "step": 44950
    },
    {
      "epoch": 6.229735347097132,
      "grad_norm": 24.577146530151367,
      "learning_rate": 1.5092143549951506e-05,
      "loss": 1.6279,
      "step": 44960
    },
    {
      "epoch": 6.2311209643896355,
      "grad_norm": 9.223227500915527,
      "learning_rate": 1.5086601080781488e-05,
      "loss": 2.0993,
      "step": 44970
    },
    {
      "epoch": 6.23250658168214,
      "grad_norm": 19.214860916137695,
      "learning_rate": 1.5081058611611475e-05,
      "loss": 2.1856,
      "step": 44980
    },
    {
      "epoch": 6.233892198974643,
      "grad_norm": 14.933971405029297,
      "learning_rate": 1.507551614244146e-05,
      "loss": 2.2061,
      "step": 44990
    },
    {
      "epoch": 6.235277816267147,
      "grad_norm": 12.869872093200684,
      "learning_rate": 1.5069973673271444e-05,
      "loss": 1.7303,
      "step": 45000
    },
    {
      "epoch": 6.2366634335596505,
      "grad_norm": 16.440771102905273,
      "learning_rate": 1.5064431204101429e-05,
      "loss": 1.9189,
      "step": 45010
    },
    {
      "epoch": 6.238049050852155,
      "grad_norm": 14.420807838439941,
      "learning_rate": 1.5058888734931414e-05,
      "loss": 1.6906,
      "step": 45020
    },
    {
      "epoch": 6.239434668144659,
      "grad_norm": 12.602564811706543,
      "learning_rate": 1.5053346265761398e-05,
      "loss": 1.7878,
      "step": 45030
    },
    {
      "epoch": 6.240820285437162,
      "grad_norm": 11.243276596069336,
      "learning_rate": 1.5047803796591383e-05,
      "loss": 1.6655,
      "step": 45040
    },
    {
      "epoch": 6.242205902729666,
      "grad_norm": 18.291255950927734,
      "learning_rate": 1.5042261327421368e-05,
      "loss": 2.4986,
      "step": 45050
    },
    {
      "epoch": 6.24359152002217,
      "grad_norm": 8.09941577911377,
      "learning_rate": 1.5036718858251352e-05,
      "loss": 2.07,
      "step": 45060
    },
    {
      "epoch": 6.244977137314674,
      "grad_norm": 13.454910278320312,
      "learning_rate": 1.5031176389081337e-05,
      "loss": 1.8182,
      "step": 45070
    },
    {
      "epoch": 6.246362754607177,
      "grad_norm": 16.26447296142578,
      "learning_rate": 1.5025633919911322e-05,
      "loss": 1.6228,
      "step": 45080
    },
    {
      "epoch": 6.247748371899681,
      "grad_norm": 11.364152908325195,
      "learning_rate": 1.5020091450741308e-05,
      "loss": 2.0217,
      "step": 45090
    },
    {
      "epoch": 6.2491339891921855,
      "grad_norm": 6.157052040100098,
      "learning_rate": 1.5015103228488292e-05,
      "loss": 1.8247,
      "step": 45100
    },
    {
      "epoch": 6.250519606484689,
      "grad_norm": 7.844686508178711,
      "learning_rate": 1.5009560759318278e-05,
      "loss": 1.7514,
      "step": 45110
    },
    {
      "epoch": 6.251905223777193,
      "grad_norm": 14.91981315612793,
      "learning_rate": 1.5004018290148263e-05,
      "loss": 2.0243,
      "step": 45120
    },
    {
      "epoch": 6.253290841069696,
      "grad_norm": 12.704614639282227,
      "learning_rate": 1.4998475820978246e-05,
      "loss": 2.1806,
      "step": 45130
    },
    {
      "epoch": 6.2546764583622005,
      "grad_norm": 12.64356803894043,
      "learning_rate": 1.4992933351808232e-05,
      "loss": 1.8114,
      "step": 45140
    },
    {
      "epoch": 6.256062075654704,
      "grad_norm": 16.667634963989258,
      "learning_rate": 1.4987390882638217e-05,
      "loss": 1.7693,
      "step": 45150
    },
    {
      "epoch": 6.257447692947208,
      "grad_norm": 20.659027099609375,
      "learning_rate": 1.49818484134682e-05,
      "loss": 2.1556,
      "step": 45160
    },
    {
      "epoch": 6.258833310239712,
      "grad_norm": 16.906394958496094,
      "learning_rate": 1.4976305944298186e-05,
      "loss": 1.8915,
      "step": 45170
    },
    {
      "epoch": 6.2602189275322155,
      "grad_norm": 7.8183746337890625,
      "learning_rate": 1.4970763475128171e-05,
      "loss": 2.2148,
      "step": 45180
    },
    {
      "epoch": 6.26160454482472,
      "grad_norm": 13.391571044921875,
      "learning_rate": 1.4965221005958155e-05,
      "loss": 1.832,
      "step": 45190
    },
    {
      "epoch": 6.262990162117223,
      "grad_norm": 13.574828147888184,
      "learning_rate": 1.495967853678814e-05,
      "loss": 1.5818,
      "step": 45200
    },
    {
      "epoch": 6.264375779409727,
      "grad_norm": 9.358041763305664,
      "learning_rate": 1.4954136067618125e-05,
      "loss": 1.9913,
      "step": 45210
    },
    {
      "epoch": 6.2657613967022305,
      "grad_norm": 10.399988174438477,
      "learning_rate": 1.4948593598448109e-05,
      "loss": 2.0503,
      "step": 45220
    },
    {
      "epoch": 6.267147013994735,
      "grad_norm": 16.566659927368164,
      "learning_rate": 1.4943051129278094e-05,
      "loss": 1.9038,
      "step": 45230
    },
    {
      "epoch": 6.268532631287238,
      "grad_norm": 10.707697868347168,
      "learning_rate": 1.4937508660108079e-05,
      "loss": 1.6554,
      "step": 45240
    },
    {
      "epoch": 6.269918248579742,
      "grad_norm": 11.93838882446289,
      "learning_rate": 1.4931966190938063e-05,
      "loss": 2.098,
      "step": 45250
    },
    {
      "epoch": 6.271303865872246,
      "grad_norm": 14.978038787841797,
      "learning_rate": 1.4926423721768048e-05,
      "loss": 1.9042,
      "step": 45260
    },
    {
      "epoch": 6.27268948316475,
      "grad_norm": 10.985363006591797,
      "learning_rate": 1.4920881252598035e-05,
      "loss": 2.013,
      "step": 45270
    },
    {
      "epoch": 6.274075100457254,
      "grad_norm": 19.1617431640625,
      "learning_rate": 1.491533878342802e-05,
      "loss": 1.8662,
      "step": 45280
    },
    {
      "epoch": 6.275460717749757,
      "grad_norm": 18.668237686157227,
      "learning_rate": 1.4909796314258002e-05,
      "loss": 1.3836,
      "step": 45290
    },
    {
      "epoch": 6.276846335042261,
      "grad_norm": 15.013484001159668,
      "learning_rate": 1.4904253845087989e-05,
      "loss": 2.1061,
      "step": 45300
    },
    {
      "epoch": 6.278231952334766,
      "grad_norm": 12.635396957397461,
      "learning_rate": 1.4898711375917974e-05,
      "loss": 1.8966,
      "step": 45310
    },
    {
      "epoch": 6.279617569627269,
      "grad_norm": 17.762208938598633,
      "learning_rate": 1.4893168906747956e-05,
      "loss": 1.8327,
      "step": 45320
    },
    {
      "epoch": 6.281003186919773,
      "grad_norm": 16.063228607177734,
      "learning_rate": 1.4887626437577943e-05,
      "loss": 1.9171,
      "step": 45330
    },
    {
      "epoch": 6.282388804212276,
      "grad_norm": 15.786063194274902,
      "learning_rate": 1.4882083968407928e-05,
      "loss": 1.4564,
      "step": 45340
    },
    {
      "epoch": 6.283774421504781,
      "grad_norm": 21.46319580078125,
      "learning_rate": 1.4876541499237912e-05,
      "loss": 1.8672,
      "step": 45350
    },
    {
      "epoch": 6.285160038797284,
      "grad_norm": 8.409079551696777,
      "learning_rate": 1.4870999030067897e-05,
      "loss": 2.0295,
      "step": 45360
    },
    {
      "epoch": 6.286545656089788,
      "grad_norm": 7.718584060668945,
      "learning_rate": 1.4865456560897882e-05,
      "loss": 1.9052,
      "step": 45370
    },
    {
      "epoch": 6.287931273382291,
      "grad_norm": 8.642048835754395,
      "learning_rate": 1.4859914091727866e-05,
      "loss": 1.6056,
      "step": 45380
    },
    {
      "epoch": 6.289316890674796,
      "grad_norm": 15.1923828125,
      "learning_rate": 1.4854371622557851e-05,
      "loss": 1.5734,
      "step": 45390
    },
    {
      "epoch": 6.2907025079673,
      "grad_norm": 14.199116706848145,
      "learning_rate": 1.4848829153387836e-05,
      "loss": 1.6056,
      "step": 45400
    },
    {
      "epoch": 6.292088125259803,
      "grad_norm": 13.528936386108398,
      "learning_rate": 1.484328668421782e-05,
      "loss": 2.2416,
      "step": 45410
    },
    {
      "epoch": 6.293473742552307,
      "grad_norm": 12.105499267578125,
      "learning_rate": 1.4837744215047805e-05,
      "loss": 2.3754,
      "step": 45420
    },
    {
      "epoch": 6.294859359844811,
      "grad_norm": 17.449655532836914,
      "learning_rate": 1.483220174587779e-05,
      "loss": 1.8442,
      "step": 45430
    },
    {
      "epoch": 6.296244977137315,
      "grad_norm": 6.72791051864624,
      "learning_rate": 1.4826659276707776e-05,
      "loss": 1.8808,
      "step": 45440
    },
    {
      "epoch": 6.297630594429818,
      "grad_norm": 10.132752418518066,
      "learning_rate": 1.4821116807537759e-05,
      "loss": 1.797,
      "step": 45450
    },
    {
      "epoch": 6.299016211722322,
      "grad_norm": 10.289023399353027,
      "learning_rate": 1.4815574338367744e-05,
      "loss": 2.0089,
      "step": 45460
    },
    {
      "epoch": 6.3004018290148265,
      "grad_norm": 13.750029563903809,
      "learning_rate": 1.481003186919773e-05,
      "loss": 1.9663,
      "step": 45470
    },
    {
      "epoch": 6.30178744630733,
      "grad_norm": 5.654365062713623,
      "learning_rate": 1.4804489400027713e-05,
      "loss": 1.9491,
      "step": 45480
    },
    {
      "epoch": 6.303173063599834,
      "grad_norm": 14.41341781616211,
      "learning_rate": 1.4798946930857698e-05,
      "loss": 1.8792,
      "step": 45490
    },
    {
      "epoch": 6.304558680892337,
      "grad_norm": 22.29203224182129,
      "learning_rate": 1.4793404461687684e-05,
      "loss": 1.9427,
      "step": 45500
    },
    {
      "epoch": 6.3059442981848415,
      "grad_norm": 7.30391263961792,
      "learning_rate": 1.4787861992517667e-05,
      "loss": 1.7759,
      "step": 45510
    },
    {
      "epoch": 6.307329915477345,
      "grad_norm": 11.803314208984375,
      "learning_rate": 1.4782319523347652e-05,
      "loss": 1.4036,
      "step": 45520
    },
    {
      "epoch": 6.308715532769849,
      "grad_norm": 9.594152450561523,
      "learning_rate": 1.4776777054177638e-05,
      "loss": 1.1189,
      "step": 45530
    },
    {
      "epoch": 6.310101150062353,
      "grad_norm": 22.894868850708008,
      "learning_rate": 1.4771234585007621e-05,
      "loss": 2.1499,
      "step": 45540
    },
    {
      "epoch": 6.3114867673548565,
      "grad_norm": 15.123235702514648,
      "learning_rate": 1.4765692115837607e-05,
      "loss": 2.0579,
      "step": 45550
    },
    {
      "epoch": 6.312872384647361,
      "grad_norm": 7.92909574508667,
      "learning_rate": 1.4760149646667592e-05,
      "loss": 1.8939,
      "step": 45560
    },
    {
      "epoch": 6.314258001939864,
      "grad_norm": 20.235567092895508,
      "learning_rate": 1.4754607177497577e-05,
      "loss": 2.383,
      "step": 45570
    },
    {
      "epoch": 6.315643619232368,
      "grad_norm": 13.287342071533203,
      "learning_rate": 1.474906470832756e-05,
      "loss": 2.2763,
      "step": 45580
    },
    {
      "epoch": 6.3170292365248715,
      "grad_norm": 11.638114929199219,
      "learning_rate": 1.4743522239157546e-05,
      "loss": 2.0364,
      "step": 45590
    },
    {
      "epoch": 6.318414853817376,
      "grad_norm": 12.653169631958008,
      "learning_rate": 1.4737979769987531e-05,
      "loss": 1.8866,
      "step": 45600
    },
    {
      "epoch": 6.31980047110988,
      "grad_norm": 15.2315673828125,
      "learning_rate": 1.4732437300817515e-05,
      "loss": 1.8679,
      "step": 45610
    },
    {
      "epoch": 6.321186088402383,
      "grad_norm": 4.671535491943359,
      "learning_rate": 1.47268948316475e-05,
      "loss": 1.7719,
      "step": 45620
    },
    {
      "epoch": 6.322571705694887,
      "grad_norm": 22.755718231201172,
      "learning_rate": 1.4721352362477485e-05,
      "loss": 1.7635,
      "step": 45630
    },
    {
      "epoch": 6.323957322987391,
      "grad_norm": 17.831119537353516,
      "learning_rate": 1.4715809893307469e-05,
      "loss": 2.1953,
      "step": 45640
    },
    {
      "epoch": 6.325342940279895,
      "grad_norm": 11.549660682678223,
      "learning_rate": 1.4710267424137454e-05,
      "loss": 1.7946,
      "step": 45650
    },
    {
      "epoch": 6.326728557572398,
      "grad_norm": 13.300615310668945,
      "learning_rate": 1.470472495496744e-05,
      "loss": 2.1054,
      "step": 45660
    },
    {
      "epoch": 6.328114174864902,
      "grad_norm": 13.035848617553711,
      "learning_rate": 1.4699182485797423e-05,
      "loss": 2.1728,
      "step": 45670
    },
    {
      "epoch": 6.3294997921574065,
      "grad_norm": 7.819351673126221,
      "learning_rate": 1.4693640016627408e-05,
      "loss": 1.6699,
      "step": 45680
    },
    {
      "epoch": 6.33088540944991,
      "grad_norm": 7.826213836669922,
      "learning_rate": 1.4688097547457393e-05,
      "loss": 2.1766,
      "step": 45690
    },
    {
      "epoch": 6.332271026742414,
      "grad_norm": 16.906734466552734,
      "learning_rate": 1.4682555078287377e-05,
      "loss": 2.4021,
      "step": 45700
    },
    {
      "epoch": 6.333656644034917,
      "grad_norm": 16.274282455444336,
      "learning_rate": 1.4677012609117362e-05,
      "loss": 1.7631,
      "step": 45710
    },
    {
      "epoch": 6.3350422613274215,
      "grad_norm": 16.17551040649414,
      "learning_rate": 1.4671470139947347e-05,
      "loss": 2.0135,
      "step": 45720
    },
    {
      "epoch": 6.336427878619925,
      "grad_norm": 9.956146240234375,
      "learning_rate": 1.4665927670777334e-05,
      "loss": 1.6746,
      "step": 45730
    },
    {
      "epoch": 6.337813495912429,
      "grad_norm": 6.963425636291504,
      "learning_rate": 1.4660385201607316e-05,
      "loss": 1.9957,
      "step": 45740
    },
    {
      "epoch": 6.339199113204932,
      "grad_norm": 15.426787376403809,
      "learning_rate": 1.4654842732437301e-05,
      "loss": 2.1952,
      "step": 45750
    },
    {
      "epoch": 6.3405847304974365,
      "grad_norm": 9.714064598083496,
      "learning_rate": 1.4649300263267288e-05,
      "loss": 1.7697,
      "step": 45760
    },
    {
      "epoch": 6.341970347789941,
      "grad_norm": 9.976037979125977,
      "learning_rate": 1.464375779409727e-05,
      "loss": 1.6133,
      "step": 45770
    },
    {
      "epoch": 6.343355965082444,
      "grad_norm": 13.829812049865723,
      "learning_rate": 1.4638215324927257e-05,
      "loss": 1.8601,
      "step": 45780
    },
    {
      "epoch": 6.344741582374948,
      "grad_norm": 11.284379005432129,
      "learning_rate": 1.4632672855757242e-05,
      "loss": 1.8098,
      "step": 45790
    },
    {
      "epoch": 6.3461271996674515,
      "grad_norm": 15.759222984313965,
      "learning_rate": 1.4627130386587224e-05,
      "loss": 1.9317,
      "step": 45800
    },
    {
      "epoch": 6.347512816959956,
      "grad_norm": 9.353776931762695,
      "learning_rate": 1.4621587917417211e-05,
      "loss": 1.5828,
      "step": 45810
    },
    {
      "epoch": 6.34889843425246,
      "grad_norm": 9.917346000671387,
      "learning_rate": 1.4616045448247196e-05,
      "loss": 1.7014,
      "step": 45820
    },
    {
      "epoch": 6.350284051544963,
      "grad_norm": 19.921953201293945,
      "learning_rate": 1.4610502979077178e-05,
      "loss": 2.1358,
      "step": 45830
    },
    {
      "epoch": 6.351669668837467,
      "grad_norm": 8.723174095153809,
      "learning_rate": 1.4604960509907165e-05,
      "loss": 1.7915,
      "step": 45840
    },
    {
      "epoch": 6.353055286129971,
      "grad_norm": 23.518428802490234,
      "learning_rate": 1.459941804073715e-05,
      "loss": 1.6514,
      "step": 45850
    },
    {
      "epoch": 6.354440903422475,
      "grad_norm": 11.483316421508789,
      "learning_rate": 1.4593875571567136e-05,
      "loss": 1.411,
      "step": 45860
    },
    {
      "epoch": 6.355826520714978,
      "grad_norm": 12.158675193786621,
      "learning_rate": 1.458833310239712e-05,
      "loss": 1.7233,
      "step": 45870
    },
    {
      "epoch": 6.357212138007482,
      "grad_norm": 11.655858993530273,
      "learning_rate": 1.4582790633227105e-05,
      "loss": 1.8265,
      "step": 45880
    },
    {
      "epoch": 6.358597755299986,
      "grad_norm": 11.908564567565918,
      "learning_rate": 1.457724816405709e-05,
      "loss": 1.7718,
      "step": 45890
    },
    {
      "epoch": 6.35998337259249,
      "grad_norm": 17.16512107849121,
      "learning_rate": 1.4571705694887073e-05,
      "loss": 1.9629,
      "step": 45900
    },
    {
      "epoch": 6.361368989884994,
      "grad_norm": 11.166722297668457,
      "learning_rate": 1.4566163225717059e-05,
      "loss": 2.0923,
      "step": 45910
    },
    {
      "epoch": 6.362754607177497,
      "grad_norm": 12.315522193908691,
      "learning_rate": 1.4560620756547044e-05,
      "loss": 1.8058,
      "step": 45920
    },
    {
      "epoch": 6.364140224470002,
      "grad_norm": 12.195391654968262,
      "learning_rate": 1.4555078287377027e-05,
      "loss": 2.3384,
      "step": 45930
    },
    {
      "epoch": 6.365525841762505,
      "grad_norm": 14.997904777526855,
      "learning_rate": 1.4549535818207013e-05,
      "loss": 2.3395,
      "step": 45940
    },
    {
      "epoch": 6.366911459055009,
      "grad_norm": 14.993483543395996,
      "learning_rate": 1.4543993349036998e-05,
      "loss": 2.2318,
      "step": 45950
    },
    {
      "epoch": 6.368297076347512,
      "grad_norm": 17.17268180847168,
      "learning_rate": 1.4538450879866981e-05,
      "loss": 1.6549,
      "step": 45960
    },
    {
      "epoch": 6.369682693640017,
      "grad_norm": 9.72492790222168,
      "learning_rate": 1.4532908410696967e-05,
      "loss": 1.6069,
      "step": 45970
    },
    {
      "epoch": 6.371068310932521,
      "grad_norm": 10.653616905212402,
      "learning_rate": 1.4527365941526952e-05,
      "loss": 1.9671,
      "step": 45980
    },
    {
      "epoch": 6.372453928225024,
      "grad_norm": 14.836141586303711,
      "learning_rate": 1.4521823472356935e-05,
      "loss": 1.8329,
      "step": 45990
    },
    {
      "epoch": 6.373839545517528,
      "grad_norm": 9.280027389526367,
      "learning_rate": 1.451628100318692e-05,
      "loss": 2.2113,
      "step": 46000
    },
    {
      "epoch": 6.375225162810032,
      "grad_norm": 11.18445873260498,
      "learning_rate": 1.4510738534016906e-05,
      "loss": 1.5279,
      "step": 46010
    },
    {
      "epoch": 6.376610780102536,
      "grad_norm": 12.83940601348877,
      "learning_rate": 1.4505196064846891e-05,
      "loss": 1.7452,
      "step": 46020
    },
    {
      "epoch": 6.377996397395039,
      "grad_norm": 10.347373962402344,
      "learning_rate": 1.4499653595676875e-05,
      "loss": 1.4992,
      "step": 46030
    },
    {
      "epoch": 6.379382014687543,
      "grad_norm": 4.699162006378174,
      "learning_rate": 1.449411112650686e-05,
      "loss": 1.8421,
      "step": 46040
    },
    {
      "epoch": 6.3807676319800475,
      "grad_norm": 19.610637664794922,
      "learning_rate": 1.4488568657336845e-05,
      "loss": 1.7798,
      "step": 46050
    },
    {
      "epoch": 6.382153249272551,
      "grad_norm": 19.36354637145996,
      "learning_rate": 1.4483026188166829e-05,
      "loss": 2.306,
      "step": 46060
    },
    {
      "epoch": 6.383538866565055,
      "grad_norm": 12.298836708068848,
      "learning_rate": 1.4477483718996814e-05,
      "loss": 1.7146,
      "step": 46070
    },
    {
      "epoch": 6.384924483857558,
      "grad_norm": 13.853116035461426,
      "learning_rate": 1.44719412498268e-05,
      "loss": 1.6795,
      "step": 46080
    },
    {
      "epoch": 6.3863101011500625,
      "grad_norm": 18.24338150024414,
      "learning_rate": 1.4466398780656783e-05,
      "loss": 1.5554,
      "step": 46090
    },
    {
      "epoch": 6.387695718442566,
      "grad_norm": 11.7031888961792,
      "learning_rate": 1.4460856311486768e-05,
      "loss": 1.7463,
      "step": 46100
    },
    {
      "epoch": 6.38908133573507,
      "grad_norm": 18.134963989257812,
      "learning_rate": 1.4455313842316753e-05,
      "loss": 1.5285,
      "step": 46110
    },
    {
      "epoch": 6.390466953027574,
      "grad_norm": 9.721597671508789,
      "learning_rate": 1.4449771373146737e-05,
      "loss": 1.6444,
      "step": 46120
    },
    {
      "epoch": 6.3918525703200775,
      "grad_norm": 14.893159866333008,
      "learning_rate": 1.4444228903976722e-05,
      "loss": 1.7896,
      "step": 46130
    },
    {
      "epoch": 6.393238187612582,
      "grad_norm": 11.522760391235352,
      "learning_rate": 1.4438686434806707e-05,
      "loss": 1.7603,
      "step": 46140
    },
    {
      "epoch": 6.394623804905085,
      "grad_norm": 20.293703079223633,
      "learning_rate": 1.4433143965636693e-05,
      "loss": 1.9328,
      "step": 46150
    },
    {
      "epoch": 6.396009422197589,
      "grad_norm": 10.513821601867676,
      "learning_rate": 1.4427601496466676e-05,
      "loss": 2.1002,
      "step": 46160
    },
    {
      "epoch": 6.3973950394900925,
      "grad_norm": 8.07098388671875,
      "learning_rate": 1.4422059027296662e-05,
      "loss": 1.7647,
      "step": 46170
    },
    {
      "epoch": 6.398780656782597,
      "grad_norm": 12.516287803649902,
      "learning_rate": 1.4416516558126647e-05,
      "loss": 2.0415,
      "step": 46180
    },
    {
      "epoch": 6.400166274075101,
      "grad_norm": 7.524134635925293,
      "learning_rate": 1.441097408895663e-05,
      "loss": 1.8793,
      "step": 46190
    },
    {
      "epoch": 6.401551891367604,
      "grad_norm": 12.517850875854492,
      "learning_rate": 1.4405431619786616e-05,
      "loss": 2.093,
      "step": 46200
    },
    {
      "epoch": 6.402937508660108,
      "grad_norm": 9.948463439941406,
      "learning_rate": 1.43998891506166e-05,
      "loss": 1.6495,
      "step": 46210
    },
    {
      "epoch": 6.404323125952612,
      "grad_norm": 10.831645965576172,
      "learning_rate": 1.4394346681446584e-05,
      "loss": 2.1972,
      "step": 46220
    },
    {
      "epoch": 6.405708743245116,
      "grad_norm": 10.517358779907227,
      "learning_rate": 1.438880421227657e-05,
      "loss": 1.5937,
      "step": 46230
    },
    {
      "epoch": 6.407094360537619,
      "grad_norm": 10.568371772766113,
      "learning_rate": 1.4383261743106557e-05,
      "loss": 1.425,
      "step": 46240
    },
    {
      "epoch": 6.408479977830123,
      "grad_norm": 8.955689430236816,
      "learning_rate": 1.4377719273936538e-05,
      "loss": 1.729,
      "step": 46250
    },
    {
      "epoch": 6.4098655951226275,
      "grad_norm": 11.367764472961426,
      "learning_rate": 1.4372176804766524e-05,
      "loss": 1.9076,
      "step": 46260
    },
    {
      "epoch": 6.411251212415131,
      "grad_norm": 17.777423858642578,
      "learning_rate": 1.436663433559651e-05,
      "loss": 1.9756,
      "step": 46270
    },
    {
      "epoch": 6.412636829707635,
      "grad_norm": 14.509844779968262,
      "learning_rate": 1.4361091866426492e-05,
      "loss": 1.8898,
      "step": 46280
    },
    {
      "epoch": 6.414022447000138,
      "grad_norm": 13.347043991088867,
      "learning_rate": 1.4355549397256478e-05,
      "loss": 1.8536,
      "step": 46290
    },
    {
      "epoch": 6.4154080642926425,
      "grad_norm": 9.596056938171387,
      "learning_rate": 1.4350006928086465e-05,
      "loss": 1.6712,
      "step": 46300
    },
    {
      "epoch": 6.416793681585146,
      "grad_norm": 6.336299419403076,
      "learning_rate": 1.434446445891645e-05,
      "loss": 1.5864,
      "step": 46310
    },
    {
      "epoch": 6.41817929887765,
      "grad_norm": 13.999273300170898,
      "learning_rate": 1.4338921989746433e-05,
      "loss": 2.1457,
      "step": 46320
    },
    {
      "epoch": 6.419564916170154,
      "grad_norm": 9.836764335632324,
      "learning_rate": 1.4333379520576419e-05,
      "loss": 1.7223,
      "step": 46330
    },
    {
      "epoch": 6.4209505334626575,
      "grad_norm": 15.538956642150879,
      "learning_rate": 1.4327837051406404e-05,
      "loss": 1.5427,
      "step": 46340
    },
    {
      "epoch": 6.422336150755162,
      "grad_norm": 6.0344367027282715,
      "learning_rate": 1.4322294582236388e-05,
      "loss": 1.927,
      "step": 46350
    },
    {
      "epoch": 6.423721768047665,
      "grad_norm": 16.349071502685547,
      "learning_rate": 1.4316752113066373e-05,
      "loss": 2.1653,
      "step": 46360
    },
    {
      "epoch": 6.425107385340169,
      "grad_norm": 10.809529304504395,
      "learning_rate": 1.4311209643896358e-05,
      "loss": 1.9417,
      "step": 46370
    },
    {
      "epoch": 6.4264930026326725,
      "grad_norm": 16.46148681640625,
      "learning_rate": 1.4305667174726342e-05,
      "loss": 1.9556,
      "step": 46380
    },
    {
      "epoch": 6.427878619925177,
      "grad_norm": 12.540241241455078,
      "learning_rate": 1.4300124705556327e-05,
      "loss": 1.812,
      "step": 46390
    },
    {
      "epoch": 6.42926423721768,
      "grad_norm": 11.644621849060059,
      "learning_rate": 1.4294582236386312e-05,
      "loss": 2.0086,
      "step": 46400
    },
    {
      "epoch": 6.430649854510184,
      "grad_norm": 10.16502571105957,
      "learning_rate": 1.4289039767216296e-05,
      "loss": 2.04,
      "step": 46410
    },
    {
      "epoch": 6.432035471802688,
      "grad_norm": 19.491926193237305,
      "learning_rate": 1.4283497298046281e-05,
      "loss": 1.8709,
      "step": 46420
    },
    {
      "epoch": 6.433421089095192,
      "grad_norm": 18.890453338623047,
      "learning_rate": 1.4277954828876266e-05,
      "loss": 2.0726,
      "step": 46430
    },
    {
      "epoch": 6.434806706387696,
      "grad_norm": 20.114574432373047,
      "learning_rate": 1.4272412359706251e-05,
      "loss": 1.8518,
      "step": 46440
    },
    {
      "epoch": 6.436192323680199,
      "grad_norm": 10.80359935760498,
      "learning_rate": 1.4266869890536235e-05,
      "loss": 1.4874,
      "step": 46450
    },
    {
      "epoch": 6.437577940972703,
      "grad_norm": 12.902926445007324,
      "learning_rate": 1.426132742136622e-05,
      "loss": 1.6449,
      "step": 46460
    },
    {
      "epoch": 6.438963558265208,
      "grad_norm": 14.298046112060547,
      "learning_rate": 1.4255784952196205e-05,
      "loss": 1.9449,
      "step": 46470
    },
    {
      "epoch": 6.440349175557711,
      "grad_norm": 21.26935577392578,
      "learning_rate": 1.4250242483026189e-05,
      "loss": 1.8038,
      "step": 46480
    },
    {
      "epoch": 6.441734792850215,
      "grad_norm": 5.541801929473877,
      "learning_rate": 1.4244700013856174e-05,
      "loss": 1.726,
      "step": 46490
    },
    {
      "epoch": 6.443120410142718,
      "grad_norm": 6.411977767944336,
      "learning_rate": 1.423915754468616e-05,
      "loss": 2.1793,
      "step": 46500
    },
    {
      "epoch": 6.444506027435223,
      "grad_norm": 11.573492050170898,
      "learning_rate": 1.4233615075516143e-05,
      "loss": 2.2467,
      "step": 46510
    },
    {
      "epoch": 6.445891644727726,
      "grad_norm": 8.192522048950195,
      "learning_rate": 1.4228072606346128e-05,
      "loss": 1.8436,
      "step": 46520
    },
    {
      "epoch": 6.44727726202023,
      "grad_norm": 17.19960594177246,
      "learning_rate": 1.4222530137176114e-05,
      "loss": 1.797,
      "step": 46530
    },
    {
      "epoch": 6.448662879312733,
      "grad_norm": 18.10294532775879,
      "learning_rate": 1.4216987668006097e-05,
      "loss": 1.984,
      "step": 46540
    },
    {
      "epoch": 6.450048496605238,
      "grad_norm": 15.361332893371582,
      "learning_rate": 1.4211445198836082e-05,
      "loss": 1.7781,
      "step": 46550
    },
    {
      "epoch": 6.451434113897742,
      "grad_norm": 9.550597190856934,
      "learning_rate": 1.4205902729666068e-05,
      "loss": 1.7118,
      "step": 46560
    },
    {
      "epoch": 6.452819731190245,
      "grad_norm": 14.783368110656738,
      "learning_rate": 1.4200360260496051e-05,
      "loss": 1.9391,
      "step": 46570
    },
    {
      "epoch": 6.454205348482749,
      "grad_norm": 14.925813674926758,
      "learning_rate": 1.4194817791326036e-05,
      "loss": 1.9543,
      "step": 46580
    },
    {
      "epoch": 6.455590965775253,
      "grad_norm": 14.31411361694336,
      "learning_rate": 1.4189275322156022e-05,
      "loss": 1.8227,
      "step": 46590
    },
    {
      "epoch": 6.456976583067757,
      "grad_norm": 9.364887237548828,
      "learning_rate": 1.4183732852986007e-05,
      "loss": 2.0843,
      "step": 46600
    },
    {
      "epoch": 6.45836220036026,
      "grad_norm": 25.71718406677246,
      "learning_rate": 1.417819038381599e-05,
      "loss": 1.674,
      "step": 46610
    },
    {
      "epoch": 6.459747817652764,
      "grad_norm": 15.677270889282227,
      "learning_rate": 1.4172647914645976e-05,
      "loss": 1.9302,
      "step": 46620
    },
    {
      "epoch": 6.4611334349452685,
      "grad_norm": 10.664752960205078,
      "learning_rate": 1.4167105445475961e-05,
      "loss": 2.0611,
      "step": 46630
    },
    {
      "epoch": 6.462519052237772,
      "grad_norm": 12.300222396850586,
      "learning_rate": 1.4161562976305945e-05,
      "loss": 1.6097,
      "step": 46640
    },
    {
      "epoch": 6.463904669530276,
      "grad_norm": 20.305286407470703,
      "learning_rate": 1.415602050713593e-05,
      "loss": 1.9855,
      "step": 46650
    },
    {
      "epoch": 6.465290286822779,
      "grad_norm": 13.323301315307617,
      "learning_rate": 1.4150478037965915e-05,
      "loss": 2.2131,
      "step": 46660
    },
    {
      "epoch": 6.4666759041152835,
      "grad_norm": 12.172158241271973,
      "learning_rate": 1.4144935568795899e-05,
      "loss": 2.3336,
      "step": 46670
    },
    {
      "epoch": 6.468061521407787,
      "grad_norm": 17.187644958496094,
      "learning_rate": 1.4139393099625884e-05,
      "loss": 2.1589,
      "step": 46680
    },
    {
      "epoch": 6.469447138700291,
      "grad_norm": 18.96476936340332,
      "learning_rate": 1.4133850630455869e-05,
      "loss": 2.3163,
      "step": 46690
    },
    {
      "epoch": 6.470832755992795,
      "grad_norm": 17.08424949645996,
      "learning_rate": 1.4128308161285853e-05,
      "loss": 1.8893,
      "step": 46700
    },
    {
      "epoch": 6.4722183732852985,
      "grad_norm": 14.214877128601074,
      "learning_rate": 1.4122765692115838e-05,
      "loss": 1.8029,
      "step": 46710
    },
    {
      "epoch": 6.473603990577803,
      "grad_norm": 20.639699935913086,
      "learning_rate": 1.4117223222945823e-05,
      "loss": 2.3291,
      "step": 46720
    },
    {
      "epoch": 6.474989607870306,
      "grad_norm": 14.875896453857422,
      "learning_rate": 1.411168075377581e-05,
      "loss": 2.0484,
      "step": 46730
    },
    {
      "epoch": 6.47637522516281,
      "grad_norm": 8.18868637084961,
      "learning_rate": 1.4106138284605792e-05,
      "loss": 1.9147,
      "step": 46740
    },
    {
      "epoch": 6.4777608424553135,
      "grad_norm": 16.113840103149414,
      "learning_rate": 1.4100595815435779e-05,
      "loss": 2.1295,
      "step": 46750
    },
    {
      "epoch": 6.479146459747818,
      "grad_norm": 17.323991775512695,
      "learning_rate": 1.4095053346265764e-05,
      "loss": 1.8992,
      "step": 46760
    },
    {
      "epoch": 6.480532077040322,
      "grad_norm": 14.440195083618164,
      "learning_rate": 1.4089510877095746e-05,
      "loss": 2.2657,
      "step": 46770
    },
    {
      "epoch": 6.481917694332825,
      "grad_norm": 19.456256866455078,
      "learning_rate": 1.4083968407925733e-05,
      "loss": 1.7704,
      "step": 46780
    },
    {
      "epoch": 6.483303311625329,
      "grad_norm": 8.247701644897461,
      "learning_rate": 1.4078425938755718e-05,
      "loss": 1.9808,
      "step": 46790
    },
    {
      "epoch": 6.484688928917833,
      "grad_norm": 20.73255729675293,
      "learning_rate": 1.40728834695857e-05,
      "loss": 1.962,
      "step": 46800
    },
    {
      "epoch": 6.486074546210337,
      "grad_norm": 10.636055946350098,
      "learning_rate": 1.4067341000415687e-05,
      "loss": 1.9443,
      "step": 46810
    },
    {
      "epoch": 6.48746016350284,
      "grad_norm": 10.81678295135498,
      "learning_rate": 1.4061798531245672e-05,
      "loss": 1.8375,
      "step": 46820
    },
    {
      "epoch": 6.488845780795344,
      "grad_norm": 10.145094871520996,
      "learning_rate": 1.4056256062075656e-05,
      "loss": 1.7013,
      "step": 46830
    },
    {
      "epoch": 6.4902313980878485,
      "grad_norm": 14.109857559204102,
      "learning_rate": 1.4050713592905641e-05,
      "loss": 2.0837,
      "step": 46840
    },
    {
      "epoch": 6.491617015380352,
      "grad_norm": 8.094244956970215,
      "learning_rate": 1.4045171123735626e-05,
      "loss": 1.736,
      "step": 46850
    },
    {
      "epoch": 6.493002632672856,
      "grad_norm": 6.684130668640137,
      "learning_rate": 1.403962865456561e-05,
      "loss": 2.0391,
      "step": 46860
    },
    {
      "epoch": 6.494388249965359,
      "grad_norm": 14.522138595581055,
      "learning_rate": 1.4034086185395595e-05,
      "loss": 1.9116,
      "step": 46870
    },
    {
      "epoch": 6.4957738672578635,
      "grad_norm": 14.405234336853027,
      "learning_rate": 1.402854371622558e-05,
      "loss": 2.1824,
      "step": 46880
    },
    {
      "epoch": 6.497159484550367,
      "grad_norm": 12.056116104125977,
      "learning_rate": 1.4023001247055566e-05,
      "loss": 1.6782,
      "step": 46890
    },
    {
      "epoch": 6.498545101842871,
      "grad_norm": 11.189685821533203,
      "learning_rate": 1.4017458777885549e-05,
      "loss": 2.3133,
      "step": 46900
    },
    {
      "epoch": 6.499930719135374,
      "grad_norm": 8.002290725708008,
      "learning_rate": 1.4011916308715534e-05,
      "loss": 1.7897,
      "step": 46910
    },
    {
      "epoch": 6.5013163364278785,
      "grad_norm": 26.44431495666504,
      "learning_rate": 1.400637383954552e-05,
      "loss": 1.6543,
      "step": 46920
    },
    {
      "epoch": 6.502701953720383,
      "grad_norm": 15.445606231689453,
      "learning_rate": 1.4000831370375503e-05,
      "loss": 1.8491,
      "step": 46930
    },
    {
      "epoch": 6.504087571012886,
      "grad_norm": 18.10012435913086,
      "learning_rate": 1.3995288901205488e-05,
      "loss": 2.149,
      "step": 46940
    },
    {
      "epoch": 6.50547318830539,
      "grad_norm": 7.354936122894287,
      "learning_rate": 1.3989746432035474e-05,
      "loss": 1.5957,
      "step": 46950
    },
    {
      "epoch": 6.5068588055978935,
      "grad_norm": 36.74070739746094,
      "learning_rate": 1.3984203962865457e-05,
      "loss": 2.0208,
      "step": 46960
    },
    {
      "epoch": 6.508244422890398,
      "grad_norm": 6.635312080383301,
      "learning_rate": 1.3978661493695442e-05,
      "loss": 1.9591,
      "step": 46970
    },
    {
      "epoch": 6.509630040182902,
      "grad_norm": 11.159857749938965,
      "learning_rate": 1.3973119024525428e-05,
      "loss": 2.1282,
      "step": 46980
    },
    {
      "epoch": 6.511015657475405,
      "grad_norm": 16.708911895751953,
      "learning_rate": 1.3967576555355411e-05,
      "loss": 1.8364,
      "step": 46990
    },
    {
      "epoch": 6.512401274767909,
      "grad_norm": 19.55483627319336,
      "learning_rate": 1.3962034086185397e-05,
      "loss": 2.0959,
      "step": 47000
    },
    {
      "epoch": 6.513786892060413,
      "grad_norm": 10.778672218322754,
      "learning_rate": 1.3956491617015382e-05,
      "loss": 1.9281,
      "step": 47010
    },
    {
      "epoch": 6.515172509352917,
      "grad_norm": 13.31178092956543,
      "learning_rate": 1.3950949147845367e-05,
      "loss": 1.6789,
      "step": 47020
    },
    {
      "epoch": 6.51655812664542,
      "grad_norm": 20.69632339477539,
      "learning_rate": 1.394540667867535e-05,
      "loss": 2.0953,
      "step": 47030
    },
    {
      "epoch": 6.517943743937924,
      "grad_norm": 12.084859848022461,
      "learning_rate": 1.3939864209505336e-05,
      "loss": 2.1772,
      "step": 47040
    },
    {
      "epoch": 6.519329361230428,
      "grad_norm": 14.705925941467285,
      "learning_rate": 1.3934321740335321e-05,
      "loss": 1.5292,
      "step": 47050
    },
    {
      "epoch": 6.520714978522932,
      "grad_norm": 12.390105247497559,
      "learning_rate": 1.3928779271165305e-05,
      "loss": 1.8915,
      "step": 47060
    },
    {
      "epoch": 6.522100595815436,
      "grad_norm": 6.765432834625244,
      "learning_rate": 1.392323680199529e-05,
      "loss": 1.5689,
      "step": 47070
    },
    {
      "epoch": 6.523486213107939,
      "grad_norm": 9.812827110290527,
      "learning_rate": 1.3917694332825275e-05,
      "loss": 1.5181,
      "step": 47080
    },
    {
      "epoch": 6.524871830400444,
      "grad_norm": 14.190889358520508,
      "learning_rate": 1.3912151863655259e-05,
      "loss": 1.4566,
      "step": 47090
    },
    {
      "epoch": 6.526257447692947,
      "grad_norm": 8.992239952087402,
      "learning_rate": 1.3906609394485244e-05,
      "loss": 2.0094,
      "step": 47100
    },
    {
      "epoch": 6.527643064985451,
      "grad_norm": NaN,
      "learning_rate": 1.390106692531523e-05,
      "loss": 2.0703,
      "step": 47110
    },
    {
      "epoch": 6.529028682277955,
      "grad_norm": 19.059703826904297,
      "learning_rate": 1.3896078703062214e-05,
      "loss": 1.5317,
      "step": 47120
    },
    {
      "epoch": 6.530414299570459,
      "grad_norm": 9.983344078063965,
      "learning_rate": 1.38905362338922e-05,
      "loss": 1.8902,
      "step": 47130
    },
    {
      "epoch": 6.531799916862963,
      "grad_norm": 15.523026466369629,
      "learning_rate": 1.3884993764722186e-05,
      "loss": 2.1471,
      "step": 47140
    },
    {
      "epoch": 6.533185534155466,
      "grad_norm": 13.720674514770508,
      "learning_rate": 1.3879451295552168e-05,
      "loss": 2.2671,
      "step": 47150
    },
    {
      "epoch": 6.53457115144797,
      "grad_norm": 20.050809860229492,
      "learning_rate": 1.3873908826382155e-05,
      "loss": 1.8981,
      "step": 47160
    },
    {
      "epoch": 6.535956768740474,
      "grad_norm": 8.216472625732422,
      "learning_rate": 1.386836635721214e-05,
      "loss": 1.6042,
      "step": 47170
    },
    {
      "epoch": 6.537342386032978,
      "grad_norm": 17.88943862915039,
      "learning_rate": 1.3862823888042124e-05,
      "loss": 2.4189,
      "step": 47180
    },
    {
      "epoch": 6.538728003325481,
      "grad_norm": 8.021111488342285,
      "learning_rate": 1.3857281418872109e-05,
      "loss": 1.8929,
      "step": 47190
    },
    {
      "epoch": 6.540113620617985,
      "grad_norm": 13.42774772644043,
      "learning_rate": 1.3851738949702094e-05,
      "loss": 1.9623,
      "step": 47200
    },
    {
      "epoch": 6.5414992379104895,
      "grad_norm": 9.67792797088623,
      "learning_rate": 1.384619648053208e-05,
      "loss": 2.5503,
      "step": 47210
    },
    {
      "epoch": 6.542884855202993,
      "grad_norm": 10.951967239379883,
      "learning_rate": 1.3840654011362063e-05,
      "loss": 2.3153,
      "step": 47220
    },
    {
      "epoch": 6.544270472495497,
      "grad_norm": 17.5009765625,
      "learning_rate": 1.3835111542192048e-05,
      "loss": 1.6278,
      "step": 47230
    },
    {
      "epoch": 6.545656089788,
      "grad_norm": 8.67015552520752,
      "learning_rate": 1.3829569073022034e-05,
      "loss": 1.9455,
      "step": 47240
    },
    {
      "epoch": 6.5470417070805045,
      "grad_norm": 19.949291229248047,
      "learning_rate": 1.3824026603852017e-05,
      "loss": 1.7659,
      "step": 47250
    },
    {
      "epoch": 6.548427324373009,
      "grad_norm": 9.3748197555542,
      "learning_rate": 1.3818484134682002e-05,
      "loss": 1.7763,
      "step": 47260
    },
    {
      "epoch": 6.549812941665512,
      "grad_norm": 14.674798965454102,
      "learning_rate": 1.3812941665511988e-05,
      "loss": 2.134,
      "step": 47270
    },
    {
      "epoch": 6.551198558958016,
      "grad_norm": 15.338847160339355,
      "learning_rate": 1.3807399196341971e-05,
      "loss": 1.359,
      "step": 47280
    },
    {
      "epoch": 6.5525841762505195,
      "grad_norm": 7.599662780761719,
      "learning_rate": 1.3801856727171956e-05,
      "loss": 1.9459,
      "step": 47290
    },
    {
      "epoch": 6.553969793543024,
      "grad_norm": 12.72581672668457,
      "learning_rate": 1.3796314258001942e-05,
      "loss": 1.8678,
      "step": 47300
    },
    {
      "epoch": 6.555355410835527,
      "grad_norm": 9.142359733581543,
      "learning_rate": 1.3790771788831925e-05,
      "loss": 1.4843,
      "step": 47310
    },
    {
      "epoch": 6.556741028128031,
      "grad_norm": 18.389562606811523,
      "learning_rate": 1.378522931966191e-05,
      "loss": 2.1451,
      "step": 47320
    },
    {
      "epoch": 6.5581266454205345,
      "grad_norm": 14.295429229736328,
      "learning_rate": 1.3779686850491896e-05,
      "loss": 1.9493,
      "step": 47330
    },
    {
      "epoch": 6.559512262713039,
      "grad_norm": 10.830421447753906,
      "learning_rate": 1.377414438132188e-05,
      "loss": 1.6855,
      "step": 47340
    },
    {
      "epoch": 6.560897880005543,
      "grad_norm": 15.79521656036377,
      "learning_rate": 1.3768601912151864e-05,
      "loss": 2.2971,
      "step": 47350
    },
    {
      "epoch": 6.562283497298046,
      "grad_norm": 20.427961349487305,
      "learning_rate": 1.376305944298185e-05,
      "loss": 1.9184,
      "step": 47360
    },
    {
      "epoch": 6.56366911459055,
      "grad_norm": 12.870916366577148,
      "learning_rate": 1.3757516973811835e-05,
      "loss": 1.9503,
      "step": 47370
    },
    {
      "epoch": 6.565054731883054,
      "grad_norm": 20.02153205871582,
      "learning_rate": 1.3751974504641819e-05,
      "loss": 2.0415,
      "step": 47380
    },
    {
      "epoch": 6.566440349175558,
      "grad_norm": 12.370559692382812,
      "learning_rate": 1.3746432035471804e-05,
      "loss": 1.8928,
      "step": 47390
    },
    {
      "epoch": 6.567825966468061,
      "grad_norm": 16.78324317932129,
      "learning_rate": 1.3740889566301789e-05,
      "loss": 1.7968,
      "step": 47400
    },
    {
      "epoch": 6.569211583760565,
      "grad_norm": 8.105249404907227,
      "learning_rate": 1.3735347097131773e-05,
      "loss": 1.7054,
      "step": 47410
    },
    {
      "epoch": 6.570597201053069,
      "grad_norm": 12.26307487487793,
      "learning_rate": 1.3729804627961758e-05,
      "loss": 1.8283,
      "step": 47420
    },
    {
      "epoch": 6.571982818345573,
      "grad_norm": 12.732878684997559,
      "learning_rate": 1.3724262158791743e-05,
      "loss": 1.7948,
      "step": 47430
    },
    {
      "epoch": 6.573368435638077,
      "grad_norm": 13.790338516235352,
      "learning_rate": 1.3718719689621727e-05,
      "loss": 2.3289,
      "step": 47440
    },
    {
      "epoch": 6.57475405293058,
      "grad_norm": 16.065834045410156,
      "learning_rate": 1.3713177220451712e-05,
      "loss": 2.1052,
      "step": 47450
    },
    {
      "epoch": 6.5761396702230845,
      "grad_norm": 15.135347366333008,
      "learning_rate": 1.3707634751281697e-05,
      "loss": 1.7631,
      "step": 47460
    },
    {
      "epoch": 6.577525287515588,
      "grad_norm": 8.395171165466309,
      "learning_rate": 1.370209228211168e-05,
      "loss": 2.2759,
      "step": 47470
    },
    {
      "epoch": 6.578910904808092,
      "grad_norm": 20.546205520629883,
      "learning_rate": 1.3696549812941666e-05,
      "loss": 2.2215,
      "step": 47480
    },
    {
      "epoch": 6.580296522100596,
      "grad_norm": 15.176533699035645,
      "learning_rate": 1.3691007343771651e-05,
      "loss": 2.145,
      "step": 47490
    },
    {
      "epoch": 6.5816821393930995,
      "grad_norm": 10.465841293334961,
      "learning_rate": 1.3685464874601636e-05,
      "loss": 2.0314,
      "step": 47500
    },
    {
      "epoch": 6.583067756685604,
      "grad_norm": 9.394119262695312,
      "learning_rate": 1.367992240543162e-05,
      "loss": 1.8881,
      "step": 47510
    },
    {
      "epoch": 6.584453373978107,
      "grad_norm": 9.373268127441406,
      "learning_rate": 1.3674379936261605e-05,
      "loss": 1.6473,
      "step": 47520
    },
    {
      "epoch": 6.585838991270611,
      "grad_norm": 10.485692977905273,
      "learning_rate": 1.366883746709159e-05,
      "loss": 1.9068,
      "step": 47530
    },
    {
      "epoch": 6.5872246085631145,
      "grad_norm": 13.196953773498535,
      "learning_rate": 1.3663294997921574e-05,
      "loss": 1.73,
      "step": 47540
    },
    {
      "epoch": 6.588610225855619,
      "grad_norm": 22.65822982788086,
      "learning_rate": 1.365775252875156e-05,
      "loss": 1.9215,
      "step": 47550
    },
    {
      "epoch": 6.589995843148122,
      "grad_norm": 19.36301040649414,
      "learning_rate": 1.3652210059581546e-05,
      "loss": 1.8351,
      "step": 47560
    },
    {
      "epoch": 6.591381460440626,
      "grad_norm": 8.273637771606445,
      "learning_rate": 1.3646667590411528e-05,
      "loss": 2.2199,
      "step": 47570
    },
    {
      "epoch": 6.59276707773313,
      "grad_norm": 12.480233192443848,
      "learning_rate": 1.3641125121241513e-05,
      "loss": 1.5369,
      "step": 47580
    },
    {
      "epoch": 6.594152695025634,
      "grad_norm": 15.031339645385742,
      "learning_rate": 1.36355826520715e-05,
      "loss": 1.8347,
      "step": 47590
    },
    {
      "epoch": 6.595538312318138,
      "grad_norm": 15.771479606628418,
      "learning_rate": 1.3630040182901482e-05,
      "loss": 1.7614,
      "step": 47600
    },
    {
      "epoch": 6.596923929610641,
      "grad_norm": 14.799605369567871,
      "learning_rate": 1.3624497713731467e-05,
      "loss": 2.1789,
      "step": 47610
    },
    {
      "epoch": 6.598309546903145,
      "grad_norm": 14.125528335571289,
      "learning_rate": 1.3618955244561454e-05,
      "loss": 1.7687,
      "step": 47620
    },
    {
      "epoch": 6.59969516419565,
      "grad_norm": 12.714886665344238,
      "learning_rate": 1.3613412775391436e-05,
      "loss": 1.8669,
      "step": 47630
    },
    {
      "epoch": 6.601080781488153,
      "grad_norm": 13.070796966552734,
      "learning_rate": 1.3607870306221423e-05,
      "loss": 1.8207,
      "step": 47640
    },
    {
      "epoch": 6.602466398780657,
      "grad_norm": 10.701482772827148,
      "learning_rate": 1.3602327837051408e-05,
      "loss": 1.77,
      "step": 47650
    },
    {
      "epoch": 6.60385201607316,
      "grad_norm": 4.132876396179199,
      "learning_rate": 1.3596785367881394e-05,
      "loss": 1.8314,
      "step": 47660
    },
    {
      "epoch": 6.605237633365665,
      "grad_norm": 14.334027290344238,
      "learning_rate": 1.3591242898711377e-05,
      "loss": 2.4145,
      "step": 47670
    },
    {
      "epoch": 6.606623250658168,
      "grad_norm": 18.092802047729492,
      "learning_rate": 1.3585700429541362e-05,
      "loss": 1.9056,
      "step": 47680
    },
    {
      "epoch": 6.608008867950672,
      "grad_norm": 16.687746047973633,
      "learning_rate": 1.3580157960371348e-05,
      "loss": 2.0304,
      "step": 47690
    },
    {
      "epoch": 6.609394485243175,
      "grad_norm": 13.11326789855957,
      "learning_rate": 1.3574615491201331e-05,
      "loss": 1.97,
      "step": 47700
    },
    {
      "epoch": 6.61078010253568,
      "grad_norm": 16.649328231811523,
      "learning_rate": 1.3569073022031316e-05,
      "loss": 1.6556,
      "step": 47710
    },
    {
      "epoch": 6.612165719828184,
      "grad_norm": 13.159723281860352,
      "learning_rate": 1.3563530552861302e-05,
      "loss": 2.2309,
      "step": 47720
    },
    {
      "epoch": 6.613551337120687,
      "grad_norm": 15.354476928710938,
      "learning_rate": 1.3557988083691285e-05,
      "loss": 2.2229,
      "step": 47730
    },
    {
      "epoch": 6.614936954413191,
      "grad_norm": 17.18191146850586,
      "learning_rate": 1.355244561452127e-05,
      "loss": 1.8282,
      "step": 47740
    },
    {
      "epoch": 6.616322571705695,
      "grad_norm": 16.208358764648438,
      "learning_rate": 1.3546903145351256e-05,
      "loss": 1.8557,
      "step": 47750
    },
    {
      "epoch": 6.617708188998199,
      "grad_norm": 9.592673301696777,
      "learning_rate": 1.354136067618124e-05,
      "loss": 2.1362,
      "step": 47760
    },
    {
      "epoch": 6.619093806290703,
      "grad_norm": 6.604108810424805,
      "learning_rate": 1.3535818207011225e-05,
      "loss": 2.2288,
      "step": 47770
    },
    {
      "epoch": 6.620479423583206,
      "grad_norm": 19.76890754699707,
      "learning_rate": 1.353027573784121e-05,
      "loss": 1.8376,
      "step": 47780
    },
    {
      "epoch": 6.6218650408757105,
      "grad_norm": 7.757640838623047,
      "learning_rate": 1.3524733268671195e-05,
      "loss": 1.9629,
      "step": 47790
    },
    {
      "epoch": 6.623250658168214,
      "grad_norm": 13.78955078125,
      "learning_rate": 1.3519190799501179e-05,
      "loss": 1.9829,
      "step": 47800
    },
    {
      "epoch": 6.624636275460718,
      "grad_norm": 10.391948699951172,
      "learning_rate": 1.3513648330331164e-05,
      "loss": 2.007,
      "step": 47810
    },
    {
      "epoch": 6.626021892753221,
      "grad_norm": 11.106463432312012,
      "learning_rate": 1.350810586116115e-05,
      "loss": 1.9573,
      "step": 47820
    },
    {
      "epoch": 6.6274075100457255,
      "grad_norm": 19.891672134399414,
      "learning_rate": 1.3502563391991133e-05,
      "loss": 2.3262,
      "step": 47830
    },
    {
      "epoch": 6.628793127338229,
      "grad_norm": 14.446587562561035,
      "learning_rate": 1.3497020922821118e-05,
      "loss": 2.4574,
      "step": 47840
    },
    {
      "epoch": 6.630178744630733,
      "grad_norm": 12.180705070495605,
      "learning_rate": 1.3491478453651103e-05,
      "loss": 1.6306,
      "step": 47850
    },
    {
      "epoch": 6.631564361923237,
      "grad_norm": 9.634988784790039,
      "learning_rate": 1.3485935984481087e-05,
      "loss": 1.6968,
      "step": 47860
    },
    {
      "epoch": 6.6329499792157405,
      "grad_norm": 11.161224365234375,
      "learning_rate": 1.3480393515311072e-05,
      "loss": 2.1234,
      "step": 47870
    },
    {
      "epoch": 6.634335596508245,
      "grad_norm": 11.850483894348145,
      "learning_rate": 1.3474851046141057e-05,
      "loss": 2.1174,
      "step": 47880
    },
    {
      "epoch": 6.635721213800748,
      "grad_norm": 15.930742263793945,
      "learning_rate": 1.346930857697104e-05,
      "loss": 1.6918,
      "step": 47890
    },
    {
      "epoch": 6.637106831093252,
      "grad_norm": 13.940186500549316,
      "learning_rate": 1.3463766107801026e-05,
      "loss": 1.5381,
      "step": 47900
    },
    {
      "epoch": 6.6384924483857555,
      "grad_norm": 14.573734283447266,
      "learning_rate": 1.3458223638631011e-05,
      "loss": 1.8618,
      "step": 47910
    },
    {
      "epoch": 6.63987806567826,
      "grad_norm": 10.238973617553711,
      "learning_rate": 1.3452681169460995e-05,
      "loss": 1.6504,
      "step": 47920
    },
    {
      "epoch": 6.641263682970764,
      "grad_norm": 11.894013404846191,
      "learning_rate": 1.344713870029098e-05,
      "loss": 1.8923,
      "step": 47930
    },
    {
      "epoch": 6.642649300263267,
      "grad_norm": 16.958024978637695,
      "learning_rate": 1.3441596231120965e-05,
      "loss": 2.1392,
      "step": 47940
    },
    {
      "epoch": 6.644034917555771,
      "grad_norm": 9.576492309570312,
      "learning_rate": 1.343605376195095e-05,
      "loss": 2.155,
      "step": 47950
    },
    {
      "epoch": 6.645420534848275,
      "grad_norm": 7.305116653442383,
      "learning_rate": 1.3430511292780934e-05,
      "loss": 1.6501,
      "step": 47960
    },
    {
      "epoch": 6.646806152140779,
      "grad_norm": 15.813631057739258,
      "learning_rate": 1.342496882361092e-05,
      "loss": 2.5003,
      "step": 47970
    },
    {
      "epoch": 6.648191769433282,
      "grad_norm": 19.092592239379883,
      "learning_rate": 1.3419426354440905e-05,
      "loss": 2.0256,
      "step": 47980
    },
    {
      "epoch": 6.649577386725786,
      "grad_norm": 10.122380256652832,
      "learning_rate": 1.3413883885270888e-05,
      "loss": 1.4129,
      "step": 47990
    },
    {
      "epoch": 6.6509630040182905,
      "grad_norm": 12.220268249511719,
      "learning_rate": 1.3408341416100873e-05,
      "loss": 1.9817,
      "step": 48000
    },
    {
      "epoch": 6.652348621310794,
      "grad_norm": 11.881760597229004,
      "learning_rate": 1.3402798946930859e-05,
      "loss": 1.9173,
      "step": 48010
    },
    {
      "epoch": 6.653734238603298,
      "grad_norm": 12.163268089294434,
      "learning_rate": 1.3397256477760842e-05,
      "loss": 1.8121,
      "step": 48020
    },
    {
      "epoch": 6.655119855895801,
      "grad_norm": 23.91097068786621,
      "learning_rate": 1.3391714008590828e-05,
      "loss": 2.3419,
      "step": 48030
    },
    {
      "epoch": 6.6565054731883055,
      "grad_norm": 14.101982116699219,
      "learning_rate": 1.3386171539420813e-05,
      "loss": 2.0574,
      "step": 48040
    },
    {
      "epoch": 6.657891090480809,
      "grad_norm": 9.001395225524902,
      "learning_rate": 1.3380629070250796e-05,
      "loss": 1.6424,
      "step": 48050
    },
    {
      "epoch": 6.659276707773313,
      "grad_norm": 17.81953239440918,
      "learning_rate": 1.3375086601080782e-05,
      "loss": 1.987,
      "step": 48060
    },
    {
      "epoch": 6.660662325065816,
      "grad_norm": 7.017705917358398,
      "learning_rate": 1.3369544131910769e-05,
      "loss": 1.7896,
      "step": 48070
    },
    {
      "epoch": 6.6620479423583205,
      "grad_norm": 13.583393096923828,
      "learning_rate": 1.3364001662740754e-05,
      "loss": 2.4562,
      "step": 48080
    },
    {
      "epoch": 6.663433559650825,
      "grad_norm": 15.49866008758545,
      "learning_rate": 1.3358459193570736e-05,
      "loss": 1.987,
      "step": 48090
    },
    {
      "epoch": 6.664819176943328,
      "grad_norm": 10.165116310119629,
      "learning_rate": 1.3352916724400723e-05,
      "loss": 1.7777,
      "step": 48100
    },
    {
      "epoch": 6.666204794235832,
      "grad_norm": 18.268787384033203,
      "learning_rate": 1.3347374255230708e-05,
      "loss": 1.5964,
      "step": 48110
    },
    {
      "epoch": 6.6675904115283355,
      "grad_norm": 11.136770248413086,
      "learning_rate": 1.334183178606069e-05,
      "loss": 1.6538,
      "step": 48120
    },
    {
      "epoch": 6.66897602882084,
      "grad_norm": 9.498119354248047,
      "learning_rate": 1.3336289316890677e-05,
      "loss": 1.3559,
      "step": 48130
    },
    {
      "epoch": 6.670361646113344,
      "grad_norm": 13.598661422729492,
      "learning_rate": 1.3330746847720662e-05,
      "loss": 2.0078,
      "step": 48140
    },
    {
      "epoch": 6.671747263405847,
      "grad_norm": 24.5501766204834,
      "learning_rate": 1.3325204378550645e-05,
      "loss": 1.9472,
      "step": 48150
    },
    {
      "epoch": 6.673132880698351,
      "grad_norm": 10.717686653137207,
      "learning_rate": 1.331966190938063e-05,
      "loss": 2.0831,
      "step": 48160
    },
    {
      "epoch": 6.674518497990855,
      "grad_norm": 8.6716947555542,
      "learning_rate": 1.3314119440210616e-05,
      "loss": 2.1334,
      "step": 48170
    },
    {
      "epoch": 6.675904115283359,
      "grad_norm": 19.02987289428711,
      "learning_rate": 1.33085769710406e-05,
      "loss": 1.7304,
      "step": 48180
    },
    {
      "epoch": 6.677289732575862,
      "grad_norm": 6.967350006103516,
      "learning_rate": 1.3303034501870585e-05,
      "loss": 2.0268,
      "step": 48190
    },
    {
      "epoch": 6.678675349868366,
      "grad_norm": 13.580726623535156,
      "learning_rate": 1.329749203270057e-05,
      "loss": 1.8994,
      "step": 48200
    },
    {
      "epoch": 6.68006096716087,
      "grad_norm": 14.920215606689453,
      "learning_rate": 1.3291949563530554e-05,
      "loss": 1.8656,
      "step": 48210
    },
    {
      "epoch": 6.681446584453374,
      "grad_norm": 12.255455017089844,
      "learning_rate": 1.3286407094360539e-05,
      "loss": 1.5441,
      "step": 48220
    },
    {
      "epoch": 6.682832201745878,
      "grad_norm": 10.571935653686523,
      "learning_rate": 1.3280864625190524e-05,
      "loss": 1.6404,
      "step": 48230
    },
    {
      "epoch": 6.684217819038381,
      "grad_norm": 9.640629768371582,
      "learning_rate": 1.327532215602051e-05,
      "loss": 1.9518,
      "step": 48240
    },
    {
      "epoch": 6.685603436330886,
      "grad_norm": 10.678086280822754,
      "learning_rate": 1.3269779686850493e-05,
      "loss": 1.7682,
      "step": 48250
    },
    {
      "epoch": 6.686989053623389,
      "grad_norm": 10.9913969039917,
      "learning_rate": 1.3264237217680478e-05,
      "loss": 1.9062,
      "step": 48260
    },
    {
      "epoch": 6.688374670915893,
      "grad_norm": 16.47221565246582,
      "learning_rate": 1.3258694748510463e-05,
      "loss": 1.6908,
      "step": 48270
    },
    {
      "epoch": 6.689760288208397,
      "grad_norm": 9.166971206665039,
      "learning_rate": 1.3253152279340447e-05,
      "loss": 1.49,
      "step": 48280
    },
    {
      "epoch": 6.691145905500901,
      "grad_norm": 15.929049491882324,
      "learning_rate": 1.3247609810170432e-05,
      "loss": 1.8051,
      "step": 48290
    },
    {
      "epoch": 6.692531522793405,
      "grad_norm": 11.92410659790039,
      "learning_rate": 1.3242067341000417e-05,
      "loss": 1.7989,
      "step": 48300
    },
    {
      "epoch": 6.693917140085908,
      "grad_norm": 16.062557220458984,
      "learning_rate": 1.3236524871830401e-05,
      "loss": 1.6877,
      "step": 48310
    },
    {
      "epoch": 6.695302757378412,
      "grad_norm": 10.008437156677246,
      "learning_rate": 1.3230982402660386e-05,
      "loss": 1.9145,
      "step": 48320
    },
    {
      "epoch": 6.696688374670916,
      "grad_norm": 17.759071350097656,
      "learning_rate": 1.3225439933490371e-05,
      "loss": 1.6969,
      "step": 48330
    },
    {
      "epoch": 6.69807399196342,
      "grad_norm": 7.784555435180664,
      "learning_rate": 1.3219897464320355e-05,
      "loss": 1.5399,
      "step": 48340
    },
    {
      "epoch": 6.699459609255923,
      "grad_norm": 8.743520736694336,
      "learning_rate": 1.321435499515034e-05,
      "loss": 1.8544,
      "step": 48350
    },
    {
      "epoch": 6.700845226548427,
      "grad_norm": 16.864070892333984,
      "learning_rate": 1.3208812525980326e-05,
      "loss": 2.2092,
      "step": 48360
    },
    {
      "epoch": 6.7022308438409315,
      "grad_norm": 15.449349403381348,
      "learning_rate": 1.320327005681031e-05,
      "loss": 1.8328,
      "step": 48370
    },
    {
      "epoch": 6.703616461133435,
      "grad_norm": 12.503813743591309,
      "learning_rate": 1.3197727587640294e-05,
      "loss": 2.0863,
      "step": 48380
    },
    {
      "epoch": 6.705002078425939,
      "grad_norm": 12.005868911743164,
      "learning_rate": 1.319218511847028e-05,
      "loss": 2.4287,
      "step": 48390
    },
    {
      "epoch": 6.706387695718442,
      "grad_norm": 18.040435791015625,
      "learning_rate": 1.3186642649300265e-05,
      "loss": 2.0418,
      "step": 48400
    },
    {
      "epoch": 6.7077733130109465,
      "grad_norm": 12.435028076171875,
      "learning_rate": 1.3181100180130248e-05,
      "loss": 1.9772,
      "step": 48410
    },
    {
      "epoch": 6.709158930303451,
      "grad_norm": 14.868168830871582,
      "learning_rate": 1.3175557710960234e-05,
      "loss": 1.929,
      "step": 48420
    },
    {
      "epoch": 6.710544547595954,
      "grad_norm": 11.843306541442871,
      "learning_rate": 1.3170015241790219e-05,
      "loss": 1.8326,
      "step": 48430
    },
    {
      "epoch": 6.711930164888458,
      "grad_norm": 14.885619163513184,
      "learning_rate": 1.3164472772620202e-05,
      "loss": 1.8001,
      "step": 48440
    },
    {
      "epoch": 6.7133157821809615,
      "grad_norm": 10.369468688964844,
      "learning_rate": 1.3158930303450188e-05,
      "loss": 1.9406,
      "step": 48450
    },
    {
      "epoch": 6.714701399473466,
      "grad_norm": 17.220308303833008,
      "learning_rate": 1.3153387834280173e-05,
      "loss": 1.7339,
      "step": 48460
    },
    {
      "epoch": 6.716087016765969,
      "grad_norm": 10.933573722839355,
      "learning_rate": 1.3147845365110156e-05,
      "loss": 1.6162,
      "step": 48470
    },
    {
      "epoch": 6.717472634058473,
      "grad_norm": 14.54939079284668,
      "learning_rate": 1.3142302895940142e-05,
      "loss": 1.9521,
      "step": 48480
    },
    {
      "epoch": 6.7188582513509765,
      "grad_norm": 7.631688594818115,
      "learning_rate": 1.3136760426770127e-05,
      "loss": 1.9791,
      "step": 48490
    },
    {
      "epoch": 6.720243868643481,
      "grad_norm": 6.602275371551514,
      "learning_rate": 1.313121795760011e-05,
      "loss": 2.0341,
      "step": 48500
    },
    {
      "epoch": 6.721629485935985,
      "grad_norm": 17.45539665222168,
      "learning_rate": 1.3125675488430096e-05,
      "loss": 1.7381,
      "step": 48510
    },
    {
      "epoch": 6.723015103228488,
      "grad_norm": 10.007150650024414,
      "learning_rate": 1.3120133019260081e-05,
      "loss": 1.4757,
      "step": 48520
    },
    {
      "epoch": 6.724400720520992,
      "grad_norm": 9.032757759094238,
      "learning_rate": 1.3114590550090068e-05,
      "loss": 2.1425,
      "step": 48530
    },
    {
      "epoch": 6.725786337813496,
      "grad_norm": 8.565345764160156,
      "learning_rate": 1.310904808092005e-05,
      "loss": 1.7068,
      "step": 48540
    },
    {
      "epoch": 6.727171955106,
      "grad_norm": 22.6444091796875,
      "learning_rate": 1.3103505611750035e-05,
      "loss": 1.7094,
      "step": 48550
    },
    {
      "epoch": 6.728557572398503,
      "grad_norm": 12.12431812286377,
      "learning_rate": 1.3097963142580022e-05,
      "loss": 2.0161,
      "step": 48560
    },
    {
      "epoch": 6.729943189691007,
      "grad_norm": 7.850942611694336,
      "learning_rate": 1.3092420673410004e-05,
      "loss": 1.8447,
      "step": 48570
    },
    {
      "epoch": 6.731328806983511,
      "grad_norm": 11.210389137268066,
      "learning_rate": 1.308687820423999e-05,
      "loss": 1.5161,
      "step": 48580
    },
    {
      "epoch": 6.732714424276015,
      "grad_norm": 17.632360458374023,
      "learning_rate": 1.3081335735069976e-05,
      "loss": 2.0881,
      "step": 48590
    },
    {
      "epoch": 6.734100041568519,
      "grad_norm": 9.608840942382812,
      "learning_rate": 1.3075793265899958e-05,
      "loss": 1.9436,
      "step": 48600
    },
    {
      "epoch": 6.735485658861022,
      "grad_norm": 16.030290603637695,
      "learning_rate": 1.3070250796729945e-05,
      "loss": 1.6731,
      "step": 48610
    },
    {
      "epoch": 6.7368712761535265,
      "grad_norm": 8.04154109954834,
      "learning_rate": 1.306470832755993e-05,
      "loss": 1.589,
      "step": 48620
    },
    {
      "epoch": 6.73825689344603,
      "grad_norm": 16.00307273864746,
      "learning_rate": 1.3059165858389912e-05,
      "loss": 1.8069,
      "step": 48630
    },
    {
      "epoch": 6.739642510738534,
      "grad_norm": 20.565673828125,
      "learning_rate": 1.3053623389219899e-05,
      "loss": 1.8347,
      "step": 48640
    },
    {
      "epoch": 6.741028128031038,
      "grad_norm": 8.030505180358887,
      "learning_rate": 1.3048080920049884e-05,
      "loss": 1.6906,
      "step": 48650
    },
    {
      "epoch": 6.7424137453235415,
      "grad_norm": 9.987751007080078,
      "learning_rate": 1.304253845087987e-05,
      "loss": 2.0035,
      "step": 48660
    },
    {
      "epoch": 6.743799362616046,
      "grad_norm": 15.879510879516602,
      "learning_rate": 1.3036995981709853e-05,
      "loss": 2.0234,
      "step": 48670
    },
    {
      "epoch": 6.745184979908549,
      "grad_norm": 19.249011993408203,
      "learning_rate": 1.3031453512539838e-05,
      "loss": 2.074,
      "step": 48680
    },
    {
      "epoch": 6.746570597201053,
      "grad_norm": 22.900617599487305,
      "learning_rate": 1.3025911043369823e-05,
      "loss": 2.2075,
      "step": 48690
    },
    {
      "epoch": 6.7479562144935565,
      "grad_norm": 15.678581237792969,
      "learning_rate": 1.3020368574199807e-05,
      "loss": 2.0324,
      "step": 48700
    },
    {
      "epoch": 6.749341831786061,
      "grad_norm": 6.6300225257873535,
      "learning_rate": 1.3014826105029792e-05,
      "loss": 2.0168,
      "step": 48710
    },
    {
      "epoch": 6.750727449078564,
      "grad_norm": 12.365769386291504,
      "learning_rate": 1.3009283635859778e-05,
      "loss": 2.0819,
      "step": 48720
    },
    {
      "epoch": 6.752113066371068,
      "grad_norm": 7.834549427032471,
      "learning_rate": 1.3003741166689761e-05,
      "loss": 2.109,
      "step": 48730
    },
    {
      "epoch": 6.753498683663572,
      "grad_norm": 13.3527250289917,
      "learning_rate": 1.2998198697519746e-05,
      "loss": 1.8749,
      "step": 48740
    },
    {
      "epoch": 6.754884300956076,
      "grad_norm": 26.49446678161621,
      "learning_rate": 1.2992656228349732e-05,
      "loss": 2.1403,
      "step": 48750
    },
    {
      "epoch": 6.75626991824858,
      "grad_norm": 19.952533721923828,
      "learning_rate": 1.2987113759179715e-05,
      "loss": 1.8965,
      "step": 48760
    },
    {
      "epoch": 6.757655535541083,
      "grad_norm": 13.661849975585938,
      "learning_rate": 1.29815712900097e-05,
      "loss": 1.9807,
      "step": 48770
    },
    {
      "epoch": 6.759041152833587,
      "grad_norm": 11.210929870605469,
      "learning_rate": 1.2976028820839686e-05,
      "loss": 1.4425,
      "step": 48780
    },
    {
      "epoch": 6.760426770126092,
      "grad_norm": 12.43126106262207,
      "learning_rate": 1.297048635166967e-05,
      "loss": 1.939,
      "step": 48790
    },
    {
      "epoch": 6.761812387418595,
      "grad_norm": 13.704140663146973,
      "learning_rate": 1.2964943882499654e-05,
      "loss": 1.5247,
      "step": 48800
    },
    {
      "epoch": 6.763198004711099,
      "grad_norm": 16.73300552368164,
      "learning_rate": 1.295940141332964e-05,
      "loss": 1.8971,
      "step": 48810
    },
    {
      "epoch": 6.764583622003602,
      "grad_norm": 11.991018295288086,
      "learning_rate": 1.2953858944159625e-05,
      "loss": 2.1998,
      "step": 48820
    },
    {
      "epoch": 6.765969239296107,
      "grad_norm": 28.768857955932617,
      "learning_rate": 1.2948316474989609e-05,
      "loss": 1.6213,
      "step": 48830
    },
    {
      "epoch": 6.76735485658861,
      "grad_norm": 14.720407485961914,
      "learning_rate": 1.2942774005819594e-05,
      "loss": 2.0573,
      "step": 48840
    },
    {
      "epoch": 6.768740473881114,
      "grad_norm": 11.992547988891602,
      "learning_rate": 1.2937231536649579e-05,
      "loss": 1.9232,
      "step": 48850
    },
    {
      "epoch": 6.770126091173617,
      "grad_norm": 11.170584678649902,
      "learning_rate": 1.2931689067479563e-05,
      "loss": 2.0031,
      "step": 48860
    },
    {
      "epoch": 6.771511708466122,
      "grad_norm": 13.67764949798584,
      "learning_rate": 1.2926146598309548e-05,
      "loss": 1.9882,
      "step": 48870
    },
    {
      "epoch": 6.772897325758626,
      "grad_norm": 12.116572380065918,
      "learning_rate": 1.2920604129139533e-05,
      "loss": 2.0702,
      "step": 48880
    },
    {
      "epoch": 6.774282943051129,
      "grad_norm": 10.916040420532227,
      "learning_rate": 1.2915061659969517e-05,
      "loss": 2.0523,
      "step": 48890
    },
    {
      "epoch": 6.775668560343633,
      "grad_norm": 13.717260360717773,
      "learning_rate": 1.2909519190799502e-05,
      "loss": 2.0194,
      "step": 48900
    },
    {
      "epoch": 6.777054177636137,
      "grad_norm": 9.9658784866333,
      "learning_rate": 1.2903976721629487e-05,
      "loss": 1.9468,
      "step": 48910
    },
    {
      "epoch": 6.778439794928641,
      "grad_norm": 11.79793930053711,
      "learning_rate": 1.289843425245947e-05,
      "loss": 1.4033,
      "step": 48920
    },
    {
      "epoch": 6.779825412221145,
      "grad_norm": 22.079124450683594,
      "learning_rate": 1.2892891783289456e-05,
      "loss": 2.0281,
      "step": 48930
    },
    {
      "epoch": 6.781211029513648,
      "grad_norm": 11.769842147827148,
      "learning_rate": 1.2887349314119441e-05,
      "loss": 2.0511,
      "step": 48940
    },
    {
      "epoch": 6.7825966468061525,
      "grad_norm": 14.621139526367188,
      "learning_rate": 1.2881806844949426e-05,
      "loss": 1.9168,
      "step": 48950
    },
    {
      "epoch": 6.783982264098656,
      "grad_norm": 27.316913604736328,
      "learning_rate": 1.2876818622696413e-05,
      "loss": 1.7337,
      "step": 48960
    },
    {
      "epoch": 6.78536788139116,
      "grad_norm": 10.868560791015625,
      "learning_rate": 1.2871276153526398e-05,
      "loss": 1.8851,
      "step": 48970
    },
    {
      "epoch": 6.786753498683663,
      "grad_norm": 13.737728118896484,
      "learning_rate": 1.2865733684356383e-05,
      "loss": 2.2013,
      "step": 48980
    },
    {
      "epoch": 6.7881391159761675,
      "grad_norm": 6.8399481773376465,
      "learning_rate": 1.2860191215186367e-05,
      "loss": 1.7797,
      "step": 48990
    },
    {
      "epoch": 6.789524733268671,
      "grad_norm": 9.313740730285645,
      "learning_rate": 1.2854648746016352e-05,
      "loss": 2.3834,
      "step": 49000
    },
    {
      "epoch": 6.790910350561175,
      "grad_norm": 18.458980560302734,
      "learning_rate": 1.2849106276846337e-05,
      "loss": 2.8937,
      "step": 49010
    },
    {
      "epoch": 6.792295967853679,
      "grad_norm": 14.243513107299805,
      "learning_rate": 1.2843563807676321e-05,
      "loss": 2.0206,
      "step": 49020
    },
    {
      "epoch": 6.7936815851461825,
      "grad_norm": 10.392801284790039,
      "learning_rate": 1.2838021338506306e-05,
      "loss": 1.8667,
      "step": 49030
    },
    {
      "epoch": 6.795067202438687,
      "grad_norm": 11.67113208770752,
      "learning_rate": 1.2832478869336291e-05,
      "loss": 2.1383,
      "step": 49040
    },
    {
      "epoch": 6.79645281973119,
      "grad_norm": 22.7415828704834,
      "learning_rate": 1.2826936400166275e-05,
      "loss": 1.9476,
      "step": 49050
    },
    {
      "epoch": 6.797838437023694,
      "grad_norm": 20.97039794921875,
      "learning_rate": 1.282139393099626e-05,
      "loss": 1.9213,
      "step": 49060
    },
    {
      "epoch": 6.799224054316198,
      "grad_norm": 26.199819564819336,
      "learning_rate": 1.2815851461826245e-05,
      "loss": 1.8329,
      "step": 49070
    },
    {
      "epoch": 6.800609671608702,
      "grad_norm": 11.614482879638672,
      "learning_rate": 1.2810308992656229e-05,
      "loss": 2.1556,
      "step": 49080
    },
    {
      "epoch": 6.801995288901206,
      "grad_norm": 14.217156410217285,
      "learning_rate": 1.2804766523486214e-05,
      "loss": 1.7131,
      "step": 49090
    },
    {
      "epoch": 6.803380906193709,
      "grad_norm": 18.318944931030273,
      "learning_rate": 1.27992240543162e-05,
      "loss": 1.9811,
      "step": 49100
    },
    {
      "epoch": 6.804766523486213,
      "grad_norm": 14.302298545837402,
      "learning_rate": 1.2793681585146183e-05,
      "loss": 1.88,
      "step": 49110
    },
    {
      "epoch": 6.806152140778717,
      "grad_norm": 10.459745407104492,
      "learning_rate": 1.2788139115976168e-05,
      "loss": 1.581,
      "step": 49120
    },
    {
      "epoch": 6.807537758071221,
      "grad_norm": 12.549145698547363,
      "learning_rate": 1.2782596646806154e-05,
      "loss": 2.0595,
      "step": 49130
    },
    {
      "epoch": 6.808923375363724,
      "grad_norm": 10.078529357910156,
      "learning_rate": 1.2777054177636139e-05,
      "loss": 1.6976,
      "step": 49140
    },
    {
      "epoch": 6.810308992656228,
      "grad_norm": 11.671260833740234,
      "learning_rate": 1.2771511708466122e-05,
      "loss": 2.2241,
      "step": 49150
    },
    {
      "epoch": 6.8116946099487325,
      "grad_norm": 10.926835060119629,
      "learning_rate": 1.2765969239296108e-05,
      "loss": 1.7892,
      "step": 49160
    },
    {
      "epoch": 6.813080227241236,
      "grad_norm": 8.059759140014648,
      "learning_rate": 1.2760426770126093e-05,
      "loss": 1.5179,
      "step": 49170
    },
    {
      "epoch": 6.81446584453374,
      "grad_norm": 11.761439323425293,
      "learning_rate": 1.2754884300956076e-05,
      "loss": 1.788,
      "step": 49180
    },
    {
      "epoch": 6.815851461826243,
      "grad_norm": 11.568829536437988,
      "learning_rate": 1.2749341831786062e-05,
      "loss": 1.8373,
      "step": 49190
    },
    {
      "epoch": 6.8172370791187475,
      "grad_norm": 15.006284713745117,
      "learning_rate": 1.2743799362616047e-05,
      "loss": 2.5282,
      "step": 49200
    },
    {
      "epoch": 6.818622696411251,
      "grad_norm": 7.254584789276123,
      "learning_rate": 1.273825689344603e-05,
      "loss": 1.5809,
      "step": 49210
    },
    {
      "epoch": 6.820008313703755,
      "grad_norm": 13.48732852935791,
      "learning_rate": 1.2732714424276016e-05,
      "loss": 1.747,
      "step": 49220
    },
    {
      "epoch": 6.821393930996258,
      "grad_norm": 11.513823509216309,
      "learning_rate": 1.2727171955106001e-05,
      "loss": 2.0799,
      "step": 49230
    },
    {
      "epoch": 6.8227795482887625,
      "grad_norm": 10.053812980651855,
      "learning_rate": 1.2721629485935985e-05,
      "loss": 2.0296,
      "step": 49240
    },
    {
      "epoch": 6.824165165581267,
      "grad_norm": 18.523014068603516,
      "learning_rate": 1.271608701676597e-05,
      "loss": 1.9377,
      "step": 49250
    },
    {
      "epoch": 6.82555078287377,
      "grad_norm": 13.369449615478516,
      "learning_rate": 1.2710544547595955e-05,
      "loss": 2.1631,
      "step": 49260
    },
    {
      "epoch": 6.826936400166274,
      "grad_norm": 19.83697509765625,
      "learning_rate": 1.2705002078425939e-05,
      "loss": 1.8718,
      "step": 49270
    },
    {
      "epoch": 6.8283220174587775,
      "grad_norm": 16.076522827148438,
      "learning_rate": 1.2699459609255924e-05,
      "loss": 1.9801,
      "step": 49280
    },
    {
      "epoch": 6.829707634751282,
      "grad_norm": 16.96306037902832,
      "learning_rate": 1.2693917140085909e-05,
      "loss": 1.802,
      "step": 49290
    },
    {
      "epoch": 6.831093252043786,
      "grad_norm": 14.925638198852539,
      "learning_rate": 1.2688374670915894e-05,
      "loss": 1.9089,
      "step": 49300
    },
    {
      "epoch": 6.832478869336289,
      "grad_norm": 9.083992958068848,
      "learning_rate": 1.2682832201745878e-05,
      "loss": 1.7955,
      "step": 49310
    },
    {
      "epoch": 6.833864486628793,
      "grad_norm": 14.691743850708008,
      "learning_rate": 1.2677289732575863e-05,
      "loss": 1.5899,
      "step": 49320
    },
    {
      "epoch": 6.835250103921297,
      "grad_norm": 22.02202033996582,
      "learning_rate": 1.2671747263405848e-05,
      "loss": 1.7785,
      "step": 49330
    },
    {
      "epoch": 6.836635721213801,
      "grad_norm": 11.107169151306152,
      "learning_rate": 1.2666204794235832e-05,
      "loss": 1.7817,
      "step": 49340
    },
    {
      "epoch": 6.838021338506304,
      "grad_norm": 18.015478134155273,
      "learning_rate": 1.2660662325065817e-05,
      "loss": 2.1132,
      "step": 49350
    },
    {
      "epoch": 6.839406955798808,
      "grad_norm": 6.705733776092529,
      "learning_rate": 1.2655119855895802e-05,
      "loss": 1.7418,
      "step": 49360
    },
    {
      "epoch": 6.840792573091312,
      "grad_norm": 10.147285461425781,
      "learning_rate": 1.2649577386725786e-05,
      "loss": 2.0518,
      "step": 49370
    },
    {
      "epoch": 6.842178190383816,
      "grad_norm": 8.378418922424316,
      "learning_rate": 1.2644034917555771e-05,
      "loss": 1.5239,
      "step": 49380
    },
    {
      "epoch": 6.84356380767632,
      "grad_norm": 12.099312782287598,
      "learning_rate": 1.2638492448385758e-05,
      "loss": 1.4731,
      "step": 49390
    },
    {
      "epoch": 6.844949424968823,
      "grad_norm": 19.657848358154297,
      "learning_rate": 1.263294997921574e-05,
      "loss": 1.9108,
      "step": 49400
    },
    {
      "epoch": 6.846335042261328,
      "grad_norm": 14.448751449584961,
      "learning_rate": 1.2627407510045725e-05,
      "loss": 1.7098,
      "step": 49410
    },
    {
      "epoch": 6.847720659553831,
      "grad_norm": 22.500972747802734,
      "learning_rate": 1.2621865040875712e-05,
      "loss": 2.0602,
      "step": 49420
    },
    {
      "epoch": 6.849106276846335,
      "grad_norm": 14.558570861816406,
      "learning_rate": 1.2616322571705698e-05,
      "loss": 2.1101,
      "step": 49430
    },
    {
      "epoch": 6.850491894138839,
      "grad_norm": 14.777463912963867,
      "learning_rate": 1.261078010253568e-05,
      "loss": 2.0862,
      "step": 49440
    },
    {
      "epoch": 6.851877511431343,
      "grad_norm": 14.096564292907715,
      "learning_rate": 1.2605237633365666e-05,
      "loss": 1.8105,
      "step": 49450
    },
    {
      "epoch": 6.853263128723847,
      "grad_norm": 15.768111228942871,
      "learning_rate": 1.2599695164195652e-05,
      "loss": 2.2915,
      "step": 49460
    },
    {
      "epoch": 6.85464874601635,
      "grad_norm": 12.460687637329102,
      "learning_rate": 1.2594152695025635e-05,
      "loss": 2.0708,
      "step": 49470
    },
    {
      "epoch": 6.856034363308854,
      "grad_norm": 8.542485237121582,
      "learning_rate": 1.258861022585562e-05,
      "loss": 2.1433,
      "step": 49480
    },
    {
      "epoch": 6.857419980601358,
      "grad_norm": 9.025757789611816,
      "learning_rate": 1.2583067756685606e-05,
      "loss": 1.9044,
      "step": 49490
    },
    {
      "epoch": 6.858805597893862,
      "grad_norm": 14.088888168334961,
      "learning_rate": 1.257752528751559e-05,
      "loss": 1.8497,
      "step": 49500
    },
    {
      "epoch": 6.860191215186365,
      "grad_norm": 10.310491561889648,
      "learning_rate": 1.2571982818345574e-05,
      "loss": 1.9488,
      "step": 49510
    },
    {
      "epoch": 6.861576832478869,
      "grad_norm": 21.80890464782715,
      "learning_rate": 1.256644034917556e-05,
      "loss": 1.8213,
      "step": 49520
    },
    {
      "epoch": 6.8629624497713735,
      "grad_norm": 16.23206901550293,
      "learning_rate": 1.2560897880005543e-05,
      "loss": 1.6467,
      "step": 49530
    },
    {
      "epoch": 6.864348067063877,
      "grad_norm": 13.576860427856445,
      "learning_rate": 1.2555355410835528e-05,
      "loss": 1.9676,
      "step": 49540
    },
    {
      "epoch": 6.865733684356381,
      "grad_norm": 12.222113609313965,
      "learning_rate": 1.2549812941665514e-05,
      "loss": 2.2276,
      "step": 49550
    },
    {
      "epoch": 6.867119301648884,
      "grad_norm": 17.48398208618164,
      "learning_rate": 1.2544270472495497e-05,
      "loss": 1.8523,
      "step": 49560
    },
    {
      "epoch": 6.8685049189413885,
      "grad_norm": 10.704626083374023,
      "learning_rate": 1.2538728003325483e-05,
      "loss": 2.0996,
      "step": 49570
    },
    {
      "epoch": 6.869890536233893,
      "grad_norm": 19.877819061279297,
      "learning_rate": 1.2533185534155468e-05,
      "loss": 2.0168,
      "step": 49580
    },
    {
      "epoch": 6.871276153526396,
      "grad_norm": 17.828754425048828,
      "learning_rate": 1.2527643064985453e-05,
      "loss": 2.1505,
      "step": 49590
    },
    {
      "epoch": 6.8726617708189,
      "grad_norm": 14.441047668457031,
      "learning_rate": 1.2522100595815437e-05,
      "loss": 2.1967,
      "step": 49600
    },
    {
      "epoch": 6.8740473881114035,
      "grad_norm": 12.717619895935059,
      "learning_rate": 1.2516558126645422e-05,
      "loss": 1.3831,
      "step": 49610
    },
    {
      "epoch": 6.875433005403908,
      "grad_norm": 16.123090744018555,
      "learning_rate": 1.2511015657475407e-05,
      "loss": 1.7846,
      "step": 49620
    },
    {
      "epoch": 6.876818622696411,
      "grad_norm": 17.881610870361328,
      "learning_rate": 1.250547318830539e-05,
      "loss": 1.5939,
      "step": 49630
    },
    {
      "epoch": 6.878204239988915,
      "grad_norm": 9.766188621520996,
      "learning_rate": 1.2499930719135376e-05,
      "loss": 1.9897,
      "step": 49640
    },
    {
      "epoch": 6.8795898572814185,
      "grad_norm": 13.44609546661377,
      "learning_rate": 1.2494388249965361e-05,
      "loss": 2.0882,
      "step": 49650
    },
    {
      "epoch": 6.880975474573923,
      "grad_norm": 18.093626022338867,
      "learning_rate": 1.2488845780795345e-05,
      "loss": 1.8763,
      "step": 49660
    },
    {
      "epoch": 6.882361091866427,
      "grad_norm": 9.600439071655273,
      "learning_rate": 1.248330331162533e-05,
      "loss": 2.0224,
      "step": 49670
    },
    {
      "epoch": 6.88374670915893,
      "grad_norm": 14.275860786437988,
      "learning_rate": 1.2477760842455315e-05,
      "loss": 1.9831,
      "step": 49680
    },
    {
      "epoch": 6.885132326451434,
      "grad_norm": 16.686275482177734,
      "learning_rate": 1.2472218373285299e-05,
      "loss": 2.1653,
      "step": 49690
    },
    {
      "epoch": 6.886517943743938,
      "grad_norm": 22.09076690673828,
      "learning_rate": 1.2466675904115284e-05,
      "loss": 2.0365,
      "step": 49700
    },
    {
      "epoch": 6.887903561036442,
      "grad_norm": 10.725715637207031,
      "learning_rate": 1.246113343494527e-05,
      "loss": 1.8029,
      "step": 49710
    },
    {
      "epoch": 6.889289178328945,
      "grad_norm": 9.873047828674316,
      "learning_rate": 1.2455590965775254e-05,
      "loss": 1.9722,
      "step": 49720
    },
    {
      "epoch": 6.890674795621449,
      "grad_norm": 12.858006477355957,
      "learning_rate": 1.2450048496605238e-05,
      "loss": 1.8047,
      "step": 49730
    },
    {
      "epoch": 6.8920604129139535,
      "grad_norm": 8.873475074768066,
      "learning_rate": 1.2444506027435223e-05,
      "loss": 2.533,
      "step": 49740
    },
    {
      "epoch": 6.893446030206457,
      "grad_norm": 12.561365127563477,
      "learning_rate": 1.2438963558265209e-05,
      "loss": 1.5631,
      "step": 49750
    },
    {
      "epoch": 6.894831647498961,
      "grad_norm": 12.789769172668457,
      "learning_rate": 1.2433421089095192e-05,
      "loss": 1.9268,
      "step": 49760
    },
    {
      "epoch": 6.896217264791464,
      "grad_norm": 17.635217666625977,
      "learning_rate": 1.2427878619925177e-05,
      "loss": 2.121,
      "step": 49770
    },
    {
      "epoch": 6.8976028820839685,
      "grad_norm": 6.456995964050293,
      "learning_rate": 1.2422336150755163e-05,
      "loss": 2.1053,
      "step": 49780
    },
    {
      "epoch": 6.898988499376472,
      "grad_norm": 11.296070098876953,
      "learning_rate": 1.2416793681585146e-05,
      "loss": 1.7637,
      "step": 49790
    },
    {
      "epoch": 6.900374116668976,
      "grad_norm": 15.996532440185547,
      "learning_rate": 1.2411251212415131e-05,
      "loss": 1.9294,
      "step": 49800
    },
    {
      "epoch": 6.90175973396148,
      "grad_norm": 7.825758934020996,
      "learning_rate": 1.2405708743245117e-05,
      "loss": 1.6416,
      "step": 49810
    },
    {
      "epoch": 6.9031453512539835,
      "grad_norm": 10.68472957611084,
      "learning_rate": 1.24001662740751e-05,
      "loss": 2.3506,
      "step": 49820
    },
    {
      "epoch": 6.904530968546488,
      "grad_norm": 13.559295654296875,
      "learning_rate": 1.2394623804905085e-05,
      "loss": 1.8053,
      "step": 49830
    },
    {
      "epoch": 6.905916585838991,
      "grad_norm": 14.770870208740234,
      "learning_rate": 1.238908133573507e-05,
      "loss": 1.6529,
      "step": 49840
    },
    {
      "epoch": 6.907302203131495,
      "grad_norm": 26.577125549316406,
      "learning_rate": 1.2383538866565054e-05,
      "loss": 1.4197,
      "step": 49850
    },
    {
      "epoch": 6.9086878204239985,
      "grad_norm": 10.392426490783691,
      "learning_rate": 1.237799639739504e-05,
      "loss": 2.3809,
      "step": 49860
    },
    {
      "epoch": 6.910073437716503,
      "grad_norm": 8.481742858886719,
      "learning_rate": 1.2372453928225025e-05,
      "loss": 1.5268,
      "step": 49870
    },
    {
      "epoch": 6.911459055009006,
      "grad_norm": 21.63142204284668,
      "learning_rate": 1.2366911459055012e-05,
      "loss": 2.0255,
      "step": 49880
    },
    {
      "epoch": 6.91284467230151,
      "grad_norm": 16.555898666381836,
      "learning_rate": 1.2361368989884994e-05,
      "loss": 1.8731,
      "step": 49890
    },
    {
      "epoch": 6.914230289594014,
      "grad_norm": 16.3935604095459,
      "learning_rate": 1.235582652071498e-05,
      "loss": 1.9173,
      "step": 49900
    },
    {
      "epoch": 6.915615906886518,
      "grad_norm": 12.227615356445312,
      "learning_rate": 1.2350284051544966e-05,
      "loss": 1.913,
      "step": 49910
    },
    {
      "epoch": 6.917001524179022,
      "grad_norm": 21.526580810546875,
      "learning_rate": 1.2344741582374948e-05,
      "loss": 1.9487,
      "step": 49920
    },
    {
      "epoch": 6.918387141471525,
      "grad_norm": 14.032005310058594,
      "learning_rate": 1.2339199113204935e-05,
      "loss": 1.9252,
      "step": 49930
    },
    {
      "epoch": 6.919772758764029,
      "grad_norm": 15.039803504943848,
      "learning_rate": 1.233365664403492e-05,
      "loss": 2.0541,
      "step": 49940
    },
    {
      "epoch": 6.921158376056534,
      "grad_norm": 14.762155532836914,
      "learning_rate": 1.2328114174864902e-05,
      "loss": 2.0188,
      "step": 49950
    },
    {
      "epoch": 6.922543993349037,
      "grad_norm": 16.07925033569336,
      "learning_rate": 1.2322571705694889e-05,
      "loss": 1.6221,
      "step": 49960
    },
    {
      "epoch": 6.923929610641541,
      "grad_norm": 8.47598934173584,
      "learning_rate": 1.2317029236524874e-05,
      "loss": 1.9188,
      "step": 49970
    },
    {
      "epoch": 6.925315227934044,
      "grad_norm": 11.829097747802734,
      "learning_rate": 1.2311486767354857e-05,
      "loss": 1.9931,
      "step": 49980
    },
    {
      "epoch": 6.926700845226549,
      "grad_norm": 9.675738334655762,
      "learning_rate": 1.2305944298184843e-05,
      "loss": 1.9958,
      "step": 49990
    },
    {
      "epoch": 6.928086462519052,
      "grad_norm": 15.217538833618164,
      "learning_rate": 1.2300401829014828e-05,
      "loss": 1.6957,
      "step": 50000
    },
    {
      "epoch": 6.929472079811556,
      "grad_norm": 15.502787590026855,
      "learning_rate": 1.2294859359844813e-05,
      "loss": 2.0118,
      "step": 50010
    },
    {
      "epoch": 6.930857697104059,
      "grad_norm": 12.28775405883789,
      "learning_rate": 1.2289316890674797e-05,
      "loss": 1.7869,
      "step": 50020
    },
    {
      "epoch": 6.932243314396564,
      "grad_norm": 12.493881225585938,
      "learning_rate": 1.2283774421504782e-05,
      "loss": 1.7462,
      "step": 50030
    },
    {
      "epoch": 6.933628931689068,
      "grad_norm": 14.483233451843262,
      "learning_rate": 1.2278231952334767e-05,
      "loss": 2.2239,
      "step": 50040
    },
    {
      "epoch": 6.935014548981571,
      "grad_norm": 12.72824478149414,
      "learning_rate": 1.227268948316475e-05,
      "loss": 1.9183,
      "step": 50050
    },
    {
      "epoch": 6.936400166274075,
      "grad_norm": 13.571030616760254,
      "learning_rate": 1.2267147013994736e-05,
      "loss": 1.7087,
      "step": 50060
    },
    {
      "epoch": 6.937785783566579,
      "grad_norm": 8.457987785339355,
      "learning_rate": 1.2261604544824721e-05,
      "loss": 2.0268,
      "step": 50070
    },
    {
      "epoch": 6.939171400859083,
      "grad_norm": 34.21791076660156,
      "learning_rate": 1.2256062075654705e-05,
      "loss": 1.9512,
      "step": 50080
    },
    {
      "epoch": 6.940557018151587,
      "grad_norm": 13.316874504089355,
      "learning_rate": 1.225051960648469e-05,
      "loss": 2.3123,
      "step": 50090
    },
    {
      "epoch": 6.94194263544409,
      "grad_norm": 11.825521469116211,
      "learning_rate": 1.2244977137314675e-05,
      "loss": 2.2105,
      "step": 50100
    },
    {
      "epoch": 6.9433282527365945,
      "grad_norm": 11.096749305725098,
      "learning_rate": 1.2239434668144659e-05,
      "loss": 2.0365,
      "step": 50110
    },
    {
      "epoch": 6.944713870029098,
      "grad_norm": 17.984607696533203,
      "learning_rate": 1.2233892198974644e-05,
      "loss": 2.2028,
      "step": 50120
    },
    {
      "epoch": 6.946099487321602,
      "grad_norm": 13.645357131958008,
      "learning_rate": 1.222834972980463e-05,
      "loss": 1.8112,
      "step": 50130
    },
    {
      "epoch": 6.947485104614105,
      "grad_norm": 12.302380561828613,
      "learning_rate": 1.2222807260634613e-05,
      "loss": 1.8649,
      "step": 50140
    },
    {
      "epoch": 6.9488707219066095,
      "grad_norm": 5.327244281768799,
      "learning_rate": 1.2217264791464598e-05,
      "loss": 1.9505,
      "step": 50150
    },
    {
      "epoch": 6.950256339199113,
      "grad_norm": 9.055081367492676,
      "learning_rate": 1.2211722322294583e-05,
      "loss": 1.6948,
      "step": 50160
    },
    {
      "epoch": 6.951641956491617,
      "grad_norm": 6.385087966918945,
      "learning_rate": 1.2206179853124569e-05,
      "loss": 1.4178,
      "step": 50170
    },
    {
      "epoch": 6.953027573784121,
      "grad_norm": 22.1436824798584,
      "learning_rate": 1.2200637383954552e-05,
      "loss": 1.938,
      "step": 50180
    },
    {
      "epoch": 6.9544131910766245,
      "grad_norm": 11.03735637664795,
      "learning_rate": 1.2195094914784537e-05,
      "loss": 2.563,
      "step": 50190
    },
    {
      "epoch": 6.955798808369129,
      "grad_norm": 14.411739349365234,
      "learning_rate": 1.2189552445614523e-05,
      "loss": 1.4622,
      "step": 50200
    },
    {
      "epoch": 6.957184425661632,
      "grad_norm": 5.888562202453613,
      "learning_rate": 1.2184009976444506e-05,
      "loss": 1.7929,
      "step": 50210
    },
    {
      "epoch": 6.958570042954136,
      "grad_norm": 16.732879638671875,
      "learning_rate": 1.2178467507274492e-05,
      "loss": 1.8172,
      "step": 50220
    },
    {
      "epoch": 6.95995566024664,
      "grad_norm": 21.90033531188965,
      "learning_rate": 1.2172925038104477e-05,
      "loss": 1.7226,
      "step": 50230
    },
    {
      "epoch": 6.961341277539144,
      "grad_norm": 15.34982967376709,
      "learning_rate": 1.216738256893446e-05,
      "loss": 1.8292,
      "step": 50240
    },
    {
      "epoch": 6.962726894831648,
      "grad_norm": 11.082958221435547,
      "learning_rate": 1.2161840099764446e-05,
      "loss": 1.7608,
      "step": 50250
    },
    {
      "epoch": 6.964112512124151,
      "grad_norm": 30.09527587890625,
      "learning_rate": 1.215629763059443e-05,
      "loss": 1.8005,
      "step": 50260
    },
    {
      "epoch": 6.965498129416655,
      "grad_norm": 10.880014419555664,
      "learning_rate": 1.2150755161424414e-05,
      "loss": 1.8976,
      "step": 50270
    },
    {
      "epoch": 6.966883746709159,
      "grad_norm": 4.263045310974121,
      "learning_rate": 1.21452126922544e-05,
      "loss": 1.589,
      "step": 50280
    },
    {
      "epoch": 6.968269364001663,
      "grad_norm": 11.790842056274414,
      "learning_rate": 1.2139670223084385e-05,
      "loss": 1.9248,
      "step": 50290
    },
    {
      "epoch": 6.969654981294166,
      "grad_norm": 12.254383087158203,
      "learning_rate": 1.213412775391437e-05,
      "loss": 1.9721,
      "step": 50300
    },
    {
      "epoch": 6.97104059858667,
      "grad_norm": 10.972877502441406,
      "learning_rate": 1.2128585284744354e-05,
      "loss": 2.0467,
      "step": 50310
    },
    {
      "epoch": 6.9724262158791745,
      "grad_norm": 8.603904724121094,
      "learning_rate": 1.2123042815574339e-05,
      "loss": 1.6858,
      "step": 50320
    },
    {
      "epoch": 6.973811833171678,
      "grad_norm": 14.022631645202637,
      "learning_rate": 1.2117500346404324e-05,
      "loss": 1.6573,
      "step": 50330
    },
    {
      "epoch": 6.975197450464182,
      "grad_norm": 13.475678443908691,
      "learning_rate": 1.2111957877234308e-05,
      "loss": 2.1851,
      "step": 50340
    },
    {
      "epoch": 6.976583067756685,
      "grad_norm": 7.243092060089111,
      "learning_rate": 1.2106415408064293e-05,
      "loss": 1.7233,
      "step": 50350
    },
    {
      "epoch": 6.9779686850491895,
      "grad_norm": 12.650873184204102,
      "learning_rate": 1.210087293889428e-05,
      "loss": 1.8736,
      "step": 50360
    },
    {
      "epoch": 6.979354302341693,
      "grad_norm": 14.800111770629883,
      "learning_rate": 1.2095330469724262e-05,
      "loss": 1.5316,
      "step": 50370
    },
    {
      "epoch": 6.980739919634197,
      "grad_norm": 13.696953773498535,
      "learning_rate": 1.2089788000554247e-05,
      "loss": 1.9911,
      "step": 50380
    },
    {
      "epoch": 6.9821255369267,
      "grad_norm": 9.292529106140137,
      "learning_rate": 1.2084245531384234e-05,
      "loss": 1.9697,
      "step": 50390
    },
    {
      "epoch": 6.9835111542192045,
      "grad_norm": 18.833070755004883,
      "learning_rate": 1.2078703062214216e-05,
      "loss": 1.836,
      "step": 50400
    },
    {
      "epoch": 6.984896771511709,
      "grad_norm": 14.761469841003418,
      "learning_rate": 1.2073160593044203e-05,
      "loss": 1.6295,
      "step": 50410
    },
    {
      "epoch": 6.986282388804212,
      "grad_norm": 13.26773452758789,
      "learning_rate": 1.2067618123874188e-05,
      "loss": 2.0433,
      "step": 50420
    },
    {
      "epoch": 6.987668006096716,
      "grad_norm": 16.82174301147461,
      "learning_rate": 1.206207565470417e-05,
      "loss": 1.4336,
      "step": 50430
    },
    {
      "epoch": 6.9890536233892195,
      "grad_norm": 12.510298728942871,
      "learning_rate": 1.2056533185534157e-05,
      "loss": 1.6912,
      "step": 50440
    },
    {
      "epoch": 6.990439240681724,
      "grad_norm": 10.197585105895996,
      "learning_rate": 1.2050990716364142e-05,
      "loss": 1.6598,
      "step": 50450
    },
    {
      "epoch": 6.991824857974228,
      "grad_norm": 18.086902618408203,
      "learning_rate": 1.2045448247194127e-05,
      "loss": 1.7093,
      "step": 50460
    },
    {
      "epoch": 6.993210475266731,
      "grad_norm": 11.39276123046875,
      "learning_rate": 1.2039905778024111e-05,
      "loss": 1.8918,
      "step": 50470
    },
    {
      "epoch": 6.994596092559235,
      "grad_norm": 8.036104202270508,
      "learning_rate": 1.2034363308854096e-05,
      "loss": 1.989,
      "step": 50480
    },
    {
      "epoch": 6.995981709851739,
      "grad_norm": 8.702296257019043,
      "learning_rate": 1.2028820839684081e-05,
      "loss": 1.3995,
      "step": 50490
    },
    {
      "epoch": 6.997367327144243,
      "grad_norm": 18.729244232177734,
      "learning_rate": 1.2023278370514065e-05,
      "loss": 1.7136,
      "step": 50500
    },
    {
      "epoch": 6.998752944436746,
      "grad_norm": 14.0715913772583,
      "learning_rate": 1.201773590134405e-05,
      "loss": 1.9588,
      "step": 50510
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.5318087318087318,
      "eval_bert_f1": 0.9882360696792603,
      "eval_bert_precision": 0.990032434463501,
      "eval_bert_recall": 0.9868359565734863,
      "eval_f1": 0.07356826199901201,
      "eval_loss": 2.1336729526519775,
      "eval_runtime": 280.1038,
      "eval_samples_per_second": 51.517,
      "eval_steps_per_second": 6.44,
      "eval_synonym_accuracy": 0.5444213444213444,
      "step": 50519
    },
    {
      "epoch": 7.00013856172925,
      "grad_norm": 25.04451560974121,
      "learning_rate": 1.2012193432174035e-05,
      "loss": 2.0133,
      "step": 50520
    },
    {
      "epoch": 7.001524179021755,
      "grad_norm": 9.12547492980957,
      "learning_rate": 1.2006650963004019e-05,
      "loss": 1.913,
      "step": 50530
    },
    {
      "epoch": 7.002909796314258,
      "grad_norm": 8.965474128723145,
      "learning_rate": 1.2001108493834004e-05,
      "loss": 2.0901,
      "step": 50540
    },
    {
      "epoch": 7.004295413606762,
      "grad_norm": 14.68580150604248,
      "learning_rate": 1.199556602466399e-05,
      "loss": 1.7869,
      "step": 50550
    },
    {
      "epoch": 7.005681030899265,
      "grad_norm": 8.508548736572266,
      "learning_rate": 1.1990023555493973e-05,
      "loss": 1.8311,
      "step": 50560
    },
    {
      "epoch": 7.00706664819177,
      "grad_norm": 20.606163024902344,
      "learning_rate": 1.1984481086323958e-05,
      "loss": 1.5376,
      "step": 50570
    },
    {
      "epoch": 7.008452265484273,
      "grad_norm": 16.668153762817383,
      "learning_rate": 1.1978938617153944e-05,
      "loss": 1.8916,
      "step": 50580
    },
    {
      "epoch": 7.009837882776777,
      "grad_norm": 7.887907981872559,
      "learning_rate": 1.1973396147983929e-05,
      "loss": 1.7472,
      "step": 50590
    },
    {
      "epoch": 7.011223500069281,
      "grad_norm": 17.363595962524414,
      "learning_rate": 1.1967853678813912e-05,
      "loss": 1.896,
      "step": 50600
    },
    {
      "epoch": 7.012609117361785,
      "grad_norm": 13.650079727172852,
      "learning_rate": 1.1962311209643898e-05,
      "loss": 1.4029,
      "step": 50610
    },
    {
      "epoch": 7.013994734654289,
      "grad_norm": 17.877700805664062,
      "learning_rate": 1.1956768740473883e-05,
      "loss": 1.8505,
      "step": 50620
    },
    {
      "epoch": 7.015380351946792,
      "grad_norm": 10.223145484924316,
      "learning_rate": 1.1951226271303866e-05,
      "loss": 2.0317,
      "step": 50630
    },
    {
      "epoch": 7.016765969239296,
      "grad_norm": 12.456276893615723,
      "learning_rate": 1.1945683802133852e-05,
      "loss": 1.4924,
      "step": 50640
    },
    {
      "epoch": 7.0181515865318,
      "grad_norm": 9.70611572265625,
      "learning_rate": 1.1940141332963837e-05,
      "loss": 1.9909,
      "step": 50650
    },
    {
      "epoch": 7.019537203824304,
      "grad_norm": 11.640019416809082,
      "learning_rate": 1.193459886379382e-05,
      "loss": 1.8439,
      "step": 50660
    },
    {
      "epoch": 7.020922821116807,
      "grad_norm": 13.642772674560547,
      "learning_rate": 1.1929056394623806e-05,
      "loss": 2.1506,
      "step": 50670
    },
    {
      "epoch": 7.022308438409311,
      "grad_norm": 13.746353149414062,
      "learning_rate": 1.1923513925453791e-05,
      "loss": 2.1925,
      "step": 50680
    },
    {
      "epoch": 7.0236940557018155,
      "grad_norm": 8.944897651672363,
      "learning_rate": 1.1917971456283775e-05,
      "loss": 1.8875,
      "step": 50690
    },
    {
      "epoch": 7.025079672994319,
      "grad_norm": 16.14792251586914,
      "learning_rate": 1.191242898711376e-05,
      "loss": 1.9158,
      "step": 50700
    },
    {
      "epoch": 7.026465290286823,
      "grad_norm": 14.77883529663086,
      "learning_rate": 1.1906886517943745e-05,
      "loss": 1.9023,
      "step": 50710
    },
    {
      "epoch": 7.027850907579326,
      "grad_norm": 13.757935523986816,
      "learning_rate": 1.1901344048773729e-05,
      "loss": 1.9874,
      "step": 50720
    },
    {
      "epoch": 7.0292365248718305,
      "grad_norm": 11.839675903320312,
      "learning_rate": 1.1895801579603714e-05,
      "loss": 1.9239,
      "step": 50730
    },
    {
      "epoch": 7.030622142164334,
      "grad_norm": 8.594076156616211,
      "learning_rate": 1.1890259110433699e-05,
      "loss": 1.6387,
      "step": 50740
    },
    {
      "epoch": 7.032007759456838,
      "grad_norm": 8.773295402526855,
      "learning_rate": 1.1884716641263684e-05,
      "loss": 2.2088,
      "step": 50750
    },
    {
      "epoch": 7.033393376749342,
      "grad_norm": 10.12637710571289,
      "learning_rate": 1.1879174172093668e-05,
      "loss": 1.7679,
      "step": 50760
    },
    {
      "epoch": 7.0347789940418455,
      "grad_norm": 13.66878890991211,
      "learning_rate": 1.1873631702923653e-05,
      "loss": 1.4415,
      "step": 50770
    },
    {
      "epoch": 7.03616461133435,
      "grad_norm": 12.852863311767578,
      "learning_rate": 1.1868089233753638e-05,
      "loss": 1.9781,
      "step": 50780
    },
    {
      "epoch": 7.037550228626853,
      "grad_norm": 18.084354400634766,
      "learning_rate": 1.1862546764583622e-05,
      "loss": 1.8357,
      "step": 50790
    },
    {
      "epoch": 7.038935845919357,
      "grad_norm": 7.80521297454834,
      "learning_rate": 1.1857004295413607e-05,
      "loss": 1.8219,
      "step": 50800
    },
    {
      "epoch": 7.0403214632118605,
      "grad_norm": 17.03194236755371,
      "learning_rate": 1.1851461826243592e-05,
      "loss": 1.5858,
      "step": 50810
    },
    {
      "epoch": 7.041707080504365,
      "grad_norm": 11.298080444335938,
      "learning_rate": 1.1845919357073576e-05,
      "loss": 2.0114,
      "step": 50820
    },
    {
      "epoch": 7.043092697796869,
      "grad_norm": 12.401262283325195,
      "learning_rate": 1.1840376887903561e-05,
      "loss": 1.6062,
      "step": 50830
    },
    {
      "epoch": 7.044478315089372,
      "grad_norm": 12.732489585876465,
      "learning_rate": 1.1834834418733547e-05,
      "loss": 1.5396,
      "step": 50840
    },
    {
      "epoch": 7.045863932381876,
      "grad_norm": 16.50591468811035,
      "learning_rate": 1.182929194956353e-05,
      "loss": 1.8443,
      "step": 50850
    },
    {
      "epoch": 7.04724954967438,
      "grad_norm": 14.096792221069336,
      "learning_rate": 1.1823749480393515e-05,
      "loss": 1.9482,
      "step": 50860
    },
    {
      "epoch": 7.048635166966884,
      "grad_norm": 12.757354736328125,
      "learning_rate": 1.1818207011223502e-05,
      "loss": 1.8445,
      "step": 50870
    },
    {
      "epoch": 7.050020784259387,
      "grad_norm": 11.759042739868164,
      "learning_rate": 1.1812664542053487e-05,
      "loss": 2.003,
      "step": 50880
    },
    {
      "epoch": 7.051406401551891,
      "grad_norm": 18.997011184692383,
      "learning_rate": 1.180712207288347e-05,
      "loss": 1.7902,
      "step": 50890
    },
    {
      "epoch": 7.0527920188443955,
      "grad_norm": 8.797693252563477,
      "learning_rate": 1.1801579603713456e-05,
      "loss": 1.779,
      "step": 50900
    },
    {
      "epoch": 7.054177636136899,
      "grad_norm": 11.922112464904785,
      "learning_rate": 1.1796037134543442e-05,
      "loss": 1.7648,
      "step": 50910
    },
    {
      "epoch": 7.055563253429403,
      "grad_norm": 16.021991729736328,
      "learning_rate": 1.1790494665373423e-05,
      "loss": 1.6939,
      "step": 50920
    },
    {
      "epoch": 7.056948870721906,
      "grad_norm": 12.42635440826416,
      "learning_rate": 1.178495219620341e-05,
      "loss": 2.1944,
      "step": 50930
    },
    {
      "epoch": 7.0583344880144105,
      "grad_norm": 17.732994079589844,
      "learning_rate": 1.1779409727033396e-05,
      "loss": 2.1766,
      "step": 50940
    },
    {
      "epoch": 7.059720105306914,
      "grad_norm": 10.266453742980957,
      "learning_rate": 1.177386725786338e-05,
      "loss": 1.9779,
      "step": 50950
    },
    {
      "epoch": 7.061105722599418,
      "grad_norm": 7.655534267425537,
      "learning_rate": 1.1768324788693364e-05,
      "loss": 1.7982,
      "step": 50960
    },
    {
      "epoch": 7.062491339891922,
      "grad_norm": 7.169644832611084,
      "learning_rate": 1.176278231952335e-05,
      "loss": 1.5759,
      "step": 50970
    },
    {
      "epoch": 7.0638769571844255,
      "grad_norm": 11.210095405578613,
      "learning_rate": 1.1757239850353333e-05,
      "loss": 1.5606,
      "step": 50980
    },
    {
      "epoch": 7.06526257447693,
      "grad_norm": 14.203448295593262,
      "learning_rate": 1.1751697381183318e-05,
      "loss": 1.6827,
      "step": 50990
    },
    {
      "epoch": 7.066648191769433,
      "grad_norm": 14.217348098754883,
      "learning_rate": 1.1746154912013304e-05,
      "loss": 1.8821,
      "step": 51000
    },
    {
      "epoch": 7.068033809061937,
      "grad_norm": 15.238809585571289,
      "learning_rate": 1.1740612442843287e-05,
      "loss": 1.7044,
      "step": 51010
    },
    {
      "epoch": 7.0694194263544405,
      "grad_norm": 25.08834457397461,
      "learning_rate": 1.1735069973673273e-05,
      "loss": 1.808,
      "step": 51020
    },
    {
      "epoch": 7.070805043646945,
      "grad_norm": 8.59521770477295,
      "learning_rate": 1.1729527504503258e-05,
      "loss": 1.7684,
      "step": 51030
    },
    {
      "epoch": 7.072190660939449,
      "grad_norm": 9.928277969360352,
      "learning_rate": 1.1723985035333243e-05,
      "loss": 1.6924,
      "step": 51040
    },
    {
      "epoch": 7.073576278231952,
      "grad_norm": 18.817808151245117,
      "learning_rate": 1.1718442566163227e-05,
      "loss": 1.5815,
      "step": 51050
    },
    {
      "epoch": 7.074961895524456,
      "grad_norm": 10.548883438110352,
      "learning_rate": 1.1712900096993212e-05,
      "loss": 1.3739,
      "step": 51060
    },
    {
      "epoch": 7.07634751281696,
      "grad_norm": 9.281869888305664,
      "learning_rate": 1.1707357627823197e-05,
      "loss": 1.7047,
      "step": 51070
    },
    {
      "epoch": 7.077733130109464,
      "grad_norm": 15.888456344604492,
      "learning_rate": 1.170181515865318e-05,
      "loss": 1.9592,
      "step": 51080
    },
    {
      "epoch": 7.079118747401967,
      "grad_norm": 5.57621431350708,
      "learning_rate": 1.1696272689483166e-05,
      "loss": 1.7543,
      "step": 51090
    },
    {
      "epoch": 7.080504364694471,
      "grad_norm": 11.903804779052734,
      "learning_rate": 1.1690730220313151e-05,
      "loss": 1.6085,
      "step": 51100
    },
    {
      "epoch": 7.081889981986976,
      "grad_norm": 18.74913215637207,
      "learning_rate": 1.1685187751143135e-05,
      "loss": 1.9156,
      "step": 51110
    },
    {
      "epoch": 7.083275599279479,
      "grad_norm": 6.290655136108398,
      "learning_rate": 1.167964528197312e-05,
      "loss": 2.162,
      "step": 51120
    },
    {
      "epoch": 7.084661216571983,
      "grad_norm": 19.4044132232666,
      "learning_rate": 1.1674102812803105e-05,
      "loss": 2.1019,
      "step": 51130
    },
    {
      "epoch": 7.086046833864486,
      "grad_norm": 15.956181526184082,
      "learning_rate": 1.1668560343633089e-05,
      "loss": 1.8715,
      "step": 51140
    },
    {
      "epoch": 7.087432451156991,
      "grad_norm": 11.348182678222656,
      "learning_rate": 1.1663017874463074e-05,
      "loss": 1.8158,
      "step": 51150
    },
    {
      "epoch": 7.088818068449494,
      "grad_norm": 12.20079231262207,
      "learning_rate": 1.165747540529306e-05,
      "loss": 1.2612,
      "step": 51160
    },
    {
      "epoch": 7.090203685741998,
      "grad_norm": 19.2413387298584,
      "learning_rate": 1.1651932936123043e-05,
      "loss": 2.0993,
      "step": 51170
    },
    {
      "epoch": 7.091589303034502,
      "grad_norm": 11.314833641052246,
      "learning_rate": 1.1646390466953028e-05,
      "loss": 1.7723,
      "step": 51180
    },
    {
      "epoch": 7.092974920327006,
      "grad_norm": 15.547758102416992,
      "learning_rate": 1.1640847997783013e-05,
      "loss": 2.2661,
      "step": 51190
    },
    {
      "epoch": 7.09436053761951,
      "grad_norm": 9.320924758911133,
      "learning_rate": 1.1635305528612999e-05,
      "loss": 2.0855,
      "step": 51200
    },
    {
      "epoch": 7.095746154912013,
      "grad_norm": 10.717719078063965,
      "learning_rate": 1.1629763059442982e-05,
      "loss": 2.027,
      "step": 51210
    },
    {
      "epoch": 7.097131772204517,
      "grad_norm": 24.116464614868164,
      "learning_rate": 1.1624220590272967e-05,
      "loss": 1.5109,
      "step": 51220
    },
    {
      "epoch": 7.098517389497021,
      "grad_norm": 11.960206031799316,
      "learning_rate": 1.1618678121102953e-05,
      "loss": 2.3493,
      "step": 51230
    },
    {
      "epoch": 7.099903006789525,
      "grad_norm": 14.588623046875,
      "learning_rate": 1.1613135651932936e-05,
      "loss": 1.7084,
      "step": 51240
    },
    {
      "epoch": 7.101288624082029,
      "grad_norm": 8.46792984008789,
      "learning_rate": 1.1607593182762921e-05,
      "loss": 2.1405,
      "step": 51250
    },
    {
      "epoch": 7.102674241374532,
      "grad_norm": 20.409650802612305,
      "learning_rate": 1.1602050713592907e-05,
      "loss": 2.0034,
      "step": 51260
    },
    {
      "epoch": 7.1040598586670365,
      "grad_norm": 15.14937686920166,
      "learning_rate": 1.159650824442289e-05,
      "loss": 1.7268,
      "step": 51270
    },
    {
      "epoch": 7.10544547595954,
      "grad_norm": 18.058155059814453,
      "learning_rate": 1.1590965775252875e-05,
      "loss": 1.9425,
      "step": 51280
    },
    {
      "epoch": 7.106831093252044,
      "grad_norm": 8.819859504699707,
      "learning_rate": 1.158542330608286e-05,
      "loss": 2.0887,
      "step": 51290
    },
    {
      "epoch": 7.108216710544547,
      "grad_norm": 15.666120529174805,
      "learning_rate": 1.1579880836912844e-05,
      "loss": 1.8079,
      "step": 51300
    },
    {
      "epoch": 7.1096023278370515,
      "grad_norm": 26.73042106628418,
      "learning_rate": 1.157433836774283e-05,
      "loss": 1.5576,
      "step": 51310
    },
    {
      "epoch": 7.110987945129555,
      "grad_norm": 13.62419605255127,
      "learning_rate": 1.1568795898572815e-05,
      "loss": 2.2691,
      "step": 51320
    },
    {
      "epoch": 7.112373562422059,
      "grad_norm": 7.052210330963135,
      "learning_rate": 1.1563253429402802e-05,
      "loss": 2.0441,
      "step": 51330
    },
    {
      "epoch": 7.113759179714563,
      "grad_norm": 10.434797286987305,
      "learning_rate": 1.1557710960232784e-05,
      "loss": 1.5619,
      "step": 51340
    },
    {
      "epoch": 7.1151447970070665,
      "grad_norm": 12.11933422088623,
      "learning_rate": 1.1552168491062769e-05,
      "loss": 2.1084,
      "step": 51350
    },
    {
      "epoch": 7.116530414299571,
      "grad_norm": 17.63082504272461,
      "learning_rate": 1.1546626021892756e-05,
      "loss": 1.8888,
      "step": 51360
    },
    {
      "epoch": 7.117916031592074,
      "grad_norm": 17.675403594970703,
      "learning_rate": 1.1541083552722738e-05,
      "loss": 2.1667,
      "step": 51370
    },
    {
      "epoch": 7.119301648884578,
      "grad_norm": 16.138778686523438,
      "learning_rate": 1.1535541083552725e-05,
      "loss": 1.7396,
      "step": 51380
    },
    {
      "epoch": 7.1206872661770815,
      "grad_norm": 9.935975074768066,
      "learning_rate": 1.152999861438271e-05,
      "loss": 2.0318,
      "step": 51390
    },
    {
      "epoch": 7.122072883469586,
      "grad_norm": 13.445430755615234,
      "learning_rate": 1.1524456145212692e-05,
      "loss": 1.7017,
      "step": 51400
    },
    {
      "epoch": 7.12345850076209,
      "grad_norm": 12.086447715759277,
      "learning_rate": 1.1518913676042679e-05,
      "loss": 2.1847,
      "step": 51410
    },
    {
      "epoch": 7.124844118054593,
      "grad_norm": 15.247739791870117,
      "learning_rate": 1.1513371206872664e-05,
      "loss": 1.9639,
      "step": 51420
    },
    {
      "epoch": 7.126229735347097,
      "grad_norm": 16.129404067993164,
      "learning_rate": 1.1507828737702646e-05,
      "loss": 1.7283,
      "step": 51430
    },
    {
      "epoch": 7.127615352639601,
      "grad_norm": 13.228449821472168,
      "learning_rate": 1.1502286268532633e-05,
      "loss": 2.1116,
      "step": 51440
    },
    {
      "epoch": 7.129000969932105,
      "grad_norm": 10.926410675048828,
      "learning_rate": 1.1496743799362618e-05,
      "loss": 1.766,
      "step": 51450
    },
    {
      "epoch": 7.130386587224608,
      "grad_norm": 11.691513061523438,
      "learning_rate": 1.1491201330192601e-05,
      "loss": 1.6759,
      "step": 51460
    },
    {
      "epoch": 7.131772204517112,
      "grad_norm": 6.762946128845215,
      "learning_rate": 1.1485658861022587e-05,
      "loss": 1.6707,
      "step": 51470
    },
    {
      "epoch": 7.1331578218096166,
      "grad_norm": 22.434432983398438,
      "learning_rate": 1.1480116391852572e-05,
      "loss": 2.1205,
      "step": 51480
    },
    {
      "epoch": 7.13454343910212,
      "grad_norm": 13.116268157958984,
      "learning_rate": 1.1474573922682557e-05,
      "loss": 1.6665,
      "step": 51490
    },
    {
      "epoch": 7.135929056394624,
      "grad_norm": 12.941546440124512,
      "learning_rate": 1.146903145351254e-05,
      "loss": 2.0997,
      "step": 51500
    },
    {
      "epoch": 7.137314673687127,
      "grad_norm": 14.48519229888916,
      "learning_rate": 1.1463488984342526e-05,
      "loss": 1.4616,
      "step": 51510
    },
    {
      "epoch": 7.1387002909796315,
      "grad_norm": 15.308467864990234,
      "learning_rate": 1.1457946515172511e-05,
      "loss": 2.0965,
      "step": 51520
    },
    {
      "epoch": 7.140085908272135,
      "grad_norm": 7.709055423736572,
      "learning_rate": 1.1452404046002495e-05,
      "loss": 1.7086,
      "step": 51530
    },
    {
      "epoch": 7.141471525564639,
      "grad_norm": 7.12296724319458,
      "learning_rate": 1.144686157683248e-05,
      "loss": 2.1063,
      "step": 51540
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 9.56745433807373,
      "learning_rate": 1.1441319107662465e-05,
      "loss": 2.1647,
      "step": 51550
    },
    {
      "epoch": 7.1442427601496465,
      "grad_norm": 13.051599502563477,
      "learning_rate": 1.1435776638492449e-05,
      "loss": 1.732,
      "step": 51560
    },
    {
      "epoch": 7.145628377442151,
      "grad_norm": 19.9296875,
      "learning_rate": 1.1430234169322434e-05,
      "loss": 1.8568,
      "step": 51570
    },
    {
      "epoch": 7.147013994734654,
      "grad_norm": 24.53359031677246,
      "learning_rate": 1.142469170015242e-05,
      "loss": 1.6704,
      "step": 51580
    },
    {
      "epoch": 7.148399612027158,
      "grad_norm": 14.416803359985352,
      "learning_rate": 1.1419149230982403e-05,
      "loss": 2.0068,
      "step": 51590
    },
    {
      "epoch": 7.1497852293196615,
      "grad_norm": 13.601101875305176,
      "learning_rate": 1.1413606761812388e-05,
      "loss": 1.6789,
      "step": 51600
    },
    {
      "epoch": 7.151170846612166,
      "grad_norm": 8.692950248718262,
      "learning_rate": 1.1408064292642373e-05,
      "loss": 1.5644,
      "step": 51610
    },
    {
      "epoch": 7.15255646390467,
      "grad_norm": 17.667448043823242,
      "learning_rate": 1.1402521823472359e-05,
      "loss": 1.9155,
      "step": 51620
    },
    {
      "epoch": 7.153942081197173,
      "grad_norm": 11.58066177368164,
      "learning_rate": 1.1396979354302342e-05,
      "loss": 1.811,
      "step": 51630
    },
    {
      "epoch": 7.155327698489677,
      "grad_norm": 10.618087768554688,
      "learning_rate": 1.1391436885132327e-05,
      "loss": 1.7534,
      "step": 51640
    },
    {
      "epoch": 7.156713315782181,
      "grad_norm": 13.159997940063477,
      "learning_rate": 1.1385894415962313e-05,
      "loss": 2.1503,
      "step": 51650
    },
    {
      "epoch": 7.158098933074685,
      "grad_norm": 9.621098518371582,
      "learning_rate": 1.1380351946792296e-05,
      "loss": 1.7837,
      "step": 51660
    },
    {
      "epoch": 7.159484550367188,
      "grad_norm": 13.973389625549316,
      "learning_rate": 1.1374809477622282e-05,
      "loss": 1.9511,
      "step": 51670
    },
    {
      "epoch": 7.160870167659692,
      "grad_norm": 11.244667053222656,
      "learning_rate": 1.1369267008452267e-05,
      "loss": 2.003,
      "step": 51680
    },
    {
      "epoch": 7.162255784952197,
      "grad_norm": 16.953351974487305,
      "learning_rate": 1.136372453928225e-05,
      "loss": 2.0442,
      "step": 51690
    },
    {
      "epoch": 7.1636414022447,
      "grad_norm": 13.134794235229492,
      "learning_rate": 1.1358182070112236e-05,
      "loss": 1.7136,
      "step": 51700
    },
    {
      "epoch": 7.165027019537204,
      "grad_norm": 16.70814323425293,
      "learning_rate": 1.135263960094222e-05,
      "loss": 1.8821,
      "step": 51710
    },
    {
      "epoch": 7.166412636829707,
      "grad_norm": 8.495076179504395,
      "learning_rate": 1.1347097131772204e-05,
      "loss": 1.646,
      "step": 51720
    },
    {
      "epoch": 7.167798254122212,
      "grad_norm": 15.180150985717773,
      "learning_rate": 1.134155466260219e-05,
      "loss": 1.9367,
      "step": 51730
    },
    {
      "epoch": 7.169183871414715,
      "grad_norm": 11.730792999267578,
      "learning_rate": 1.1336012193432175e-05,
      "loss": 1.8367,
      "step": 51740
    },
    {
      "epoch": 7.170569488707219,
      "grad_norm": 7.306857109069824,
      "learning_rate": 1.1330469724262158e-05,
      "loss": 1.9359,
      "step": 51750
    },
    {
      "epoch": 7.171955105999723,
      "grad_norm": 10.722821235656738,
      "learning_rate": 1.1324927255092144e-05,
      "loss": 1.8466,
      "step": 51760
    },
    {
      "epoch": 7.173340723292227,
      "grad_norm": 12.77379035949707,
      "learning_rate": 1.1319384785922129e-05,
      "loss": 1.4678,
      "step": 51770
    },
    {
      "epoch": 7.174726340584731,
      "grad_norm": 11.730840682983398,
      "learning_rate": 1.1313842316752114e-05,
      "loss": 1.8806,
      "step": 51780
    },
    {
      "epoch": 7.176111957877234,
      "grad_norm": 17.553499221801758,
      "learning_rate": 1.1308299847582098e-05,
      "loss": 2.0693,
      "step": 51790
    },
    {
      "epoch": 7.177497575169738,
      "grad_norm": 21.41438102722168,
      "learning_rate": 1.1302757378412083e-05,
      "loss": 1.8558,
      "step": 51800
    },
    {
      "epoch": 7.178883192462242,
      "grad_norm": 13.912025451660156,
      "learning_rate": 1.1297214909242068e-05,
      "loss": 1.966,
      "step": 51810
    },
    {
      "epoch": 7.180268809754746,
      "grad_norm": 14.243131637573242,
      "learning_rate": 1.1291672440072052e-05,
      "loss": 1.6566,
      "step": 51820
    },
    {
      "epoch": 7.181654427047249,
      "grad_norm": 14.241271018981934,
      "learning_rate": 1.1286129970902037e-05,
      "loss": 1.7484,
      "step": 51830
    },
    {
      "epoch": 7.183040044339753,
      "grad_norm": 12.872343063354492,
      "learning_rate": 1.1280587501732024e-05,
      "loss": 1.6711,
      "step": 51840
    },
    {
      "epoch": 7.1844256616322575,
      "grad_norm": 15.063424110412598,
      "learning_rate": 1.1275045032562006e-05,
      "loss": 2.1491,
      "step": 51850
    },
    {
      "epoch": 7.185811278924761,
      "grad_norm": 16.130115509033203,
      "learning_rate": 1.1269502563391991e-05,
      "loss": 1.9148,
      "step": 51860
    },
    {
      "epoch": 7.187196896217265,
      "grad_norm": 11.727334976196289,
      "learning_rate": 1.1263960094221978e-05,
      "loss": 2.2372,
      "step": 51870
    },
    {
      "epoch": 7.188582513509768,
      "grad_norm": 18.30166244506836,
      "learning_rate": 1.125841762505196e-05,
      "loss": 1.5863,
      "step": 51880
    },
    {
      "epoch": 7.1899681308022725,
      "grad_norm": 9.419647216796875,
      "learning_rate": 1.1252875155881947e-05,
      "loss": 1.7846,
      "step": 51890
    },
    {
      "epoch": 7.191353748094777,
      "grad_norm": 9.070294380187988,
      "learning_rate": 1.1247332686711932e-05,
      "loss": 1.7461,
      "step": 51900
    },
    {
      "epoch": 7.19273936538728,
      "grad_norm": 15.244428634643555,
      "learning_rate": 1.1241790217541917e-05,
      "loss": 1.6509,
      "step": 51910
    },
    {
      "epoch": 7.194124982679784,
      "grad_norm": 8.456097602844238,
      "learning_rate": 1.1236247748371901e-05,
      "loss": 2.2114,
      "step": 51920
    },
    {
      "epoch": 7.1955105999722875,
      "grad_norm": 12.414069175720215,
      "learning_rate": 1.1230705279201886e-05,
      "loss": 1.5187,
      "step": 51930
    },
    {
      "epoch": 7.196896217264792,
      "grad_norm": 11.077513694763184,
      "learning_rate": 1.1225162810031871e-05,
      "loss": 2.0699,
      "step": 51940
    },
    {
      "epoch": 7.198281834557295,
      "grad_norm": 13.661972045898438,
      "learning_rate": 1.1219620340861855e-05,
      "loss": 1.8415,
      "step": 51950
    },
    {
      "epoch": 7.199667451849799,
      "grad_norm": 8.663350105285645,
      "learning_rate": 1.121407787169184e-05,
      "loss": 1.8988,
      "step": 51960
    },
    {
      "epoch": 7.2010530691423025,
      "grad_norm": 13.15139389038086,
      "learning_rate": 1.1208535402521825e-05,
      "loss": 1.8423,
      "step": 51970
    },
    {
      "epoch": 7.202438686434807,
      "grad_norm": 12.860363006591797,
      "learning_rate": 1.1202992933351809e-05,
      "loss": 2.1259,
      "step": 51980
    },
    {
      "epoch": 7.203824303727311,
      "grad_norm": 10.610527992248535,
      "learning_rate": 1.1197450464181794e-05,
      "loss": 1.5797,
      "step": 51990
    },
    {
      "epoch": 7.205209921019814,
      "grad_norm": 20.210491180419922,
      "learning_rate": 1.119190799501178e-05,
      "loss": 1.5983,
      "step": 52000
    },
    {
      "epoch": 7.206595538312318,
      "grad_norm": 13.108633995056152,
      "learning_rate": 1.1186365525841763e-05,
      "loss": 2.0008,
      "step": 52010
    },
    {
      "epoch": 7.207981155604822,
      "grad_norm": 15.936856269836426,
      "learning_rate": 1.1180823056671748e-05,
      "loss": 1.7194,
      "step": 52020
    },
    {
      "epoch": 7.209366772897326,
      "grad_norm": 18.335693359375,
      "learning_rate": 1.1175280587501734e-05,
      "loss": 1.8221,
      "step": 52030
    },
    {
      "epoch": 7.210752390189829,
      "grad_norm": 15.604353904724121,
      "learning_rate": 1.1169738118331717e-05,
      "loss": 1.4386,
      "step": 52040
    },
    {
      "epoch": 7.212138007482333,
      "grad_norm": 16.4299259185791,
      "learning_rate": 1.1164195649161702e-05,
      "loss": 1.7122,
      "step": 52050
    },
    {
      "epoch": 7.2135236247748376,
      "grad_norm": 14.301675796508789,
      "learning_rate": 1.1158653179991688e-05,
      "loss": 2.4169,
      "step": 52060
    },
    {
      "epoch": 7.214909242067341,
      "grad_norm": 11.29694652557373,
      "learning_rate": 1.1153110710821673e-05,
      "loss": 2.043,
      "step": 52070
    },
    {
      "epoch": 7.216294859359845,
      "grad_norm": 11.998190879821777,
      "learning_rate": 1.1147568241651656e-05,
      "loss": 1.6341,
      "step": 52080
    },
    {
      "epoch": 7.217680476652348,
      "grad_norm": 11.24821662902832,
      "learning_rate": 1.1142025772481642e-05,
      "loss": 1.3967,
      "step": 52090
    },
    {
      "epoch": 7.2190660939448525,
      "grad_norm": 17.740476608276367,
      "learning_rate": 1.1136483303311627e-05,
      "loss": 2.3384,
      "step": 52100
    },
    {
      "epoch": 7.220451711237356,
      "grad_norm": 11.866504669189453,
      "learning_rate": 1.113094083414161e-05,
      "loss": 1.6307,
      "step": 52110
    },
    {
      "epoch": 7.22183732852986,
      "grad_norm": 14.792283058166504,
      "learning_rate": 1.1125398364971596e-05,
      "loss": 1.6152,
      "step": 52120
    },
    {
      "epoch": 7.223222945822364,
      "grad_norm": 13.049473762512207,
      "learning_rate": 1.1119855895801581e-05,
      "loss": 1.8028,
      "step": 52130
    },
    {
      "epoch": 7.2246085631148675,
      "grad_norm": 15.672119140625,
      "learning_rate": 1.1114313426631565e-05,
      "loss": 1.864,
      "step": 52140
    },
    {
      "epoch": 7.225994180407372,
      "grad_norm": 16.116540908813477,
      "learning_rate": 1.110877095746155e-05,
      "loss": 1.988,
      "step": 52150
    },
    {
      "epoch": 7.227379797699875,
      "grad_norm": 13.73437213897705,
      "learning_rate": 1.1103228488291535e-05,
      "loss": 1.8703,
      "step": 52160
    },
    {
      "epoch": 7.228765414992379,
      "grad_norm": 15.00793170928955,
      "learning_rate": 1.1097686019121519e-05,
      "loss": 1.7833,
      "step": 52170
    },
    {
      "epoch": 7.2301510322848825,
      "grad_norm": 16.53685188293457,
      "learning_rate": 1.1092143549951504e-05,
      "loss": 2.077,
      "step": 52180
    },
    {
      "epoch": 7.231536649577387,
      "grad_norm": 11.937737464904785,
      "learning_rate": 1.1086601080781489e-05,
      "loss": 1.9517,
      "step": 52190
    },
    {
      "epoch": 7.232922266869891,
      "grad_norm": 5.95000696182251,
      "learning_rate": 1.1081058611611474e-05,
      "loss": 1.88,
      "step": 52200
    },
    {
      "epoch": 7.234307884162394,
      "grad_norm": 10.963598251342773,
      "learning_rate": 1.1075516142441458e-05,
      "loss": 2.0047,
      "step": 52210
    },
    {
      "epoch": 7.235693501454898,
      "grad_norm": 20.86649513244629,
      "learning_rate": 1.1069973673271443e-05,
      "loss": 1.8064,
      "step": 52220
    },
    {
      "epoch": 7.237079118747402,
      "grad_norm": 12.28896713256836,
      "learning_rate": 1.1064431204101428e-05,
      "loss": 1.8372,
      "step": 52230
    },
    {
      "epoch": 7.238464736039906,
      "grad_norm": 15.083314895629883,
      "learning_rate": 1.1058888734931412e-05,
      "loss": 2.2212,
      "step": 52240
    },
    {
      "epoch": 7.239850353332409,
      "grad_norm": 11.924522399902344,
      "learning_rate": 1.1053346265761397e-05,
      "loss": 1.91,
      "step": 52250
    },
    {
      "epoch": 7.241235970624913,
      "grad_norm": 11.775452613830566,
      "learning_rate": 1.1047803796591382e-05,
      "loss": 1.9792,
      "step": 52260
    },
    {
      "epoch": 7.242621587917418,
      "grad_norm": 14.385826110839844,
      "learning_rate": 1.1042261327421366e-05,
      "loss": 2.1381,
      "step": 52270
    },
    {
      "epoch": 7.244007205209921,
      "grad_norm": 10.383428573608398,
      "learning_rate": 1.1036718858251351e-05,
      "loss": 1.5564,
      "step": 52280
    },
    {
      "epoch": 7.245392822502425,
      "grad_norm": 17.85296058654785,
      "learning_rate": 1.1031176389081336e-05,
      "loss": 1.8596,
      "step": 52290
    },
    {
      "epoch": 7.246778439794928,
      "grad_norm": 13.38623332977295,
      "learning_rate": 1.102563391991132e-05,
      "loss": 1.4142,
      "step": 52300
    },
    {
      "epoch": 7.248164057087433,
      "grad_norm": 18.44287109375,
      "learning_rate": 1.1020091450741305e-05,
      "loss": 2.078,
      "step": 52310
    },
    {
      "epoch": 7.249549674379936,
      "grad_norm": 11.532577514648438,
      "learning_rate": 1.101454898157129e-05,
      "loss": 1.4771,
      "step": 52320
    },
    {
      "epoch": 7.25093529167244,
      "grad_norm": 20.336471557617188,
      "learning_rate": 1.1009006512401274e-05,
      "loss": 1.884,
      "step": 52330
    },
    {
      "epoch": 7.252320908964943,
      "grad_norm": 25.686084747314453,
      "learning_rate": 1.100346404323126e-05,
      "loss": 2.1146,
      "step": 52340
    },
    {
      "epoch": 7.253706526257448,
      "grad_norm": 24.13315773010254,
      "learning_rate": 1.0997921574061246e-05,
      "loss": 1.9179,
      "step": 52350
    },
    {
      "epoch": 7.255092143549952,
      "grad_norm": 13.836343765258789,
      "learning_rate": 1.0992379104891232e-05,
      "loss": 1.8152,
      "step": 52360
    },
    {
      "epoch": 7.256477760842455,
      "grad_norm": 13.159014701843262,
      "learning_rate": 1.0986836635721213e-05,
      "loss": 2.1399,
      "step": 52370
    },
    {
      "epoch": 7.257863378134959,
      "grad_norm": 12.494355201721191,
      "learning_rate": 1.09812941665512e-05,
      "loss": 1.8019,
      "step": 52380
    },
    {
      "epoch": 7.259248995427463,
      "grad_norm": 15.199368476867676,
      "learning_rate": 1.0975751697381186e-05,
      "loss": 2.1637,
      "step": 52390
    },
    {
      "epoch": 7.260634612719967,
      "grad_norm": 20.551340103149414,
      "learning_rate": 1.0970209228211167e-05,
      "loss": 1.4287,
      "step": 52400
    },
    {
      "epoch": 7.262020230012471,
      "grad_norm": 17.82367515563965,
      "learning_rate": 1.0964666759041154e-05,
      "loss": 2.2392,
      "step": 52410
    },
    {
      "epoch": 7.263405847304974,
      "grad_norm": 14.445902824401855,
      "learning_rate": 1.095912428987114e-05,
      "loss": 1.7551,
      "step": 52420
    },
    {
      "epoch": 7.2647914645974785,
      "grad_norm": 7.962295055389404,
      "learning_rate": 1.0953581820701123e-05,
      "loss": 1.3275,
      "step": 52430
    },
    {
      "epoch": 7.266177081889982,
      "grad_norm": 11.108590126037598,
      "learning_rate": 1.0948039351531108e-05,
      "loss": 1.5973,
      "step": 52440
    },
    {
      "epoch": 7.267562699182486,
      "grad_norm": 9.551033973693848,
      "learning_rate": 1.0942496882361094e-05,
      "loss": 1.7289,
      "step": 52450
    },
    {
      "epoch": 7.268948316474989,
      "grad_norm": 9.383914947509766,
      "learning_rate": 1.0936954413191077e-05,
      "loss": 1.8795,
      "step": 52460
    },
    {
      "epoch": 7.2703339337674935,
      "grad_norm": 21.13209342956543,
      "learning_rate": 1.0931411944021063e-05,
      "loss": 2.0967,
      "step": 52470
    },
    {
      "epoch": 7.271719551059997,
      "grad_norm": 14.56693172454834,
      "learning_rate": 1.0925869474851048e-05,
      "loss": 1.8875,
      "step": 52480
    },
    {
      "epoch": 7.273105168352501,
      "grad_norm": 16.890607833862305,
      "learning_rate": 1.0920327005681033e-05,
      "loss": 1.7415,
      "step": 52490
    },
    {
      "epoch": 7.274490785645005,
      "grad_norm": 17.18498420715332,
      "learning_rate": 1.0914784536511017e-05,
      "loss": 1.4682,
      "step": 52500
    },
    {
      "epoch": 7.2758764029375085,
      "grad_norm": 19.501602172851562,
      "learning_rate": 1.0909242067341002e-05,
      "loss": 1.8071,
      "step": 52510
    },
    {
      "epoch": 7.277262020230013,
      "grad_norm": 17.997766494750977,
      "learning_rate": 1.0903699598170987e-05,
      "loss": 1.7285,
      "step": 52520
    },
    {
      "epoch": 7.278647637522516,
      "grad_norm": 13.292196273803711,
      "learning_rate": 1.089815712900097e-05,
      "loss": 1.9979,
      "step": 52530
    },
    {
      "epoch": 7.28003325481502,
      "grad_norm": 12.772260665893555,
      "learning_rate": 1.0892614659830956e-05,
      "loss": 2.0484,
      "step": 52540
    },
    {
      "epoch": 7.281418872107524,
      "grad_norm": 13.606001853942871,
      "learning_rate": 1.0887072190660941e-05,
      "loss": 1.6776,
      "step": 52550
    },
    {
      "epoch": 7.282804489400028,
      "grad_norm": 5.678750038146973,
      "learning_rate": 1.0881529721490925e-05,
      "loss": 1.642,
      "step": 52560
    },
    {
      "epoch": 7.284190106692532,
      "grad_norm": 11.796878814697266,
      "learning_rate": 1.087598725232091e-05,
      "loss": 2.1749,
      "step": 52570
    },
    {
      "epoch": 7.285575723985035,
      "grad_norm": 16.53708839416504,
      "learning_rate": 1.0870444783150895e-05,
      "loss": 2.0119,
      "step": 52580
    },
    {
      "epoch": 7.286961341277539,
      "grad_norm": 14.983945846557617,
      "learning_rate": 1.0864902313980879e-05,
      "loss": 1.9222,
      "step": 52590
    },
    {
      "epoch": 7.288346958570043,
      "grad_norm": 20.912290573120117,
      "learning_rate": 1.0859359844810864e-05,
      "loss": 1.9709,
      "step": 52600
    },
    {
      "epoch": 7.289732575862547,
      "grad_norm": 12.216814041137695,
      "learning_rate": 1.085381737564085e-05,
      "loss": 1.9943,
      "step": 52610
    },
    {
      "epoch": 7.29111819315505,
      "grad_norm": 13.783454895019531,
      "learning_rate": 1.0848274906470833e-05,
      "loss": 1.7106,
      "step": 52620
    },
    {
      "epoch": 7.292503810447554,
      "grad_norm": 10.081366539001465,
      "learning_rate": 1.0842732437300818e-05,
      "loss": 1.818,
      "step": 52630
    },
    {
      "epoch": 7.2938894277400586,
      "grad_norm": 16.416608810424805,
      "learning_rate": 1.0837189968130803e-05,
      "loss": 1.6779,
      "step": 52640
    },
    {
      "epoch": 7.295275045032562,
      "grad_norm": 8.891231536865234,
      "learning_rate": 1.0831647498960789e-05,
      "loss": 1.7473,
      "step": 52650
    },
    {
      "epoch": 7.296660662325066,
      "grad_norm": 17.117677688598633,
      "learning_rate": 1.0826105029790772e-05,
      "loss": 1.8963,
      "step": 52660
    },
    {
      "epoch": 7.298046279617569,
      "grad_norm": 15.179738998413086,
      "learning_rate": 1.0820562560620757e-05,
      "loss": 2.3229,
      "step": 52670
    },
    {
      "epoch": 7.2994318969100735,
      "grad_norm": 16.369415283203125,
      "learning_rate": 1.0815020091450743e-05,
      "loss": 2.2995,
      "step": 52680
    },
    {
      "epoch": 7.300817514202577,
      "grad_norm": 21.296871185302734,
      "learning_rate": 1.0809477622280726e-05,
      "loss": 1.7256,
      "step": 52690
    },
    {
      "epoch": 7.302203131495081,
      "grad_norm": 11.362128257751465,
      "learning_rate": 1.0803935153110711e-05,
      "loss": 1.7071,
      "step": 52700
    },
    {
      "epoch": 7.303588748787585,
      "grad_norm": 13.672744750976562,
      "learning_rate": 1.0798392683940697e-05,
      "loss": 1.7862,
      "step": 52710
    },
    {
      "epoch": 7.3049743660800885,
      "grad_norm": 16.5943546295166,
      "learning_rate": 1.079285021477068e-05,
      "loss": 1.6188,
      "step": 52720
    },
    {
      "epoch": 7.306359983372593,
      "grad_norm": 28.103166580200195,
      "learning_rate": 1.0787307745600665e-05,
      "loss": 1.8786,
      "step": 52730
    },
    {
      "epoch": 7.307745600665096,
      "grad_norm": 10.928138732910156,
      "learning_rate": 1.078176527643065e-05,
      "loss": 1.5353,
      "step": 52740
    },
    {
      "epoch": 7.3091312179576,
      "grad_norm": 9.211477279663086,
      "learning_rate": 1.0776222807260634e-05,
      "loss": 1.6862,
      "step": 52750
    },
    {
      "epoch": 7.3105168352501035,
      "grad_norm": 14.602869033813477,
      "learning_rate": 1.077068033809062e-05,
      "loss": 1.4435,
      "step": 52760
    },
    {
      "epoch": 7.311902452542608,
      "grad_norm": 12.705572128295898,
      "learning_rate": 1.0765137868920605e-05,
      "loss": 1.7101,
      "step": 52770
    },
    {
      "epoch": 7.313288069835112,
      "grad_norm": 19.824539184570312,
      "learning_rate": 1.0759595399750592e-05,
      "loss": 1.4991,
      "step": 52780
    },
    {
      "epoch": 7.314673687127615,
      "grad_norm": 15.853137969970703,
      "learning_rate": 1.0754052930580574e-05,
      "loss": 1.8637,
      "step": 52790
    },
    {
      "epoch": 7.316059304420119,
      "grad_norm": 14.764866828918457,
      "learning_rate": 1.0748510461410559e-05,
      "loss": 1.3474,
      "step": 52800
    },
    {
      "epoch": 7.317444921712623,
      "grad_norm": 8.152328491210938,
      "learning_rate": 1.0742967992240546e-05,
      "loss": 1.7717,
      "step": 52810
    },
    {
      "epoch": 7.318830539005127,
      "grad_norm": 12.867709159851074,
      "learning_rate": 1.0737425523070528e-05,
      "loss": 1.8139,
      "step": 52820
    },
    {
      "epoch": 7.32021615629763,
      "grad_norm": 11.111184120178223,
      "learning_rate": 1.0731883053900513e-05,
      "loss": 2.2014,
      "step": 52830
    },
    {
      "epoch": 7.321601773590134,
      "grad_norm": 14.090611457824707,
      "learning_rate": 1.07263405847305e-05,
      "loss": 1.8973,
      "step": 52840
    },
    {
      "epoch": 7.322987390882639,
      "grad_norm": 7.984696388244629,
      "learning_rate": 1.0721352362477484e-05,
      "loss": 1.45,
      "step": 52850
    },
    {
      "epoch": 7.324373008175142,
      "grad_norm": 13.482983589172363,
      "learning_rate": 1.071580989330747e-05,
      "loss": 1.8998,
      "step": 52860
    },
    {
      "epoch": 7.325758625467646,
      "grad_norm": 14.770210266113281,
      "learning_rate": 1.0710267424137455e-05,
      "loss": 1.6562,
      "step": 52870
    },
    {
      "epoch": 7.327144242760149,
      "grad_norm": 10.180550575256348,
      "learning_rate": 1.0704724954967439e-05,
      "loss": 1.8009,
      "step": 52880
    },
    {
      "epoch": 7.328529860052654,
      "grad_norm": 12.368016242980957,
      "learning_rate": 1.0699182485797424e-05,
      "loss": 1.8193,
      "step": 52890
    },
    {
      "epoch": 7.329915477345157,
      "grad_norm": 9.433987617492676,
      "learning_rate": 1.0693640016627409e-05,
      "loss": 1.7171,
      "step": 52900
    },
    {
      "epoch": 7.331301094637661,
      "grad_norm": 6.9593424797058105,
      "learning_rate": 1.0688097547457393e-05,
      "loss": 1.4509,
      "step": 52910
    },
    {
      "epoch": 7.332686711930165,
      "grad_norm": 15.489447593688965,
      "learning_rate": 1.0682555078287378e-05,
      "loss": 1.7036,
      "step": 52920
    },
    {
      "epoch": 7.334072329222669,
      "grad_norm": 11.930054664611816,
      "learning_rate": 1.0677012609117363e-05,
      "loss": 1.9532,
      "step": 52930
    },
    {
      "epoch": 7.335457946515173,
      "grad_norm": 16.17500877380371,
      "learning_rate": 1.0671470139947347e-05,
      "loss": 1.6932,
      "step": 52940
    },
    {
      "epoch": 7.336843563807676,
      "grad_norm": 17.695890426635742,
      "learning_rate": 1.0665927670777332e-05,
      "loss": 1.9731,
      "step": 52950
    },
    {
      "epoch": 7.33822918110018,
      "grad_norm": 12.642877578735352,
      "learning_rate": 1.0660385201607317e-05,
      "loss": 2.0071,
      "step": 52960
    },
    {
      "epoch": 7.339614798392684,
      "grad_norm": 17.361175537109375,
      "learning_rate": 1.0654842732437302e-05,
      "loss": 2.1659,
      "step": 52970
    },
    {
      "epoch": 7.341000415685188,
      "grad_norm": 11.232858657836914,
      "learning_rate": 1.0649300263267286e-05,
      "loss": 1.8363,
      "step": 52980
    },
    {
      "epoch": 7.342386032977691,
      "grad_norm": 10.845355033874512,
      "learning_rate": 1.0643757794097271e-05,
      "loss": 1.4242,
      "step": 52990
    },
    {
      "epoch": 7.343771650270195,
      "grad_norm": 13.807024002075195,
      "learning_rate": 1.0638215324927256e-05,
      "loss": 1.7957,
      "step": 53000
    },
    {
      "epoch": 7.3451572675626995,
      "grad_norm": 16.773345947265625,
      "learning_rate": 1.063267285575724e-05,
      "loss": 2.0764,
      "step": 53010
    },
    {
      "epoch": 7.346542884855203,
      "grad_norm": 11.460758209228516,
      "learning_rate": 1.0627130386587225e-05,
      "loss": 2.1771,
      "step": 53020
    },
    {
      "epoch": 7.347928502147707,
      "grad_norm": 18.240859985351562,
      "learning_rate": 1.062158791741721e-05,
      "loss": 2.106,
      "step": 53030
    },
    {
      "epoch": 7.34931411944021,
      "grad_norm": 7.428849220275879,
      "learning_rate": 1.0616045448247194e-05,
      "loss": 1.5381,
      "step": 53040
    },
    {
      "epoch": 7.3506997367327145,
      "grad_norm": 12.896259307861328,
      "learning_rate": 1.061050297907718e-05,
      "loss": 1.6775,
      "step": 53050
    },
    {
      "epoch": 7.352085354025219,
      "grad_norm": 20.946277618408203,
      "learning_rate": 1.0604960509907165e-05,
      "loss": 1.5793,
      "step": 53060
    },
    {
      "epoch": 7.353470971317722,
      "grad_norm": 11.714119911193848,
      "learning_rate": 1.0599418040737148e-05,
      "loss": 2.0333,
      "step": 53070
    },
    {
      "epoch": 7.354856588610226,
      "grad_norm": 13.930925369262695,
      "learning_rate": 1.0593875571567133e-05,
      "loss": 1.9849,
      "step": 53080
    },
    {
      "epoch": 7.3562422059027295,
      "grad_norm": 9.365911483764648,
      "learning_rate": 1.0588333102397119e-05,
      "loss": 2.1771,
      "step": 53090
    },
    {
      "epoch": 7.357627823195234,
      "grad_norm": 13.061589241027832,
      "learning_rate": 1.0582790633227104e-05,
      "loss": 1.644,
      "step": 53100
    },
    {
      "epoch": 7.359013440487737,
      "grad_norm": 18.227434158325195,
      "learning_rate": 1.0577248164057087e-05,
      "loss": 1.7314,
      "step": 53110
    },
    {
      "epoch": 7.360399057780241,
      "grad_norm": 9.854875564575195,
      "learning_rate": 1.0571705694887073e-05,
      "loss": 1.5266,
      "step": 53120
    },
    {
      "epoch": 7.3617846750727445,
      "grad_norm": 13.621021270751953,
      "learning_rate": 1.056616322571706e-05,
      "loss": 2.0758,
      "step": 53130
    },
    {
      "epoch": 7.363170292365249,
      "grad_norm": 6.663400173187256,
      "learning_rate": 1.0560620756547041e-05,
      "loss": 1.5911,
      "step": 53140
    },
    {
      "epoch": 7.364555909657753,
      "grad_norm": 7.022164821624756,
      "learning_rate": 1.0555078287377027e-05,
      "loss": 1.8139,
      "step": 53150
    },
    {
      "epoch": 7.365941526950256,
      "grad_norm": 14.630020141601562,
      "learning_rate": 1.0549535818207014e-05,
      "loss": 2.048,
      "step": 53160
    },
    {
      "epoch": 7.36732714424276,
      "grad_norm": 14.449623107910156,
      "learning_rate": 1.0543993349036996e-05,
      "loss": 2.0537,
      "step": 53170
    },
    {
      "epoch": 7.368712761535264,
      "grad_norm": 17.561185836791992,
      "learning_rate": 1.053845087986698e-05,
      "loss": 1.4688,
      "step": 53180
    },
    {
      "epoch": 7.370098378827768,
      "grad_norm": 10.193426132202148,
      "learning_rate": 1.0532908410696968e-05,
      "loss": 1.9803,
      "step": 53190
    },
    {
      "epoch": 7.371483996120271,
      "grad_norm": 32.359737396240234,
      "learning_rate": 1.052736594152695e-05,
      "loss": 2.2618,
      "step": 53200
    },
    {
      "epoch": 7.372869613412775,
      "grad_norm": 8.831109046936035,
      "learning_rate": 1.0521823472356937e-05,
      "loss": 1.7525,
      "step": 53210
    },
    {
      "epoch": 7.3742552307052796,
      "grad_norm": 9.47619342803955,
      "learning_rate": 1.0516281003186922e-05,
      "loss": 2.0723,
      "step": 53220
    },
    {
      "epoch": 7.375640847997783,
      "grad_norm": 9.022014617919922,
      "learning_rate": 1.0510738534016904e-05,
      "loss": 1.7837,
      "step": 53230
    },
    {
      "epoch": 7.377026465290287,
      "grad_norm": 7.241079330444336,
      "learning_rate": 1.050519606484689e-05,
      "loss": 2.2156,
      "step": 53240
    },
    {
      "epoch": 7.37841208258279,
      "grad_norm": 16.332582473754883,
      "learning_rate": 1.0499653595676876e-05,
      "loss": 1.8907,
      "step": 53250
    },
    {
      "epoch": 7.3797976998752945,
      "grad_norm": 26.41318702697754,
      "learning_rate": 1.0494111126506861e-05,
      "loss": 1.8055,
      "step": 53260
    },
    {
      "epoch": 7.381183317167798,
      "grad_norm": 13.72757339477539,
      "learning_rate": 1.0488568657336845e-05,
      "loss": 1.8611,
      "step": 53270
    },
    {
      "epoch": 7.382568934460302,
      "grad_norm": 8.11916732788086,
      "learning_rate": 1.048302618816683e-05,
      "loss": 1.4819,
      "step": 53280
    },
    {
      "epoch": 7.383954551752806,
      "grad_norm": 14.934794425964355,
      "learning_rate": 1.0477483718996815e-05,
      "loss": 2.1324,
      "step": 53290
    },
    {
      "epoch": 7.3853401690453095,
      "grad_norm": 16.29048728942871,
      "learning_rate": 1.0471941249826799e-05,
      "loss": 1.925,
      "step": 53300
    },
    {
      "epoch": 7.386725786337814,
      "grad_norm": 19.400489807128906,
      "learning_rate": 1.0466398780656784e-05,
      "loss": 1.7481,
      "step": 53310
    },
    {
      "epoch": 7.388111403630317,
      "grad_norm": 15.864120483398438,
      "learning_rate": 1.046085631148677e-05,
      "loss": 2.095,
      "step": 53320
    },
    {
      "epoch": 7.389497020922821,
      "grad_norm": 19.815479278564453,
      "learning_rate": 1.0455313842316753e-05,
      "loss": 1.8269,
      "step": 53330
    },
    {
      "epoch": 7.3908826382153245,
      "grad_norm": 19.27256202697754,
      "learning_rate": 1.0449771373146738e-05,
      "loss": 2.1626,
      "step": 53340
    },
    {
      "epoch": 7.392268255507829,
      "grad_norm": 12.143044471740723,
      "learning_rate": 1.0444228903976723e-05,
      "loss": 1.633,
      "step": 53350
    },
    {
      "epoch": 7.393653872800333,
      "grad_norm": 24.0537052154541,
      "learning_rate": 1.0438686434806707e-05,
      "loss": 1.7353,
      "step": 53360
    },
    {
      "epoch": 7.395039490092836,
      "grad_norm": 14.337175369262695,
      "learning_rate": 1.0433143965636692e-05,
      "loss": 2.1438,
      "step": 53370
    },
    {
      "epoch": 7.39642510738534,
      "grad_norm": 12.209989547729492,
      "learning_rate": 1.0427601496466677e-05,
      "loss": 1.8618,
      "step": 53380
    },
    {
      "epoch": 7.397810724677844,
      "grad_norm": 15.472552299499512,
      "learning_rate": 1.042205902729666e-05,
      "loss": 1.5692,
      "step": 53390
    },
    {
      "epoch": 7.399196341970348,
      "grad_norm": 7.366177082061768,
      "learning_rate": 1.0416516558126646e-05,
      "loss": 1.4606,
      "step": 53400
    },
    {
      "epoch": 7.400581959262851,
      "grad_norm": 7.02461051940918,
      "learning_rate": 1.0410974088956631e-05,
      "loss": 1.8857,
      "step": 53410
    },
    {
      "epoch": 7.401967576555355,
      "grad_norm": 20.679988861083984,
      "learning_rate": 1.0405431619786617e-05,
      "loss": 1.8726,
      "step": 53420
    },
    {
      "epoch": 7.40335319384786,
      "grad_norm": 13.344878196716309,
      "learning_rate": 1.03998891506166e-05,
      "loss": 2.2519,
      "step": 53430
    },
    {
      "epoch": 7.404738811140363,
      "grad_norm": 21.477590560913086,
      "learning_rate": 1.0394346681446585e-05,
      "loss": 1.8464,
      "step": 53440
    },
    {
      "epoch": 7.406124428432867,
      "grad_norm": 11.78748607635498,
      "learning_rate": 1.038880421227657e-05,
      "loss": 1.4205,
      "step": 53450
    },
    {
      "epoch": 7.40751004572537,
      "grad_norm": 17.22431755065918,
      "learning_rate": 1.0383261743106554e-05,
      "loss": 2.0115,
      "step": 53460
    },
    {
      "epoch": 7.408895663017875,
      "grad_norm": 10.056737899780273,
      "learning_rate": 1.037771927393654e-05,
      "loss": 1.9003,
      "step": 53470
    },
    {
      "epoch": 7.410281280310378,
      "grad_norm": 12.995378494262695,
      "learning_rate": 1.0372176804766525e-05,
      "loss": 1.7111,
      "step": 53480
    },
    {
      "epoch": 7.411666897602882,
      "grad_norm": 11.833453178405762,
      "learning_rate": 1.0366634335596508e-05,
      "loss": 1.7049,
      "step": 53490
    },
    {
      "epoch": 7.413052514895386,
      "grad_norm": 11.681926727294922,
      "learning_rate": 1.0361091866426494e-05,
      "loss": 2.3593,
      "step": 53500
    },
    {
      "epoch": 7.41443813218789,
      "grad_norm": 8.507880210876465,
      "learning_rate": 1.0355549397256479e-05,
      "loss": 1.6815,
      "step": 53510
    },
    {
      "epoch": 7.415823749480394,
      "grad_norm": 10.80126667022705,
      "learning_rate": 1.0350006928086462e-05,
      "loss": 1.918,
      "step": 53520
    },
    {
      "epoch": 7.417209366772897,
      "grad_norm": 15.653180122375488,
      "learning_rate": 1.0344464458916448e-05,
      "loss": 1.7998,
      "step": 53530
    },
    {
      "epoch": 7.418594984065401,
      "grad_norm": 9.685221672058105,
      "learning_rate": 1.0338921989746433e-05,
      "loss": 2.3456,
      "step": 53540
    },
    {
      "epoch": 7.419980601357905,
      "grad_norm": 10.966548919677734,
      "learning_rate": 1.0333379520576418e-05,
      "loss": 1.8991,
      "step": 53550
    },
    {
      "epoch": 7.421366218650409,
      "grad_norm": 21.875837326049805,
      "learning_rate": 1.0327837051406402e-05,
      "loss": 1.6641,
      "step": 53560
    },
    {
      "epoch": 7.422751835942913,
      "grad_norm": 8.114571571350098,
      "learning_rate": 1.0322294582236387e-05,
      "loss": 1.8281,
      "step": 53570
    },
    {
      "epoch": 7.424137453235416,
      "grad_norm": 10.34603500366211,
      "learning_rate": 1.0316752113066372e-05,
      "loss": 1.9829,
      "step": 53580
    },
    {
      "epoch": 7.4255230705279205,
      "grad_norm": 16.340822219848633,
      "learning_rate": 1.0311209643896356e-05,
      "loss": 2.3608,
      "step": 53590
    },
    {
      "epoch": 7.426908687820424,
      "grad_norm": 12.017361640930176,
      "learning_rate": 1.0305667174726341e-05,
      "loss": 1.929,
      "step": 53600
    },
    {
      "epoch": 7.428294305112928,
      "grad_norm": 23.202512741088867,
      "learning_rate": 1.0300124705556326e-05,
      "loss": 1.9758,
      "step": 53610
    },
    {
      "epoch": 7.429679922405431,
      "grad_norm": 19.500062942504883,
      "learning_rate": 1.029458223638631e-05,
      "loss": 2.0715,
      "step": 53620
    },
    {
      "epoch": 7.4310655396979355,
      "grad_norm": 13.762869834899902,
      "learning_rate": 1.0289039767216295e-05,
      "loss": 2.2018,
      "step": 53630
    },
    {
      "epoch": 7.432451156990439,
      "grad_norm": 13.844375610351562,
      "learning_rate": 1.028349729804628e-05,
      "loss": 1.7997,
      "step": 53640
    },
    {
      "epoch": 7.433836774282943,
      "grad_norm": 24.944799423217773,
      "learning_rate": 1.0277954828876264e-05,
      "loss": 2.0622,
      "step": 53650
    },
    {
      "epoch": 7.435222391575447,
      "grad_norm": 18.3402099609375,
      "learning_rate": 1.0272412359706249e-05,
      "loss": 1.9583,
      "step": 53660
    },
    {
      "epoch": 7.4366080088679505,
      "grad_norm": 21.274991989135742,
      "learning_rate": 1.0266869890536236e-05,
      "loss": 1.8844,
      "step": 53670
    },
    {
      "epoch": 7.437993626160455,
      "grad_norm": 15.871596336364746,
      "learning_rate": 1.0261327421366218e-05,
      "loss": 1.749,
      "step": 53680
    },
    {
      "epoch": 7.439379243452958,
      "grad_norm": 17.18325424194336,
      "learning_rate": 1.0255784952196203e-05,
      "loss": 1.4842,
      "step": 53690
    },
    {
      "epoch": 7.440764860745462,
      "grad_norm": 11.275157928466797,
      "learning_rate": 1.025024248302619e-05,
      "loss": 2.1024,
      "step": 53700
    },
    {
      "epoch": 7.442150478037966,
      "grad_norm": 16.517372131347656,
      "learning_rate": 1.0244700013856175e-05,
      "loss": 2.152,
      "step": 53710
    },
    {
      "epoch": 7.44353609533047,
      "grad_norm": 14.167878150939941,
      "learning_rate": 1.0239157544686157e-05,
      "loss": 1.7086,
      "step": 53720
    },
    {
      "epoch": 7.444921712622974,
      "grad_norm": 16.93964195251465,
      "learning_rate": 1.0233615075516144e-05,
      "loss": 1.8171,
      "step": 53730
    },
    {
      "epoch": 7.446307329915477,
      "grad_norm": 24.0177059173584,
      "learning_rate": 1.022807260634613e-05,
      "loss": 1.7912,
      "step": 53740
    },
    {
      "epoch": 7.447692947207981,
      "grad_norm": 16.19133949279785,
      "learning_rate": 1.0222530137176113e-05,
      "loss": 1.4114,
      "step": 53750
    },
    {
      "epoch": 7.449078564500485,
      "grad_norm": 12.298781394958496,
      "learning_rate": 1.0216987668006098e-05,
      "loss": 1.5989,
      "step": 53760
    },
    {
      "epoch": 7.450464181792989,
      "grad_norm": 7.7374773025512695,
      "learning_rate": 1.0211445198836083e-05,
      "loss": 1.4758,
      "step": 53770
    },
    {
      "epoch": 7.451849799085492,
      "grad_norm": 6.981048107147217,
      "learning_rate": 1.0205902729666067e-05,
      "loss": 1.9481,
      "step": 53780
    },
    {
      "epoch": 7.453235416377996,
      "grad_norm": 19.22743797302246,
      "learning_rate": 1.0200360260496052e-05,
      "loss": 1.8964,
      "step": 53790
    },
    {
      "epoch": 7.4546210336705006,
      "grad_norm": 11.957779884338379,
      "learning_rate": 1.0194817791326037e-05,
      "loss": 2.0365,
      "step": 53800
    },
    {
      "epoch": 7.456006650963004,
      "grad_norm": 13.110053062438965,
      "learning_rate": 1.0189275322156021e-05,
      "loss": 1.9628,
      "step": 53810
    },
    {
      "epoch": 7.457392268255508,
      "grad_norm": 9.945098876953125,
      "learning_rate": 1.0183732852986006e-05,
      "loss": 1.7937,
      "step": 53820
    },
    {
      "epoch": 7.458777885548011,
      "grad_norm": 9.419065475463867,
      "learning_rate": 1.0178190383815991e-05,
      "loss": 1.5686,
      "step": 53830
    },
    {
      "epoch": 7.4601635028405155,
      "grad_norm": 12.780505180358887,
      "learning_rate": 1.0172647914645977e-05,
      "loss": 1.8048,
      "step": 53840
    },
    {
      "epoch": 7.461549120133019,
      "grad_norm": 12.078471183776855,
      "learning_rate": 1.016710544547596e-05,
      "loss": 1.9647,
      "step": 53850
    },
    {
      "epoch": 7.462934737425523,
      "grad_norm": 9.706116676330566,
      "learning_rate": 1.0161562976305946e-05,
      "loss": 1.9087,
      "step": 53860
    },
    {
      "epoch": 7.464320354718027,
      "grad_norm": 16.551517486572266,
      "learning_rate": 1.015602050713593e-05,
      "loss": 2.0322,
      "step": 53870
    },
    {
      "epoch": 7.4657059720105305,
      "grad_norm": 15.27335262298584,
      "learning_rate": 1.0150478037965914e-05,
      "loss": 1.6721,
      "step": 53880
    },
    {
      "epoch": 7.467091589303035,
      "grad_norm": 11.304088592529297,
      "learning_rate": 1.01449355687959e-05,
      "loss": 1.7025,
      "step": 53890
    },
    {
      "epoch": 7.468477206595538,
      "grad_norm": 15.81507396697998,
      "learning_rate": 1.0139393099625885e-05,
      "loss": 1.7213,
      "step": 53900
    },
    {
      "epoch": 7.469862823888042,
      "grad_norm": 9.160017013549805,
      "learning_rate": 1.0133850630455868e-05,
      "loss": 1.8807,
      "step": 53910
    },
    {
      "epoch": 7.4712484411805455,
      "grad_norm": 10.685203552246094,
      "learning_rate": 1.0128308161285854e-05,
      "loss": 1.9565,
      "step": 53920
    },
    {
      "epoch": 7.47263405847305,
      "grad_norm": 12.571722030639648,
      "learning_rate": 1.0122765692115839e-05,
      "loss": 1.7535,
      "step": 53930
    },
    {
      "epoch": 7.474019675765554,
      "grad_norm": 17.420095443725586,
      "learning_rate": 1.0117223222945822e-05,
      "loss": 2.4324,
      "step": 53940
    },
    {
      "epoch": 7.475405293058057,
      "grad_norm": 11.12160587310791,
      "learning_rate": 1.0111680753775808e-05,
      "loss": 1.918,
      "step": 53950
    },
    {
      "epoch": 7.476790910350561,
      "grad_norm": 14.096295356750488,
      "learning_rate": 1.0106138284605793e-05,
      "loss": 1.8871,
      "step": 53960
    },
    {
      "epoch": 7.478176527643065,
      "grad_norm": 19.94080924987793,
      "learning_rate": 1.0100595815435777e-05,
      "loss": 1.8656,
      "step": 53970
    },
    {
      "epoch": 7.479562144935569,
      "grad_norm": 17.657495498657227,
      "learning_rate": 1.0095053346265762e-05,
      "loss": 1.8998,
      "step": 53980
    },
    {
      "epoch": 7.480947762228072,
      "grad_norm": 20.65123176574707,
      "learning_rate": 1.0089510877095747e-05,
      "loss": 1.9793,
      "step": 53990
    },
    {
      "epoch": 7.482333379520576,
      "grad_norm": 9.705218315124512,
      "learning_rate": 1.0083968407925732e-05,
      "loss": 1.7552,
      "step": 54000
    },
    {
      "epoch": 7.483718996813081,
      "grad_norm": 8.031852722167969,
      "learning_rate": 1.0078425938755716e-05,
      "loss": 1.8631,
      "step": 54010
    },
    {
      "epoch": 7.485104614105584,
      "grad_norm": 12.882072448730469,
      "learning_rate": 1.0072883469585701e-05,
      "loss": 1.7609,
      "step": 54020
    },
    {
      "epoch": 7.486490231398088,
      "grad_norm": 11.793965339660645,
      "learning_rate": 1.0067341000415686e-05,
      "loss": 1.7251,
      "step": 54030
    },
    {
      "epoch": 7.487875848690591,
      "grad_norm": 14.48499870300293,
      "learning_rate": 1.006179853124567e-05,
      "loss": 1.5076,
      "step": 54040
    },
    {
      "epoch": 7.489261465983096,
      "grad_norm": 19.141895294189453,
      "learning_rate": 1.0056256062075655e-05,
      "loss": 1.8374,
      "step": 54050
    },
    {
      "epoch": 7.490647083275599,
      "grad_norm": 13.21143913269043,
      "learning_rate": 1.005071359290564e-05,
      "loss": 2.2663,
      "step": 54060
    },
    {
      "epoch": 7.492032700568103,
      "grad_norm": 16.365501403808594,
      "learning_rate": 1.0045171123735624e-05,
      "loss": 2.2181,
      "step": 54070
    },
    {
      "epoch": 7.493418317860607,
      "grad_norm": 13.155804634094238,
      "learning_rate": 1.003962865456561e-05,
      "loss": 1.4757,
      "step": 54080
    },
    {
      "epoch": 7.494803935153111,
      "grad_norm": 15.609814643859863,
      "learning_rate": 1.0034086185395594e-05,
      "loss": 2.0251,
      "step": 54090
    },
    {
      "epoch": 7.496189552445615,
      "grad_norm": 14.449393272399902,
      "learning_rate": 1.0028543716225578e-05,
      "loss": 1.8593,
      "step": 54100
    },
    {
      "epoch": 7.497575169738118,
      "grad_norm": 21.058279037475586,
      "learning_rate": 1.0023001247055563e-05,
      "loss": 1.8198,
      "step": 54110
    },
    {
      "epoch": 7.498960787030622,
      "grad_norm": 12.223337173461914,
      "learning_rate": 1.0017458777885548e-05,
      "loss": 1.7994,
      "step": 54120
    },
    {
      "epoch": 7.500346404323126,
      "grad_norm": 15.139548301696777,
      "learning_rate": 1.0011916308715535e-05,
      "loss": 2.1926,
      "step": 54130
    },
    {
      "epoch": 7.50173202161563,
      "grad_norm": 16.93840789794922,
      "learning_rate": 1.0006373839545517e-05,
      "loss": 1.9848,
      "step": 54140
    },
    {
      "epoch": 7.503117638908133,
      "grad_norm": 10.187240600585938,
      "learning_rate": 1.0000831370375503e-05,
      "loss": 2.0988,
      "step": 54150
    },
    {
      "epoch": 7.504503256200637,
      "grad_norm": 9.911056518554688,
      "learning_rate": 9.995288901205488e-06,
      "loss": 1.5753,
      "step": 54160
    },
    {
      "epoch": 7.5058888734931415,
      "grad_norm": 28.602413177490234,
      "learning_rate": 9.989746432035473e-06,
      "loss": 2.1119,
      "step": 54170
    },
    {
      "epoch": 7.507274490785645,
      "grad_norm": 10.614479064941406,
      "learning_rate": 9.984203962865458e-06,
      "loss": 2.0066,
      "step": 54180
    },
    {
      "epoch": 7.508660108078149,
      "grad_norm": 19.92827606201172,
      "learning_rate": 9.978661493695442e-06,
      "loss": 1.8539,
      "step": 54190
    },
    {
      "epoch": 7.510045725370652,
      "grad_norm": 15.64614200592041,
      "learning_rate": 9.973119024525427e-06,
      "loss": 1.399,
      "step": 54200
    },
    {
      "epoch": 7.5114313426631565,
      "grad_norm": 23.470680236816406,
      "learning_rate": 9.967576555355412e-06,
      "loss": 2.2364,
      "step": 54210
    },
    {
      "epoch": 7.512816959955661,
      "grad_norm": 12.702422142028809,
      "learning_rate": 9.962034086185396e-06,
      "loss": 1.831,
      "step": 54220
    },
    {
      "epoch": 7.514202577248164,
      "grad_norm": 13.835948944091797,
      "learning_rate": 9.956491617015381e-06,
      "loss": 1.8207,
      "step": 54230
    },
    {
      "epoch": 7.515588194540668,
      "grad_norm": 13.155832290649414,
      "learning_rate": 9.950949147845366e-06,
      "loss": 1.5155,
      "step": 54240
    },
    {
      "epoch": 7.5169738118331715,
      "grad_norm": 19.99155044555664,
      "learning_rate": 9.945406678675352e-06,
      "loss": 2.2505,
      "step": 54250
    },
    {
      "epoch": 7.518359429125676,
      "grad_norm": 28.22969627380371,
      "learning_rate": 9.939864209505335e-06,
      "loss": 2.2075,
      "step": 54260
    },
    {
      "epoch": 7.519745046418179,
      "grad_norm": 8.763991355895996,
      "learning_rate": 9.93432174033532e-06,
      "loss": 1.8559,
      "step": 54270
    },
    {
      "epoch": 7.521130663710683,
      "grad_norm": 21.948713302612305,
      "learning_rate": 9.928779271165306e-06,
      "loss": 2.1235,
      "step": 54280
    },
    {
      "epoch": 7.5225162810031865,
      "grad_norm": 11.374062538146973,
      "learning_rate": 9.92323680199529e-06,
      "loss": 1.4724,
      "step": 54290
    },
    {
      "epoch": 7.523901898295691,
      "grad_norm": 16.859378814697266,
      "learning_rate": 9.917694332825274e-06,
      "loss": 1.6889,
      "step": 54300
    },
    {
      "epoch": 7.525287515588195,
      "grad_norm": 13.775287628173828,
      "learning_rate": 9.91215186365526e-06,
      "loss": 1.8445,
      "step": 54310
    },
    {
      "epoch": 7.526673132880698,
      "grad_norm": 12.304252624511719,
      "learning_rate": 9.906609394485243e-06,
      "loss": 1.9948,
      "step": 54320
    },
    {
      "epoch": 7.528058750173202,
      "grad_norm": 11.768036842346191,
      "learning_rate": 9.901066925315229e-06,
      "loss": 1.5823,
      "step": 54330
    },
    {
      "epoch": 7.529444367465706,
      "grad_norm": 11.902985572814941,
      "learning_rate": 9.895524456145214e-06,
      "loss": 1.8283,
      "step": 54340
    },
    {
      "epoch": 7.53082998475821,
      "grad_norm": 14.113903999328613,
      "learning_rate": 9.889981986975197e-06,
      "loss": 1.8198,
      "step": 54350
    },
    {
      "epoch": 7.532215602050714,
      "grad_norm": 7.054471492767334,
      "learning_rate": 9.884439517805183e-06,
      "loss": 1.4955,
      "step": 54360
    },
    {
      "epoch": 7.533601219343217,
      "grad_norm": 12.529129981994629,
      "learning_rate": 9.878897048635168e-06,
      "loss": 1.6874,
      "step": 54370
    },
    {
      "epoch": 7.5349868366357216,
      "grad_norm": 7.028296947479248,
      "learning_rate": 9.873354579465153e-06,
      "loss": 1.5878,
      "step": 54380
    },
    {
      "epoch": 7.536372453928225,
      "grad_norm": 21.383832931518555,
      "learning_rate": 9.867812110295137e-06,
      "loss": 1.9574,
      "step": 54390
    },
    {
      "epoch": 7.537758071220729,
      "grad_norm": 23.990154266357422,
      "learning_rate": 9.862269641125122e-06,
      "loss": 2.0348,
      "step": 54400
    },
    {
      "epoch": 7.539143688513232,
      "grad_norm": 18.60575294494629,
      "learning_rate": 9.856727171955107e-06,
      "loss": 1.8882,
      "step": 54410
    },
    {
      "epoch": 7.5405293058057365,
      "grad_norm": 12.749295234680176,
      "learning_rate": 9.85118470278509e-06,
      "loss": 1.8632,
      "step": 54420
    },
    {
      "epoch": 7.54191492309824,
      "grad_norm": 18.667236328125,
      "learning_rate": 9.845642233615076e-06,
      "loss": 1.6524,
      "step": 54430
    },
    {
      "epoch": 7.543300540390744,
      "grad_norm": 18.376174926757812,
      "learning_rate": 9.840099764445061e-06,
      "loss": 2.0548,
      "step": 54440
    },
    {
      "epoch": 7.544686157683248,
      "grad_norm": 19.512165069580078,
      "learning_rate": 9.834557295275046e-06,
      "loss": 1.3874,
      "step": 54450
    },
    {
      "epoch": 7.5460717749757515,
      "grad_norm": 19.256032943725586,
      "learning_rate": 9.829014826105032e-06,
      "loss": 1.3723,
      "step": 54460
    },
    {
      "epoch": 7.547457392268256,
      "grad_norm": 12.808754920959473,
      "learning_rate": 9.823472356935015e-06,
      "loss": 2.3953,
      "step": 54470
    },
    {
      "epoch": 7.548843009560759,
      "grad_norm": 25.13616180419922,
      "learning_rate": 9.817929887765e-06,
      "loss": 1.8081,
      "step": 54480
    },
    {
      "epoch": 7.550228626853263,
      "grad_norm": 18.218830108642578,
      "learning_rate": 9.812387418594986e-06,
      "loss": 2.1257,
      "step": 54490
    },
    {
      "epoch": 7.551614244145767,
      "grad_norm": 13.676544189453125,
      "learning_rate": 9.80684494942497e-06,
      "loss": 1.5717,
      "step": 54500
    },
    {
      "epoch": 7.552999861438271,
      "grad_norm": 10.982564926147461,
      "learning_rate": 9.801302480254955e-06,
      "loss": 1.7703,
      "step": 54510
    },
    {
      "epoch": 7.554385478730775,
      "grad_norm": 13.802417755126953,
      "learning_rate": 9.79576001108494e-06,
      "loss": 1.6954,
      "step": 54520
    },
    {
      "epoch": 7.555771096023278,
      "grad_norm": 17.53096580505371,
      "learning_rate": 9.790217541914923e-06,
      "loss": 1.6132,
      "step": 54530
    },
    {
      "epoch": 7.557156713315782,
      "grad_norm": 10.74460506439209,
      "learning_rate": 9.784675072744909e-06,
      "loss": 1.7792,
      "step": 54540
    },
    {
      "epoch": 7.558542330608286,
      "grad_norm": 8.466741561889648,
      "learning_rate": 9.779132603574894e-06,
      "loss": 1.8967,
      "step": 54550
    },
    {
      "epoch": 7.55992794790079,
      "grad_norm": 11.79660701751709,
      "learning_rate": 9.773590134404877e-06,
      "loss": 1.4036,
      "step": 54560
    },
    {
      "epoch": 7.561313565193293,
      "grad_norm": 14.203688621520996,
      "learning_rate": 9.768047665234863e-06,
      "loss": 2.084,
      "step": 54570
    },
    {
      "epoch": 7.562699182485797,
      "grad_norm": 13.87195873260498,
      "learning_rate": 9.762505196064848e-06,
      "loss": 1.901,
      "step": 54580
    },
    {
      "epoch": 7.564084799778302,
      "grad_norm": 18.36532211303711,
      "learning_rate": 9.756962726894831e-06,
      "loss": 1.9317,
      "step": 54590
    },
    {
      "epoch": 7.565470417070805,
      "grad_norm": 13.271316528320312,
      "learning_rate": 9.751420257724817e-06,
      "loss": 2.0231,
      "step": 54600
    },
    {
      "epoch": 7.566856034363309,
      "grad_norm": 9.683751106262207,
      "learning_rate": 9.745877788554802e-06,
      "loss": 1.8701,
      "step": 54610
    },
    {
      "epoch": 7.568241651655812,
      "grad_norm": 17.540401458740234,
      "learning_rate": 9.740335319384787e-06,
      "loss": 1.9172,
      "step": 54620
    },
    {
      "epoch": 7.569627268948317,
      "grad_norm": 11.048032760620117,
      "learning_rate": 9.73479285021477e-06,
      "loss": 2.0079,
      "step": 54630
    },
    {
      "epoch": 7.57101288624082,
      "grad_norm": 25.73866081237793,
      "learning_rate": 9.729250381044756e-06,
      "loss": 1.2965,
      "step": 54640
    },
    {
      "epoch": 7.572398503533324,
      "grad_norm": 22.44935417175293,
      "learning_rate": 9.723707911874741e-06,
      "loss": 2.1939,
      "step": 54650
    },
    {
      "epoch": 7.573784120825827,
      "grad_norm": 14.799971580505371,
      "learning_rate": 9.718165442704725e-06,
      "loss": 1.5669,
      "step": 54660
    },
    {
      "epoch": 7.575169738118332,
      "grad_norm": 10.135936737060547,
      "learning_rate": 9.71262297353471e-06,
      "loss": 1.8176,
      "step": 54670
    },
    {
      "epoch": 7.576555355410836,
      "grad_norm": 7.597978591918945,
      "learning_rate": 9.707080504364695e-06,
      "loss": 1.7644,
      "step": 54680
    },
    {
      "epoch": 7.577940972703339,
      "grad_norm": 10.691000938415527,
      "learning_rate": 9.70153803519468e-06,
      "loss": 1.6424,
      "step": 54690
    },
    {
      "epoch": 7.579326589995843,
      "grad_norm": 21.577035903930664,
      "learning_rate": 9.695995566024666e-06,
      "loss": 1.9819,
      "step": 54700
    },
    {
      "epoch": 7.580712207288347,
      "grad_norm": 10.055460929870605,
      "learning_rate": 9.69045309685465e-06,
      "loss": 2.1327,
      "step": 54710
    },
    {
      "epoch": 7.582097824580851,
      "grad_norm": 23.63839340209961,
      "learning_rate": 9.684910627684635e-06,
      "loss": 1.6187,
      "step": 54720
    },
    {
      "epoch": 7.583483441873355,
      "grad_norm": 18.484128952026367,
      "learning_rate": 9.67936815851462e-06,
      "loss": 1.744,
      "step": 54730
    },
    {
      "epoch": 7.584869059165858,
      "grad_norm": 10.470916748046875,
      "learning_rate": 9.673825689344603e-06,
      "loss": 1.743,
      "step": 54740
    },
    {
      "epoch": 7.5862546764583625,
      "grad_norm": 10.371249198913574,
      "learning_rate": 9.668283220174589e-06,
      "loss": 1.8378,
      "step": 54750
    },
    {
      "epoch": 7.587640293750866,
      "grad_norm": 13.785185813903809,
      "learning_rate": 9.662740751004574e-06,
      "loss": 1.816,
      "step": 54760
    },
    {
      "epoch": 7.58902591104337,
      "grad_norm": 19.292171478271484,
      "learning_rate": 9.657198281834557e-06,
      "loss": 2.1139,
      "step": 54770
    },
    {
      "epoch": 7.590411528335873,
      "grad_norm": 10.612712860107422,
      "learning_rate": 9.651655812664543e-06,
      "loss": 1.6304,
      "step": 54780
    },
    {
      "epoch": 7.5917971456283775,
      "grad_norm": 10.525689125061035,
      "learning_rate": 9.646113343494528e-06,
      "loss": 1.6475,
      "step": 54790
    },
    {
      "epoch": 7.593182762920881,
      "grad_norm": 9.748112678527832,
      "learning_rate": 9.640570874324512e-06,
      "loss": 1.8232,
      "step": 54800
    },
    {
      "epoch": 7.594568380213385,
      "grad_norm": 16.194915771484375,
      "learning_rate": 9.635028405154497e-06,
      "loss": 1.989,
      "step": 54810
    },
    {
      "epoch": 7.595953997505889,
      "grad_norm": 14.801027297973633,
      "learning_rate": 9.629485935984482e-06,
      "loss": 1.69,
      "step": 54820
    },
    {
      "epoch": 7.5973396147983925,
      "grad_norm": 13.178948402404785,
      "learning_rate": 9.623943466814467e-06,
      "loss": 1.4871,
      "step": 54830
    },
    {
      "epoch": 7.598725232090897,
      "grad_norm": 11.822851181030273,
      "learning_rate": 9.61840099764445e-06,
      "loss": 1.9954,
      "step": 54840
    },
    {
      "epoch": 7.6001108493834,
      "grad_norm": 10.023974418640137,
      "learning_rate": 9.612858528474436e-06,
      "loss": 2.1074,
      "step": 54850
    },
    {
      "epoch": 7.601496466675904,
      "grad_norm": 18.51883316040039,
      "learning_rate": 9.607316059304421e-06,
      "loss": 2.1268,
      "step": 54860
    },
    {
      "epoch": 7.602882083968408,
      "grad_norm": 9.315447807312012,
      "learning_rate": 9.601773590134405e-06,
      "loss": 2.0502,
      "step": 54870
    },
    {
      "epoch": 7.604267701260912,
      "grad_norm": 14.599422454833984,
      "learning_rate": 9.59623112096439e-06,
      "loss": 1.6262,
      "step": 54880
    },
    {
      "epoch": 7.605653318553416,
      "grad_norm": 11.95676326751709,
      "learning_rate": 9.590688651794375e-06,
      "loss": 1.8776,
      "step": 54890
    },
    {
      "epoch": 7.607038935845919,
      "grad_norm": 21.61780548095703,
      "learning_rate": 9.585146182624359e-06,
      "loss": 1.8804,
      "step": 54900
    },
    {
      "epoch": 7.608424553138423,
      "grad_norm": 30.439664840698242,
      "learning_rate": 9.579603713454346e-06,
      "loss": 2.1543,
      "step": 54910
    },
    {
      "epoch": 7.609810170430927,
      "grad_norm": 17.214618682861328,
      "learning_rate": 9.57406124428433e-06,
      "loss": 1.9067,
      "step": 54920
    },
    {
      "epoch": 7.611195787723431,
      "grad_norm": 15.880331993103027,
      "learning_rate": 9.568518775114313e-06,
      "loss": 1.6006,
      "step": 54930
    },
    {
      "epoch": 7.612581405015934,
      "grad_norm": 16.02093505859375,
      "learning_rate": 9.5629763059443e-06,
      "loss": 2.2844,
      "step": 54940
    },
    {
      "epoch": 7.613967022308438,
      "grad_norm": 15.700470924377441,
      "learning_rate": 9.557433836774284e-06,
      "loss": 1.3788,
      "step": 54950
    },
    {
      "epoch": 7.6153526396009426,
      "grad_norm": 12.864236831665039,
      "learning_rate": 9.55244561452127e-06,
      "loss": 1.822,
      "step": 54960
    },
    {
      "epoch": 7.616738256893446,
      "grad_norm": 17.75296401977539,
      "learning_rate": 9.546903145351255e-06,
      "loss": 1.8647,
      "step": 54970
    },
    {
      "epoch": 7.61812387418595,
      "grad_norm": 27.816097259521484,
      "learning_rate": 9.541360676181239e-06,
      "loss": 1.8232,
      "step": 54980
    },
    {
      "epoch": 7.619509491478453,
      "grad_norm": 11.193167686462402,
      "learning_rate": 9.535818207011224e-06,
      "loss": 1.5712,
      "step": 54990
    },
    {
      "epoch": 7.6208951087709575,
      "grad_norm": 10.971802711486816,
      "learning_rate": 9.53027573784121e-06,
      "loss": 1.348,
      "step": 55000
    },
    {
      "epoch": 7.622280726063462,
      "grad_norm": 11.038824081420898,
      "learning_rate": 9.524733268671193e-06,
      "loss": 1.9406,
      "step": 55010
    },
    {
      "epoch": 7.623666343355965,
      "grad_norm": 8.651226997375488,
      "learning_rate": 9.51919079950118e-06,
      "loss": 2.1728,
      "step": 55020
    },
    {
      "epoch": 7.625051960648469,
      "grad_norm": 15.900270462036133,
      "learning_rate": 9.513648330331163e-06,
      "loss": 1.7353,
      "step": 55030
    },
    {
      "epoch": 7.6264375779409725,
      "grad_norm": 15.759828567504883,
      "learning_rate": 9.508105861161147e-06,
      "loss": 1.9783,
      "step": 55040
    },
    {
      "epoch": 7.627823195233477,
      "grad_norm": 7.721922874450684,
      "learning_rate": 9.502563391991134e-06,
      "loss": 1.6834,
      "step": 55050
    },
    {
      "epoch": 7.62920881252598,
      "grad_norm": 23.71133041381836,
      "learning_rate": 9.497020922821117e-06,
      "loss": 2.0619,
      "step": 55060
    },
    {
      "epoch": 7.630594429818484,
      "grad_norm": 10.626744270324707,
      "learning_rate": 9.491478453651103e-06,
      "loss": 1.8048,
      "step": 55070
    },
    {
      "epoch": 7.6319800471109875,
      "grad_norm": 8.13493824005127,
      "learning_rate": 9.485935984481088e-06,
      "loss": 1.9408,
      "step": 55080
    },
    {
      "epoch": 7.633365664403492,
      "grad_norm": 13.135516166687012,
      "learning_rate": 9.480393515311071e-06,
      "loss": 2.075,
      "step": 55090
    },
    {
      "epoch": 7.634751281695996,
      "grad_norm": 11.491194725036621,
      "learning_rate": 9.474851046141057e-06,
      "loss": 2.1118,
      "step": 55100
    },
    {
      "epoch": 7.636136898988499,
      "grad_norm": 14.44388198852539,
      "learning_rate": 9.469308576971042e-06,
      "loss": 1.8676,
      "step": 55110
    },
    {
      "epoch": 7.637522516281003,
      "grad_norm": 13.777344703674316,
      "learning_rate": 9.463766107801025e-06,
      "loss": 1.8499,
      "step": 55120
    },
    {
      "epoch": 7.638908133573507,
      "grad_norm": 15.395556449890137,
      "learning_rate": 9.45822363863101e-06,
      "loss": 1.7153,
      "step": 55130
    },
    {
      "epoch": 7.640293750866011,
      "grad_norm": 14.657089233398438,
      "learning_rate": 9.452681169460996e-06,
      "loss": 1.7705,
      "step": 55140
    },
    {
      "epoch": 7.641679368158515,
      "grad_norm": 19.76470947265625,
      "learning_rate": 9.447138700290981e-06,
      "loss": 1.7209,
      "step": 55150
    },
    {
      "epoch": 7.643064985451018,
      "grad_norm": 11.524848937988281,
      "learning_rate": 9.441596231120965e-06,
      "loss": 1.4066,
      "step": 55160
    },
    {
      "epoch": 7.644450602743523,
      "grad_norm": 12.797879219055176,
      "learning_rate": 9.43605376195095e-06,
      "loss": 1.8021,
      "step": 55170
    },
    {
      "epoch": 7.645836220036026,
      "grad_norm": 14.854573249816895,
      "learning_rate": 9.430511292780935e-06,
      "loss": 1.7912,
      "step": 55180
    },
    {
      "epoch": 7.64722183732853,
      "grad_norm": 15.664596557617188,
      "learning_rate": 9.424968823610919e-06,
      "loss": 1.7181,
      "step": 55190
    },
    {
      "epoch": 7.648607454621033,
      "grad_norm": 8.741943359375,
      "learning_rate": 9.419426354440904e-06,
      "loss": 1.9593,
      "step": 55200
    },
    {
      "epoch": 7.649993071913538,
      "grad_norm": 12.120953559875488,
      "learning_rate": 9.41388388527089e-06,
      "loss": 2.0742,
      "step": 55210
    },
    {
      "epoch": 7.651378689206041,
      "grad_norm": 9.641873359680176,
      "learning_rate": 9.408341416100873e-06,
      "loss": 1.8842,
      "step": 55220
    },
    {
      "epoch": 7.652764306498545,
      "grad_norm": 15.060379028320312,
      "learning_rate": 9.40279894693086e-06,
      "loss": 2.3204,
      "step": 55230
    },
    {
      "epoch": 7.654149923791049,
      "grad_norm": 20.37339973449707,
      "learning_rate": 9.397256477760843e-06,
      "loss": 1.8101,
      "step": 55240
    },
    {
      "epoch": 7.655535541083553,
      "grad_norm": 13.260226249694824,
      "learning_rate": 9.391714008590827e-06,
      "loss": 1.7154,
      "step": 55250
    },
    {
      "epoch": 7.656921158376057,
      "grad_norm": 12.67065143585205,
      "learning_rate": 9.386171539420814e-06,
      "loss": 1.6802,
      "step": 55260
    },
    {
      "epoch": 7.65830677566856,
      "grad_norm": 8.474597930908203,
      "learning_rate": 9.380629070250797e-06,
      "loss": 1.5996,
      "step": 55270
    },
    {
      "epoch": 7.659692392961064,
      "grad_norm": 13.730447769165039,
      "learning_rate": 9.375086601080781e-06,
      "loss": 1.6241,
      "step": 55280
    },
    {
      "epoch": 7.661078010253568,
      "grad_norm": 11.791352272033691,
      "learning_rate": 9.369544131910768e-06,
      "loss": 2.3342,
      "step": 55290
    },
    {
      "epoch": 7.662463627546072,
      "grad_norm": 10.59618854522705,
      "learning_rate": 9.364001662740751e-06,
      "loss": 1.7304,
      "step": 55300
    },
    {
      "epoch": 7.663849244838575,
      "grad_norm": 11.019493103027344,
      "learning_rate": 9.358459193570737e-06,
      "loss": 1.9326,
      "step": 55310
    },
    {
      "epoch": 7.665234862131079,
      "grad_norm": 18.409563064575195,
      "learning_rate": 9.352916724400722e-06,
      "loss": 1.4555,
      "step": 55320
    },
    {
      "epoch": 7.6666204794235835,
      "grad_norm": 11.535239219665527,
      "learning_rate": 9.347374255230705e-06,
      "loss": 1.0491,
      "step": 55330
    },
    {
      "epoch": 7.668006096716087,
      "grad_norm": 20.431318283081055,
      "learning_rate": 9.34183178606069e-06,
      "loss": 1.7376,
      "step": 55340
    },
    {
      "epoch": 7.669391714008591,
      "grad_norm": 8.804464340209961,
      "learning_rate": 9.336289316890676e-06,
      "loss": 1.5655,
      "step": 55350
    },
    {
      "epoch": 7.670777331301094,
      "grad_norm": 24.023530960083008,
      "learning_rate": 9.330746847720661e-06,
      "loss": 1.384,
      "step": 55360
    },
    {
      "epoch": 7.6721629485935985,
      "grad_norm": 9.288497924804688,
      "learning_rate": 9.325204378550645e-06,
      "loss": 1.2838,
      "step": 55370
    },
    {
      "epoch": 7.673548565886103,
      "grad_norm": 5.805301666259766,
      "learning_rate": 9.31966190938063e-06,
      "loss": 1.6022,
      "step": 55380
    },
    {
      "epoch": 7.674934183178606,
      "grad_norm": 37.66447830200195,
      "learning_rate": 9.314119440210615e-06,
      "loss": 1.6036,
      "step": 55390
    },
    {
      "epoch": 7.67631980047111,
      "grad_norm": 20.42789649963379,
      "learning_rate": 9.308576971040599e-06,
      "loss": 2.2622,
      "step": 55400
    },
    {
      "epoch": 7.6777054177636135,
      "grad_norm": 20.616649627685547,
      "learning_rate": 9.303034501870584e-06,
      "loss": 2.0925,
      "step": 55410
    },
    {
      "epoch": 7.679091035056118,
      "grad_norm": 7.663896083831787,
      "learning_rate": 9.29749203270057e-06,
      "loss": 1.9194,
      "step": 55420
    },
    {
      "epoch": 7.680476652348621,
      "grad_norm": 7.3585991859436035,
      "learning_rate": 9.291949563530553e-06,
      "loss": 1.5502,
      "step": 55430
    },
    {
      "epoch": 7.681862269641125,
      "grad_norm": 14.934696197509766,
      "learning_rate": 9.286407094360538e-06,
      "loss": 1.4795,
      "step": 55440
    },
    {
      "epoch": 7.6832478869336285,
      "grad_norm": 10.197196006774902,
      "learning_rate": 9.280864625190523e-06,
      "loss": 1.7918,
      "step": 55450
    },
    {
      "epoch": 7.684633504226133,
      "grad_norm": 22.79794692993164,
      "learning_rate": 9.275322156020507e-06,
      "loss": 2.4099,
      "step": 55460
    },
    {
      "epoch": 7.686019121518637,
      "grad_norm": 8.551107406616211,
      "learning_rate": 9.269779686850492e-06,
      "loss": 1.4512,
      "step": 55470
    },
    {
      "epoch": 7.68740473881114,
      "grad_norm": 14.354070663452148,
      "learning_rate": 9.264237217680477e-06,
      "loss": 1.7,
      "step": 55480
    },
    {
      "epoch": 7.688790356103644,
      "grad_norm": 19.08686637878418,
      "learning_rate": 9.258694748510461e-06,
      "loss": 2.0069,
      "step": 55490
    },
    {
      "epoch": 7.690175973396148,
      "grad_norm": 8.756402015686035,
      "learning_rate": 9.253152279340448e-06,
      "loss": 1.6818,
      "step": 55500
    },
    {
      "epoch": 7.691561590688652,
      "grad_norm": 6.32798957824707,
      "learning_rate": 9.247609810170431e-06,
      "loss": 1.6025,
      "step": 55510
    },
    {
      "epoch": 7.692947207981156,
      "grad_norm": 19.7276554107666,
      "learning_rate": 9.242067341000417e-06,
      "loss": 1.5381,
      "step": 55520
    },
    {
      "epoch": 7.694332825273659,
      "grad_norm": 10.693596839904785,
      "learning_rate": 9.236524871830402e-06,
      "loss": 1.9252,
      "step": 55530
    },
    {
      "epoch": 7.6957184425661636,
      "grad_norm": 11.693355560302734,
      "learning_rate": 9.230982402660386e-06,
      "loss": 1.9515,
      "step": 55540
    },
    {
      "epoch": 7.697104059858667,
      "grad_norm": 20.352651596069336,
      "learning_rate": 9.22543993349037e-06,
      "loss": 1.8878,
      "step": 55550
    },
    {
      "epoch": 7.698489677151171,
      "grad_norm": 8.113434791564941,
      "learning_rate": 9.219897464320356e-06,
      "loss": 2.0138,
      "step": 55560
    },
    {
      "epoch": 7.699875294443674,
      "grad_norm": 9.690372467041016,
      "learning_rate": 9.21435499515034e-06,
      "loss": 1.7154,
      "step": 55570
    },
    {
      "epoch": 7.7012609117361785,
      "grad_norm": 16.806608200073242,
      "learning_rate": 9.208812525980325e-06,
      "loss": 1.9975,
      "step": 55580
    },
    {
      "epoch": 7.702646529028682,
      "grad_norm": 12.814949989318848,
      "learning_rate": 9.20327005681031e-06,
      "loss": 1.6456,
      "step": 55590
    },
    {
      "epoch": 7.704032146321186,
      "grad_norm": 9.307059288024902,
      "learning_rate": 9.197727587640295e-06,
      "loss": 1.8766,
      "step": 55600
    },
    {
      "epoch": 7.70541776361369,
      "grad_norm": 12.945798873901367,
      "learning_rate": 9.192185118470279e-06,
      "loss": 2.3124,
      "step": 55610
    },
    {
      "epoch": 7.7068033809061935,
      "grad_norm": 11.782620429992676,
      "learning_rate": 9.186642649300264e-06,
      "loss": 2.1217,
      "step": 55620
    },
    {
      "epoch": 7.708188998198698,
      "grad_norm": 8.131348609924316,
      "learning_rate": 9.18110018013025e-06,
      "loss": 1.5933,
      "step": 55630
    },
    {
      "epoch": 7.709574615491201,
      "grad_norm": 10.706440925598145,
      "learning_rate": 9.175557710960233e-06,
      "loss": 1.8126,
      "step": 55640
    },
    {
      "epoch": 7.710960232783705,
      "grad_norm": 15.973016738891602,
      "learning_rate": 9.170015241790218e-06,
      "loss": 1.5428,
      "step": 55650
    },
    {
      "epoch": 7.712345850076209,
      "grad_norm": 11.231106758117676,
      "learning_rate": 9.164472772620203e-06,
      "loss": 2.0111,
      "step": 55660
    },
    {
      "epoch": 7.713731467368713,
      "grad_norm": 13.462005615234375,
      "learning_rate": 9.158930303450187e-06,
      "loss": 1.7989,
      "step": 55670
    },
    {
      "epoch": 7.715117084661217,
      "grad_norm": 17.75602912902832,
      "learning_rate": 9.153387834280172e-06,
      "loss": 1.5217,
      "step": 55680
    },
    {
      "epoch": 7.71650270195372,
      "grad_norm": 12.355367660522461,
      "learning_rate": 9.147845365110158e-06,
      "loss": 1.9109,
      "step": 55690
    },
    {
      "epoch": 7.717888319246224,
      "grad_norm": 15.5610933303833,
      "learning_rate": 9.142302895940141e-06,
      "loss": 1.6149,
      "step": 55700
    },
    {
      "epoch": 7.719273936538728,
      "grad_norm": 3.8689892292022705,
      "learning_rate": 9.136760426770126e-06,
      "loss": 1.8175,
      "step": 55710
    },
    {
      "epoch": 7.720659553831232,
      "grad_norm": 16.432571411132812,
      "learning_rate": 9.131217957600112e-06,
      "loss": 1.6318,
      "step": 55720
    },
    {
      "epoch": 7.722045171123735,
      "grad_norm": 24.78916358947754,
      "learning_rate": 9.125675488430097e-06,
      "loss": 1.6647,
      "step": 55730
    },
    {
      "epoch": 7.723430788416239,
      "grad_norm": 16.10268211364746,
      "learning_rate": 9.12013301926008e-06,
      "loss": 2.0726,
      "step": 55740
    },
    {
      "epoch": 7.724816405708744,
      "grad_norm": 12.567192077636719,
      "learning_rate": 9.114590550090066e-06,
      "loss": 1.7212,
      "step": 55750
    },
    {
      "epoch": 7.726202023001247,
      "grad_norm": 10.024190902709961,
      "learning_rate": 9.109048080920051e-06,
      "loss": 2.1921,
      "step": 55760
    },
    {
      "epoch": 7.727587640293751,
      "grad_norm": 13.149005889892578,
      "learning_rate": 9.103505611750036e-06,
      "loss": 1.7116,
      "step": 55770
    },
    {
      "epoch": 7.728973257586254,
      "grad_norm": 10.446922302246094,
      "learning_rate": 9.09796314258002e-06,
      "loss": 1.9228,
      "step": 55780
    },
    {
      "epoch": 7.730358874878759,
      "grad_norm": 13.618471145629883,
      "learning_rate": 9.092420673410005e-06,
      "loss": 1.6611,
      "step": 55790
    },
    {
      "epoch": 7.731744492171262,
      "grad_norm": 10.538497924804688,
      "learning_rate": 9.08687820423999e-06,
      "loss": 1.9022,
      "step": 55800
    },
    {
      "epoch": 7.733130109463766,
      "grad_norm": 13.968840599060059,
      "learning_rate": 9.081335735069975e-06,
      "loss": 2.0216,
      "step": 55810
    },
    {
      "epoch": 7.73451572675627,
      "grad_norm": 10.441837310791016,
      "learning_rate": 9.075793265899959e-06,
      "loss": 2.0909,
      "step": 55820
    },
    {
      "epoch": 7.735901344048774,
      "grad_norm": 13.874685287475586,
      "learning_rate": 9.070250796729944e-06,
      "loss": 1.7516,
      "step": 55830
    },
    {
      "epoch": 7.737286961341278,
      "grad_norm": 20.387060165405273,
      "learning_rate": 9.06470832755993e-06,
      "loss": 1.748,
      "step": 55840
    },
    {
      "epoch": 7.738672578633781,
      "grad_norm": 11.233125686645508,
      "learning_rate": 9.059165858389913e-06,
      "loss": 1.5822,
      "step": 55850
    },
    {
      "epoch": 7.740058195926285,
      "grad_norm": 24.311864852905273,
      "learning_rate": 9.053623389219898e-06,
      "loss": 2.0369,
      "step": 55860
    },
    {
      "epoch": 7.741443813218789,
      "grad_norm": 9.603998184204102,
      "learning_rate": 9.048080920049884e-06,
      "loss": 1.6685,
      "step": 55870
    },
    {
      "epoch": 7.742829430511293,
      "grad_norm": 11.476932525634766,
      "learning_rate": 9.042538450879867e-06,
      "loss": 1.6178,
      "step": 55880
    },
    {
      "epoch": 7.744215047803797,
      "grad_norm": 30.286474227905273,
      "learning_rate": 9.036995981709852e-06,
      "loss": 2.1481,
      "step": 55890
    },
    {
      "epoch": 7.7456006650963,
      "grad_norm": 21.926769256591797,
      "learning_rate": 9.031453512539838e-06,
      "loss": 1.5458,
      "step": 55900
    },
    {
      "epoch": 7.7469862823888045,
      "grad_norm": 6.435207366943359,
      "learning_rate": 9.025911043369821e-06,
      "loss": 2.1599,
      "step": 55910
    },
    {
      "epoch": 7.748371899681308,
      "grad_norm": 19.927202224731445,
      "learning_rate": 9.020368574199806e-06,
      "loss": 1.685,
      "step": 55920
    },
    {
      "epoch": 7.749757516973812,
      "grad_norm": 16.527870178222656,
      "learning_rate": 9.014826105029792e-06,
      "loss": 1.545,
      "step": 55930
    },
    {
      "epoch": 7.751143134266315,
      "grad_norm": 7.770015716552734,
      "learning_rate": 9.009283635859775e-06,
      "loss": 1.7467,
      "step": 55940
    },
    {
      "epoch": 7.7525287515588195,
      "grad_norm": 15.052133560180664,
      "learning_rate": 9.00374116668976e-06,
      "loss": 2.0269,
      "step": 55950
    },
    {
      "epoch": 7.753914368851323,
      "grad_norm": 9.33011245727539,
      "learning_rate": 8.998198697519746e-06,
      "loss": 1.7775,
      "step": 55960
    },
    {
      "epoch": 7.755299986143827,
      "grad_norm": 19.732778549194336,
      "learning_rate": 8.992656228349731e-06,
      "loss": 1.5664,
      "step": 55970
    },
    {
      "epoch": 7.756685603436331,
      "grad_norm": 9.45535659790039,
      "learning_rate": 8.987113759179714e-06,
      "loss": 1.6992,
      "step": 55980
    },
    {
      "epoch": 7.7580712207288345,
      "grad_norm": 13.45671272277832,
      "learning_rate": 8.9815712900097e-06,
      "loss": 1.5444,
      "step": 55990
    },
    {
      "epoch": 7.759456838021339,
      "grad_norm": 9.869607925415039,
      "learning_rate": 8.976028820839685e-06,
      "loss": 1.6091,
      "step": 56000
    },
    {
      "epoch": 7.760842455313842,
      "grad_norm": 8.826014518737793,
      "learning_rate": 8.97048635166967e-06,
      "loss": 1.7859,
      "step": 56010
    },
    {
      "epoch": 7.762228072606346,
      "grad_norm": 13.60996150970459,
      "learning_rate": 8.964943882499655e-06,
      "loss": 1.7477,
      "step": 56020
    },
    {
      "epoch": 7.76361368989885,
      "grad_norm": 12.891029357910156,
      "learning_rate": 8.959401413329639e-06,
      "loss": 1.9025,
      "step": 56030
    },
    {
      "epoch": 7.764999307191354,
      "grad_norm": 13.304003715515137,
      "learning_rate": 8.953858944159624e-06,
      "loss": 1.6841,
      "step": 56040
    },
    {
      "epoch": 7.766384924483858,
      "grad_norm": 14.271028518676758,
      "learning_rate": 8.94831647498961e-06,
      "loss": 1.6699,
      "step": 56050
    },
    {
      "epoch": 7.767770541776361,
      "grad_norm": 6.845343589782715,
      "learning_rate": 8.942774005819593e-06,
      "loss": 1.8945,
      "step": 56060
    },
    {
      "epoch": 7.769156159068865,
      "grad_norm": 17.872093200683594,
      "learning_rate": 8.937231536649578e-06,
      "loss": 1.8643,
      "step": 56070
    },
    {
      "epoch": 7.770541776361369,
      "grad_norm": 26.145301818847656,
      "learning_rate": 8.931689067479564e-06,
      "loss": 2.3513,
      "step": 56080
    },
    {
      "epoch": 7.771927393653873,
      "grad_norm": 17.813011169433594,
      "learning_rate": 8.926146598309547e-06,
      "loss": 2.0669,
      "step": 56090
    },
    {
      "epoch": 7.773313010946376,
      "grad_norm": 10.065899848937988,
      "learning_rate": 8.920604129139532e-06,
      "loss": 2.1117,
      "step": 56100
    },
    {
      "epoch": 7.77469862823888,
      "grad_norm": 14.957991600036621,
      "learning_rate": 8.915061659969518e-06,
      "loss": 1.367,
      "step": 56110
    },
    {
      "epoch": 7.7760842455313846,
      "grad_norm": 28.594385147094727,
      "learning_rate": 8.909519190799501e-06,
      "loss": 1.6774,
      "step": 56120
    },
    {
      "epoch": 7.777469862823888,
      "grad_norm": 13.102082252502441,
      "learning_rate": 8.903976721629486e-06,
      "loss": 1.7733,
      "step": 56130
    },
    {
      "epoch": 7.778855480116392,
      "grad_norm": 12.484142303466797,
      "learning_rate": 8.898434252459472e-06,
      "loss": 2.1484,
      "step": 56140
    },
    {
      "epoch": 7.780241097408895,
      "grad_norm": 13.090814590454102,
      "learning_rate": 8.892891783289455e-06,
      "loss": 1.8972,
      "step": 56150
    },
    {
      "epoch": 7.7816267147013995,
      "grad_norm": 12.756227493286133,
      "learning_rate": 8.88734931411944e-06,
      "loss": 1.6281,
      "step": 56160
    },
    {
      "epoch": 7.783012331993904,
      "grad_norm": 15.346816062927246,
      "learning_rate": 8.881806844949426e-06,
      "loss": 1.8632,
      "step": 56170
    },
    {
      "epoch": 7.784397949286407,
      "grad_norm": 12.24859619140625,
      "learning_rate": 8.876264375779411e-06,
      "loss": 2.0393,
      "step": 56180
    },
    {
      "epoch": 7.785783566578911,
      "grad_norm": 11.57082462310791,
      "learning_rate": 8.870721906609395e-06,
      "loss": 1.9777,
      "step": 56190
    },
    {
      "epoch": 7.7871691838714145,
      "grad_norm": 22.464818954467773,
      "learning_rate": 8.86517943743938e-06,
      "loss": 1.7026,
      "step": 56200
    },
    {
      "epoch": 7.788554801163919,
      "grad_norm": 21.943456649780273,
      "learning_rate": 8.859636968269365e-06,
      "loss": 2.1527,
      "step": 56210
    },
    {
      "epoch": 7.789940418456422,
      "grad_norm": 18.968093872070312,
      "learning_rate": 8.854094499099349e-06,
      "loss": 1.792,
      "step": 56220
    },
    {
      "epoch": 7.791326035748926,
      "grad_norm": 12.321073532104492,
      "learning_rate": 8.848552029929334e-06,
      "loss": 2.066,
      "step": 56230
    },
    {
      "epoch": 7.7927116530414295,
      "grad_norm": 12.204239845275879,
      "learning_rate": 8.843009560759319e-06,
      "loss": 1.9033,
      "step": 56240
    },
    {
      "epoch": 7.794097270333934,
      "grad_norm": 11.915163040161133,
      "learning_rate": 8.837467091589303e-06,
      "loss": 1.5841,
      "step": 56250
    },
    {
      "epoch": 7.795482887626438,
      "grad_norm": 10.404224395751953,
      "learning_rate": 8.83192462241929e-06,
      "loss": 1.7998,
      "step": 56260
    },
    {
      "epoch": 7.796868504918941,
      "grad_norm": 13.844762802124023,
      "learning_rate": 8.826382153249273e-06,
      "loss": 1.6586,
      "step": 56270
    },
    {
      "epoch": 7.798254122211445,
      "grad_norm": 15.183650016784668,
      "learning_rate": 8.820839684079258e-06,
      "loss": 2.2291,
      "step": 56280
    },
    {
      "epoch": 7.799639739503949,
      "grad_norm": 26.12037467956543,
      "learning_rate": 8.815297214909244e-06,
      "loss": 1.5904,
      "step": 56290
    },
    {
      "epoch": 7.801025356796453,
      "grad_norm": 13.313135147094727,
      "learning_rate": 8.809754745739227e-06,
      "loss": 2.0237,
      "step": 56300
    },
    {
      "epoch": 7.802410974088957,
      "grad_norm": 17.93354034423828,
      "learning_rate": 8.804212276569212e-06,
      "loss": 2.2144,
      "step": 56310
    },
    {
      "epoch": 7.80379659138146,
      "grad_norm": 15.451197624206543,
      "learning_rate": 8.798669807399198e-06,
      "loss": 2.079,
      "step": 56320
    },
    {
      "epoch": 7.805182208673965,
      "grad_norm": 16.268478393554688,
      "learning_rate": 8.793127338229181e-06,
      "loss": 1.7438,
      "step": 56330
    },
    {
      "epoch": 7.806567825966468,
      "grad_norm": 17.060810089111328,
      "learning_rate": 8.787584869059167e-06,
      "loss": 1.815,
      "step": 56340
    },
    {
      "epoch": 7.807953443258972,
      "grad_norm": 14.372410774230957,
      "learning_rate": 8.782042399889152e-06,
      "loss": 1.5005,
      "step": 56350
    },
    {
      "epoch": 7.809339060551475,
      "grad_norm": 16.26369857788086,
      "learning_rate": 8.776499930719135e-06,
      "loss": 1.6689,
      "step": 56360
    },
    {
      "epoch": 7.81072467784398,
      "grad_norm": 12.632426261901855,
      "learning_rate": 8.77095746154912e-06,
      "loss": 1.7683,
      "step": 56370
    },
    {
      "epoch": 7.812110295136483,
      "grad_norm": 8.978050231933594,
      "learning_rate": 8.765414992379106e-06,
      "loss": 1.9111,
      "step": 56380
    },
    {
      "epoch": 7.813495912428987,
      "grad_norm": 14.516940116882324,
      "learning_rate": 8.759872523209091e-06,
      "loss": 1.873,
      "step": 56390
    },
    {
      "epoch": 7.814881529721491,
      "grad_norm": 11.01163101196289,
      "learning_rate": 8.754330054039075e-06,
      "loss": 1.2891,
      "step": 56400
    },
    {
      "epoch": 7.816267147013995,
      "grad_norm": 13.0259370803833,
      "learning_rate": 8.749341831786061e-06,
      "loss": 1.8912,
      "step": 56410
    },
    {
      "epoch": 7.817652764306499,
      "grad_norm": 15.510885238647461,
      "learning_rate": 8.743799362616046e-06,
      "loss": 1.9891,
      "step": 56420
    },
    {
      "epoch": 7.819038381599002,
      "grad_norm": 9.323614120483398,
      "learning_rate": 8.738256893446032e-06,
      "loss": 2.2177,
      "step": 56430
    },
    {
      "epoch": 7.820423998891506,
      "grad_norm": 15.674873352050781,
      "learning_rate": 8.732714424276015e-06,
      "loss": 1.8548,
      "step": 56440
    },
    {
      "epoch": 7.82180961618401,
      "grad_norm": 6.195551872253418,
      "learning_rate": 8.727171955106e-06,
      "loss": 1.948,
      "step": 56450
    },
    {
      "epoch": 7.823195233476514,
      "grad_norm": 17.62425422668457,
      "learning_rate": 8.721629485935986e-06,
      "loss": 1.6293,
      "step": 56460
    },
    {
      "epoch": 7.824580850769017,
      "grad_norm": 11.284950256347656,
      "learning_rate": 8.716087016765969e-06,
      "loss": 1.603,
      "step": 56470
    },
    {
      "epoch": 7.825966468061521,
      "grad_norm": 11.710408210754395,
      "learning_rate": 8.710544547595954e-06,
      "loss": 2.1315,
      "step": 56480
    },
    {
      "epoch": 7.8273520853540255,
      "grad_norm": 20.552574157714844,
      "learning_rate": 8.70500207842594e-06,
      "loss": 1.9782,
      "step": 56490
    },
    {
      "epoch": 7.828737702646529,
      "grad_norm": 14.145918846130371,
      "learning_rate": 8.699459609255925e-06,
      "loss": 2.0791,
      "step": 56500
    },
    {
      "epoch": 7.830123319939033,
      "grad_norm": 15.923341751098633,
      "learning_rate": 8.693917140085908e-06,
      "loss": 1.7188,
      "step": 56510
    },
    {
      "epoch": 7.831508937231536,
      "grad_norm": 15.943145751953125,
      "learning_rate": 8.688374670915894e-06,
      "loss": 1.7962,
      "step": 56520
    },
    {
      "epoch": 7.8328945545240405,
      "grad_norm": 15.06080436706543,
      "learning_rate": 8.682832201745879e-06,
      "loss": 1.3793,
      "step": 56530
    },
    {
      "epoch": 7.834280171816545,
      "grad_norm": 8.507241249084473,
      "learning_rate": 8.677289732575862e-06,
      "loss": 1.7788,
      "step": 56540
    },
    {
      "epoch": 7.835665789109048,
      "grad_norm": 15.202817916870117,
      "learning_rate": 8.671747263405848e-06,
      "loss": 1.9626,
      "step": 56550
    },
    {
      "epoch": 7.837051406401552,
      "grad_norm": 8.681802749633789,
      "learning_rate": 8.666204794235833e-06,
      "loss": 1.72,
      "step": 56560
    },
    {
      "epoch": 7.8384370236940555,
      "grad_norm": 15.131939888000488,
      "learning_rate": 8.660662325065817e-06,
      "loss": 1.8634,
      "step": 56570
    },
    {
      "epoch": 7.83982264098656,
      "grad_norm": 14.522377014160156,
      "learning_rate": 8.655119855895803e-06,
      "loss": 1.8242,
      "step": 56580
    },
    {
      "epoch": 7.841208258279063,
      "grad_norm": 15.151687622070312,
      "learning_rate": 8.649577386725787e-06,
      "loss": 1.6878,
      "step": 56590
    },
    {
      "epoch": 7.842593875571567,
      "grad_norm": 7.3108930587768555,
      "learning_rate": 8.64403491755577e-06,
      "loss": 1.5721,
      "step": 56600
    },
    {
      "epoch": 7.8439794928640705,
      "grad_norm": 28.41453742980957,
      "learning_rate": 8.638492448385758e-06,
      "loss": 1.8481,
      "step": 56610
    },
    {
      "epoch": 7.845365110156575,
      "grad_norm": 20.72397804260254,
      "learning_rate": 8.632949979215741e-06,
      "loss": 2.0883,
      "step": 56620
    },
    {
      "epoch": 7.846750727449079,
      "grad_norm": 13.973066329956055,
      "learning_rate": 8.627407510045726e-06,
      "loss": 1.6311,
      "step": 56630
    },
    {
      "epoch": 7.848136344741582,
      "grad_norm": 7.948145866394043,
      "learning_rate": 8.621865040875712e-06,
      "loss": 2.0922,
      "step": 56640
    },
    {
      "epoch": 7.849521962034086,
      "grad_norm": 12.626565933227539,
      "learning_rate": 8.616322571705695e-06,
      "loss": 1.7163,
      "step": 56650
    },
    {
      "epoch": 7.85090757932659,
      "grad_norm": 18.186683654785156,
      "learning_rate": 8.61078010253568e-06,
      "loss": 1.9004,
      "step": 56660
    },
    {
      "epoch": 7.852293196619094,
      "grad_norm": 12.94946575164795,
      "learning_rate": 8.605237633365666e-06,
      "loss": 1.9626,
      "step": 56670
    },
    {
      "epoch": 7.853678813911598,
      "grad_norm": 6.632101058959961,
      "learning_rate": 8.59969516419565e-06,
      "loss": 1.8497,
      "step": 56680
    },
    {
      "epoch": 7.855064431204101,
      "grad_norm": 12.19333267211914,
      "learning_rate": 8.594152695025634e-06,
      "loss": 1.843,
      "step": 56690
    },
    {
      "epoch": 7.8564500484966056,
      "grad_norm": 17.983844757080078,
      "learning_rate": 8.58861022585562e-06,
      "loss": 1.8614,
      "step": 56700
    },
    {
      "epoch": 7.857835665789109,
      "grad_norm": 16.714792251586914,
      "learning_rate": 8.583067756685605e-06,
      "loss": 1.5817,
      "step": 56710
    },
    {
      "epoch": 7.859221283081613,
      "grad_norm": 15.046895027160645,
      "learning_rate": 8.577525287515589e-06,
      "loss": 1.7596,
      "step": 56720
    },
    {
      "epoch": 7.860606900374116,
      "grad_norm": 6.698017597198486,
      "learning_rate": 8.571982818345574e-06,
      "loss": 1.8885,
      "step": 56730
    },
    {
      "epoch": 7.8619925176666206,
      "grad_norm": 24.03560447692871,
      "learning_rate": 8.566440349175559e-06,
      "loss": 1.9575,
      "step": 56740
    },
    {
      "epoch": 7.863378134959124,
      "grad_norm": 9.8497953414917,
      "learning_rate": 8.560897880005543e-06,
      "loss": 1.711,
      "step": 56750
    },
    {
      "epoch": 7.864763752251628,
      "grad_norm": 14.572351455688477,
      "learning_rate": 8.555355410835528e-06,
      "loss": 2.0158,
      "step": 56760
    },
    {
      "epoch": 7.866149369544132,
      "grad_norm": 18.506305694580078,
      "learning_rate": 8.549812941665513e-06,
      "loss": 1.6496,
      "step": 56770
    },
    {
      "epoch": 7.8675349868366355,
      "grad_norm": 18.222938537597656,
      "learning_rate": 8.544270472495497e-06,
      "loss": 1.8828,
      "step": 56780
    },
    {
      "epoch": 7.86892060412914,
      "grad_norm": 14.228886604309082,
      "learning_rate": 8.538728003325482e-06,
      "loss": 1.9893,
      "step": 56790
    },
    {
      "epoch": 7.870306221421643,
      "grad_norm": 11.392340660095215,
      "learning_rate": 8.533185534155467e-06,
      "loss": 2.1132,
      "step": 56800
    },
    {
      "epoch": 7.871691838714147,
      "grad_norm": 16.826276779174805,
      "learning_rate": 8.52764306498545e-06,
      "loss": 1.7245,
      "step": 56810
    },
    {
      "epoch": 7.873077456006651,
      "grad_norm": 7.774780750274658,
      "learning_rate": 8.522100595815438e-06,
      "loss": 1.5777,
      "step": 56820
    },
    {
      "epoch": 7.874463073299155,
      "grad_norm": 17.52096176147461,
      "learning_rate": 8.516558126645421e-06,
      "loss": 1.6951,
      "step": 56830
    },
    {
      "epoch": 7.875848690591659,
      "grad_norm": 16.62558364868164,
      "learning_rate": 8.511015657475405e-06,
      "loss": 1.8607,
      "step": 56840
    },
    {
      "epoch": 7.877234307884162,
      "grad_norm": 14.104851722717285,
      "learning_rate": 8.505473188305392e-06,
      "loss": 1.9031,
      "step": 56850
    },
    {
      "epoch": 7.878619925176666,
      "grad_norm": 8.03451919555664,
      "learning_rate": 8.499930719135375e-06,
      "loss": 1.941,
      "step": 56860
    },
    {
      "epoch": 7.88000554246917,
      "grad_norm": 17.753610610961914,
      "learning_rate": 8.49438824996536e-06,
      "loss": 2.1768,
      "step": 56870
    },
    {
      "epoch": 7.881391159761674,
      "grad_norm": 14.034107208251953,
      "learning_rate": 8.488845780795346e-06,
      "loss": 2.1004,
      "step": 56880
    },
    {
      "epoch": 7.882776777054177,
      "grad_norm": 18.755334854125977,
      "learning_rate": 8.48330331162533e-06,
      "loss": 2.0627,
      "step": 56890
    },
    {
      "epoch": 7.884162394346681,
      "grad_norm": 9.699142456054688,
      "learning_rate": 8.477760842455315e-06,
      "loss": 1.9132,
      "step": 56900
    },
    {
      "epoch": 7.885548011639186,
      "grad_norm": 10.021687507629395,
      "learning_rate": 8.4722183732853e-06,
      "loss": 1.5421,
      "step": 56910
    },
    {
      "epoch": 7.886933628931689,
      "grad_norm": 14.86461353302002,
      "learning_rate": 8.466675904115283e-06,
      "loss": 2.0344,
      "step": 56920
    },
    {
      "epoch": 7.888319246224193,
      "grad_norm": 5.79923677444458,
      "learning_rate": 8.461133434945269e-06,
      "loss": 1.7849,
      "step": 56930
    },
    {
      "epoch": 7.889704863516696,
      "grad_norm": 13.385485649108887,
      "learning_rate": 8.455590965775254e-06,
      "loss": 1.6507,
      "step": 56940
    },
    {
      "epoch": 7.891090480809201,
      "grad_norm": 14.034317016601562,
      "learning_rate": 8.450048496605239e-06,
      "loss": 1.74,
      "step": 56950
    },
    {
      "epoch": 7.892476098101705,
      "grad_norm": 11.835474967956543,
      "learning_rate": 8.444506027435223e-06,
      "loss": 1.9487,
      "step": 56960
    },
    {
      "epoch": 7.893861715394208,
      "grad_norm": 8.343596458435059,
      "learning_rate": 8.438963558265208e-06,
      "loss": 1.7538,
      "step": 56970
    },
    {
      "epoch": 7.895247332686712,
      "grad_norm": 12.97216796875,
      "learning_rate": 8.433421089095193e-06,
      "loss": 1.8845,
      "step": 56980
    },
    {
      "epoch": 7.896632949979216,
      "grad_norm": 11.27567195892334,
      "learning_rate": 8.427878619925177e-06,
      "loss": 1.9785,
      "step": 56990
    },
    {
      "epoch": 7.89801856727172,
      "grad_norm": 16.598127365112305,
      "learning_rate": 8.422336150755162e-06,
      "loss": 1.6183,
      "step": 57000
    },
    {
      "epoch": 7.899404184564223,
      "grad_norm": 18.057785034179688,
      "learning_rate": 8.416793681585147e-06,
      "loss": 1.8965,
      "step": 57010
    },
    {
      "epoch": 7.900789801856727,
      "grad_norm": 18.86441421508789,
      "learning_rate": 8.41125121241513e-06,
      "loss": 1.7257,
      "step": 57020
    },
    {
      "epoch": 7.902175419149231,
      "grad_norm": 13.854992866516113,
      "learning_rate": 8.405708743245116e-06,
      "loss": 2.2274,
      "step": 57030
    },
    {
      "epoch": 7.903561036441735,
      "grad_norm": 20.476112365722656,
      "learning_rate": 8.400166274075101e-06,
      "loss": 2.3535,
      "step": 57040
    },
    {
      "epoch": 7.904946653734239,
      "grad_norm": 9.022394180297852,
      "learning_rate": 8.394623804905085e-06,
      "loss": 1.9287,
      "step": 57050
    },
    {
      "epoch": 7.906332271026742,
      "grad_norm": 9.74074649810791,
      "learning_rate": 8.389081335735072e-06,
      "loss": 1.6699,
      "step": 57060
    },
    {
      "epoch": 7.9077178883192465,
      "grad_norm": 18.029869079589844,
      "learning_rate": 8.383538866565055e-06,
      "loss": 2.1051,
      "step": 57070
    },
    {
      "epoch": 7.90910350561175,
      "grad_norm": 21.266149520874023,
      "learning_rate": 8.37799639739504e-06,
      "loss": 1.7193,
      "step": 57080
    },
    {
      "epoch": 7.910489122904254,
      "grad_norm": 22.03299903869629,
      "learning_rate": 8.372453928225026e-06,
      "loss": 1.7195,
      "step": 57090
    },
    {
      "epoch": 7.911874740196757,
      "grad_norm": 13.237809181213379,
      "learning_rate": 8.36691145905501e-06,
      "loss": 2.0287,
      "step": 57100
    },
    {
      "epoch": 7.9132603574892615,
      "grad_norm": 19.76451873779297,
      "learning_rate": 8.361368989884995e-06,
      "loss": 1.9215,
      "step": 57110
    },
    {
      "epoch": 7.914645974781765,
      "grad_norm": 19.89264488220215,
      "learning_rate": 8.35582652071498e-06,
      "loss": 2.3232,
      "step": 57120
    },
    {
      "epoch": 7.916031592074269,
      "grad_norm": 15.750178337097168,
      "learning_rate": 8.350284051544963e-06,
      "loss": 1.5698,
      "step": 57130
    },
    {
      "epoch": 7.917417209366773,
      "grad_norm": 17.75145149230957,
      "learning_rate": 8.344741582374949e-06,
      "loss": 1.7399,
      "step": 57140
    },
    {
      "epoch": 7.9188028266592765,
      "grad_norm": 21.666759490966797,
      "learning_rate": 8.339199113204934e-06,
      "loss": 1.6111,
      "step": 57150
    },
    {
      "epoch": 7.920188443951781,
      "grad_norm": 15.27166748046875,
      "learning_rate": 8.333656644034919e-06,
      "loss": 2.0934,
      "step": 57160
    },
    {
      "epoch": 7.921574061244284,
      "grad_norm": 24.145915985107422,
      "learning_rate": 8.328114174864903e-06,
      "loss": 1.7663,
      "step": 57170
    },
    {
      "epoch": 7.922959678536788,
      "grad_norm": 13.061516761779785,
      "learning_rate": 8.322571705694888e-06,
      "loss": 2.2189,
      "step": 57180
    },
    {
      "epoch": 7.924345295829292,
      "grad_norm": 14.010557174682617,
      "learning_rate": 8.317029236524873e-06,
      "loss": 1.6924,
      "step": 57190
    },
    {
      "epoch": 7.925730913121796,
      "grad_norm": 16.609113693237305,
      "learning_rate": 8.311486767354857e-06,
      "loss": 1.4369,
      "step": 57200
    },
    {
      "epoch": 7.9271165304143,
      "grad_norm": 13.275301933288574,
      "learning_rate": 8.305944298184842e-06,
      "loss": 1.7819,
      "step": 57210
    },
    {
      "epoch": 7.928502147706803,
      "grad_norm": 17.400156021118164,
      "learning_rate": 8.300401829014827e-06,
      "loss": 1.7516,
      "step": 57220
    },
    {
      "epoch": 7.929887764999307,
      "grad_norm": 12.708028793334961,
      "learning_rate": 8.29485935984481e-06,
      "loss": 2.2278,
      "step": 57230
    },
    {
      "epoch": 7.931273382291811,
      "grad_norm": 16.955604553222656,
      "learning_rate": 8.289316890674796e-06,
      "loss": 2.0549,
      "step": 57240
    },
    {
      "epoch": 7.932658999584315,
      "grad_norm": 9.6438627243042,
      "learning_rate": 8.283774421504781e-06,
      "loss": 1.7286,
      "step": 57250
    },
    {
      "epoch": 7.934044616876818,
      "grad_norm": 14.928160667419434,
      "learning_rate": 8.278231952334765e-06,
      "loss": 1.8141,
      "step": 57260
    },
    {
      "epoch": 7.935430234169322,
      "grad_norm": 17.506317138671875,
      "learning_rate": 8.27268948316475e-06,
      "loss": 1.9883,
      "step": 57270
    },
    {
      "epoch": 7.9368158514618266,
      "grad_norm": 10.086967468261719,
      "learning_rate": 8.267147013994735e-06,
      "loss": 1.8524,
      "step": 57280
    },
    {
      "epoch": 7.93820146875433,
      "grad_norm": 13.729201316833496,
      "learning_rate": 8.26160454482472e-06,
      "loss": 1.4964,
      "step": 57290
    },
    {
      "epoch": 7.939587086046834,
      "grad_norm": 10.020607948303223,
      "learning_rate": 8.256062075654704e-06,
      "loss": 1.8671,
      "step": 57300
    },
    {
      "epoch": 7.940972703339337,
      "grad_norm": 5.19010591506958,
      "learning_rate": 8.25051960648469e-06,
      "loss": 1.7396,
      "step": 57310
    },
    {
      "epoch": 7.9423583206318416,
      "grad_norm": 10.316567420959473,
      "learning_rate": 8.244977137314675e-06,
      "loss": 1.9715,
      "step": 57320
    },
    {
      "epoch": 7.943743937924346,
      "grad_norm": 7.732901096343994,
      "learning_rate": 8.23943466814466e-06,
      "loss": 1.8744,
      "step": 57330
    },
    {
      "epoch": 7.945129555216849,
      "grad_norm": 11.1493558883667,
      "learning_rate": 8.233892198974643e-06,
      "loss": 2.0395,
      "step": 57340
    },
    {
      "epoch": 7.946515172509353,
      "grad_norm": 12.991228103637695,
      "learning_rate": 8.228349729804629e-06,
      "loss": 1.9575,
      "step": 57350
    },
    {
      "epoch": 7.9479007898018565,
      "grad_norm": 6.871796607971191,
      "learning_rate": 8.222807260634614e-06,
      "loss": 1.8611,
      "step": 57360
    },
    {
      "epoch": 7.949286407094361,
      "grad_norm": 14.548921585083008,
      "learning_rate": 8.2172647914646e-06,
      "loss": 1.3897,
      "step": 57370
    },
    {
      "epoch": 7.950672024386864,
      "grad_norm": 12.255705833435059,
      "learning_rate": 8.211722322294583e-06,
      "loss": 1.7303,
      "step": 57380
    },
    {
      "epoch": 7.952057641679368,
      "grad_norm": 15.94884204864502,
      "learning_rate": 8.206179853124568e-06,
      "loss": 1.632,
      "step": 57390
    },
    {
      "epoch": 7.9534432589718715,
      "grad_norm": 12.808612823486328,
      "learning_rate": 8.200637383954553e-06,
      "loss": 1.6936,
      "step": 57400
    },
    {
      "epoch": 7.954828876264376,
      "grad_norm": 11.236963272094727,
      "learning_rate": 8.195094914784537e-06,
      "loss": 1.8583,
      "step": 57410
    },
    {
      "epoch": 7.95621449355688,
      "grad_norm": 20.60673713684082,
      "learning_rate": 8.189552445614522e-06,
      "loss": 2.2542,
      "step": 57420
    },
    {
      "epoch": 7.957600110849383,
      "grad_norm": 20.40363311767578,
      "learning_rate": 8.184009976444507e-06,
      "loss": 1.9131,
      "step": 57430
    },
    {
      "epoch": 7.958985728141887,
      "grad_norm": 11.162962913513184,
      "learning_rate": 8.178467507274491e-06,
      "loss": 1.2026,
      "step": 57440
    },
    {
      "epoch": 7.960371345434391,
      "grad_norm": 22.319690704345703,
      "learning_rate": 8.172925038104476e-06,
      "loss": 2.5202,
      "step": 57450
    },
    {
      "epoch": 7.961756962726895,
      "grad_norm": 18.458791732788086,
      "learning_rate": 8.167382568934461e-06,
      "loss": 1.7447,
      "step": 57460
    },
    {
      "epoch": 7.963142580019399,
      "grad_norm": 14.150196075439453,
      "learning_rate": 8.161840099764445e-06,
      "loss": 1.5472,
      "step": 57470
    },
    {
      "epoch": 7.964528197311902,
      "grad_norm": 9.599056243896484,
      "learning_rate": 8.15629763059443e-06,
      "loss": 1.9871,
      "step": 57480
    },
    {
      "epoch": 7.965913814604407,
      "grad_norm": 18.066608428955078,
      "learning_rate": 8.150755161424415e-06,
      "loss": 1.5457,
      "step": 57490
    },
    {
      "epoch": 7.96729943189691,
      "grad_norm": 11.99055004119873,
      "learning_rate": 8.145212692254399e-06,
      "loss": 1.9142,
      "step": 57500
    },
    {
      "epoch": 7.968685049189414,
      "grad_norm": 12.811783790588379,
      "learning_rate": 8.139670223084384e-06,
      "loss": 1.9061,
      "step": 57510
    },
    {
      "epoch": 7.970070666481917,
      "grad_norm": 11.847129821777344,
      "learning_rate": 8.13412775391437e-06,
      "loss": 2.0804,
      "step": 57520
    },
    {
      "epoch": 7.971456283774422,
      "grad_norm": 6.4080610275268555,
      "learning_rate": 8.128585284744355e-06,
      "loss": 1.8736,
      "step": 57530
    },
    {
      "epoch": 7.972841901066925,
      "grad_norm": 14.884296417236328,
      "learning_rate": 8.123042815574338e-06,
      "loss": 1.5151,
      "step": 57540
    },
    {
      "epoch": 7.974227518359429,
      "grad_norm": 12.02150821685791,
      "learning_rate": 8.117500346404324e-06,
      "loss": 1.8916,
      "step": 57550
    },
    {
      "epoch": 7.975613135651933,
      "grad_norm": 21.972673416137695,
      "learning_rate": 8.111957877234309e-06,
      "loss": 2.3437,
      "step": 57560
    },
    {
      "epoch": 7.976998752944437,
      "grad_norm": 15.105745315551758,
      "learning_rate": 8.106415408064292e-06,
      "loss": 2.0968,
      "step": 57570
    },
    {
      "epoch": 7.978384370236941,
      "grad_norm": 9.059043884277344,
      "learning_rate": 8.10087293889428e-06,
      "loss": 1.9162,
      "step": 57580
    },
    {
      "epoch": 7.979769987529444,
      "grad_norm": 18.405948638916016,
      "learning_rate": 8.095330469724263e-06,
      "loss": 1.6513,
      "step": 57590
    },
    {
      "epoch": 7.981155604821948,
      "grad_norm": 18.13367462158203,
      "learning_rate": 8.089788000554248e-06,
      "loss": 1.6664,
      "step": 57600
    },
    {
      "epoch": 7.982541222114452,
      "grad_norm": 13.997238159179688,
      "learning_rate": 8.084245531384233e-06,
      "loss": 2.1365,
      "step": 57610
    },
    {
      "epoch": 7.983926839406956,
      "grad_norm": 16.215835571289062,
      "learning_rate": 8.078703062214217e-06,
      "loss": 2.0542,
      "step": 57620
    },
    {
      "epoch": 7.98531245669946,
      "grad_norm": 13.392897605895996,
      "learning_rate": 8.073160593044202e-06,
      "loss": 1.8494,
      "step": 57630
    },
    {
      "epoch": 7.986698073991963,
      "grad_norm": 14.568830490112305,
      "learning_rate": 8.067618123874187e-06,
      "loss": 1.6701,
      "step": 57640
    },
    {
      "epoch": 7.9880836912844675,
      "grad_norm": 13.245806694030762,
      "learning_rate": 8.062075654704171e-06,
      "loss": 1.4745,
      "step": 57650
    },
    {
      "epoch": 7.989469308576971,
      "grad_norm": 14.39504623413086,
      "learning_rate": 8.056533185534156e-06,
      "loss": 1.7412,
      "step": 57660
    },
    {
      "epoch": 7.990854925869475,
      "grad_norm": 10.028003692626953,
      "learning_rate": 8.050990716364141e-06,
      "loss": 1.7731,
      "step": 57670
    },
    {
      "epoch": 7.992240543161978,
      "grad_norm": 9.003904342651367,
      "learning_rate": 8.045448247194125e-06,
      "loss": 1.4183,
      "step": 57680
    },
    {
      "epoch": 7.9936261604544825,
      "grad_norm": 8.216276168823242,
      "learning_rate": 8.03990577802411e-06,
      "loss": 1.8956,
      "step": 57690
    },
    {
      "epoch": 7.995011777746987,
      "grad_norm": 10.265165328979492,
      "learning_rate": 8.034363308854095e-06,
      "loss": 1.9185,
      "step": 57700
    },
    {
      "epoch": 7.99639739503949,
      "grad_norm": 15.826386451721191,
      "learning_rate": 8.028820839684079e-06,
      "loss": 1.6633,
      "step": 57710
    },
    {
      "epoch": 7.997783012331994,
      "grad_norm": 14.300018310546875,
      "learning_rate": 8.023278370514064e-06,
      "loss": 1.7019,
      "step": 57720
    },
    {
      "epoch": 7.9991686296244975,
      "grad_norm": 8.893994331359863,
      "learning_rate": 8.01773590134405e-06,
      "loss": 1.5117,
      "step": 57730
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.5404019404019405,
      "eval_bert_f1": 0.9879783391952515,
      "eval_bert_precision": 0.9894562363624573,
      "eval_bert_recall": 0.9869152903556824,
      "eval_f1": 0.08235375767464737,
      "eval_loss": 2.100969076156616,
      "eval_runtime": 296.7534,
      "eval_samples_per_second": 48.626,
      "eval_steps_per_second": 6.079,
      "eval_synonym_accuracy": 0.5533610533610533,
      "step": 57736
    },
    {
      "epoch": 8.000554246917002,
      "grad_norm": 15.492256164550781,
      "learning_rate": 8.012193432174035e-06,
      "loss": 2.1927,
      "step": 57740
    },
    {
      "epoch": 8.001939864209506,
      "grad_norm": 12.393101692199707,
      "learning_rate": 8.006650963004018e-06,
      "loss": 1.7125,
      "step": 57750
    },
    {
      "epoch": 8.003325481502008,
      "grad_norm": 7.308781147003174,
      "learning_rate": 8.001108493834004e-06,
      "loss": 1.7463,
      "step": 57760
    },
    {
      "epoch": 8.004711098794512,
      "grad_norm": 15.762659072875977,
      "learning_rate": 7.995566024663989e-06,
      "loss": 1.8998,
      "step": 57770
    },
    {
      "epoch": 8.006096716087017,
      "grad_norm": 10.5081787109375,
      "learning_rate": 7.990023555493972e-06,
      "loss": 1.4843,
      "step": 57780
    },
    {
      "epoch": 8.00748233337952,
      "grad_norm": 9.136817932128906,
      "learning_rate": 7.984481086323958e-06,
      "loss": 1.8501,
      "step": 57790
    },
    {
      "epoch": 8.008867950672025,
      "grad_norm": 11.079121589660645,
      "learning_rate": 7.978938617153943e-06,
      "loss": 1.9145,
      "step": 57800
    },
    {
      "epoch": 8.010253567964527,
      "grad_norm": 14.465910911560059,
      "learning_rate": 7.973396147983926e-06,
      "loss": 1.7534,
      "step": 57810
    },
    {
      "epoch": 8.011639185257032,
      "grad_norm": 7.522882461547852,
      "learning_rate": 7.967853678813913e-06,
      "loss": 1.9352,
      "step": 57820
    },
    {
      "epoch": 8.013024802549536,
      "grad_norm": 19.450307846069336,
      "learning_rate": 7.962311209643897e-06,
      "loss": 2.0768,
      "step": 57830
    },
    {
      "epoch": 8.01441041984204,
      "grad_norm": 15.913387298583984,
      "learning_rate": 7.956768740473882e-06,
      "loss": 2.0535,
      "step": 57840
    },
    {
      "epoch": 8.015796037134544,
      "grad_norm": 14.437298774719238,
      "learning_rate": 7.951226271303867e-06,
      "loss": 1.7751,
      "step": 57850
    },
    {
      "epoch": 8.017181654427047,
      "grad_norm": 11.367911338806152,
      "learning_rate": 7.945683802133851e-06,
      "loss": 1.8007,
      "step": 57860
    },
    {
      "epoch": 8.01856727171955,
      "grad_norm": 14.000771522521973,
      "learning_rate": 7.940141332963836e-06,
      "loss": 1.6542,
      "step": 57870
    },
    {
      "epoch": 8.019952889012055,
      "grad_norm": 18.765625,
      "learning_rate": 7.934598863793822e-06,
      "loss": 1.9989,
      "step": 57880
    },
    {
      "epoch": 8.02133850630456,
      "grad_norm": 13.581629753112793,
      "learning_rate": 7.929056394623805e-06,
      "loss": 1.5515,
      "step": 57890
    },
    {
      "epoch": 8.022724123597062,
      "grad_norm": 13.005570411682129,
      "learning_rate": 7.92351392545379e-06,
      "loss": 2.1826,
      "step": 57900
    },
    {
      "epoch": 8.024109740889566,
      "grad_norm": 11.961254119873047,
      "learning_rate": 7.917971456283776e-06,
      "loss": 1.9999,
      "step": 57910
    },
    {
      "epoch": 8.02549535818207,
      "grad_norm": 14.760512351989746,
      "learning_rate": 7.912428987113759e-06,
      "loss": 1.407,
      "step": 57920
    },
    {
      "epoch": 8.026880975474574,
      "grad_norm": 15.565720558166504,
      "learning_rate": 7.906886517943744e-06,
      "loss": 1.6054,
      "step": 57930
    },
    {
      "epoch": 8.028266592767078,
      "grad_norm": 18.585222244262695,
      "learning_rate": 7.90134404877373e-06,
      "loss": 2.1557,
      "step": 57940
    },
    {
      "epoch": 8.02965221005958,
      "grad_norm": 19.427797317504883,
      "learning_rate": 7.895801579603715e-06,
      "loss": 1.5854,
      "step": 57950
    },
    {
      "epoch": 8.031037827352085,
      "grad_norm": 8.775895118713379,
      "learning_rate": 7.890259110433698e-06,
      "loss": 1.7382,
      "step": 57960
    },
    {
      "epoch": 8.03242344464459,
      "grad_norm": 10.005560874938965,
      "learning_rate": 7.884716641263684e-06,
      "loss": 1.6477,
      "step": 57970
    },
    {
      "epoch": 8.033809061937093,
      "grad_norm": 18.313798904418945,
      "learning_rate": 7.879174172093669e-06,
      "loss": 2.1411,
      "step": 57980
    },
    {
      "epoch": 8.035194679229598,
      "grad_norm": 16.797361373901367,
      "learning_rate": 7.873631702923652e-06,
      "loss": 1.8088,
      "step": 57990
    },
    {
      "epoch": 8.0365802965221,
      "grad_norm": 7.536926746368408,
      "learning_rate": 7.868089233753638e-06,
      "loss": 1.7448,
      "step": 58000
    },
    {
      "epoch": 8.037965913814604,
      "grad_norm": 8.499919891357422,
      "learning_rate": 7.862546764583623e-06,
      "loss": 1.7755,
      "step": 58010
    },
    {
      "epoch": 8.039351531107108,
      "grad_norm": 9.893911361694336,
      "learning_rate": 7.857004295413607e-06,
      "loss": 2.1724,
      "step": 58020
    },
    {
      "epoch": 8.040737148399613,
      "grad_norm": 12.97269058227539,
      "learning_rate": 7.851461826243593e-06,
      "loss": 1.5477,
      "step": 58030
    },
    {
      "epoch": 8.042122765692115,
      "grad_norm": 14.918886184692383,
      "learning_rate": 7.845919357073577e-06,
      "loss": 1.4486,
      "step": 58040
    },
    {
      "epoch": 8.04350838298462,
      "grad_norm": 9.213949203491211,
      "learning_rate": 7.84037688790356e-06,
      "loss": 1.5533,
      "step": 58050
    },
    {
      "epoch": 8.044894000277123,
      "grad_norm": 10.904190063476562,
      "learning_rate": 7.834834418733548e-06,
      "loss": 1.7327,
      "step": 58060
    },
    {
      "epoch": 8.046279617569628,
      "grad_norm": 6.0697832107543945,
      "learning_rate": 7.829291949563531e-06,
      "loss": 1.6994,
      "step": 58070
    },
    {
      "epoch": 8.047665234862132,
      "grad_norm": 10.515461921691895,
      "learning_rate": 7.823749480393515e-06,
      "loss": 1.9988,
      "step": 58080
    },
    {
      "epoch": 8.049050852154634,
      "grad_norm": 10.690964698791504,
      "learning_rate": 7.818207011223502e-06,
      "loss": 2.1015,
      "step": 58090
    },
    {
      "epoch": 8.050436469447138,
      "grad_norm": 18.307205200195312,
      "learning_rate": 7.812664542053485e-06,
      "loss": 2.0306,
      "step": 58100
    },
    {
      "epoch": 8.051822086739643,
      "grad_norm": 13.205333709716797,
      "learning_rate": 7.80712207288347e-06,
      "loss": 1.9313,
      "step": 58110
    },
    {
      "epoch": 8.053207704032147,
      "grad_norm": 8.755678176879883,
      "learning_rate": 7.801579603713456e-06,
      "loss": 1.6255,
      "step": 58120
    },
    {
      "epoch": 8.054593321324651,
      "grad_norm": 17.16533088684082,
      "learning_rate": 7.79603713454344e-06,
      "loss": 1.5723,
      "step": 58130
    },
    {
      "epoch": 8.055978938617153,
      "grad_norm": 9.050546646118164,
      "learning_rate": 7.790494665373424e-06,
      "loss": 2.0842,
      "step": 58140
    },
    {
      "epoch": 8.057364555909658,
      "grad_norm": 18.23305320739746,
      "learning_rate": 7.78495219620341e-06,
      "loss": 2.0519,
      "step": 58150
    },
    {
      "epoch": 8.058750173202162,
      "grad_norm": 14.479170799255371,
      "learning_rate": 7.779409727033393e-06,
      "loss": 1.9841,
      "step": 58160
    },
    {
      "epoch": 8.060135790494666,
      "grad_norm": 16.957931518554688,
      "learning_rate": 7.773867257863378e-06,
      "loss": 1.5478,
      "step": 58170
    },
    {
      "epoch": 8.061521407787168,
      "grad_norm": 18.67167854309082,
      "learning_rate": 7.768324788693364e-06,
      "loss": 1.7069,
      "step": 58180
    },
    {
      "epoch": 8.062907025079673,
      "grad_norm": 6.924760341644287,
      "learning_rate": 7.762782319523349e-06,
      "loss": 1.5588,
      "step": 58190
    },
    {
      "epoch": 8.064292642372177,
      "grad_norm": 22.08629035949707,
      "learning_rate": 7.757239850353333e-06,
      "loss": 1.9653,
      "step": 58200
    },
    {
      "epoch": 8.065678259664681,
      "grad_norm": 11.628681182861328,
      "learning_rate": 7.751697381183318e-06,
      "loss": 1.6661,
      "step": 58210
    },
    {
      "epoch": 8.067063876957185,
      "grad_norm": 20.331411361694336,
      "learning_rate": 7.746154912013303e-06,
      "loss": 1.8204,
      "step": 58220
    },
    {
      "epoch": 8.068449494249688,
      "grad_norm": 8.337740898132324,
      "learning_rate": 7.740612442843287e-06,
      "loss": 1.8525,
      "step": 58230
    },
    {
      "epoch": 8.069835111542192,
      "grad_norm": 11.851592063903809,
      "learning_rate": 7.735069973673272e-06,
      "loss": 1.7501,
      "step": 58240
    },
    {
      "epoch": 8.071220728834696,
      "grad_norm": 19.102264404296875,
      "learning_rate": 7.729527504503257e-06,
      "loss": 1.8461,
      "step": 58250
    },
    {
      "epoch": 8.0726063461272,
      "grad_norm": 16.584102630615234,
      "learning_rate": 7.72398503533324e-06,
      "loss": 2.0865,
      "step": 58260
    },
    {
      "epoch": 8.073991963419703,
      "grad_norm": 14.954276084899902,
      "learning_rate": 7.718442566163226e-06,
      "loss": 1.6147,
      "step": 58270
    },
    {
      "epoch": 8.075377580712207,
      "grad_norm": 11.857068061828613,
      "learning_rate": 7.712900096993211e-06,
      "loss": 1.8965,
      "step": 58280
    },
    {
      "epoch": 8.076763198004711,
      "grad_norm": 10.186911582946777,
      "learning_rate": 7.707357627823195e-06,
      "loss": 1.4522,
      "step": 58290
    },
    {
      "epoch": 8.078148815297215,
      "grad_norm": 11.881729125976562,
      "learning_rate": 7.701815158653182e-06,
      "loss": 1.6687,
      "step": 58300
    },
    {
      "epoch": 8.07953443258972,
      "grad_norm": 11.280681610107422,
      "learning_rate": 7.696272689483165e-06,
      "loss": 1.4532,
      "step": 58310
    },
    {
      "epoch": 8.080920049882222,
      "grad_norm": 17.144935607910156,
      "learning_rate": 7.69073022031315e-06,
      "loss": 1.9249,
      "step": 58320
    },
    {
      "epoch": 8.082305667174726,
      "grad_norm": 20.17771339416504,
      "learning_rate": 7.685187751143136e-06,
      "loss": 1.8155,
      "step": 58330
    },
    {
      "epoch": 8.08369128446723,
      "grad_norm": 12.99544620513916,
      "learning_rate": 7.67964528197312e-06,
      "loss": 1.8157,
      "step": 58340
    },
    {
      "epoch": 8.085076901759734,
      "grad_norm": 11.271157264709473,
      "learning_rate": 7.674102812803105e-06,
      "loss": 1.8785,
      "step": 58350
    },
    {
      "epoch": 8.086462519052239,
      "grad_norm": 8.70472526550293,
      "learning_rate": 7.66856034363309e-06,
      "loss": 2.0085,
      "step": 58360
    },
    {
      "epoch": 8.087848136344741,
      "grad_norm": 19.449525833129883,
      "learning_rate": 7.663017874463073e-06,
      "loss": 1.741,
      "step": 58370
    },
    {
      "epoch": 8.089233753637245,
      "grad_norm": 19.180326461791992,
      "learning_rate": 7.657475405293059e-06,
      "loss": 1.2136,
      "step": 58380
    },
    {
      "epoch": 8.09061937092975,
      "grad_norm": 11.838737487792969,
      "learning_rate": 7.651932936123044e-06,
      "loss": 1.5716,
      "step": 58390
    },
    {
      "epoch": 8.092004988222254,
      "grad_norm": 14.621496200561523,
      "learning_rate": 7.646390466953029e-06,
      "loss": 2.0311,
      "step": 58400
    },
    {
      "epoch": 8.093390605514756,
      "grad_norm": 13.459667205810547,
      "learning_rate": 7.640847997783013e-06,
      "loss": 1.9339,
      "step": 58410
    },
    {
      "epoch": 8.09477622280726,
      "grad_norm": 9.163602828979492,
      "learning_rate": 7.635305528612998e-06,
      "loss": 1.3754,
      "step": 58420
    },
    {
      "epoch": 8.096161840099764,
      "grad_norm": 17.64179229736328,
      "learning_rate": 7.629763059442983e-06,
      "loss": 1.4695,
      "step": 58430
    },
    {
      "epoch": 8.097547457392269,
      "grad_norm": 17.00397300720215,
      "learning_rate": 7.6242205902729675e-06,
      "loss": 1.4689,
      "step": 58440
    },
    {
      "epoch": 8.098933074684773,
      "grad_norm": 10.977508544921875,
      "learning_rate": 7.618678121102951e-06,
      "loss": 1.7578,
      "step": 58450
    },
    {
      "epoch": 8.100318691977275,
      "grad_norm": 20.387590408325195,
      "learning_rate": 7.613135651932937e-06,
      "loss": 1.891,
      "step": 58460
    },
    {
      "epoch": 8.10170430926978,
      "grad_norm": 11.041669845581055,
      "learning_rate": 7.6075931827629216e-06,
      "loss": 2.1119,
      "step": 58470
    },
    {
      "epoch": 8.103089926562284,
      "grad_norm": 7.337284564971924,
      "learning_rate": 7.602050713592907e-06,
      "loss": 1.4541,
      "step": 58480
    },
    {
      "epoch": 8.104475543854788,
      "grad_norm": 16.199275970458984,
      "learning_rate": 7.596508244422891e-06,
      "loss": 1.4541,
      "step": 58490
    },
    {
      "epoch": 8.105861161147292,
      "grad_norm": 9.617892265319824,
      "learning_rate": 7.590965775252876e-06,
      "loss": 1.5254,
      "step": 58500
    },
    {
      "epoch": 8.107246778439794,
      "grad_norm": 14.358933448791504,
      "learning_rate": 7.585423306082861e-06,
      "loss": 1.9551,
      "step": 58510
    },
    {
      "epoch": 8.108632395732299,
      "grad_norm": 14.960466384887695,
      "learning_rate": 7.579880836912845e-06,
      "loss": 2.194,
      "step": 58520
    },
    {
      "epoch": 8.110018013024803,
      "grad_norm": 10.770683288574219,
      "learning_rate": 7.5743383677428305e-06,
      "loss": 2.0402,
      "step": 58530
    },
    {
      "epoch": 8.111403630317307,
      "grad_norm": 16.8601016998291,
      "learning_rate": 7.568795898572815e-06,
      "loss": 1.5356,
      "step": 58540
    },
    {
      "epoch": 8.11278924760981,
      "grad_norm": 14.085695266723633,
      "learning_rate": 7.563253429402799e-06,
      "loss": 2.1475,
      "step": 58550
    },
    {
      "epoch": 8.114174864902314,
      "grad_norm": 13.622706413269043,
      "learning_rate": 7.557710960232785e-06,
      "loss": 1.7477,
      "step": 58560
    },
    {
      "epoch": 8.115560482194818,
      "grad_norm": 8.34560489654541,
      "learning_rate": 7.552168491062769e-06,
      "loss": 1.5413,
      "step": 58570
    },
    {
      "epoch": 8.116946099487322,
      "grad_norm": 8.010430335998535,
      "learning_rate": 7.546626021892753e-06,
      "loss": 1.4406,
      "step": 58580
    },
    {
      "epoch": 8.118331716779826,
      "grad_norm": 9.182659149169922,
      "learning_rate": 7.541083552722739e-06,
      "loss": 1.4117,
      "step": 58590
    },
    {
      "epoch": 8.119717334072329,
      "grad_norm": 8.44705867767334,
      "learning_rate": 7.535541083552723e-06,
      "loss": 1.7775,
      "step": 58600
    },
    {
      "epoch": 8.121102951364833,
      "grad_norm": 14.69674015045166,
      "learning_rate": 7.529998614382708e-06,
      "loss": 2.2054,
      "step": 58610
    },
    {
      "epoch": 8.122488568657337,
      "grad_norm": 11.798578262329102,
      "learning_rate": 7.524456145212693e-06,
      "loss": 1.9805,
      "step": 58620
    },
    {
      "epoch": 8.123874185949841,
      "grad_norm": 12.96423625946045,
      "learning_rate": 7.518913676042677e-06,
      "loss": 1.8071,
      "step": 58630
    },
    {
      "epoch": 8.125259803242345,
      "grad_norm": 15.880542755126953,
      "learning_rate": 7.513371206872662e-06,
      "loss": 2.1838,
      "step": 58640
    },
    {
      "epoch": 8.126645420534848,
      "grad_norm": 9.060291290283203,
      "learning_rate": 7.507828737702647e-06,
      "loss": 1.6313,
      "step": 58650
    },
    {
      "epoch": 8.128031037827352,
      "grad_norm": 16.503633499145508,
      "learning_rate": 7.502286268532631e-06,
      "loss": 1.8731,
      "step": 58660
    },
    {
      "epoch": 8.129416655119856,
      "grad_norm": 17.177783966064453,
      "learning_rate": 7.496743799362617e-06,
      "loss": 2.1218,
      "step": 58670
    },
    {
      "epoch": 8.13080227241236,
      "grad_norm": 13.730679512023926,
      "learning_rate": 7.491201330192601e-06,
      "loss": 1.6277,
      "step": 58680
    },
    {
      "epoch": 8.132187889704863,
      "grad_norm": 19.440690994262695,
      "learning_rate": 7.485658861022587e-06,
      "loss": 1.9858,
      "step": 58690
    },
    {
      "epoch": 8.133573506997367,
      "grad_norm": 8.328540802001953,
      "learning_rate": 7.480116391852571e-06,
      "loss": 1.3467,
      "step": 58700
    },
    {
      "epoch": 8.134959124289871,
      "grad_norm": 14.569621086120605,
      "learning_rate": 7.474573922682556e-06,
      "loss": 1.8292,
      "step": 58710
    },
    {
      "epoch": 8.136344741582375,
      "grad_norm": 29.28300666809082,
      "learning_rate": 7.469031453512541e-06,
      "loss": 1.5924,
      "step": 58720
    },
    {
      "epoch": 8.13773035887488,
      "grad_norm": 18.715211868286133,
      "learning_rate": 7.463488984342525e-06,
      "loss": 1.7617,
      "step": 58730
    },
    {
      "epoch": 8.139115976167382,
      "grad_norm": 8.20562744140625,
      "learning_rate": 7.45794651517251e-06,
      "loss": 1.6127,
      "step": 58740
    },
    {
      "epoch": 8.140501593459886,
      "grad_norm": 13.110675811767578,
      "learning_rate": 7.452404046002495e-06,
      "loss": 1.5856,
      "step": 58750
    },
    {
      "epoch": 8.14188721075239,
      "grad_norm": 16.998197555541992,
      "learning_rate": 7.446861576832479e-06,
      "loss": 1.7213,
      "step": 58760
    },
    {
      "epoch": 8.143272828044894,
      "grad_norm": 13.643441200256348,
      "learning_rate": 7.441319107662465e-06,
      "loss": 1.8534,
      "step": 58770
    },
    {
      "epoch": 8.144658445337399,
      "grad_norm": 8.905184745788574,
      "learning_rate": 7.435776638492449e-06,
      "loss": 2.0184,
      "step": 58780
    },
    {
      "epoch": 8.146044062629901,
      "grad_norm": 10.590982437133789,
      "learning_rate": 7.4302341693224335e-06,
      "loss": 1.657,
      "step": 58790
    },
    {
      "epoch": 8.147429679922405,
      "grad_norm": 12.379274368286133,
      "learning_rate": 7.424691700152419e-06,
      "loss": 1.5523,
      "step": 58800
    },
    {
      "epoch": 8.14881529721491,
      "grad_norm": 10.116243362426758,
      "learning_rate": 7.419149230982403e-06,
      "loss": 1.5864,
      "step": 58810
    },
    {
      "epoch": 8.150200914507414,
      "grad_norm": 10.938846588134766,
      "learning_rate": 7.4136067618123875e-06,
      "loss": 1.5099,
      "step": 58820
    },
    {
      "epoch": 8.151586531799916,
      "grad_norm": 14.566667556762695,
      "learning_rate": 7.408064292642373e-06,
      "loss": 1.6135,
      "step": 58830
    },
    {
      "epoch": 8.15297214909242,
      "grad_norm": 11.140793800354004,
      "learning_rate": 7.402521823472357e-06,
      "loss": 1.6287,
      "step": 58840
    },
    {
      "epoch": 8.154357766384924,
      "grad_norm": 10.5495023727417,
      "learning_rate": 7.396979354302342e-06,
      "loss": 1.9009,
      "step": 58850
    },
    {
      "epoch": 8.155743383677429,
      "grad_norm": 11.412301063537598,
      "learning_rate": 7.391436885132327e-06,
      "loss": 1.8538,
      "step": 58860
    },
    {
      "epoch": 8.157129000969933,
      "grad_norm": 10.953258514404297,
      "learning_rate": 7.385894415962311e-06,
      "loss": 1.5649,
      "step": 58870
    },
    {
      "epoch": 8.158514618262435,
      "grad_norm": 17.18425178527832,
      "learning_rate": 7.3803519467922965e-06,
      "loss": 1.6165,
      "step": 58880
    },
    {
      "epoch": 8.15990023555494,
      "grad_norm": 10.790581703186035,
      "learning_rate": 7.374809477622281e-06,
      "loss": 1.7988,
      "step": 58890
    },
    {
      "epoch": 8.161285852847444,
      "grad_norm": 17.823211669921875,
      "learning_rate": 7.369267008452267e-06,
      "loss": 1.792,
      "step": 58900
    },
    {
      "epoch": 8.162671470139948,
      "grad_norm": 11.28922176361084,
      "learning_rate": 7.363724539282251e-06,
      "loss": 1.5452,
      "step": 58910
    },
    {
      "epoch": 8.16405708743245,
      "grad_norm": 9.958037376403809,
      "learning_rate": 7.358182070112235e-06,
      "loss": 1.8073,
      "step": 58920
    },
    {
      "epoch": 8.165442704724954,
      "grad_norm": 14.7430419921875,
      "learning_rate": 7.352639600942221e-06,
      "loss": 2.6922,
      "step": 58930
    },
    {
      "epoch": 8.166828322017459,
      "grad_norm": 13.760003089904785,
      "learning_rate": 7.347097131772205e-06,
      "loss": 1.8413,
      "step": 58940
    },
    {
      "epoch": 8.168213939309963,
      "grad_norm": 9.250702857971191,
      "learning_rate": 7.34155466260219e-06,
      "loss": 1.9829,
      "step": 58950
    },
    {
      "epoch": 8.169599556602467,
      "grad_norm": 19.000539779663086,
      "learning_rate": 7.336012193432175e-06,
      "loss": 1.7658,
      "step": 58960
    },
    {
      "epoch": 8.17098517389497,
      "grad_norm": 14.869714736938477,
      "learning_rate": 7.3304697242621595e-06,
      "loss": 1.9094,
      "step": 58970
    },
    {
      "epoch": 8.172370791187474,
      "grad_norm": 8.913189888000488,
      "learning_rate": 7.324927255092145e-06,
      "loss": 2.079,
      "step": 58980
    },
    {
      "epoch": 8.173756408479978,
      "grad_norm": 16.161420822143555,
      "learning_rate": 7.319384785922129e-06,
      "loss": 1.9842,
      "step": 58990
    },
    {
      "epoch": 8.175142025772482,
      "grad_norm": 17.689720153808594,
      "learning_rate": 7.3138423167521135e-06,
      "loss": 1.7865,
      "step": 59000
    },
    {
      "epoch": 8.176527643064986,
      "grad_norm": 6.2940287590026855,
      "learning_rate": 7.308299847582099e-06,
      "loss": 1.7462,
      "step": 59010
    },
    {
      "epoch": 8.177913260357489,
      "grad_norm": 9.220551490783691,
      "learning_rate": 7.302757378412083e-06,
      "loss": 1.6284,
      "step": 59020
    },
    {
      "epoch": 8.179298877649993,
      "grad_norm": 11.046258926391602,
      "learning_rate": 7.297214909242068e-06,
      "loss": 1.32,
      "step": 59030
    },
    {
      "epoch": 8.180684494942497,
      "grad_norm": 19.90744972229004,
      "learning_rate": 7.291672440072053e-06,
      "loss": 1.8174,
      "step": 59040
    },
    {
      "epoch": 8.182070112235001,
      "grad_norm": 15.460055351257324,
      "learning_rate": 7.286129970902037e-06,
      "loss": 1.8315,
      "step": 59050
    },
    {
      "epoch": 8.183455729527504,
      "grad_norm": 19.628381729125977,
      "learning_rate": 7.2805875017320225e-06,
      "loss": 1.1967,
      "step": 59060
    },
    {
      "epoch": 8.184841346820008,
      "grad_norm": 11.920642852783203,
      "learning_rate": 7.275045032562007e-06,
      "loss": 1.9269,
      "step": 59070
    },
    {
      "epoch": 8.186226964112512,
      "grad_norm": 11.622859001159668,
      "learning_rate": 7.269502563391991e-06,
      "loss": 2.3404,
      "step": 59080
    },
    {
      "epoch": 8.187612581405016,
      "grad_norm": 17.432598114013672,
      "learning_rate": 7.2639600942219765e-06,
      "loss": 1.6968,
      "step": 59090
    },
    {
      "epoch": 8.18899819869752,
      "grad_norm": 29.704654693603516,
      "learning_rate": 7.258417625051961e-06,
      "loss": 1.9278,
      "step": 59100
    },
    {
      "epoch": 8.190383815990023,
      "grad_norm": 8.60853385925293,
      "learning_rate": 7.252875155881945e-06,
      "loss": 2.0252,
      "step": 59110
    },
    {
      "epoch": 8.191769433282527,
      "grad_norm": 13.55083179473877,
      "learning_rate": 7.247332686711931e-06,
      "loss": 2.064,
      "step": 59120
    },
    {
      "epoch": 8.193155050575031,
      "grad_norm": 11.84615421295166,
      "learning_rate": 7.241790217541915e-06,
      "loss": 2.1396,
      "step": 59130
    },
    {
      "epoch": 8.194540667867535,
      "grad_norm": 13.927048683166504,
      "learning_rate": 7.236247748371901e-06,
      "loss": 1.9923,
      "step": 59140
    },
    {
      "epoch": 8.19592628516004,
      "grad_norm": 14.447853088378906,
      "learning_rate": 7.230705279201885e-06,
      "loss": 1.8284,
      "step": 59150
    },
    {
      "epoch": 8.197311902452542,
      "grad_norm": 11.352375984191895,
      "learning_rate": 7.225162810031869e-06,
      "loss": 1.8998,
      "step": 59160
    },
    {
      "epoch": 8.198697519745046,
      "grad_norm": 9.945466995239258,
      "learning_rate": 7.219620340861855e-06,
      "loss": 1.5796,
      "step": 59170
    },
    {
      "epoch": 8.20008313703755,
      "grad_norm": 12.487665176391602,
      "learning_rate": 7.2140778716918395e-06,
      "loss": 1.252,
      "step": 59180
    },
    {
      "epoch": 8.201468754330055,
      "grad_norm": 14.917903900146484,
      "learning_rate": 7.208535402521825e-06,
      "loss": 1.7733,
      "step": 59190
    },
    {
      "epoch": 8.202854371622557,
      "grad_norm": 14.294964790344238,
      "learning_rate": 7.202992933351809e-06,
      "loss": 1.6191,
      "step": 59200
    },
    {
      "epoch": 8.204239988915061,
      "grad_norm": 19.09927749633789,
      "learning_rate": 7.197450464181794e-06,
      "loss": 1.8194,
      "step": 59210
    },
    {
      "epoch": 8.205625606207565,
      "grad_norm": 28.686626434326172,
      "learning_rate": 7.191907995011779e-06,
      "loss": 1.5681,
      "step": 59220
    },
    {
      "epoch": 8.20701122350007,
      "grad_norm": 12.658227920532227,
      "learning_rate": 7.186365525841763e-06,
      "loss": 1.6431,
      "step": 59230
    },
    {
      "epoch": 8.208396840792574,
      "grad_norm": 13.904925346374512,
      "learning_rate": 7.180823056671748e-06,
      "loss": 1.9499,
      "step": 59240
    },
    {
      "epoch": 8.209782458085076,
      "grad_norm": 15.613987922668457,
      "learning_rate": 7.175280587501733e-06,
      "loss": 1.771,
      "step": 59250
    },
    {
      "epoch": 8.21116807537758,
      "grad_norm": 14.416398048400879,
      "learning_rate": 7.169738118331717e-06,
      "loss": 1.4925,
      "step": 59260
    },
    {
      "epoch": 8.212553692670085,
      "grad_norm": 10.238116264343262,
      "learning_rate": 7.1641956491617026e-06,
      "loss": 1.6293,
      "step": 59270
    },
    {
      "epoch": 8.213939309962589,
      "grad_norm": 23.811614990234375,
      "learning_rate": 7.158653179991687e-06,
      "loss": 1.886,
      "step": 59280
    },
    {
      "epoch": 8.215324927255093,
      "grad_norm": 11.85948657989502,
      "learning_rate": 7.153110710821671e-06,
      "loss": 2.1496,
      "step": 59290
    },
    {
      "epoch": 8.216710544547595,
      "grad_norm": 7.114025592803955,
      "learning_rate": 7.147568241651657e-06,
      "loss": 1.4998,
      "step": 59300
    },
    {
      "epoch": 8.2180961618401,
      "grad_norm": 21.957233428955078,
      "learning_rate": 7.142025772481641e-06,
      "loss": 1.7886,
      "step": 59310
    },
    {
      "epoch": 8.219481779132604,
      "grad_norm": 12.248634338378906,
      "learning_rate": 7.136483303311625e-06,
      "loss": 1.7683,
      "step": 59320
    },
    {
      "epoch": 8.220867396425108,
      "grad_norm": 17.208343505859375,
      "learning_rate": 7.130940834141611e-06,
      "loss": 2.1175,
      "step": 59330
    },
    {
      "epoch": 8.22225301371761,
      "grad_norm": 20.709264755249023,
      "learning_rate": 7.125398364971595e-06,
      "loss": 1.9205,
      "step": 59340
    },
    {
      "epoch": 8.223638631010115,
      "grad_norm": 12.632709503173828,
      "learning_rate": 7.11985589580158e-06,
      "loss": 1.5448,
      "step": 59350
    },
    {
      "epoch": 8.225024248302619,
      "grad_norm": 19.365341186523438,
      "learning_rate": 7.114313426631565e-06,
      "loss": 1.6656,
      "step": 59360
    },
    {
      "epoch": 8.226409865595123,
      "grad_norm": 11.685567855834961,
      "learning_rate": 7.108770957461549e-06,
      "loss": 1.8288,
      "step": 59370
    },
    {
      "epoch": 8.227795482887627,
      "grad_norm": 28.88709831237793,
      "learning_rate": 7.103228488291534e-06,
      "loss": 1.5445,
      "step": 59380
    },
    {
      "epoch": 8.22918110018013,
      "grad_norm": 14.524658203125,
      "learning_rate": 7.097686019121519e-06,
      "loss": 1.8997,
      "step": 59390
    },
    {
      "epoch": 8.230566717472634,
      "grad_norm": 13.321407318115234,
      "learning_rate": 7.092143549951503e-06,
      "loss": 2.055,
      "step": 59400
    },
    {
      "epoch": 8.231952334765138,
      "grad_norm": 11.093925476074219,
      "learning_rate": 7.086601080781489e-06,
      "loss": 2.0174,
      "step": 59410
    },
    {
      "epoch": 8.233337952057642,
      "grad_norm": 12.327885627746582,
      "learning_rate": 7.081058611611473e-06,
      "loss": 1.3981,
      "step": 59420
    },
    {
      "epoch": 8.234723569350145,
      "grad_norm": 13.220869064331055,
      "learning_rate": 7.075516142441459e-06,
      "loss": 2.1736,
      "step": 59430
    },
    {
      "epoch": 8.236109186642649,
      "grad_norm": 14.859396934509277,
      "learning_rate": 7.069973673271443e-06,
      "loss": 1.557,
      "step": 59440
    },
    {
      "epoch": 8.237494803935153,
      "grad_norm": 13.345833778381348,
      "learning_rate": 7.064431204101428e-06,
      "loss": 1.9105,
      "step": 59450
    },
    {
      "epoch": 8.238880421227657,
      "grad_norm": 15.369101524353027,
      "learning_rate": 7.058888734931413e-06,
      "loss": 1.9007,
      "step": 59460
    },
    {
      "epoch": 8.240266038520161,
      "grad_norm": 11.242201805114746,
      "learning_rate": 7.053346265761397e-06,
      "loss": 1.8598,
      "step": 59470
    },
    {
      "epoch": 8.241651655812664,
      "grad_norm": 18.707597732543945,
      "learning_rate": 7.047803796591383e-06,
      "loss": 1.6187,
      "step": 59480
    },
    {
      "epoch": 8.243037273105168,
      "grad_norm": 11.470410346984863,
      "learning_rate": 7.042261327421367e-06,
      "loss": 1.9918,
      "step": 59490
    },
    {
      "epoch": 8.244422890397672,
      "grad_norm": 14.566814422607422,
      "learning_rate": 7.0367188582513514e-06,
      "loss": 2.0899,
      "step": 59500
    },
    {
      "epoch": 8.245808507690176,
      "grad_norm": 21.53010368347168,
      "learning_rate": 7.031176389081337e-06,
      "loss": 1.5418,
      "step": 59510
    },
    {
      "epoch": 8.24719412498268,
      "grad_norm": 17.916078567504883,
      "learning_rate": 7.025633919911321e-06,
      "loss": 1.9828,
      "step": 59520
    },
    {
      "epoch": 8.248579742275183,
      "grad_norm": 10.791767120361328,
      "learning_rate": 7.0200914507413055e-06,
      "loss": 1.7106,
      "step": 59530
    },
    {
      "epoch": 8.249965359567687,
      "grad_norm": 13.690057754516602,
      "learning_rate": 7.014548981571291e-06,
      "loss": 1.6336,
      "step": 59540
    },
    {
      "epoch": 8.251350976860191,
      "grad_norm": 23.047412872314453,
      "learning_rate": 7.009006512401275e-06,
      "loss": 2.1692,
      "step": 59550
    },
    {
      "epoch": 8.252736594152696,
      "grad_norm": 12.335274696350098,
      "learning_rate": 7.00346404323126e-06,
      "loss": 1.7173,
      "step": 59560
    },
    {
      "epoch": 8.254122211445198,
      "grad_norm": 7.4276442527771,
      "learning_rate": 6.997921574061245e-06,
      "loss": 1.8878,
      "step": 59570
    },
    {
      "epoch": 8.255507828737702,
      "grad_norm": 15.727643966674805,
      "learning_rate": 6.992379104891229e-06,
      "loss": 1.6126,
      "step": 59580
    },
    {
      "epoch": 8.256893446030206,
      "grad_norm": 13.187237739562988,
      "learning_rate": 6.9868366357212144e-06,
      "loss": 1.4917,
      "step": 59590
    },
    {
      "epoch": 8.25827906332271,
      "grad_norm": 6.862652778625488,
      "learning_rate": 6.981294166551199e-06,
      "loss": 1.8394,
      "step": 59600
    },
    {
      "epoch": 8.259664680615215,
      "grad_norm": 10.751138687133789,
      "learning_rate": 6.975751697381183e-06,
      "loss": 1.6537,
      "step": 59610
    },
    {
      "epoch": 8.261050297907717,
      "grad_norm": 11.871854782104492,
      "learning_rate": 6.9702092282111685e-06,
      "loss": 1.6173,
      "step": 59620
    },
    {
      "epoch": 8.262435915200221,
      "grad_norm": 11.813139915466309,
      "learning_rate": 6.964666759041153e-06,
      "loss": 1.6134,
      "step": 59630
    },
    {
      "epoch": 8.263821532492726,
      "grad_norm": 6.014608860015869,
      "learning_rate": 6.959124289871139e-06,
      "loss": 1.8087,
      "step": 59640
    },
    {
      "epoch": 8.26520714978523,
      "grad_norm": 11.130179405212402,
      "learning_rate": 6.953581820701123e-06,
      "loss": 1.8552,
      "step": 59650
    },
    {
      "epoch": 8.266592767077734,
      "grad_norm": 15.385286331176758,
      "learning_rate": 6.948039351531107e-06,
      "loss": 1.9489,
      "step": 59660
    },
    {
      "epoch": 8.267978384370236,
      "grad_norm": 17.971338272094727,
      "learning_rate": 6.942496882361093e-06,
      "loss": 1.9679,
      "step": 59670
    },
    {
      "epoch": 8.26936400166274,
      "grad_norm": 8.66018295288086,
      "learning_rate": 6.9369544131910774e-06,
      "loss": 1.4051,
      "step": 59680
    },
    {
      "epoch": 8.270749618955245,
      "grad_norm": 15.697033882141113,
      "learning_rate": 6.931411944021062e-06,
      "loss": 1.5604,
      "step": 59690
    },
    {
      "epoch": 8.272135236247749,
      "grad_norm": 15.417296409606934,
      "learning_rate": 6.925869474851047e-06,
      "loss": 1.4323,
      "step": 59700
    },
    {
      "epoch": 8.273520853540251,
      "grad_norm": 14.038293838500977,
      "learning_rate": 6.9203270056810315e-06,
      "loss": 1.767,
      "step": 59710
    },
    {
      "epoch": 8.274906470832756,
      "grad_norm": 11.84948444366455,
      "learning_rate": 6.914784536511017e-06,
      "loss": 1.4672,
      "step": 59720
    },
    {
      "epoch": 8.27629208812526,
      "grad_norm": 26.31174087524414,
      "learning_rate": 6.909242067341001e-06,
      "loss": 1.6743,
      "step": 59730
    },
    {
      "epoch": 8.277677705417764,
      "grad_norm": 11.527636528015137,
      "learning_rate": 6.9036995981709856e-06,
      "loss": 2.0778,
      "step": 59740
    },
    {
      "epoch": 8.279063322710268,
      "grad_norm": 22.87640953063965,
      "learning_rate": 6.898157129000971e-06,
      "loss": 1.5945,
      "step": 59750
    },
    {
      "epoch": 8.28044894000277,
      "grad_norm": 16.589353561401367,
      "learning_rate": 6.892614659830955e-06,
      "loss": 1.8568,
      "step": 59760
    },
    {
      "epoch": 8.281834557295275,
      "grad_norm": 11.960465431213379,
      "learning_rate": 6.88707219066094e-06,
      "loss": 2.0386,
      "step": 59770
    },
    {
      "epoch": 8.283220174587779,
      "grad_norm": 9.964625358581543,
      "learning_rate": 6.881529721490925e-06,
      "loss": 1.5947,
      "step": 59780
    },
    {
      "epoch": 8.284605791880283,
      "grad_norm": 12.979510307312012,
      "learning_rate": 6.875987252320909e-06,
      "loss": 1.5681,
      "step": 59790
    },
    {
      "epoch": 8.285991409172787,
      "grad_norm": 9.062947273254395,
      "learning_rate": 6.8704447831508945e-06,
      "loss": 1.8293,
      "step": 59800
    },
    {
      "epoch": 8.28737702646529,
      "grad_norm": 7.692014217376709,
      "learning_rate": 6.864902313980879e-06,
      "loss": 1.9203,
      "step": 59810
    },
    {
      "epoch": 8.288762643757794,
      "grad_norm": 13.123437881469727,
      "learning_rate": 6.859359844810863e-06,
      "loss": 1.547,
      "step": 59820
    },
    {
      "epoch": 8.290148261050298,
      "grad_norm": 14.135461807250977,
      "learning_rate": 6.8538173756408486e-06,
      "loss": 1.7272,
      "step": 59830
    },
    {
      "epoch": 8.291533878342802,
      "grad_norm": 8.819643020629883,
      "learning_rate": 6.848274906470833e-06,
      "loss": 2.0399,
      "step": 59840
    },
    {
      "epoch": 8.292919495635305,
      "grad_norm": 14.674775123596191,
      "learning_rate": 6.842732437300818e-06,
      "loss": 1.7848,
      "step": 59850
    },
    {
      "epoch": 8.294305112927809,
      "grad_norm": 12.532403945922852,
      "learning_rate": 6.837189968130803e-06,
      "loss": 1.9512,
      "step": 59860
    },
    {
      "epoch": 8.295690730220313,
      "grad_norm": 22.274791717529297,
      "learning_rate": 6.831647498960787e-06,
      "loss": 2.1834,
      "step": 59870
    },
    {
      "epoch": 8.297076347512817,
      "grad_norm": 10.027535438537598,
      "learning_rate": 6.826105029790773e-06,
      "loss": 1.7557,
      "step": 59880
    },
    {
      "epoch": 8.298461964805322,
      "grad_norm": 9.835427284240723,
      "learning_rate": 6.820562560620757e-06,
      "loss": 1.5487,
      "step": 59890
    },
    {
      "epoch": 8.299847582097824,
      "grad_norm": 20.668716430664062,
      "learning_rate": 6.815020091450741e-06,
      "loss": 1.845,
      "step": 59900
    },
    {
      "epoch": 8.301233199390328,
      "grad_norm": 12.282350540161133,
      "learning_rate": 6.809477622280727e-06,
      "loss": 2.1211,
      "step": 59910
    },
    {
      "epoch": 8.302618816682832,
      "grad_norm": 15.268195152282715,
      "learning_rate": 6.8039351531107116e-06,
      "loss": 1.8701,
      "step": 59920
    },
    {
      "epoch": 8.304004433975336,
      "grad_norm": 17.804401397705078,
      "learning_rate": 6.798392683940697e-06,
      "loss": 1.8247,
      "step": 59930
    },
    {
      "epoch": 8.305390051267839,
      "grad_norm": 18.534732818603516,
      "learning_rate": 6.792850214770681e-06,
      "loss": 1.722,
      "step": 59940
    },
    {
      "epoch": 8.306775668560343,
      "grad_norm": 14.843191146850586,
      "learning_rate": 6.787307745600666e-06,
      "loss": 1.7773,
      "step": 59950
    },
    {
      "epoch": 8.308161285852847,
      "grad_norm": 4.972982883453369,
      "learning_rate": 6.781765276430651e-06,
      "loss": 1.9269,
      "step": 59960
    },
    {
      "epoch": 8.309546903145351,
      "grad_norm": 9.786080360412598,
      "learning_rate": 6.776222807260635e-06,
      "loss": 2.0692,
      "step": 59970
    },
    {
      "epoch": 8.310932520437856,
      "grad_norm": 14.956925392150879,
      "learning_rate": 6.77068033809062e-06,
      "loss": 1.9832,
      "step": 59980
    },
    {
      "epoch": 8.312318137730358,
      "grad_norm": 16.83487892150879,
      "learning_rate": 6.765137868920605e-06,
      "loss": 1.5981,
      "step": 59990
    },
    {
      "epoch": 8.313703755022862,
      "grad_norm": 11.004192352294922,
      "learning_rate": 6.759595399750589e-06,
      "loss": 1.6914,
      "step": 60000
    },
    {
      "epoch": 8.315089372315366,
      "grad_norm": 9.128416061401367,
      "learning_rate": 6.754052930580575e-06,
      "loss": 1.6495,
      "step": 60010
    },
    {
      "epoch": 8.31647498960787,
      "grad_norm": 13.476400375366211,
      "learning_rate": 6.748510461410559e-06,
      "loss": 1.76,
      "step": 60020
    },
    {
      "epoch": 8.317860606900375,
      "grad_norm": 14.688634872436523,
      "learning_rate": 6.742967992240543e-06,
      "loss": 1.8203,
      "step": 60030
    },
    {
      "epoch": 8.319246224192877,
      "grad_norm": 11.328211784362793,
      "learning_rate": 6.737425523070529e-06,
      "loss": 2.0745,
      "step": 60040
    },
    {
      "epoch": 8.320631841485381,
      "grad_norm": 22.34955596923828,
      "learning_rate": 6.731883053900513e-06,
      "loss": 1.9391,
      "step": 60050
    },
    {
      "epoch": 8.322017458777886,
      "grad_norm": 13.294991493225098,
      "learning_rate": 6.7263405847304974e-06,
      "loss": 1.8802,
      "step": 60060
    },
    {
      "epoch": 8.32340307607039,
      "grad_norm": 10.240287780761719,
      "learning_rate": 6.720798115560483e-06,
      "loss": 1.9056,
      "step": 60070
    },
    {
      "epoch": 8.324788693362894,
      "grad_norm": 9.000031471252441,
      "learning_rate": 6.715255646390467e-06,
      "loss": 1.9387,
      "step": 60080
    },
    {
      "epoch": 8.326174310655396,
      "grad_norm": 16.745473861694336,
      "learning_rate": 6.709713177220452e-06,
      "loss": 1.7579,
      "step": 60090
    },
    {
      "epoch": 8.3275599279479,
      "grad_norm": 13.867866516113281,
      "learning_rate": 6.704170708050437e-06,
      "loss": 1.5864,
      "step": 60100
    },
    {
      "epoch": 8.328945545240405,
      "grad_norm": 14.201447486877441,
      "learning_rate": 6.698628238880421e-06,
      "loss": 2.2403,
      "step": 60110
    },
    {
      "epoch": 8.330331162532909,
      "grad_norm": 22.35867691040039,
      "learning_rate": 6.693085769710406e-06,
      "loss": 1.7266,
      "step": 60120
    },
    {
      "epoch": 8.331716779825411,
      "grad_norm": 11.943038940429688,
      "learning_rate": 6.687543300540391e-06,
      "loss": 1.4264,
      "step": 60130
    },
    {
      "epoch": 8.333102397117916,
      "grad_norm": 11.036580085754395,
      "learning_rate": 6.682000831370377e-06,
      "loss": 1.5342,
      "step": 60140
    },
    {
      "epoch": 8.33448801441042,
      "grad_norm": 6.8714423179626465,
      "learning_rate": 6.676458362200361e-06,
      "loss": 1.5787,
      "step": 60150
    },
    {
      "epoch": 8.335873631702924,
      "grad_norm": 17.20224952697754,
      "learning_rate": 6.670915893030345e-06,
      "loss": 1.6466,
      "step": 60160
    },
    {
      "epoch": 8.337259248995428,
      "grad_norm": 14.265788078308105,
      "learning_rate": 6.665373423860331e-06,
      "loss": 1.6434,
      "step": 60170
    },
    {
      "epoch": 8.33864486628793,
      "grad_norm": 17.94489097595215,
      "learning_rate": 6.659830954690315e-06,
      "loss": 1.763,
      "step": 60180
    },
    {
      "epoch": 8.340030483580435,
      "grad_norm": 11.739227294921875,
      "learning_rate": 6.6542884855203e-06,
      "loss": 2.1955,
      "step": 60190
    },
    {
      "epoch": 8.341416100872939,
      "grad_norm": 11.734329223632812,
      "learning_rate": 6.648746016350285e-06,
      "loss": 1.5985,
      "step": 60200
    },
    {
      "epoch": 8.342801718165443,
      "grad_norm": 15.942313194274902,
      "learning_rate": 6.643203547180269e-06,
      "loss": 1.5859,
      "step": 60210
    },
    {
      "epoch": 8.344187335457946,
      "grad_norm": 15.700114250183105,
      "learning_rate": 6.637661078010255e-06,
      "loss": 2.0046,
      "step": 60220
    },
    {
      "epoch": 8.34557295275045,
      "grad_norm": 17.388931274414062,
      "learning_rate": 6.632118608840239e-06,
      "loss": 1.9912,
      "step": 60230
    },
    {
      "epoch": 8.346958570042954,
      "grad_norm": 11.916586875915527,
      "learning_rate": 6.6265761396702235e-06,
      "loss": 1.5738,
      "step": 60240
    },
    {
      "epoch": 8.348344187335458,
      "grad_norm": 12.166089057922363,
      "learning_rate": 6.621033670500209e-06,
      "loss": 1.8816,
      "step": 60250
    },
    {
      "epoch": 8.349729804627962,
      "grad_norm": 16.627178192138672,
      "learning_rate": 6.615491201330193e-06,
      "loss": 1.6295,
      "step": 60260
    },
    {
      "epoch": 8.351115421920465,
      "grad_norm": 19.237668991088867,
      "learning_rate": 6.6099487321601775e-06,
      "loss": 1.7403,
      "step": 60270
    },
    {
      "epoch": 8.352501039212969,
      "grad_norm": 15.57107162475586,
      "learning_rate": 6.604406262990163e-06,
      "loss": 1.9625,
      "step": 60280
    },
    {
      "epoch": 8.353886656505473,
      "grad_norm": 8.970113754272461,
      "learning_rate": 6.598863793820147e-06,
      "loss": 1.7018,
      "step": 60290
    },
    {
      "epoch": 8.355272273797977,
      "grad_norm": 13.670104026794434,
      "learning_rate": 6.593321324650132e-06,
      "loss": 1.9601,
      "step": 60300
    },
    {
      "epoch": 8.356657891090482,
      "grad_norm": 13.927051544189453,
      "learning_rate": 6.587778855480117e-06,
      "loss": 1.45,
      "step": 60310
    },
    {
      "epoch": 8.358043508382984,
      "grad_norm": 22.622873306274414,
      "learning_rate": 6.582236386310101e-06,
      "loss": 2.1853,
      "step": 60320
    },
    {
      "epoch": 8.359429125675488,
      "grad_norm": 14.080074310302734,
      "learning_rate": 6.5766939171400865e-06,
      "loss": 1.643,
      "step": 60330
    },
    {
      "epoch": 8.360814742967992,
      "grad_norm": 14.432887077331543,
      "learning_rate": 6.571151447970071e-06,
      "loss": 1.9115,
      "step": 60340
    },
    {
      "epoch": 8.362200360260497,
      "grad_norm": 8.552641868591309,
      "learning_rate": 6.565608978800055e-06,
      "loss": 1.3375,
      "step": 60350
    },
    {
      "epoch": 8.363585977552999,
      "grad_norm": 19.989391326904297,
      "learning_rate": 6.5600665096300405e-06,
      "loss": 1.9406,
      "step": 60360
    },
    {
      "epoch": 8.364971594845503,
      "grad_norm": 9.742911338806152,
      "learning_rate": 6.554524040460025e-06,
      "loss": 1.5066,
      "step": 60370
    },
    {
      "epoch": 8.366357212138007,
      "grad_norm": 17.681598663330078,
      "learning_rate": 6.548981571290011e-06,
      "loss": 2.0919,
      "step": 60380
    },
    {
      "epoch": 8.367742829430512,
      "grad_norm": 12.677412033081055,
      "learning_rate": 6.543439102119995e-06,
      "loss": 1.7975,
      "step": 60390
    },
    {
      "epoch": 8.369128446723016,
      "grad_norm": 16.208982467651367,
      "learning_rate": 6.537896632949979e-06,
      "loss": 2.3237,
      "step": 60400
    },
    {
      "epoch": 8.370514064015518,
      "grad_norm": 6.969752311706543,
      "learning_rate": 6.532354163779965e-06,
      "loss": 1.8194,
      "step": 60410
    },
    {
      "epoch": 8.371899681308022,
      "grad_norm": 15.609169006347656,
      "learning_rate": 6.5268116946099495e-06,
      "loss": 1.4606,
      "step": 60420
    },
    {
      "epoch": 8.373285298600527,
      "grad_norm": 16.696521759033203,
      "learning_rate": 6.521269225439935e-06,
      "loss": 2.1163,
      "step": 60430
    },
    {
      "epoch": 8.37467091589303,
      "grad_norm": 18.979591369628906,
      "learning_rate": 6.515726756269919e-06,
      "loss": 1.6536,
      "step": 60440
    },
    {
      "epoch": 8.376056533185535,
      "grad_norm": 13.782872200012207,
      "learning_rate": 6.5101842870999035e-06,
      "loss": 1.3419,
      "step": 60450
    },
    {
      "epoch": 8.377442150478037,
      "grad_norm": 10.53896713256836,
      "learning_rate": 6.504641817929889e-06,
      "loss": 1.6532,
      "step": 60460
    },
    {
      "epoch": 8.378827767770542,
      "grad_norm": 9.9948091506958,
      "learning_rate": 6.499099348759873e-06,
      "loss": 2.0497,
      "step": 60470
    },
    {
      "epoch": 8.380213385063046,
      "grad_norm": 10.024354934692383,
      "learning_rate": 6.493556879589858e-06,
      "loss": 1.7693,
      "step": 60480
    },
    {
      "epoch": 8.38159900235555,
      "grad_norm": 7.640834331512451,
      "learning_rate": 6.488014410419843e-06,
      "loss": 1.4237,
      "step": 60490
    },
    {
      "epoch": 8.382984619648052,
      "grad_norm": 10.220873832702637,
      "learning_rate": 6.482471941249827e-06,
      "loss": 1.8867,
      "step": 60500
    },
    {
      "epoch": 8.384370236940557,
      "grad_norm": 10.123723983764648,
      "learning_rate": 6.4769294720798125e-06,
      "loss": 1.6929,
      "step": 60510
    },
    {
      "epoch": 8.38575585423306,
      "grad_norm": 6.858945846557617,
      "learning_rate": 6.471387002909797e-06,
      "loss": 2.1097,
      "step": 60520
    },
    {
      "epoch": 8.387141471525565,
      "grad_norm": 21.417545318603516,
      "learning_rate": 6.465844533739781e-06,
      "loss": 1.8299,
      "step": 60530
    },
    {
      "epoch": 8.38852708881807,
      "grad_norm": 26.01575469970703,
      "learning_rate": 6.4603020645697665e-06,
      "loss": 1.6658,
      "step": 60540
    },
    {
      "epoch": 8.389912706110572,
      "grad_norm": 13.48815631866455,
      "learning_rate": 6.454759595399751e-06,
      "loss": 1.4841,
      "step": 60550
    },
    {
      "epoch": 8.391298323403076,
      "grad_norm": 13.19244384765625,
      "learning_rate": 6.449217126229735e-06,
      "loss": 2.3175,
      "step": 60560
    },
    {
      "epoch": 8.39268394069558,
      "grad_norm": 13.48602294921875,
      "learning_rate": 6.443674657059721e-06,
      "loss": 2.1101,
      "step": 60570
    },
    {
      "epoch": 8.394069557988084,
      "grad_norm": 8.380661010742188,
      "learning_rate": 6.438132187889705e-06,
      "loss": 1.3645,
      "step": 60580
    },
    {
      "epoch": 8.395455175280588,
      "grad_norm": 10.109672546386719,
      "learning_rate": 6.43258971871969e-06,
      "loss": 1.7482,
      "step": 60590
    },
    {
      "epoch": 8.39684079257309,
      "grad_norm": 15.536681175231934,
      "learning_rate": 6.427047249549675e-06,
      "loss": 2.3915,
      "step": 60600
    },
    {
      "epoch": 8.398226409865595,
      "grad_norm": 17.875717163085938,
      "learning_rate": 6.421504780379659e-06,
      "loss": 1.8845,
      "step": 60610
    },
    {
      "epoch": 8.3996120271581,
      "grad_norm": 8.318024635314941,
      "learning_rate": 6.415962311209645e-06,
      "loss": 2.2146,
      "step": 60620
    },
    {
      "epoch": 8.400997644450603,
      "grad_norm": 17.894346237182617,
      "learning_rate": 6.410419842039629e-06,
      "loss": 1.8287,
      "step": 60630
    },
    {
      "epoch": 8.402383261743106,
      "grad_norm": 7.702952861785889,
      "learning_rate": 6.404877372869613e-06,
      "loss": 1.5237,
      "step": 60640
    },
    {
      "epoch": 8.40376887903561,
      "grad_norm": 10.126229286193848,
      "learning_rate": 6.399334903699599e-06,
      "loss": 1.5034,
      "step": 60650
    },
    {
      "epoch": 8.405154496328114,
      "grad_norm": 11.141087532043457,
      "learning_rate": 6.393792434529584e-06,
      "loss": 1.6781,
      "step": 60660
    },
    {
      "epoch": 8.406540113620618,
      "grad_norm": 8.09555435180664,
      "learning_rate": 6.388249965359569e-06,
      "loss": 2.249,
      "step": 60670
    },
    {
      "epoch": 8.407925730913123,
      "grad_norm": 15.049251556396484,
      "learning_rate": 6.382707496189553e-06,
      "loss": 2.0058,
      "step": 60680
    },
    {
      "epoch": 8.409311348205625,
      "grad_norm": 21.63823127746582,
      "learning_rate": 6.377165027019538e-06,
      "loss": 1.9516,
      "step": 60690
    },
    {
      "epoch": 8.41069696549813,
      "grad_norm": 15.054495811462402,
      "learning_rate": 6.371622557849523e-06,
      "loss": 1.8119,
      "step": 60700
    },
    {
      "epoch": 8.412082582790633,
      "grad_norm": 15.484834671020508,
      "learning_rate": 6.366080088679507e-06,
      "loss": 1.8088,
      "step": 60710
    },
    {
      "epoch": 8.413468200083138,
      "grad_norm": 11.076665878295898,
      "learning_rate": 6.360537619509492e-06,
      "loss": 1.9269,
      "step": 60720
    },
    {
      "epoch": 8.41485381737564,
      "grad_norm": 18.259418487548828,
      "learning_rate": 6.354995150339477e-06,
      "loss": 1.6638,
      "step": 60730
    },
    {
      "epoch": 8.416239434668144,
      "grad_norm": 8.799959182739258,
      "learning_rate": 6.349452681169461e-06,
      "loss": 1.6225,
      "step": 60740
    },
    {
      "epoch": 8.417625051960648,
      "grad_norm": 8.207894325256348,
      "learning_rate": 6.343910211999447e-06,
      "loss": 1.7012,
      "step": 60750
    },
    {
      "epoch": 8.419010669253153,
      "grad_norm": 15.093670845031738,
      "learning_rate": 6.338367742829431e-06,
      "loss": 1.7564,
      "step": 60760
    },
    {
      "epoch": 8.420396286545657,
      "grad_norm": 26.074033737182617,
      "learning_rate": 6.332825273659415e-06,
      "loss": 1.9756,
      "step": 60770
    },
    {
      "epoch": 8.42178190383816,
      "grad_norm": 13.679527282714844,
      "learning_rate": 6.327282804489401e-06,
      "loss": 1.4412,
      "step": 60780
    },
    {
      "epoch": 8.423167521130663,
      "grad_norm": 12.616634368896484,
      "learning_rate": 6.321740335319385e-06,
      "loss": 1.551,
      "step": 60790
    },
    {
      "epoch": 8.424553138423168,
      "grad_norm": 26.45139503479004,
      "learning_rate": 6.31619786614937e-06,
      "loss": 1.8177,
      "step": 60800
    },
    {
      "epoch": 8.425938755715672,
      "grad_norm": 28.055885314941406,
      "learning_rate": 6.310655396979355e-06,
      "loss": 1.9761,
      "step": 60810
    },
    {
      "epoch": 8.427324373008176,
      "grad_norm": 23.542755126953125,
      "learning_rate": 6.305112927809339e-06,
      "loss": 1.9014,
      "step": 60820
    },
    {
      "epoch": 8.428709990300678,
      "grad_norm": 19.848634719848633,
      "learning_rate": 6.299570458639324e-06,
      "loss": 2.0581,
      "step": 60830
    },
    {
      "epoch": 8.430095607593183,
      "grad_norm": 11.841763496398926,
      "learning_rate": 6.294027989469309e-06,
      "loss": 2.1331,
      "step": 60840
    },
    {
      "epoch": 8.431481224885687,
      "grad_norm": 14.401906967163086,
      "learning_rate": 6.288485520299293e-06,
      "loss": 1.6329,
      "step": 60850
    },
    {
      "epoch": 8.432866842178191,
      "grad_norm": 14.093308448791504,
      "learning_rate": 6.282943051129278e-06,
      "loss": 1.801,
      "step": 60860
    },
    {
      "epoch": 8.434252459470693,
      "grad_norm": 20.29580307006836,
      "learning_rate": 6.277400581959263e-06,
      "loss": 1.9254,
      "step": 60870
    },
    {
      "epoch": 8.435638076763198,
      "grad_norm": 20.44900894165039,
      "learning_rate": 6.271858112789249e-06,
      "loss": 1.9996,
      "step": 60880
    },
    {
      "epoch": 8.437023694055702,
      "grad_norm": 18.505359649658203,
      "learning_rate": 6.266315643619233e-06,
      "loss": 1.997,
      "step": 60890
    },
    {
      "epoch": 8.438409311348206,
      "grad_norm": 17.80486488342285,
      "learning_rate": 6.260773174449217e-06,
      "loss": 1.8032,
      "step": 60900
    },
    {
      "epoch": 8.43979492864071,
      "grad_norm": 17.503015518188477,
      "learning_rate": 6.255230705279203e-06,
      "loss": 1.9176,
      "step": 60910
    },
    {
      "epoch": 8.441180545933213,
      "grad_norm": 15.623138427734375,
      "learning_rate": 6.249688236109187e-06,
      "loss": 1.3038,
      "step": 60920
    },
    {
      "epoch": 8.442566163225717,
      "grad_norm": 11.395350456237793,
      "learning_rate": 6.244145766939172e-06,
      "loss": 1.4168,
      "step": 60930
    },
    {
      "epoch": 8.443951780518221,
      "grad_norm": 10.713146209716797,
      "learning_rate": 6.238603297769157e-06,
      "loss": 1.5035,
      "step": 60940
    },
    {
      "epoch": 8.445337397810725,
      "grad_norm": 11.900801658630371,
      "learning_rate": 6.2330608285991414e-06,
      "loss": 1.6376,
      "step": 60950
    },
    {
      "epoch": 8.44672301510323,
      "grad_norm": 8.721404075622559,
      "learning_rate": 6.227518359429127e-06,
      "loss": 1.8555,
      "step": 60960
    },
    {
      "epoch": 8.448108632395732,
      "grad_norm": 16.4731502532959,
      "learning_rate": 6.221975890259111e-06,
      "loss": 2.3043,
      "step": 60970
    },
    {
      "epoch": 8.449494249688236,
      "grad_norm": 20.58173942565918,
      "learning_rate": 6.2164334210890955e-06,
      "loss": 1.7544,
      "step": 60980
    },
    {
      "epoch": 8.45087986698074,
      "grad_norm": 17.750581741333008,
      "learning_rate": 6.210890951919081e-06,
      "loss": 2.1,
      "step": 60990
    },
    {
      "epoch": 8.452265484273244,
      "grad_norm": 11.055330276489258,
      "learning_rate": 6.205348482749065e-06,
      "loss": 1.6368,
      "step": 61000
    },
    {
      "epoch": 8.453651101565747,
      "grad_norm": 13.481405258178711,
      "learning_rate": 6.1998060135790495e-06,
      "loss": 2.02,
      "step": 61010
    },
    {
      "epoch": 8.455036718858251,
      "grad_norm": 17.093748092651367,
      "learning_rate": 6.194263544409035e-06,
      "loss": 1.7792,
      "step": 61020
    },
    {
      "epoch": 8.456422336150755,
      "grad_norm": 19.416675567626953,
      "learning_rate": 6.188721075239019e-06,
      "loss": 1.7335,
      "step": 61030
    },
    {
      "epoch": 8.45780795344326,
      "grad_norm": 12.980568885803223,
      "learning_rate": 6.1831786060690044e-06,
      "loss": 1.9678,
      "step": 61040
    },
    {
      "epoch": 8.459193570735764,
      "grad_norm": 14.314202308654785,
      "learning_rate": 6.177636136898989e-06,
      "loss": 1.8541,
      "step": 61050
    },
    {
      "epoch": 8.460579188028266,
      "grad_norm": 11.927977561950684,
      "learning_rate": 6.172093667728973e-06,
      "loss": 1.7002,
      "step": 61060
    },
    {
      "epoch": 8.46196480532077,
      "grad_norm": 12.2063570022583,
      "learning_rate": 6.1665511985589585e-06,
      "loss": 1.9331,
      "step": 61070
    },
    {
      "epoch": 8.463350422613274,
      "grad_norm": 12.466530799865723,
      "learning_rate": 6.161008729388943e-06,
      "loss": 1.5976,
      "step": 61080
    },
    {
      "epoch": 8.464736039905778,
      "grad_norm": 18.99477767944336,
      "learning_rate": 6.155466260218929e-06,
      "loss": 1.8334,
      "step": 61090
    },
    {
      "epoch": 8.466121657198283,
      "grad_norm": 16.734329223632812,
      "learning_rate": 6.1499237910489125e-06,
      "loss": 1.7616,
      "step": 61100
    },
    {
      "epoch": 8.467507274490785,
      "grad_norm": 10.806408882141113,
      "learning_rate": 6.144381321878897e-06,
      "loss": 1.4141,
      "step": 61110
    },
    {
      "epoch": 8.46889289178329,
      "grad_norm": 14.222593307495117,
      "learning_rate": 6.138838852708883e-06,
      "loss": 1.495,
      "step": 61120
    },
    {
      "epoch": 8.470278509075793,
      "grad_norm": 12.864398002624512,
      "learning_rate": 6.1332963835388674e-06,
      "loss": 1.8389,
      "step": 61130
    },
    {
      "epoch": 8.471664126368298,
      "grad_norm": 5.793025016784668,
      "learning_rate": 6.127753914368851e-06,
      "loss": 1.5606,
      "step": 61140
    },
    {
      "epoch": 8.4730497436608,
      "grad_norm": 12.217825889587402,
      "learning_rate": 6.122211445198837e-06,
      "loss": 1.8044,
      "step": 61150
    },
    {
      "epoch": 8.474435360953304,
      "grad_norm": 11.5682373046875,
      "learning_rate": 6.1166689760288215e-06,
      "loss": 1.725,
      "step": 61160
    },
    {
      "epoch": 8.475820978245808,
      "grad_norm": 13.542511940002441,
      "learning_rate": 6.111126506858807e-06,
      "loss": 1.7593,
      "step": 61170
    },
    {
      "epoch": 8.477206595538313,
      "grad_norm": 23.581300735473633,
      "learning_rate": 6.105584037688791e-06,
      "loss": 2.0467,
      "step": 61180
    },
    {
      "epoch": 8.478592212830817,
      "grad_norm": 10.100044250488281,
      "learning_rate": 6.1000415685187756e-06,
      "loss": 1.9473,
      "step": 61190
    },
    {
      "epoch": 8.47997783012332,
      "grad_norm": 16.498809814453125,
      "learning_rate": 6.094499099348761e-06,
      "loss": 1.7688,
      "step": 61200
    },
    {
      "epoch": 8.481363447415823,
      "grad_norm": 19.871919631958008,
      "learning_rate": 6.088956630178745e-06,
      "loss": 2.0186,
      "step": 61210
    },
    {
      "epoch": 8.482749064708328,
      "grad_norm": 13.318164825439453,
      "learning_rate": 6.08341416100873e-06,
      "loss": 2.2339,
      "step": 61220
    },
    {
      "epoch": 8.484134682000832,
      "grad_norm": 9.905052185058594,
      "learning_rate": 6.077871691838715e-06,
      "loss": 1.9859,
      "step": 61230
    },
    {
      "epoch": 8.485520299293334,
      "grad_norm": 9.448246002197266,
      "learning_rate": 6.072329222668699e-06,
      "loss": 1.7044,
      "step": 61240
    },
    {
      "epoch": 8.486905916585838,
      "grad_norm": 9.716683387756348,
      "learning_rate": 6.0667867534986845e-06,
      "loss": 2.0004,
      "step": 61250
    },
    {
      "epoch": 8.488291533878343,
      "grad_norm": 27.325233459472656,
      "learning_rate": 6.061244284328669e-06,
      "loss": 1.7734,
      "step": 61260
    },
    {
      "epoch": 8.489677151170847,
      "grad_norm": 17.129053115844727,
      "learning_rate": 6.055701815158653e-06,
      "loss": 2.1257,
      "step": 61270
    },
    {
      "epoch": 8.491062768463351,
      "grad_norm": 13.15890884399414,
      "learning_rate": 6.0501593459886386e-06,
      "loss": 1.5779,
      "step": 61280
    },
    {
      "epoch": 8.492448385755853,
      "grad_norm": 11.84654426574707,
      "learning_rate": 6.044616876818623e-06,
      "loss": 1.3706,
      "step": 61290
    },
    {
      "epoch": 8.493834003048358,
      "grad_norm": 7.9285993576049805,
      "learning_rate": 6.039074407648607e-06,
      "loss": 1.6484,
      "step": 61300
    },
    {
      "epoch": 8.495219620340862,
      "grad_norm": 11.23454475402832,
      "learning_rate": 6.033531938478593e-06,
      "loss": 1.5974,
      "step": 61310
    },
    {
      "epoch": 8.496605237633366,
      "grad_norm": 13.763742446899414,
      "learning_rate": 6.027989469308577e-06,
      "loss": 2.0229,
      "step": 61320
    },
    {
      "epoch": 8.49799085492587,
      "grad_norm": 20.195953369140625,
      "learning_rate": 6.022447000138562e-06,
      "loss": 1.7159,
      "step": 61330
    },
    {
      "epoch": 8.499376472218373,
      "grad_norm": 16.887056350708008,
      "learning_rate": 6.016904530968547e-06,
      "loss": 2.1132,
      "step": 61340
    },
    {
      "epoch": 8.500762089510877,
      "grad_norm": 16.10062026977539,
      "learning_rate": 6.011362061798531e-06,
      "loss": 1.6006,
      "step": 61350
    },
    {
      "epoch": 8.502147706803381,
      "grad_norm": 10.067890167236328,
      "learning_rate": 6.005819592628517e-06,
      "loss": 2.2102,
      "step": 61360
    },
    {
      "epoch": 8.503533324095885,
      "grad_norm": 26.553436279296875,
      "learning_rate": 6.000277123458501e-06,
      "loss": 2.1464,
      "step": 61370
    },
    {
      "epoch": 8.50491894138839,
      "grad_norm": 20.360000610351562,
      "learning_rate": 5.994734654288487e-06,
      "loss": 1.6971,
      "step": 61380
    },
    {
      "epoch": 8.506304558680892,
      "grad_norm": 14.88298511505127,
      "learning_rate": 5.989192185118471e-06,
      "loss": 1.1662,
      "step": 61390
    },
    {
      "epoch": 8.507690175973396,
      "grad_norm": 12.54909896850586,
      "learning_rate": 5.983649715948456e-06,
      "loss": 1.7379,
      "step": 61400
    },
    {
      "epoch": 8.5090757932659,
      "grad_norm": 11.4429931640625,
      "learning_rate": 5.978107246778441e-06,
      "loss": 1.3557,
      "step": 61410
    },
    {
      "epoch": 8.510461410558404,
      "grad_norm": 7.98741340637207,
      "learning_rate": 5.972564777608425e-06,
      "loss": 1.6694,
      "step": 61420
    },
    {
      "epoch": 8.511847027850907,
      "grad_norm": 10.226372718811035,
      "learning_rate": 5.96702230843841e-06,
      "loss": 1.5882,
      "step": 61430
    },
    {
      "epoch": 8.513232645143411,
      "grad_norm": 9.998796463012695,
      "learning_rate": 5.961479839268395e-06,
      "loss": 1.9083,
      "step": 61440
    },
    {
      "epoch": 8.514618262435915,
      "grad_norm": 28.688514709472656,
      "learning_rate": 5.955937370098379e-06,
      "loss": 1.7662,
      "step": 61450
    },
    {
      "epoch": 8.51600387972842,
      "grad_norm": 15.36556625366211,
      "learning_rate": 5.9503949009283646e-06,
      "loss": 1.8509,
      "step": 61460
    },
    {
      "epoch": 8.517389497020924,
      "grad_norm": 11.742584228515625,
      "learning_rate": 5.944852431758349e-06,
      "loss": 1.9666,
      "step": 61470
    },
    {
      "epoch": 8.518775114313426,
      "grad_norm": 9.996787071228027,
      "learning_rate": 5.939309962588333e-06,
      "loss": 1.6855,
      "step": 61480
    },
    {
      "epoch": 8.52016073160593,
      "grad_norm": 19.490324020385742,
      "learning_rate": 5.933767493418319e-06,
      "loss": 1.6611,
      "step": 61490
    },
    {
      "epoch": 8.521546348898434,
      "grad_norm": 11.974469184875488,
      "learning_rate": 5.928225024248303e-06,
      "loss": 1.7846,
      "step": 61500
    },
    {
      "epoch": 8.522931966190939,
      "grad_norm": 10.095548629760742,
      "learning_rate": 5.9226825550782874e-06,
      "loss": 2.0541,
      "step": 61510
    },
    {
      "epoch": 8.524317583483441,
      "grad_norm": 18.503604888916016,
      "learning_rate": 5.917140085908273e-06,
      "loss": 1.7873,
      "step": 61520
    },
    {
      "epoch": 8.525703200775945,
      "grad_norm": 11.34644603729248,
      "learning_rate": 5.911597616738257e-06,
      "loss": 1.8845,
      "step": 61530
    },
    {
      "epoch": 8.52708881806845,
      "grad_norm": 9.632353782653809,
      "learning_rate": 5.906055147568242e-06,
      "loss": 1.5211,
      "step": 61540
    },
    {
      "epoch": 8.528474435360954,
      "grad_norm": 17.3505802154541,
      "learning_rate": 5.900512678398227e-06,
      "loss": 1.6476,
      "step": 61550
    },
    {
      "epoch": 8.529860052653458,
      "grad_norm": 14.661255836486816,
      "learning_rate": 5.894970209228211e-06,
      "loss": 1.9253,
      "step": 61560
    },
    {
      "epoch": 8.53124566994596,
      "grad_norm": 6.598758697509766,
      "learning_rate": 5.889427740058196e-06,
      "loss": 2.1257,
      "step": 61570
    },
    {
      "epoch": 8.532631287238464,
      "grad_norm": 10.882220268249512,
      "learning_rate": 5.883885270888181e-06,
      "loss": 1.9696,
      "step": 61580
    },
    {
      "epoch": 8.534016904530969,
      "grad_norm": 17.627033233642578,
      "learning_rate": 5.878342801718165e-06,
      "loss": 1.6607,
      "step": 61590
    },
    {
      "epoch": 8.535402521823473,
      "grad_norm": 9.283535957336426,
      "learning_rate": 5.8728003325481504e-06,
      "loss": 1.7703,
      "step": 61600
    },
    {
      "epoch": 8.536788139115977,
      "grad_norm": 21.448240280151367,
      "learning_rate": 5.867257863378135e-06,
      "loss": 1.9228,
      "step": 61610
    },
    {
      "epoch": 8.53817375640848,
      "grad_norm": 22.88302230834961,
      "learning_rate": 5.861715394208121e-06,
      "loss": 1.9427,
      "step": 61620
    },
    {
      "epoch": 8.539559373700984,
      "grad_norm": 14.687492370605469,
      "learning_rate": 5.856172925038105e-06,
      "loss": 1.8385,
      "step": 61630
    },
    {
      "epoch": 8.540944990993488,
      "grad_norm": 13.805463790893555,
      "learning_rate": 5.850630455868089e-06,
      "loss": 1.4426,
      "step": 61640
    },
    {
      "epoch": 8.542330608285992,
      "grad_norm": 16.62352752685547,
      "learning_rate": 5.845087986698075e-06,
      "loss": 1.4154,
      "step": 61650
    },
    {
      "epoch": 8.543716225578494,
      "grad_norm": 9.840442657470703,
      "learning_rate": 5.839545517528059e-06,
      "loss": 1.6342,
      "step": 61660
    },
    {
      "epoch": 8.545101842870999,
      "grad_norm": 9.420167922973633,
      "learning_rate": 5.834003048358045e-06,
      "loss": 1.5466,
      "step": 61670
    },
    {
      "epoch": 8.546487460163503,
      "grad_norm": 13.677691459655762,
      "learning_rate": 5.828460579188029e-06,
      "loss": 1.9111,
      "step": 61680
    },
    {
      "epoch": 8.547873077456007,
      "grad_norm": 17.78928565979004,
      "learning_rate": 5.8229181100180135e-06,
      "loss": 1.5847,
      "step": 61690
    },
    {
      "epoch": 8.549258694748511,
      "grad_norm": 18.527494430541992,
      "learning_rate": 5.817375640847999e-06,
      "loss": 1.9297,
      "step": 61700
    },
    {
      "epoch": 8.550644312041014,
      "grad_norm": 12.656061172485352,
      "learning_rate": 5.811833171677983e-06,
      "loss": 1.437,
      "step": 61710
    },
    {
      "epoch": 8.552029929333518,
      "grad_norm": 15.966341018676758,
      "learning_rate": 5.8062907025079675e-06,
      "loss": 2.288,
      "step": 61720
    },
    {
      "epoch": 8.553415546626022,
      "grad_norm": 5.871904373168945,
      "learning_rate": 5.800748233337953e-06,
      "loss": 1.5831,
      "step": 61730
    },
    {
      "epoch": 8.554801163918526,
      "grad_norm": 10.463897705078125,
      "learning_rate": 5.795205764167937e-06,
      "loss": 1.4742,
      "step": 61740
    },
    {
      "epoch": 8.556186781211029,
      "grad_norm": 16.57365608215332,
      "learning_rate": 5.789663294997922e-06,
      "loss": 1.4091,
      "step": 61750
    },
    {
      "epoch": 8.557572398503533,
      "grad_norm": 12.70382308959961,
      "learning_rate": 5.784120825827907e-06,
      "loss": 2.1098,
      "step": 61760
    },
    {
      "epoch": 8.558958015796037,
      "grad_norm": 10.890703201293945,
      "learning_rate": 5.778578356657891e-06,
      "loss": 1.9931,
      "step": 61770
    },
    {
      "epoch": 8.560343633088541,
      "grad_norm": 18.599349975585938,
      "learning_rate": 5.7730358874878765e-06,
      "loss": 1.9524,
      "step": 61780
    },
    {
      "epoch": 8.561729250381045,
      "grad_norm": 10.791993141174316,
      "learning_rate": 5.767493418317861e-06,
      "loss": 1.7543,
      "step": 61790
    },
    {
      "epoch": 8.563114867673548,
      "grad_norm": 13.540785789489746,
      "learning_rate": 5.761950949147845e-06,
      "loss": 2.004,
      "step": 61800
    },
    {
      "epoch": 8.564500484966052,
      "grad_norm": 12.423829078674316,
      "learning_rate": 5.7564084799778305e-06,
      "loss": 2.041,
      "step": 61810
    },
    {
      "epoch": 8.565886102258556,
      "grad_norm": 12.810137748718262,
      "learning_rate": 5.750866010807815e-06,
      "loss": 1.685,
      "step": 61820
    },
    {
      "epoch": 8.56727171955106,
      "grad_norm": 11.51386833190918,
      "learning_rate": 5.745323541637801e-06,
      "loss": 1.4591,
      "step": 61830
    },
    {
      "epoch": 8.568657336843565,
      "grad_norm": 14.168554306030273,
      "learning_rate": 5.7397810724677846e-06,
      "loss": 1.5117,
      "step": 61840
    },
    {
      "epoch": 8.570042954136067,
      "grad_norm": 10.988896369934082,
      "learning_rate": 5.734238603297769e-06,
      "loss": 1.8189,
      "step": 61850
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 12.975425720214844,
      "learning_rate": 5.728696134127755e-06,
      "loss": 1.4419,
      "step": 61860
    },
    {
      "epoch": 8.572814188721075,
      "grad_norm": 16.029138565063477,
      "learning_rate": 5.7231536649577395e-06,
      "loss": 1.7442,
      "step": 61870
    },
    {
      "epoch": 8.57419980601358,
      "grad_norm": 27.52664566040039,
      "learning_rate": 5.717611195787723e-06,
      "loss": 1.3535,
      "step": 61880
    },
    {
      "epoch": 8.575585423306084,
      "grad_norm": 15.347484588623047,
      "learning_rate": 5.712068726617709e-06,
      "loss": 1.8609,
      "step": 61890
    },
    {
      "epoch": 8.576971040598586,
      "grad_norm": 19.942325592041016,
      "learning_rate": 5.7065262574476935e-06,
      "loss": 1.7187,
      "step": 61900
    },
    {
      "epoch": 8.57835665789109,
      "grad_norm": 11.696895599365234,
      "learning_rate": 5.700983788277679e-06,
      "loss": 2.0701,
      "step": 61910
    },
    {
      "epoch": 8.579742275183595,
      "grad_norm": 7.458334922790527,
      "learning_rate": 5.695441319107663e-06,
      "loss": 1.9547,
      "step": 61920
    },
    {
      "epoch": 8.581127892476099,
      "grad_norm": 15.568492889404297,
      "learning_rate": 5.6898988499376476e-06,
      "loss": 1.9179,
      "step": 61930
    },
    {
      "epoch": 8.582513509768601,
      "grad_norm": 12.417105674743652,
      "learning_rate": 5.684356380767633e-06,
      "loss": 1.9049,
      "step": 61940
    },
    {
      "epoch": 8.583899127061105,
      "grad_norm": 17.249502182006836,
      "learning_rate": 5.678813911597617e-06,
      "loss": 1.6808,
      "step": 61950
    },
    {
      "epoch": 8.58528474435361,
      "grad_norm": 21.084074020385742,
      "learning_rate": 5.673271442427602e-06,
      "loss": 1.7035,
      "step": 61960
    },
    {
      "epoch": 8.586670361646114,
      "grad_norm": 20.165151596069336,
      "learning_rate": 5.667728973257587e-06,
      "loss": 1.5731,
      "step": 61970
    },
    {
      "epoch": 8.588055978938618,
      "grad_norm": 10.04245376586914,
      "learning_rate": 5.662186504087571e-06,
      "loss": 2.0632,
      "step": 61980
    },
    {
      "epoch": 8.58944159623112,
      "grad_norm": 13.0095853805542,
      "learning_rate": 5.6566440349175565e-06,
      "loss": 1.9386,
      "step": 61990
    },
    {
      "epoch": 8.590827213523625,
      "grad_norm": 28.01375389099121,
      "learning_rate": 5.651101565747541e-06,
      "loss": 2.3261,
      "step": 62000
    },
    {
      "epoch": 8.592212830816129,
      "grad_norm": 9.157097816467285,
      "learning_rate": 5.645559096577525e-06,
      "loss": 1.6008,
      "step": 62010
    },
    {
      "epoch": 8.593598448108633,
      "grad_norm": 16.319711685180664,
      "learning_rate": 5.640016627407511e-06,
      "loss": 1.6625,
      "step": 62020
    },
    {
      "epoch": 8.594984065401135,
      "grad_norm": 27.457225799560547,
      "learning_rate": 5.634474158237495e-06,
      "loss": 2.1833,
      "step": 62030
    },
    {
      "epoch": 8.59636968269364,
      "grad_norm": 9.34826946258545,
      "learning_rate": 5.62893168906748e-06,
      "loss": 2.0352,
      "step": 62040
    },
    {
      "epoch": 8.597755299986144,
      "grad_norm": 12.502079963684082,
      "learning_rate": 5.623389219897465e-06,
      "loss": 1.7994,
      "step": 62050
    },
    {
      "epoch": 8.599140917278648,
      "grad_norm": 12.849848747253418,
      "learning_rate": 5.617846750727449e-06,
      "loss": 1.7427,
      "step": 62060
    },
    {
      "epoch": 8.600526534571152,
      "grad_norm": 16.85944175720215,
      "learning_rate": 5.612304281557434e-06,
      "loss": 1.6737,
      "step": 62070
    },
    {
      "epoch": 8.601912151863655,
      "grad_norm": 6.523629188537598,
      "learning_rate": 5.606761812387419e-06,
      "loss": 1.8306,
      "step": 62080
    },
    {
      "epoch": 8.603297769156159,
      "grad_norm": 8.099065780639648,
      "learning_rate": 5.601219343217403e-06,
      "loss": 1.8923,
      "step": 62090
    },
    {
      "epoch": 8.604683386448663,
      "grad_norm": 8.103608131408691,
      "learning_rate": 5.595676874047389e-06,
      "loss": 2.193,
      "step": 62100
    },
    {
      "epoch": 8.606069003741167,
      "grad_norm": 15.152924537658691,
      "learning_rate": 5.590134404877373e-06,
      "loss": 1.7559,
      "step": 62110
    },
    {
      "epoch": 8.607454621033671,
      "grad_norm": 22.171770095825195,
      "learning_rate": 5.584591935707359e-06,
      "loss": 1.5642,
      "step": 62120
    },
    {
      "epoch": 8.608840238326174,
      "grad_norm": 16.001323699951172,
      "learning_rate": 5.579049466537343e-06,
      "loss": 1.7106,
      "step": 62130
    },
    {
      "epoch": 8.610225855618678,
      "grad_norm": 10.77430248260498,
      "learning_rate": 5.573506997367328e-06,
      "loss": 1.9263,
      "step": 62140
    },
    {
      "epoch": 8.611611472911182,
      "grad_norm": 12.79565715789795,
      "learning_rate": 5.567964528197313e-06,
      "loss": 2.0962,
      "step": 62150
    },
    {
      "epoch": 8.612997090203686,
      "grad_norm": 14.764081001281738,
      "learning_rate": 5.562422059027297e-06,
      "loss": 1.6846,
      "step": 62160
    },
    {
      "epoch": 8.614382707496189,
      "grad_norm": 7.227125644683838,
      "learning_rate": 5.556879589857282e-06,
      "loss": 1.8192,
      "step": 62170
    },
    {
      "epoch": 8.615768324788693,
      "grad_norm": 21.80853271484375,
      "learning_rate": 5.551337120687267e-06,
      "loss": 2.1651,
      "step": 62180
    },
    {
      "epoch": 8.617153942081197,
      "grad_norm": 17.94033432006836,
      "learning_rate": 5.545794651517251e-06,
      "loss": 1.9033,
      "step": 62190
    },
    {
      "epoch": 8.618539559373701,
      "grad_norm": 14.454504013061523,
      "learning_rate": 5.540252182347237e-06,
      "loss": 1.6406,
      "step": 62200
    },
    {
      "epoch": 8.619925176666206,
      "grad_norm": 15.170381546020508,
      "learning_rate": 5.534709713177221e-06,
      "loss": 1.8568,
      "step": 62210
    },
    {
      "epoch": 8.621310793958708,
      "grad_norm": 11.700806617736816,
      "learning_rate": 5.529167244007205e-06,
      "loss": 1.7261,
      "step": 62220
    },
    {
      "epoch": 8.622696411251212,
      "grad_norm": 11.156098365783691,
      "learning_rate": 5.523624774837191e-06,
      "loss": 1.781,
      "step": 62230
    },
    {
      "epoch": 8.624082028543716,
      "grad_norm": 7.671278476715088,
      "learning_rate": 5.518082305667175e-06,
      "loss": 2.2558,
      "step": 62240
    },
    {
      "epoch": 8.62546764583622,
      "grad_norm": 6.642171859741211,
      "learning_rate": 5.5125398364971595e-06,
      "loss": 1.5185,
      "step": 62250
    },
    {
      "epoch": 8.626853263128723,
      "grad_norm": 12.781670570373535,
      "learning_rate": 5.506997367327145e-06,
      "loss": 1.8836,
      "step": 62260
    },
    {
      "epoch": 8.628238880421227,
      "grad_norm": 11.69694709777832,
      "learning_rate": 5.501454898157129e-06,
      "loss": 1.6783,
      "step": 62270
    },
    {
      "epoch": 8.629624497713731,
      "grad_norm": 26.29814338684082,
      "learning_rate": 5.495912428987114e-06,
      "loss": 2.0972,
      "step": 62280
    },
    {
      "epoch": 8.631010115006235,
      "grad_norm": 17.339519500732422,
      "learning_rate": 5.490369959817099e-06,
      "loss": 1.9389,
      "step": 62290
    },
    {
      "epoch": 8.63239573229874,
      "grad_norm": 18.330020904541016,
      "learning_rate": 5.484827490647083e-06,
      "loss": 1.67,
      "step": 62300
    },
    {
      "epoch": 8.633781349591242,
      "grad_norm": 20.540977478027344,
      "learning_rate": 5.479285021477068e-06,
      "loss": 2.0206,
      "step": 62310
    },
    {
      "epoch": 8.635166966883746,
      "grad_norm": 12.402555465698242,
      "learning_rate": 5.473742552307053e-06,
      "loss": 1.814,
      "step": 62320
    },
    {
      "epoch": 8.63655258417625,
      "grad_norm": 11.732503890991211,
      "learning_rate": 5.468200083137039e-06,
      "loss": 1.6597,
      "step": 62330
    },
    {
      "epoch": 8.637938201468755,
      "grad_norm": 10.800801277160645,
      "learning_rate": 5.4626576139670225e-06,
      "loss": 1.7339,
      "step": 62340
    },
    {
      "epoch": 8.639323818761259,
      "grad_norm": 9.89618968963623,
      "learning_rate": 5.457115144797007e-06,
      "loss": 2.294,
      "step": 62350
    },
    {
      "epoch": 8.640709436053761,
      "grad_norm": 12.270345687866211,
      "learning_rate": 5.451572675626993e-06,
      "loss": 1.9243,
      "step": 62360
    },
    {
      "epoch": 8.642095053346265,
      "grad_norm": 18.829133987426758,
      "learning_rate": 5.446030206456977e-06,
      "loss": 1.8319,
      "step": 62370
    },
    {
      "epoch": 8.64348067063877,
      "grad_norm": 9.860042572021484,
      "learning_rate": 5.440487737286961e-06,
      "loss": 1.4728,
      "step": 62380
    },
    {
      "epoch": 8.644866287931274,
      "grad_norm": 14.280009269714355,
      "learning_rate": 5.434945268116947e-06,
      "loss": 1.4907,
      "step": 62390
    },
    {
      "epoch": 8.646251905223778,
      "grad_norm": 18.940099716186523,
      "learning_rate": 5.429402798946931e-06,
      "loss": 1.8654,
      "step": 62400
    },
    {
      "epoch": 8.64763752251628,
      "grad_norm": 7.527910232543945,
      "learning_rate": 5.423860329776917e-06,
      "loss": 1.7642,
      "step": 62410
    },
    {
      "epoch": 8.649023139808785,
      "grad_norm": 6.158313274383545,
      "learning_rate": 5.418872107523902e-06,
      "loss": 1.5023,
      "step": 62420
    },
    {
      "epoch": 8.650408757101289,
      "grad_norm": 12.915483474731445,
      "learning_rate": 5.413329638353887e-06,
      "loss": 1.3402,
      "step": 62430
    },
    {
      "epoch": 8.651794374393793,
      "grad_norm": 15.10372257232666,
      "learning_rate": 5.407787169183873e-06,
      "loss": 1.7086,
      "step": 62440
    },
    {
      "epoch": 8.653179991686295,
      "grad_norm": 11.267203330993652,
      "learning_rate": 5.402244700013857e-06,
      "loss": 2.1627,
      "step": 62450
    },
    {
      "epoch": 8.6545656089788,
      "grad_norm": 11.310460090637207,
      "learning_rate": 5.396702230843841e-06,
      "loss": 1.5478,
      "step": 62460
    },
    {
      "epoch": 8.655951226271304,
      "grad_norm": 8.82297134399414,
      "learning_rate": 5.391159761673827e-06,
      "loss": 1.4717,
      "step": 62470
    },
    {
      "epoch": 8.657336843563808,
      "grad_norm": 12.56529712677002,
      "learning_rate": 5.385617292503811e-06,
      "loss": 1.7849,
      "step": 62480
    },
    {
      "epoch": 8.658722460856312,
      "grad_norm": 15.1085205078125,
      "learning_rate": 5.3800748233337956e-06,
      "loss": 1.9716,
      "step": 62490
    },
    {
      "epoch": 8.660108078148815,
      "grad_norm": 10.165959358215332,
      "learning_rate": 5.374532354163781e-06,
      "loss": 1.3723,
      "step": 62500
    },
    {
      "epoch": 8.661493695441319,
      "grad_norm": 15.22922420501709,
      "learning_rate": 5.368989884993765e-06,
      "loss": 1.7657,
      "step": 62510
    },
    {
      "epoch": 8.662879312733823,
      "grad_norm": 11.21177864074707,
      "learning_rate": 5.3634474158237505e-06,
      "loss": 1.9881,
      "step": 62520
    },
    {
      "epoch": 8.664264930026327,
      "grad_norm": 17.766860961914062,
      "learning_rate": 5.357904946653735e-06,
      "loss": 1.8582,
      "step": 62530
    },
    {
      "epoch": 8.66565054731883,
      "grad_norm": 12.080766677856445,
      "learning_rate": 5.352362477483719e-06,
      "loss": 1.7596,
      "step": 62540
    },
    {
      "epoch": 8.667036164611334,
      "grad_norm": 7.561325550079346,
      "learning_rate": 5.3468200083137045e-06,
      "loss": 1.4604,
      "step": 62550
    },
    {
      "epoch": 8.668421781903838,
      "grad_norm": 29.342500686645508,
      "learning_rate": 5.341277539143689e-06,
      "loss": 1.9904,
      "step": 62560
    },
    {
      "epoch": 8.669807399196342,
      "grad_norm": 11.217934608459473,
      "learning_rate": 5.335735069973673e-06,
      "loss": 1.6294,
      "step": 62570
    },
    {
      "epoch": 8.671193016488846,
      "grad_norm": 9.992738723754883,
      "learning_rate": 5.3301926008036586e-06,
      "loss": 2.3996,
      "step": 62580
    },
    {
      "epoch": 8.672578633781349,
      "grad_norm": 14.611551284790039,
      "learning_rate": 5.324650131633643e-06,
      "loss": 2.1111,
      "step": 62590
    },
    {
      "epoch": 8.673964251073853,
      "grad_norm": 16.067834854125977,
      "learning_rate": 5.319107662463628e-06,
      "loss": 2.3204,
      "step": 62600
    },
    {
      "epoch": 8.675349868366357,
      "grad_norm": 12.931116104125977,
      "learning_rate": 5.313565193293613e-06,
      "loss": 2.1069,
      "step": 62610
    },
    {
      "epoch": 8.676735485658861,
      "grad_norm": 11.171581268310547,
      "learning_rate": 5.308022724123597e-06,
      "loss": 1.832,
      "step": 62620
    },
    {
      "epoch": 8.678121102951366,
      "grad_norm": 24.229761123657227,
      "learning_rate": 5.302480254953582e-06,
      "loss": 2.0416,
      "step": 62630
    },
    {
      "epoch": 8.679506720243868,
      "grad_norm": 10.879809379577637,
      "learning_rate": 5.296937785783567e-06,
      "loss": 1.1179,
      "step": 62640
    },
    {
      "epoch": 8.680892337536372,
      "grad_norm": 13.224574089050293,
      "learning_rate": 5.291395316613552e-06,
      "loss": 1.9787,
      "step": 62650
    },
    {
      "epoch": 8.682277954828876,
      "grad_norm": 9.357122421264648,
      "learning_rate": 5.285852847443536e-06,
      "loss": 1.8103,
      "step": 62660
    },
    {
      "epoch": 8.68366357212138,
      "grad_norm": 9.904427528381348,
      "learning_rate": 5.280310378273521e-06,
      "loss": 1.7089,
      "step": 62670
    },
    {
      "epoch": 8.685049189413885,
      "grad_norm": 10.782495498657227,
      "learning_rate": 5.274767909103507e-06,
      "loss": 1.6659,
      "step": 62680
    },
    {
      "epoch": 8.686434806706387,
      "grad_norm": 6.7543044090271,
      "learning_rate": 5.26922543993349e-06,
      "loss": 1.3703,
      "step": 62690
    },
    {
      "epoch": 8.687820423998891,
      "grad_norm": 13.062597274780273,
      "learning_rate": 5.263682970763475e-06,
      "loss": 1.7184,
      "step": 62700
    },
    {
      "epoch": 8.689206041291396,
      "grad_norm": 21.2896671295166,
      "learning_rate": 5.258140501593461e-06,
      "loss": 2.123,
      "step": 62710
    },
    {
      "epoch": 8.6905916585839,
      "grad_norm": 13.079686164855957,
      "learning_rate": 5.252598032423445e-06,
      "loss": 1.7364,
      "step": 62720
    },
    {
      "epoch": 8.691977275876402,
      "grad_norm": 15.73352336883545,
      "learning_rate": 5.2470555632534305e-06,
      "loss": 1.6896,
      "step": 62730
    },
    {
      "epoch": 8.693362893168906,
      "grad_norm": 11.177041053771973,
      "learning_rate": 5.241513094083415e-06,
      "loss": 1.8603,
      "step": 62740
    },
    {
      "epoch": 8.69474851046141,
      "grad_norm": 13.385961532592773,
      "learning_rate": 5.235970624913399e-06,
      "loss": 1.815,
      "step": 62750
    },
    {
      "epoch": 8.696134127753915,
      "grad_norm": 7.54419469833374,
      "learning_rate": 5.230428155743385e-06,
      "loss": 1.6521,
      "step": 62760
    },
    {
      "epoch": 8.697519745046419,
      "grad_norm": 6.822971343994141,
      "learning_rate": 5.224885686573369e-06,
      "loss": 1.8252,
      "step": 62770
    },
    {
      "epoch": 8.698905362338921,
      "grad_norm": 9.303971290588379,
      "learning_rate": 5.219343217403353e-06,
      "loss": 1.9901,
      "step": 62780
    },
    {
      "epoch": 8.700290979631426,
      "grad_norm": 12.219131469726562,
      "learning_rate": 5.213800748233339e-06,
      "loss": 1.9364,
      "step": 62790
    },
    {
      "epoch": 8.70167659692393,
      "grad_norm": 18.64993667602539,
      "learning_rate": 5.208258279063323e-06,
      "loss": 1.9877,
      "step": 62800
    },
    {
      "epoch": 8.703062214216434,
      "grad_norm": 16.757749557495117,
      "learning_rate": 5.202715809893308e-06,
      "loss": 1.8761,
      "step": 62810
    },
    {
      "epoch": 8.704447831508936,
      "grad_norm": 14.881125450134277,
      "learning_rate": 5.197173340723293e-06,
      "loss": 2.0315,
      "step": 62820
    },
    {
      "epoch": 8.70583344880144,
      "grad_norm": 8.834479331970215,
      "learning_rate": 5.191630871553277e-06,
      "loss": 1.7221,
      "step": 62830
    },
    {
      "epoch": 8.707219066093945,
      "grad_norm": 14.691996574401855,
      "learning_rate": 5.186088402383262e-06,
      "loss": 1.3173,
      "step": 62840
    },
    {
      "epoch": 8.708604683386449,
      "grad_norm": 10.056828498840332,
      "learning_rate": 5.180545933213247e-06,
      "loss": 1.5787,
      "step": 62850
    },
    {
      "epoch": 8.709990300678953,
      "grad_norm": 14.864571571350098,
      "learning_rate": 5.175003464043231e-06,
      "loss": 1.7911,
      "step": 62860
    },
    {
      "epoch": 8.711375917971456,
      "grad_norm": 14.42105484008789,
      "learning_rate": 5.169460994873216e-06,
      "loss": 1.9248,
      "step": 62870
    },
    {
      "epoch": 8.71276153526396,
      "grad_norm": 12.60835075378418,
      "learning_rate": 5.163918525703201e-06,
      "loss": 1.458,
      "step": 62880
    },
    {
      "epoch": 8.714147152556464,
      "grad_norm": 25.063871383666992,
      "learning_rate": 5.158376056533186e-06,
      "loss": 1.9373,
      "step": 62890
    },
    {
      "epoch": 8.715532769848968,
      "grad_norm": 19.37778091430664,
      "learning_rate": 5.1528335873631705e-06,
      "loss": 1.7588,
      "step": 62900
    },
    {
      "epoch": 8.716918387141472,
      "grad_norm": 17.415861129760742,
      "learning_rate": 5.147291118193155e-06,
      "loss": 1.5491,
      "step": 62910
    },
    {
      "epoch": 8.718304004433975,
      "grad_norm": 17.83830451965332,
      "learning_rate": 5.14174864902314e-06,
      "loss": 1.9621,
      "step": 62920
    },
    {
      "epoch": 8.719689621726479,
      "grad_norm": 16.228897094726562,
      "learning_rate": 5.1362061798531245e-06,
      "loss": 1.4566,
      "step": 62930
    },
    {
      "epoch": 8.721075239018983,
      "grad_norm": 14.740241050720215,
      "learning_rate": 5.130663710683109e-06,
      "loss": 1.5512,
      "step": 62940
    },
    {
      "epoch": 8.722460856311487,
      "grad_norm": 10.080902099609375,
      "learning_rate": 5.125121241513095e-06,
      "loss": 1.5223,
      "step": 62950
    },
    {
      "epoch": 8.72384647360399,
      "grad_norm": 19.28205108642578,
      "learning_rate": 5.1195787723430786e-06,
      "loss": 2.1083,
      "step": 62960
    },
    {
      "epoch": 8.725232090896494,
      "grad_norm": 12.272896766662598,
      "learning_rate": 5.114036303173065e-06,
      "loss": 1.7656,
      "step": 62970
    },
    {
      "epoch": 8.726617708188998,
      "grad_norm": 11.8438081741333,
      "learning_rate": 5.108493834003049e-06,
      "loss": 1.7204,
      "step": 62980
    },
    {
      "epoch": 8.728003325481502,
      "grad_norm": 12.015830993652344,
      "learning_rate": 5.1029513648330335e-06,
      "loss": 1.8346,
      "step": 62990
    },
    {
      "epoch": 8.729388942774007,
      "grad_norm": 8.903362274169922,
      "learning_rate": 5.097408895663019e-06,
      "loss": 1.6205,
      "step": 63000
    },
    {
      "epoch": 8.730774560066509,
      "grad_norm": 17.32916259765625,
      "learning_rate": 5.091866426493003e-06,
      "loss": 1.8663,
      "step": 63010
    },
    {
      "epoch": 8.732160177359013,
      "grad_norm": 5.956333637237549,
      "learning_rate": 5.086323957322988e-06,
      "loss": 1.4645,
      "step": 63020
    },
    {
      "epoch": 8.733545794651517,
      "grad_norm": 7.233635425567627,
      "learning_rate": 5.080781488152973e-06,
      "loss": 1.9172,
      "step": 63030
    },
    {
      "epoch": 8.734931411944022,
      "grad_norm": 14.039650917053223,
      "learning_rate": 5.075239018982957e-06,
      "loss": 1.8171,
      "step": 63040
    },
    {
      "epoch": 8.736317029236524,
      "grad_norm": 9.535636901855469,
      "learning_rate": 5.069696549812942e-06,
      "loss": 1.4766,
      "step": 63050
    },
    {
      "epoch": 8.737702646529028,
      "grad_norm": 13.932086944580078,
      "learning_rate": 5.064154080642927e-06,
      "loss": 1.6088,
      "step": 63060
    },
    {
      "epoch": 8.739088263821532,
      "grad_norm": 6.7269368171691895,
      "learning_rate": 5.058611611472911e-06,
      "loss": 1.429,
      "step": 63070
    },
    {
      "epoch": 8.740473881114037,
      "grad_norm": 13.868095397949219,
      "learning_rate": 5.0530691423028965e-06,
      "loss": 1.4462,
      "step": 63080
    },
    {
      "epoch": 8.74185949840654,
      "grad_norm": 19.222829818725586,
      "learning_rate": 5.047526673132881e-06,
      "loss": 1.3984,
      "step": 63090
    },
    {
      "epoch": 8.743245115699043,
      "grad_norm": 19.511259078979492,
      "learning_rate": 5.041984203962866e-06,
      "loss": 2.2936,
      "step": 63100
    },
    {
      "epoch": 8.744630732991547,
      "grad_norm": 12.732091903686523,
      "learning_rate": 5.0364417347928505e-06,
      "loss": 1.9108,
      "step": 63110
    },
    {
      "epoch": 8.746016350284052,
      "grad_norm": 7.667689323425293,
      "learning_rate": 5.030899265622835e-06,
      "loss": 1.7638,
      "step": 63120
    },
    {
      "epoch": 8.747401967576556,
      "grad_norm": 15.076803207397461,
      "learning_rate": 5.02535679645282e-06,
      "loss": 2.242,
      "step": 63130
    },
    {
      "epoch": 8.74878758486906,
      "grad_norm": 16.184362411499023,
      "learning_rate": 5.019814327282805e-06,
      "loss": 1.7626,
      "step": 63140
    },
    {
      "epoch": 8.750173202161562,
      "grad_norm": 14.270710945129395,
      "learning_rate": 5.014271858112789e-06,
      "loss": 1.8559,
      "step": 63150
    },
    {
      "epoch": 8.751558819454067,
      "grad_norm": 12.531105041503906,
      "learning_rate": 5.008729388942774e-06,
      "loss": 1.892,
      "step": 63160
    },
    {
      "epoch": 8.75294443674657,
      "grad_norm": 15.318168640136719,
      "learning_rate": 5.003186919772759e-06,
      "loss": 1.885,
      "step": 63170
    },
    {
      "epoch": 8.754330054039075,
      "grad_norm": 13.452950477600098,
      "learning_rate": 4.997644450602744e-06,
      "loss": 1.9363,
      "step": 63180
    },
    {
      "epoch": 8.75571567133158,
      "grad_norm": 11.009367942810059,
      "learning_rate": 4.992101981432729e-06,
      "loss": 1.3356,
      "step": 63190
    },
    {
      "epoch": 8.757101288624082,
      "grad_norm": 15.382522583007812,
      "learning_rate": 4.9865595122627135e-06,
      "loss": 1.7149,
      "step": 63200
    },
    {
      "epoch": 8.758486905916586,
      "grad_norm": 14.363783836364746,
      "learning_rate": 4.981017043092698e-06,
      "loss": 1.6752,
      "step": 63210
    },
    {
      "epoch": 8.75987252320909,
      "grad_norm": 11.407318115234375,
      "learning_rate": 4.975474573922683e-06,
      "loss": 2.1399,
      "step": 63220
    },
    {
      "epoch": 8.761258140501594,
      "grad_norm": 9.461670875549316,
      "learning_rate": 4.969932104752668e-06,
      "loss": 1.7686,
      "step": 63230
    },
    {
      "epoch": 8.762643757794097,
      "grad_norm": 11.281888961791992,
      "learning_rate": 4.964389635582653e-06,
      "loss": 1.7424,
      "step": 63240
    },
    {
      "epoch": 8.7640293750866,
      "grad_norm": 10.959132194519043,
      "learning_rate": 4.958847166412637e-06,
      "loss": 1.9539,
      "step": 63250
    },
    {
      "epoch": 8.765414992379105,
      "grad_norm": 15.664097785949707,
      "learning_rate": 4.953304697242622e-06,
      "loss": 2.2206,
      "step": 63260
    },
    {
      "epoch": 8.76680060967161,
      "grad_norm": 20.601194381713867,
      "learning_rate": 4.947762228072607e-06,
      "loss": 1.6405,
      "step": 63270
    },
    {
      "epoch": 8.768186226964113,
      "grad_norm": 12.448343276977539,
      "learning_rate": 4.942219758902591e-06,
      "loss": 1.9912,
      "step": 63280
    },
    {
      "epoch": 8.769571844256616,
      "grad_norm": 18.87119483947754,
      "learning_rate": 4.9366772897325765e-06,
      "loss": 1.6908,
      "step": 63290
    },
    {
      "epoch": 8.77095746154912,
      "grad_norm": 8.449320793151855,
      "learning_rate": 4.931134820562561e-06,
      "loss": 1.3963,
      "step": 63300
    },
    {
      "epoch": 8.772343078841624,
      "grad_norm": 19.55357551574707,
      "learning_rate": 4.925592351392545e-06,
      "loss": 1.6721,
      "step": 63310
    },
    {
      "epoch": 8.773728696134128,
      "grad_norm": 35.516197204589844,
      "learning_rate": 4.920049882222531e-06,
      "loss": 2.1039,
      "step": 63320
    },
    {
      "epoch": 8.77511431342663,
      "grad_norm": 14.1985445022583,
      "learning_rate": 4.914507413052516e-06,
      "loss": 2.0999,
      "step": 63330
    },
    {
      "epoch": 8.776499930719135,
      "grad_norm": 13.178043365478516,
      "learning_rate": 4.9089649438825e-06,
      "loss": 1.8017,
      "step": 63340
    },
    {
      "epoch": 8.77788554801164,
      "grad_norm": 17.81059455871582,
      "learning_rate": 4.903422474712485e-06,
      "loss": 1.9669,
      "step": 63350
    },
    {
      "epoch": 8.779271165304143,
      "grad_norm": 9.635263442993164,
      "learning_rate": 4.89788000554247e-06,
      "loss": 1.5901,
      "step": 63360
    },
    {
      "epoch": 8.780656782596648,
      "grad_norm": 16.56868553161621,
      "learning_rate": 4.8928917832894554e-06,
      "loss": 1.795,
      "step": 63370
    },
    {
      "epoch": 8.78204239988915,
      "grad_norm": 24.10676383972168,
      "learning_rate": 4.887349314119441e-06,
      "loss": 1.2232,
      "step": 63380
    },
    {
      "epoch": 8.783428017181654,
      "grad_norm": 11.630282402038574,
      "learning_rate": 4.881806844949425e-06,
      "loss": 1.5628,
      "step": 63390
    },
    {
      "epoch": 8.784813634474158,
      "grad_norm": 10.623976707458496,
      "learning_rate": 4.87626437577941e-06,
      "loss": 2.004,
      "step": 63400
    },
    {
      "epoch": 8.786199251766662,
      "grad_norm": 16.827375411987305,
      "learning_rate": 4.870721906609395e-06,
      "loss": 1.5839,
      "step": 63410
    },
    {
      "epoch": 8.787584869059167,
      "grad_norm": 9.742751121520996,
      "learning_rate": 4.86517943743938e-06,
      "loss": 1.5483,
      "step": 63420
    },
    {
      "epoch": 8.788970486351669,
      "grad_norm": 11.43833065032959,
      "learning_rate": 4.859636968269364e-06,
      "loss": 1.6437,
      "step": 63430
    },
    {
      "epoch": 8.790356103644173,
      "grad_norm": 13.131802558898926,
      "learning_rate": 4.85409449909935e-06,
      "loss": 2.1148,
      "step": 63440
    },
    {
      "epoch": 8.791741720936677,
      "grad_norm": 13.373100280761719,
      "learning_rate": 4.848552029929334e-06,
      "loss": 1.9176,
      "step": 63450
    },
    {
      "epoch": 8.793127338229182,
      "grad_norm": 18.807188034057617,
      "learning_rate": 4.8430095607593184e-06,
      "loss": 1.6413,
      "step": 63460
    },
    {
      "epoch": 8.794512955521684,
      "grad_norm": 12.210108757019043,
      "learning_rate": 4.837467091589304e-06,
      "loss": 1.7395,
      "step": 63470
    },
    {
      "epoch": 8.795898572814188,
      "grad_norm": 16.369503021240234,
      "learning_rate": 4.831924622419288e-06,
      "loss": 1.7488,
      "step": 63480
    },
    {
      "epoch": 8.797284190106692,
      "grad_norm": 16.495037078857422,
      "learning_rate": 4.8263821532492725e-06,
      "loss": 1.9411,
      "step": 63490
    },
    {
      "epoch": 8.798669807399197,
      "grad_norm": 16.286361694335938,
      "learning_rate": 4.820839684079258e-06,
      "loss": 1.8757,
      "step": 63500
    },
    {
      "epoch": 8.8000554246917,
      "grad_norm": 16.430477142333984,
      "learning_rate": 4.815297214909242e-06,
      "loss": 1.9605,
      "step": 63510
    },
    {
      "epoch": 8.801441041984203,
      "grad_norm": 10.039135932922363,
      "learning_rate": 4.809754745739227e-06,
      "loss": 2.16,
      "step": 63520
    },
    {
      "epoch": 8.802826659276707,
      "grad_norm": 15.348405838012695,
      "learning_rate": 4.804212276569212e-06,
      "loss": 1.5539,
      "step": 63530
    },
    {
      "epoch": 8.804212276569212,
      "grad_norm": 27.480628967285156,
      "learning_rate": 4.798669807399197e-06,
      "loss": 1.6583,
      "step": 63540
    },
    {
      "epoch": 8.805597893861716,
      "grad_norm": 11.560293197631836,
      "learning_rate": 4.7931273382291815e-06,
      "loss": 1.5756,
      "step": 63550
    },
    {
      "epoch": 8.806983511154218,
      "grad_norm": 10.67113208770752,
      "learning_rate": 4.787584869059167e-06,
      "loss": 1.8026,
      "step": 63560
    },
    {
      "epoch": 8.808369128446722,
      "grad_norm": 10.02811336517334,
      "learning_rate": 4.782042399889151e-06,
      "loss": 1.4187,
      "step": 63570
    },
    {
      "epoch": 8.809754745739227,
      "grad_norm": 26.713172912597656,
      "learning_rate": 4.7764999307191355e-06,
      "loss": 1.7484,
      "step": 63580
    },
    {
      "epoch": 8.81114036303173,
      "grad_norm": 9.114625930786133,
      "learning_rate": 4.770957461549121e-06,
      "loss": 1.5883,
      "step": 63590
    },
    {
      "epoch": 8.812525980324235,
      "grad_norm": 14.016242980957031,
      "learning_rate": 4.765414992379105e-06,
      "loss": 1.8147,
      "step": 63600
    },
    {
      "epoch": 8.813911597616737,
      "grad_norm": 10.574020385742188,
      "learning_rate": 4.7598725232090896e-06,
      "loss": 1.9351,
      "step": 63610
    },
    {
      "epoch": 8.815297214909242,
      "grad_norm": 18.92766571044922,
      "learning_rate": 4.754330054039075e-06,
      "loss": 1.7783,
      "step": 63620
    },
    {
      "epoch": 8.816682832201746,
      "grad_norm": 6.669447422027588,
      "learning_rate": 4.748787584869059e-06,
      "loss": 1.8358,
      "step": 63630
    },
    {
      "epoch": 8.81806844949425,
      "grad_norm": 16.417024612426758,
      "learning_rate": 4.7432451156990445e-06,
      "loss": 1.8584,
      "step": 63640
    },
    {
      "epoch": 8.819454066786754,
      "grad_norm": 14.613152503967285,
      "learning_rate": 4.73770264652903e-06,
      "loss": 2.1709,
      "step": 63650
    },
    {
      "epoch": 8.820839684079257,
      "grad_norm": 4.051434516906738,
      "learning_rate": 4.732160177359013e-06,
      "loss": 1.561,
      "step": 63660
    },
    {
      "epoch": 8.82222530137176,
      "grad_norm": 10.858330726623535,
      "learning_rate": 4.7266177081889985e-06,
      "loss": 1.8762,
      "step": 63670
    },
    {
      "epoch": 8.823610918664265,
      "grad_norm": 15.826868057250977,
      "learning_rate": 4.721075239018984e-06,
      "loss": 1.5456,
      "step": 63680
    },
    {
      "epoch": 8.82499653595677,
      "grad_norm": 12.809696197509766,
      "learning_rate": 4.715532769848968e-06,
      "loss": 1.9634,
      "step": 63690
    },
    {
      "epoch": 8.826382153249273,
      "grad_norm": 8.445879936218262,
      "learning_rate": 4.7099903006789526e-06,
      "loss": 1.4659,
      "step": 63700
    },
    {
      "epoch": 8.827767770541776,
      "grad_norm": 14.256503105163574,
      "learning_rate": 4.704447831508938e-06,
      "loss": 1.9948,
      "step": 63710
    },
    {
      "epoch": 8.82915338783428,
      "grad_norm": 7.7396111488342285,
      "learning_rate": 4.698905362338922e-06,
      "loss": 1.9862,
      "step": 63720
    },
    {
      "epoch": 8.830539005126784,
      "grad_norm": 15.600172996520996,
      "learning_rate": 4.6933628931689075e-06,
      "loss": 2.2723,
      "step": 63730
    },
    {
      "epoch": 8.831924622419288,
      "grad_norm": 21.75008773803711,
      "learning_rate": 4.687820423998892e-06,
      "loss": 1.7865,
      "step": 63740
    },
    {
      "epoch": 8.83331023971179,
      "grad_norm": 18.704450607299805,
      "learning_rate": 4.682277954828876e-06,
      "loss": 1.76,
      "step": 63750
    },
    {
      "epoch": 8.834695857004295,
      "grad_norm": 21.652963638305664,
      "learning_rate": 4.6767354856588615e-06,
      "loss": 1.7231,
      "step": 63760
    },
    {
      "epoch": 8.8360814742968,
      "grad_norm": 23.448007583618164,
      "learning_rate": 4.671193016488847e-06,
      "loss": 2.2502,
      "step": 63770
    },
    {
      "epoch": 8.837467091589303,
      "grad_norm": 15.588624000549316,
      "learning_rate": 4.66565054731883e-06,
      "loss": 1.73,
      "step": 63780
    },
    {
      "epoch": 8.838852708881808,
      "grad_norm": 9.410613059997559,
      "learning_rate": 4.660108078148816e-06,
      "loss": 1.9148,
      "step": 63790
    },
    {
      "epoch": 8.84023832617431,
      "grad_norm": 16.631330490112305,
      "learning_rate": 4.654565608978801e-06,
      "loss": 1.5577,
      "step": 63800
    },
    {
      "epoch": 8.841623943466814,
      "grad_norm": 8.062614440917969,
      "learning_rate": 4.649023139808785e-06,
      "loss": 1.7693,
      "step": 63810
    },
    {
      "epoch": 8.843009560759318,
      "grad_norm": 12.604878425598145,
      "learning_rate": 4.64348067063877e-06,
      "loss": 1.818,
      "step": 63820
    },
    {
      "epoch": 8.844395178051823,
      "grad_norm": 13.556533813476562,
      "learning_rate": 4.637938201468755e-06,
      "loss": 1.8308,
      "step": 63830
    },
    {
      "epoch": 8.845780795344325,
      "grad_norm": 7.797748565673828,
      "learning_rate": 4.632395732298739e-06,
      "loss": 1.7917,
      "step": 63840
    },
    {
      "epoch": 8.84716641263683,
      "grad_norm": 15.00527286529541,
      "learning_rate": 4.6268532631287245e-06,
      "loss": 1.7177,
      "step": 63850
    },
    {
      "epoch": 8.848552029929333,
      "grad_norm": 21.13206672668457,
      "learning_rate": 4.621310793958709e-06,
      "loss": 1.8445,
      "step": 63860
    },
    {
      "epoch": 8.849937647221838,
      "grad_norm": 20.26464080810547,
      "learning_rate": 4.615768324788693e-06,
      "loss": 2.0107,
      "step": 63870
    },
    {
      "epoch": 8.851323264514342,
      "grad_norm": 14.609793663024902,
      "learning_rate": 4.610225855618679e-06,
      "loss": 1.6876,
      "step": 63880
    },
    {
      "epoch": 8.852708881806844,
      "grad_norm": 13.429692268371582,
      "learning_rate": 4.604683386448664e-06,
      "loss": 2.033,
      "step": 63890
    },
    {
      "epoch": 8.854094499099348,
      "grad_norm": 7.210801601409912,
      "learning_rate": 4.599140917278647e-06,
      "loss": 1.4685,
      "step": 63900
    },
    {
      "epoch": 8.855480116391853,
      "grad_norm": 14.030957221984863,
      "learning_rate": 4.593598448108633e-06,
      "loss": 1.5728,
      "step": 63910
    },
    {
      "epoch": 8.856865733684357,
      "grad_norm": 11.367166519165039,
      "learning_rate": 4.588055978938618e-06,
      "loss": 1.6756,
      "step": 63920
    },
    {
      "epoch": 8.858251350976861,
      "grad_norm": 10.417434692382812,
      "learning_rate": 4.582513509768602e-06,
      "loss": 1.9372,
      "step": 63930
    },
    {
      "epoch": 8.859636968269363,
      "grad_norm": 9.510541915893555,
      "learning_rate": 4.576971040598587e-06,
      "loss": 1.4414,
      "step": 63940
    },
    {
      "epoch": 8.861022585561868,
      "grad_norm": 16.637311935424805,
      "learning_rate": 4.571428571428572e-06,
      "loss": 2.2263,
      "step": 63950
    },
    {
      "epoch": 8.862408202854372,
      "grad_norm": 12.628040313720703,
      "learning_rate": 4.565886102258556e-06,
      "loss": 1.5114,
      "step": 63960
    },
    {
      "epoch": 8.863793820146876,
      "grad_norm": 17.656259536743164,
      "learning_rate": 4.560343633088542e-06,
      "loss": 2.0048,
      "step": 63970
    },
    {
      "epoch": 8.865179437439378,
      "grad_norm": 14.996064186096191,
      "learning_rate": 4.554801163918526e-06,
      "loss": 1.5012,
      "step": 63980
    },
    {
      "epoch": 8.866565054731883,
      "grad_norm": 8.211316108703613,
      "learning_rate": 4.54925869474851e-06,
      "loss": 1.5057,
      "step": 63990
    },
    {
      "epoch": 8.867950672024387,
      "grad_norm": 11.415596008300781,
      "learning_rate": 4.543716225578496e-06,
      "loss": 1.4441,
      "step": 64000
    },
    {
      "epoch": 8.869336289316891,
      "grad_norm": 20.89410972595215,
      "learning_rate": 4.53817375640848e-06,
      "loss": 1.6797,
      "step": 64010
    },
    {
      "epoch": 8.870721906609395,
      "grad_norm": 15.411168098449707,
      "learning_rate": 4.532631287238465e-06,
      "loss": 1.7061,
      "step": 64020
    },
    {
      "epoch": 8.872107523901898,
      "grad_norm": 11.24461841583252,
      "learning_rate": 4.52708881806845e-06,
      "loss": 2.1711,
      "step": 64030
    },
    {
      "epoch": 8.873493141194402,
      "grad_norm": 11.345602989196777,
      "learning_rate": 4.521546348898435e-06,
      "loss": 1.9436,
      "step": 64040
    },
    {
      "epoch": 8.874878758486906,
      "grad_norm": 4.81028938293457,
      "learning_rate": 4.516003879728419e-06,
      "loss": 1.9667,
      "step": 64050
    },
    {
      "epoch": 8.87626437577941,
      "grad_norm": 13.088988304138184,
      "learning_rate": 4.510461410558405e-06,
      "loss": 1.7523,
      "step": 64060
    },
    {
      "epoch": 8.877649993071913,
      "grad_norm": 7.829658508300781,
      "learning_rate": 4.504918941388389e-06,
      "loss": 1.3201,
      "step": 64070
    },
    {
      "epoch": 8.879035610364417,
      "grad_norm": 10.481224060058594,
      "learning_rate": 4.499376472218373e-06,
      "loss": 1.9449,
      "step": 64080
    },
    {
      "epoch": 8.880421227656921,
      "grad_norm": 11.765968322753906,
      "learning_rate": 4.49438824996536e-06,
      "loss": 1.6578,
      "step": 64090
    },
    {
      "epoch": 8.881806844949425,
      "grad_norm": 7.81345796585083,
      "learning_rate": 4.488845780795344e-06,
      "loss": 1.6361,
      "step": 64100
    },
    {
      "epoch": 8.88319246224193,
      "grad_norm": 21.969318389892578,
      "learning_rate": 4.4833033116253294e-06,
      "loss": 1.7613,
      "step": 64110
    },
    {
      "epoch": 8.884578079534432,
      "grad_norm": 16.23931312561035,
      "learning_rate": 4.477760842455315e-06,
      "loss": 1.9223,
      "step": 64120
    },
    {
      "epoch": 8.885963696826936,
      "grad_norm": 12.461936950683594,
      "learning_rate": 4.472218373285299e-06,
      "loss": 1.7373,
      "step": 64130
    },
    {
      "epoch": 8.88734931411944,
      "grad_norm": 15.768769264221191,
      "learning_rate": 4.4666759041152835e-06,
      "loss": 1.7394,
      "step": 64140
    },
    {
      "epoch": 8.888734931411944,
      "grad_norm": 22.39780616760254,
      "learning_rate": 4.461133434945269e-06,
      "loss": 1.7669,
      "step": 64150
    },
    {
      "epoch": 8.890120548704449,
      "grad_norm": 20.699045181274414,
      "learning_rate": 4.455590965775253e-06,
      "loss": 2.0723,
      "step": 64160
    },
    {
      "epoch": 8.891506165996951,
      "grad_norm": 10.895048141479492,
      "learning_rate": 4.450048496605238e-06,
      "loss": 1.8123,
      "step": 64170
    },
    {
      "epoch": 8.892891783289455,
      "grad_norm": 17.806909561157227,
      "learning_rate": 4.444506027435223e-06,
      "loss": 1.6906,
      "step": 64180
    },
    {
      "epoch": 8.89427740058196,
      "grad_norm": 16.406009674072266,
      "learning_rate": 4.438963558265207e-06,
      "loss": 1.915,
      "step": 64190
    },
    {
      "epoch": 8.895663017874464,
      "grad_norm": 21.196483612060547,
      "learning_rate": 4.4334210890951925e-06,
      "loss": 2.4255,
      "step": 64200
    },
    {
      "epoch": 8.897048635166968,
      "grad_norm": 7.1586012840271,
      "learning_rate": 4.427878619925177e-06,
      "loss": 1.7099,
      "step": 64210
    },
    {
      "epoch": 8.89843425245947,
      "grad_norm": 14.338739395141602,
      "learning_rate": 4.422336150755161e-06,
      "loss": 1.9289,
      "step": 64220
    },
    {
      "epoch": 8.899819869751974,
      "grad_norm": 7.995492935180664,
      "learning_rate": 4.4167936815851465e-06,
      "loss": 1.7735,
      "step": 64230
    },
    {
      "epoch": 8.901205487044479,
      "grad_norm": 14.520744323730469,
      "learning_rate": 4.411251212415131e-06,
      "loss": 1.5819,
      "step": 64240
    },
    {
      "epoch": 8.902591104336983,
      "grad_norm": 11.781726837158203,
      "learning_rate": 4.405708743245116e-06,
      "loss": 1.8785,
      "step": 64250
    },
    {
      "epoch": 8.903976721629485,
      "grad_norm": 16.78973388671875,
      "learning_rate": 4.4001662740751006e-06,
      "loss": 1.9528,
      "step": 64260
    },
    {
      "epoch": 8.90536233892199,
      "grad_norm": 15.003401756286621,
      "learning_rate": 4.394623804905086e-06,
      "loss": 1.8173,
      "step": 64270
    },
    {
      "epoch": 8.906747956214494,
      "grad_norm": 14.097550392150879,
      "learning_rate": 4.38908133573507e-06,
      "loss": 1.9071,
      "step": 64280
    },
    {
      "epoch": 8.908133573506998,
      "grad_norm": 9.470846176147461,
      "learning_rate": 4.3835388665650555e-06,
      "loss": 1.9211,
      "step": 64290
    },
    {
      "epoch": 8.909519190799502,
      "grad_norm": 11.324009895324707,
      "learning_rate": 4.37799639739504e-06,
      "loss": 1.6505,
      "step": 64300
    },
    {
      "epoch": 8.910904808092004,
      "grad_norm": 13.13193130493164,
      "learning_rate": 4.372453928225024e-06,
      "loss": 1.942,
      "step": 64310
    },
    {
      "epoch": 8.912290425384509,
      "grad_norm": 15.69282341003418,
      "learning_rate": 4.3669114590550095e-06,
      "loss": 1.8816,
      "step": 64320
    },
    {
      "epoch": 8.913676042677013,
      "grad_norm": 12.79144287109375,
      "learning_rate": 4.361368989884994e-06,
      "loss": 1.7375,
      "step": 64330
    },
    {
      "epoch": 8.915061659969517,
      "grad_norm": 30.12596893310547,
      "learning_rate": 4.355826520714979e-06,
      "loss": 1.8721,
      "step": 64340
    },
    {
      "epoch": 8.91644727726202,
      "grad_norm": 16.537363052368164,
      "learning_rate": 4.3502840515449636e-06,
      "loss": 2.0832,
      "step": 64350
    },
    {
      "epoch": 8.917832894554524,
      "grad_norm": 9.791791915893555,
      "learning_rate": 4.344741582374948e-06,
      "loss": 2.0019,
      "step": 64360
    },
    {
      "epoch": 8.919218511847028,
      "grad_norm": 17.52669906616211,
      "learning_rate": 4.339199113204933e-06,
      "loss": 1.3896,
      "step": 64370
    },
    {
      "epoch": 8.920604129139532,
      "grad_norm": 12.041028022766113,
      "learning_rate": 4.3336566440349185e-06,
      "loss": 1.5919,
      "step": 64380
    },
    {
      "epoch": 8.921989746432036,
      "grad_norm": NaN,
      "learning_rate": 4.328114174864903e-06,
      "loss": 2.0558,
      "step": 64390
    },
    {
      "epoch": 8.923375363724539,
      "grad_norm": 14.092460632324219,
      "learning_rate": 4.323125952611889e-06,
      "loss": 2.106,
      "step": 64400
    },
    {
      "epoch": 8.924760981017043,
      "grad_norm": 13.755748748779297,
      "learning_rate": 4.317583483441874e-06,
      "loss": 1.8733,
      "step": 64410
    },
    {
      "epoch": 8.926146598309547,
      "grad_norm": 12.039724349975586,
      "learning_rate": 4.312041014271858e-06,
      "loss": 1.98,
      "step": 64420
    },
    {
      "epoch": 8.927532215602051,
      "grad_norm": 13.986319541931152,
      "learning_rate": 4.306498545101843e-06,
      "loss": 2.1531,
      "step": 64430
    },
    {
      "epoch": 8.928917832894555,
      "grad_norm": 22.714763641357422,
      "learning_rate": 4.300956075931828e-06,
      "loss": 1.6455,
      "step": 64440
    },
    {
      "epoch": 8.930303450187058,
      "grad_norm": 13.170611381530762,
      "learning_rate": 4.295413606761813e-06,
      "loss": 1.4936,
      "step": 64450
    },
    {
      "epoch": 8.931689067479562,
      "grad_norm": 23.42994499206543,
      "learning_rate": 4.289871137591797e-06,
      "loss": 2.1837,
      "step": 64460
    },
    {
      "epoch": 8.933074684772066,
      "grad_norm": 17.96761131286621,
      "learning_rate": 4.284328668421782e-06,
      "loss": 2.2463,
      "step": 64470
    },
    {
      "epoch": 8.93446030206457,
      "grad_norm": 19.195758819580078,
      "learning_rate": 4.278786199251767e-06,
      "loss": 2.1793,
      "step": 64480
    },
    {
      "epoch": 8.935845919357075,
      "grad_norm": 14.691657066345215,
      "learning_rate": 4.273243730081752e-06,
      "loss": 1.9104,
      "step": 64490
    },
    {
      "epoch": 8.937231536649577,
      "grad_norm": 11.510966300964355,
      "learning_rate": 4.267701260911737e-06,
      "loss": 2.1666,
      "step": 64500
    },
    {
      "epoch": 8.938617153942081,
      "grad_norm": 15.127150535583496,
      "learning_rate": 4.262158791741721e-06,
      "loss": 2.3414,
      "step": 64510
    },
    {
      "epoch": 8.940002771234585,
      "grad_norm": 13.201000213623047,
      "learning_rate": 4.256616322571706e-06,
      "loss": 1.8263,
      "step": 64520
    },
    {
      "epoch": 8.94138838852709,
      "grad_norm": 17.879680633544922,
      "learning_rate": 4.251073853401691e-06,
      "loss": 2.327,
      "step": 64530
    },
    {
      "epoch": 8.942774005819592,
      "grad_norm": 11.96193790435791,
      "learning_rate": 4.245531384231675e-06,
      "loss": 1.7894,
      "step": 64540
    },
    {
      "epoch": 8.944159623112096,
      "grad_norm": 10.3621826171875,
      "learning_rate": 4.23998891506166e-06,
      "loss": 2.0418,
      "step": 64550
    },
    {
      "epoch": 8.9455452404046,
      "grad_norm": 10.316019058227539,
      "learning_rate": 4.234446445891645e-06,
      "loss": 1.5299,
      "step": 64560
    },
    {
      "epoch": 8.946930857697104,
      "grad_norm": 18.823822021484375,
      "learning_rate": 4.22890397672163e-06,
      "loss": 1.6124,
      "step": 64570
    },
    {
      "epoch": 8.948316474989609,
      "grad_norm": 8.518437385559082,
      "learning_rate": 4.2233615075516144e-06,
      "loss": 1.5498,
      "step": 64580
    },
    {
      "epoch": 8.949702092282111,
      "grad_norm": 15.389877319335938,
      "learning_rate": 4.217819038381599e-06,
      "loss": 1.9489,
      "step": 64590
    },
    {
      "epoch": 8.951087709574615,
      "grad_norm": 11.2294921875,
      "learning_rate": 4.212276569211584e-06,
      "loss": 1.3836,
      "step": 64600
    },
    {
      "epoch": 8.95247332686712,
      "grad_norm": 7.859105587005615,
      "learning_rate": 4.206734100041569e-06,
      "loss": 1.7928,
      "step": 64610
    },
    {
      "epoch": 8.953858944159624,
      "grad_norm": 12.625776290893555,
      "learning_rate": 4.201191630871554e-06,
      "loss": 1.8507,
      "step": 64620
    },
    {
      "epoch": 8.955244561452126,
      "grad_norm": 8.144291877746582,
      "learning_rate": 4.195649161701538e-06,
      "loss": 1.8664,
      "step": 64630
    },
    {
      "epoch": 8.95663017874463,
      "grad_norm": 14.710034370422363,
      "learning_rate": 4.190106692531523e-06,
      "loss": 1.7268,
      "step": 64640
    },
    {
      "epoch": 8.958015796037134,
      "grad_norm": 17.934677124023438,
      "learning_rate": 4.184564223361508e-06,
      "loss": 1.9326,
      "step": 64650
    },
    {
      "epoch": 8.959401413329639,
      "grad_norm": 12.337697982788086,
      "learning_rate": 4.179021754191493e-06,
      "loss": 1.5509,
      "step": 64660
    },
    {
      "epoch": 8.960787030622143,
      "grad_norm": 13.556421279907227,
      "learning_rate": 4.1734792850214774e-06,
      "loss": 1.8661,
      "step": 64670
    },
    {
      "epoch": 8.962172647914645,
      "grad_norm": 7.325558185577393,
      "learning_rate": 4.167936815851462e-06,
      "loss": 1.9034,
      "step": 64680
    },
    {
      "epoch": 8.96355826520715,
      "grad_norm": 15.104254722595215,
      "learning_rate": 4.162394346681447e-06,
      "loss": 1.6676,
      "step": 64690
    },
    {
      "epoch": 8.964943882499654,
      "grad_norm": 11.827940940856934,
      "learning_rate": 4.156851877511432e-06,
      "loss": 1.9835,
      "step": 64700
    },
    {
      "epoch": 8.966329499792158,
      "grad_norm": 18.31270408630371,
      "learning_rate": 4.151309408341416e-06,
      "loss": 1.6267,
      "step": 64710
    },
    {
      "epoch": 8.967715117084662,
      "grad_norm": 14.492039680480957,
      "learning_rate": 4.145766939171401e-06,
      "loss": 1.7861,
      "step": 64720
    },
    {
      "epoch": 8.969100734377164,
      "grad_norm": 13.007990837097168,
      "learning_rate": 4.140224470001386e-06,
      "loss": 1.2337,
      "step": 64730
    },
    {
      "epoch": 8.970486351669669,
      "grad_norm": 17.17246437072754,
      "learning_rate": 4.134682000831371e-06,
      "loss": 1.7547,
      "step": 64740
    },
    {
      "epoch": 8.971871968962173,
      "grad_norm": 18.31554412841797,
      "learning_rate": 4.129139531661355e-06,
      "loss": 2.1645,
      "step": 64750
    },
    {
      "epoch": 8.973257586254677,
      "grad_norm": 17.809803009033203,
      "learning_rate": 4.1235970624913404e-06,
      "loss": 1.7808,
      "step": 64760
    },
    {
      "epoch": 8.97464320354718,
      "grad_norm": 9.641549110412598,
      "learning_rate": 4.118054593321325e-06,
      "loss": 1.4491,
      "step": 64770
    },
    {
      "epoch": 8.976028820839684,
      "grad_norm": 8.039588928222656,
      "learning_rate": 4.11251212415131e-06,
      "loss": 1.5882,
      "step": 64780
    },
    {
      "epoch": 8.977414438132188,
      "grad_norm": 10.801308631896973,
      "learning_rate": 4.1069696549812945e-06,
      "loss": 1.8665,
      "step": 64790
    },
    {
      "epoch": 8.978800055424692,
      "grad_norm": 13.024068832397461,
      "learning_rate": 4.101427185811279e-06,
      "loss": 2.074,
      "step": 64800
    },
    {
      "epoch": 8.980185672717196,
      "grad_norm": 17.608179092407227,
      "learning_rate": 4.095884716641264e-06,
      "loss": 1.6226,
      "step": 64810
    },
    {
      "epoch": 8.981571290009699,
      "grad_norm": 22.182971954345703,
      "learning_rate": 4.0903422474712486e-06,
      "loss": 1.9258,
      "step": 64820
    },
    {
      "epoch": 8.982956907302203,
      "grad_norm": 13.9362211227417,
      "learning_rate": 4.084799778301233e-06,
      "loss": 1.3917,
      "step": 64830
    },
    {
      "epoch": 8.984342524594707,
      "grad_norm": 10.80864143371582,
      "learning_rate": 4.079257309131218e-06,
      "loss": 1.8897,
      "step": 64840
    },
    {
      "epoch": 8.985728141887211,
      "grad_norm": 24.654495239257812,
      "learning_rate": 4.0737148399612035e-06,
      "loss": 1.5018,
      "step": 64850
    },
    {
      "epoch": 8.987113759179714,
      "grad_norm": 9.463214874267578,
      "learning_rate": 4.068172370791188e-06,
      "loss": 1.7251,
      "step": 64860
    },
    {
      "epoch": 8.988499376472218,
      "grad_norm": 12.609370231628418,
      "learning_rate": 4.062629901621172e-06,
      "loss": 1.5503,
      "step": 64870
    },
    {
      "epoch": 8.989884993764722,
      "grad_norm": 14.10332202911377,
      "learning_rate": 4.0570874324511575e-06,
      "loss": 1.7195,
      "step": 64880
    },
    {
      "epoch": 8.991270611057226,
      "grad_norm": 13.253886222839355,
      "learning_rate": 4.051544963281142e-06,
      "loss": 1.8389,
      "step": 64890
    },
    {
      "epoch": 8.99265622834973,
      "grad_norm": 12.24387264251709,
      "learning_rate": 4.046002494111127e-06,
      "loss": 2.4106,
      "step": 64900
    },
    {
      "epoch": 8.994041845642233,
      "grad_norm": 16.596059799194336,
      "learning_rate": 4.0404600249411116e-06,
      "loss": 1.7116,
      "step": 64910
    },
    {
      "epoch": 8.995427462934737,
      "grad_norm": 9.821025848388672,
      "learning_rate": 4.034917555771096e-06,
      "loss": 1.9874,
      "step": 64920
    },
    {
      "epoch": 8.996813080227241,
      "grad_norm": 22.04688262939453,
      "learning_rate": 4.029375086601081e-06,
      "loss": 1.4854,
      "step": 64930
    },
    {
      "epoch": 8.998198697519745,
      "grad_norm": 12.205183029174805,
      "learning_rate": 4.023832617431066e-06,
      "loss": 1.3427,
      "step": 64940
    },
    {
      "epoch": 8.99958431481225,
      "grad_norm": 18.4587345123291,
      "learning_rate": 4.01829014826105e-06,
      "loss": 1.7391,
      "step": 64950
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.5435897435897435,
      "eval_bert_f1": 0.9882568717002869,
      "eval_bert_precision": 0.9898190498352051,
      "eval_bert_recall": 0.9870941638946533,
      "eval_f1": 0.08513617488931559,
      "eval_loss": 2.0827927589416504,
      "eval_runtime": 293.7911,
      "eval_samples_per_second": 49.117,
      "eval_steps_per_second": 6.14,
      "eval_synonym_accuracy": 0.5575190575190575,
      "step": 64953
    },
    {
      "epoch": 9.000969932104752,
      "grad_norm": 17.43511962890625,
      "learning_rate": 4.012747679091035e-06,
      "loss": 2.1909,
      "step": 64960
    },
    {
      "epoch": 9.002355549397256,
      "grad_norm": 16.464487075805664,
      "learning_rate": 4.0072052099210205e-06,
      "loss": 2.0898,
      "step": 64970
    },
    {
      "epoch": 9.00374116668976,
      "grad_norm": 24.17505645751953,
      "learning_rate": 4.001662740751005e-06,
      "loss": 1.9025,
      "step": 64980
    },
    {
      "epoch": 9.005126783982265,
      "grad_norm": 11.433059692382812,
      "learning_rate": 3.99612027158099e-06,
      "loss": 1.8378,
      "step": 64990
    },
    {
      "epoch": 9.006512401274767,
      "grad_norm": 12.624988555908203,
      "learning_rate": 3.9905778024109746e-06,
      "loss": 1.5674,
      "step": 65000
    },
    {
      "epoch": 9.007898018567271,
      "grad_norm": 13.79255199432373,
      "learning_rate": 3.985035333240959e-06,
      "loss": 2.0567,
      "step": 65010
    },
    {
      "epoch": 9.009283635859775,
      "grad_norm": 17.66063117980957,
      "learning_rate": 3.979492864070944e-06,
      "loss": 1.3134,
      "step": 65020
    },
    {
      "epoch": 9.01066925315228,
      "grad_norm": 15.584206581115723,
      "learning_rate": 3.973950394900929e-06,
      "loss": 1.5476,
      "step": 65030
    },
    {
      "epoch": 9.012054870444784,
      "grad_norm": 15.735401153564453,
      "learning_rate": 3.968407925730913e-06,
      "loss": 1.8813,
      "step": 65040
    },
    {
      "epoch": 9.013440487737286,
      "grad_norm": 11.424551010131836,
      "learning_rate": 3.962865456560898e-06,
      "loss": 1.4268,
      "step": 65050
    },
    {
      "epoch": 9.01482610502979,
      "grad_norm": 10.840938568115234,
      "learning_rate": 3.957322987390883e-06,
      "loss": 1.7946,
      "step": 65060
    },
    {
      "epoch": 9.016211722322295,
      "grad_norm": 8.226773262023926,
      "learning_rate": 3.951780518220868e-06,
      "loss": 1.9726,
      "step": 65070
    },
    {
      "epoch": 9.017597339614799,
      "grad_norm": 8.484058380126953,
      "learning_rate": 3.946238049050852e-06,
      "loss": 2.4371,
      "step": 65080
    },
    {
      "epoch": 9.018982956907303,
      "grad_norm": 11.996967315673828,
      "learning_rate": 3.940695579880838e-06,
      "loss": 1.8845,
      "step": 65090
    },
    {
      "epoch": 9.020368574199805,
      "grad_norm": 16.21152687072754,
      "learning_rate": 3.935153110710822e-06,
      "loss": 1.9808,
      "step": 65100
    },
    {
      "epoch": 9.02175419149231,
      "grad_norm": 7.5177903175354,
      "learning_rate": 3.929610641540807e-06,
      "loss": 1.738,
      "step": 65110
    },
    {
      "epoch": 9.023139808784814,
      "grad_norm": 17.269798278808594,
      "learning_rate": 3.924068172370792e-06,
      "loss": 1.332,
      "step": 65120
    },
    {
      "epoch": 9.024525426077318,
      "grad_norm": 12.132073402404785,
      "learning_rate": 3.918525703200776e-06,
      "loss": 1.6755,
      "step": 65130
    },
    {
      "epoch": 9.02591104336982,
      "grad_norm": 17.912019729614258,
      "learning_rate": 3.912983234030761e-06,
      "loss": 1.93,
      "step": 65140
    },
    {
      "epoch": 9.027296660662325,
      "grad_norm": 9.379801750183105,
      "learning_rate": 3.907440764860746e-06,
      "loss": 1.3007,
      "step": 65150
    },
    {
      "epoch": 9.028682277954829,
      "grad_norm": 7.128204345703125,
      "learning_rate": 3.90189829569073e-06,
      "loss": 1.7017,
      "step": 65160
    },
    {
      "epoch": 9.030067895247333,
      "grad_norm": 12.873530387878418,
      "learning_rate": 3.896355826520715e-06,
      "loss": 2.1263,
      "step": 65170
    },
    {
      "epoch": 9.031453512539837,
      "grad_norm": 14.776007652282715,
      "learning_rate": 3.8908133573507e-06,
      "loss": 1.5394,
      "step": 65180
    },
    {
      "epoch": 9.03283912983234,
      "grad_norm": 11.249451637268066,
      "learning_rate": 3.885270888180685e-06,
      "loss": 1.3584,
      "step": 65190
    },
    {
      "epoch": 9.034224747124844,
      "grad_norm": 18.9588565826416,
      "learning_rate": 3.879728419010669e-06,
      "loss": 1.7644,
      "step": 65200
    },
    {
      "epoch": 9.035610364417348,
      "grad_norm": 12.677300453186035,
      "learning_rate": 3.874185949840654e-06,
      "loss": 1.9518,
      "step": 65210
    },
    {
      "epoch": 9.036995981709852,
      "grad_norm": 14.070024490356445,
      "learning_rate": 3.868643480670639e-06,
      "loss": 1.4809,
      "step": 65220
    },
    {
      "epoch": 9.038381599002356,
      "grad_norm": 8.067723274230957,
      "learning_rate": 3.863101011500624e-06,
      "loss": 1.4206,
      "step": 65230
    },
    {
      "epoch": 9.039767216294859,
      "grad_norm": 13.832194328308105,
      "learning_rate": 3.857558542330609e-06,
      "loss": 1.5857,
      "step": 65240
    },
    {
      "epoch": 9.041152833587363,
      "grad_norm": 18.69957733154297,
      "learning_rate": 3.852016073160593e-06,
      "loss": 1.7468,
      "step": 65250
    },
    {
      "epoch": 9.042538450879867,
      "grad_norm": 19.27188491821289,
      "learning_rate": 3.846473603990578e-06,
      "loss": 1.5706,
      "step": 65260
    },
    {
      "epoch": 9.043924068172371,
      "grad_norm": 13.433531761169434,
      "learning_rate": 3.840931134820563e-06,
      "loss": 1.4183,
      "step": 65270
    },
    {
      "epoch": 9.045309685464874,
      "grad_norm": 11.585884094238281,
      "learning_rate": 3.835388665650548e-06,
      "loss": 1.8388,
      "step": 65280
    },
    {
      "epoch": 9.046695302757378,
      "grad_norm": 17.594276428222656,
      "learning_rate": 3.829846196480532e-06,
      "loss": 1.6792,
      "step": 65290
    },
    {
      "epoch": 9.048080920049882,
      "grad_norm": 11.709505081176758,
      "learning_rate": 3.824303727310517e-06,
      "loss": 1.4241,
      "step": 65300
    },
    {
      "epoch": 9.049466537342386,
      "grad_norm": 12.792884826660156,
      "learning_rate": 3.818761258140502e-06,
      "loss": 1.864,
      "step": 65310
    },
    {
      "epoch": 9.05085215463489,
      "grad_norm": 9.074552536010742,
      "learning_rate": 3.813218788970487e-06,
      "loss": 1.5381,
      "step": 65320
    },
    {
      "epoch": 9.052237771927393,
      "grad_norm": 33.474647521972656,
      "learning_rate": 3.8076763198004713e-06,
      "loss": 1.7435,
      "step": 65330
    },
    {
      "epoch": 9.053623389219897,
      "grad_norm": 18.32089614868164,
      "learning_rate": 3.802133850630456e-06,
      "loss": 1.8539,
      "step": 65340
    },
    {
      "epoch": 9.055009006512401,
      "grad_norm": 15.691359519958496,
      "learning_rate": 3.796591381460441e-06,
      "loss": 1.6073,
      "step": 65350
    },
    {
      "epoch": 9.056394623804906,
      "grad_norm": 15.865438461303711,
      "learning_rate": 3.7910489122904258e-06,
      "loss": 1.8614,
      "step": 65360
    },
    {
      "epoch": 9.05778024109741,
      "grad_norm": 22.216140747070312,
      "learning_rate": 3.78550644312041e-06,
      "loss": 2.0747,
      "step": 65370
    },
    {
      "epoch": 9.059165858389912,
      "grad_norm": 14.634682655334473,
      "learning_rate": 3.779963973950395e-06,
      "loss": 2.1011,
      "step": 65380
    },
    {
      "epoch": 9.060551475682416,
      "grad_norm": 17.145015716552734,
      "learning_rate": 3.7744215047803802e-06,
      "loss": 1.7593,
      "step": 65390
    },
    {
      "epoch": 9.06193709297492,
      "grad_norm": 14.051513671875,
      "learning_rate": 3.768879035610365e-06,
      "loss": 1.7192,
      "step": 65400
    },
    {
      "epoch": 9.063322710267425,
      "grad_norm": 17.907176971435547,
      "learning_rate": 3.7633365664403495e-06,
      "loss": 1.8186,
      "step": 65410
    },
    {
      "epoch": 9.064708327559927,
      "grad_norm": 12.552047729492188,
      "learning_rate": 3.7577940972703343e-06,
      "loss": 1.8234,
      "step": 65420
    },
    {
      "epoch": 9.066093944852431,
      "grad_norm": 10.531363487243652,
      "learning_rate": 3.752251628100319e-06,
      "loss": 1.4675,
      "step": 65430
    },
    {
      "epoch": 9.067479562144936,
      "grad_norm": 9.007275581359863,
      "learning_rate": 3.746709158930304e-06,
      "loss": 1.5166,
      "step": 65440
    },
    {
      "epoch": 9.06886517943744,
      "grad_norm": 18.618379592895508,
      "learning_rate": 3.7411666897602883e-06,
      "loss": 1.5507,
      "step": 65450
    },
    {
      "epoch": 9.070250796729944,
      "grad_norm": 14.606024742126465,
      "learning_rate": 3.735624220590273e-06,
      "loss": 1.5379,
      "step": 65460
    },
    {
      "epoch": 9.071636414022446,
      "grad_norm": 7.47069787979126,
      "learning_rate": 3.730081751420258e-06,
      "loss": 1.4951,
      "step": 65470
    },
    {
      "epoch": 9.07302203131495,
      "grad_norm": 11.385741233825684,
      "learning_rate": 3.724539282250243e-06,
      "loss": 1.5579,
      "step": 65480
    },
    {
      "epoch": 9.074407648607455,
      "grad_norm": 19.20353889465332,
      "learning_rate": 3.7189968130802272e-06,
      "loss": 2.0131,
      "step": 65490
    },
    {
      "epoch": 9.075793265899959,
      "grad_norm": 17.89202880859375,
      "learning_rate": 3.713454343910212e-06,
      "loss": 1.2928,
      "step": 65500
    },
    {
      "epoch": 9.077178883192463,
      "grad_norm": 13.322579383850098,
      "learning_rate": 3.7079118747401973e-06,
      "loss": 1.7041,
      "step": 65510
    },
    {
      "epoch": 9.078564500484966,
      "grad_norm": 8.302082061767578,
      "learning_rate": 3.702369405570182e-06,
      "loss": 1.6066,
      "step": 65520
    },
    {
      "epoch": 9.07995011777747,
      "grad_norm": 16.69425392150879,
      "learning_rate": 3.6968269364001665e-06,
      "loss": 1.471,
      "step": 65530
    },
    {
      "epoch": 9.081335735069974,
      "grad_norm": 10.998647689819336,
      "learning_rate": 3.6912844672301514e-06,
      "loss": 1.8472,
      "step": 65540
    },
    {
      "epoch": 9.082721352362478,
      "grad_norm": 11.070998191833496,
      "learning_rate": 3.685741998060136e-06,
      "loss": 1.8674,
      "step": 65550
    },
    {
      "epoch": 9.08410696965498,
      "grad_norm": 18.991546630859375,
      "learning_rate": 3.680199528890121e-06,
      "loss": 1.7499,
      "step": 65560
    },
    {
      "epoch": 9.085492586947485,
      "grad_norm": 12.58228874206543,
      "learning_rate": 3.6746570597201054e-06,
      "loss": 1.6873,
      "step": 65570
    },
    {
      "epoch": 9.086878204239989,
      "grad_norm": 15.072092056274414,
      "learning_rate": 3.6691145905500902e-06,
      "loss": 1.5851,
      "step": 65580
    },
    {
      "epoch": 9.088263821532493,
      "grad_norm": 8.51492691040039,
      "learning_rate": 3.663572121380075e-06,
      "loss": 1.7779,
      "step": 65590
    },
    {
      "epoch": 9.089649438824997,
      "grad_norm": 16.699501037597656,
      "learning_rate": 3.65802965221006e-06,
      "loss": 1.5979,
      "step": 65600
    },
    {
      "epoch": 9.0910350561175,
      "grad_norm": 19.189678192138672,
      "learning_rate": 3.6524871830400447e-06,
      "loss": 1.833,
      "step": 65610
    },
    {
      "epoch": 9.092420673410004,
      "grad_norm": 12.624176025390625,
      "learning_rate": 3.646944713870029e-06,
      "loss": 1.753,
      "step": 65620
    },
    {
      "epoch": 9.093806290702508,
      "grad_norm": 11.580524444580078,
      "learning_rate": 3.641402244700014e-06,
      "loss": 1.8738,
      "step": 65630
    },
    {
      "epoch": 9.095191907995012,
      "grad_norm": 11.294315338134766,
      "learning_rate": 3.635859775529999e-06,
      "loss": 1.5469,
      "step": 65640
    },
    {
      "epoch": 9.096577525287515,
      "grad_norm": 26.080982208251953,
      "learning_rate": 3.630317306359984e-06,
      "loss": 1.8805,
      "step": 65650
    },
    {
      "epoch": 9.097963142580019,
      "grad_norm": 12.154790878295898,
      "learning_rate": 3.6247748371899684e-06,
      "loss": 1.6952,
      "step": 65660
    },
    {
      "epoch": 9.099348759872523,
      "grad_norm": 14.121726036071777,
      "learning_rate": 3.6192323680199532e-06,
      "loss": 1.8886,
      "step": 65670
    },
    {
      "epoch": 9.100734377165027,
      "grad_norm": 20.491931915283203,
      "learning_rate": 3.613689898849938e-06,
      "loss": 1.6313,
      "step": 65680
    },
    {
      "epoch": 9.102119994457532,
      "grad_norm": 14.106822967529297,
      "learning_rate": 3.608147429679923e-06,
      "loss": 2.1931,
      "step": 65690
    },
    {
      "epoch": 9.103505611750034,
      "grad_norm": 14.352356910705566,
      "learning_rate": 3.6026049605099073e-06,
      "loss": 1.7004,
      "step": 65700
    },
    {
      "epoch": 9.104891229042538,
      "grad_norm": 13.368825912475586,
      "learning_rate": 3.597062491339892e-06,
      "loss": 1.7964,
      "step": 65710
    },
    {
      "epoch": 9.106276846335042,
      "grad_norm": 13.737940788269043,
      "learning_rate": 3.591520022169877e-06,
      "loss": 1.9841,
      "step": 65720
    },
    {
      "epoch": 9.107662463627547,
      "grad_norm": 9.580086708068848,
      "learning_rate": 3.5859775529998618e-06,
      "loss": 1.9599,
      "step": 65730
    },
    {
      "epoch": 9.10904808092005,
      "grad_norm": 23.547208786010742,
      "learning_rate": 3.580435083829846e-06,
      "loss": 1.7659,
      "step": 65740
    },
    {
      "epoch": 9.110433698212553,
      "grad_norm": 20.40437889099121,
      "learning_rate": 3.574892614659831e-06,
      "loss": 1.596,
      "step": 65750
    },
    {
      "epoch": 9.111819315505057,
      "grad_norm": 15.54687786102295,
      "learning_rate": 3.5693501454898162e-06,
      "loss": 2.4353,
      "step": 65760
    },
    {
      "epoch": 9.113204932797561,
      "grad_norm": 11.77553939819336,
      "learning_rate": 3.563807676319801e-06,
      "loss": 2.206,
      "step": 65770
    },
    {
      "epoch": 9.114590550090066,
      "grad_norm": 15.228435516357422,
      "learning_rate": 3.5582652071497855e-06,
      "loss": 1.8134,
      "step": 65780
    },
    {
      "epoch": 9.115976167382568,
      "grad_norm": 15.170512199401855,
      "learning_rate": 3.5527227379797703e-06,
      "loss": 1.8024,
      "step": 65790
    },
    {
      "epoch": 9.117361784675072,
      "grad_norm": 13.847809791564941,
      "learning_rate": 3.547180268809755e-06,
      "loss": 1.4437,
      "step": 65800
    },
    {
      "epoch": 9.118747401967576,
      "grad_norm": 14.564984321594238,
      "learning_rate": 3.54163779963974e-06,
      "loss": 1.8291,
      "step": 65810
    },
    {
      "epoch": 9.12013301926008,
      "grad_norm": 14.669072151184082,
      "learning_rate": 3.5360953304697244e-06,
      "loss": 1.8418,
      "step": 65820
    },
    {
      "epoch": 9.121518636552585,
      "grad_norm": 8.93493938446045,
      "learning_rate": 3.530552861299709e-06,
      "loss": 1.7796,
      "step": 65830
    },
    {
      "epoch": 9.122904253845087,
      "grad_norm": 19.472806930541992,
      "learning_rate": 3.525010392129694e-06,
      "loss": 1.8727,
      "step": 65840
    },
    {
      "epoch": 9.124289871137591,
      "grad_norm": 15.630992889404297,
      "learning_rate": 3.519467922959679e-06,
      "loss": 1.7906,
      "step": 65850
    },
    {
      "epoch": 9.125675488430096,
      "grad_norm": 15.995797157287598,
      "learning_rate": 3.5139254537896632e-06,
      "loss": 1.8108,
      "step": 65860
    },
    {
      "epoch": 9.1270611057226,
      "grad_norm": 11.320611953735352,
      "learning_rate": 3.508382984619648e-06,
      "loss": 1.31,
      "step": 65870
    },
    {
      "epoch": 9.128446723015104,
      "grad_norm": 16.81305503845215,
      "learning_rate": 3.5028405154496333e-06,
      "loss": 1.8978,
      "step": 65880
    },
    {
      "epoch": 9.129832340307606,
      "grad_norm": 16.05643653869629,
      "learning_rate": 3.497298046279618e-06,
      "loss": 1.4394,
      "step": 65890
    },
    {
      "epoch": 9.13121795760011,
      "grad_norm": 12.786325454711914,
      "learning_rate": 3.491755577109603e-06,
      "loss": 1.4787,
      "step": 65900
    },
    {
      "epoch": 9.132603574892615,
      "grad_norm": 9.793930053710938,
      "learning_rate": 3.4862131079395874e-06,
      "loss": 2.0048,
      "step": 65910
    },
    {
      "epoch": 9.133989192185119,
      "grad_norm": 12.291242599487305,
      "learning_rate": 3.480670638769572e-06,
      "loss": 2.4006,
      "step": 65920
    },
    {
      "epoch": 9.135374809477621,
      "grad_norm": 10.628820419311523,
      "learning_rate": 3.475128169599557e-06,
      "loss": 1.6768,
      "step": 65930
    },
    {
      "epoch": 9.136760426770126,
      "grad_norm": 5.6964006423950195,
      "learning_rate": 3.469585700429542e-06,
      "loss": 1.7656,
      "step": 65940
    },
    {
      "epoch": 9.13814604406263,
      "grad_norm": 18.235851287841797,
      "learning_rate": 3.4640432312595262e-06,
      "loss": 1.8045,
      "step": 65950
    },
    {
      "epoch": 9.139531661355134,
      "grad_norm": 11.055797576904297,
      "learning_rate": 3.458500762089511e-06,
      "loss": 1.699,
      "step": 65960
    },
    {
      "epoch": 9.140917278647638,
      "grad_norm": 7.187053203582764,
      "learning_rate": 3.452958292919496e-06,
      "loss": 1.5061,
      "step": 65970
    },
    {
      "epoch": 9.14230289594014,
      "grad_norm": 12.547018051147461,
      "learning_rate": 3.4474158237494807e-06,
      "loss": 2.0839,
      "step": 65980
    },
    {
      "epoch": 9.143688513232645,
      "grad_norm": 16.016796112060547,
      "learning_rate": 3.441873354579465e-06,
      "loss": 1.8755,
      "step": 65990
    },
    {
      "epoch": 9.145074130525149,
      "grad_norm": 14.692268371582031,
      "learning_rate": 3.43633088540945e-06,
      "loss": 1.7558,
      "step": 66000
    },
    {
      "epoch": 9.146459747817653,
      "grad_norm": 15.282793998718262,
      "learning_rate": 3.430788416239435e-06,
      "loss": 1.7935,
      "step": 66010
    },
    {
      "epoch": 9.147845365110157,
      "grad_norm": 18.135089874267578,
      "learning_rate": 3.42524594706942e-06,
      "loss": 1.4581,
      "step": 66020
    },
    {
      "epoch": 9.14923098240266,
      "grad_norm": 18.631271362304688,
      "learning_rate": 3.4197034778994044e-06,
      "loss": 1.7565,
      "step": 66030
    },
    {
      "epoch": 9.150616599695164,
      "grad_norm": 10.20290756225586,
      "learning_rate": 3.4141610087293893e-06,
      "loss": 1.8267,
      "step": 66040
    },
    {
      "epoch": 9.152002216987668,
      "grad_norm": 7.041913986206055,
      "learning_rate": 3.408618539559374e-06,
      "loss": 1.3857,
      "step": 66050
    },
    {
      "epoch": 9.153387834280172,
      "grad_norm": 19.09686279296875,
      "learning_rate": 3.403076070389359e-06,
      "loss": 1.9891,
      "step": 66060
    },
    {
      "epoch": 9.154773451572675,
      "grad_norm": 12.885541915893555,
      "learning_rate": 3.3975336012193433e-06,
      "loss": 1.8919,
      "step": 66070
    },
    {
      "epoch": 9.156159068865179,
      "grad_norm": 15.53427505493164,
      "learning_rate": 3.391991132049328e-06,
      "loss": 1.9016,
      "step": 66080
    },
    {
      "epoch": 9.157544686157683,
      "grad_norm": 6.9820990562438965,
      "learning_rate": 3.386448662879313e-06,
      "loss": 1.8835,
      "step": 66090
    },
    {
      "epoch": 9.158930303450187,
      "grad_norm": 12.630030632019043,
      "learning_rate": 3.3809061937092978e-06,
      "loss": 1.8064,
      "step": 66100
    },
    {
      "epoch": 9.160315920742692,
      "grad_norm": 10.912115097045898,
      "learning_rate": 3.375363724539282e-06,
      "loss": 1.6844,
      "step": 66110
    },
    {
      "epoch": 9.161701538035194,
      "grad_norm": 15.2986421585083,
      "learning_rate": 3.369821255369267e-06,
      "loss": 1.7127,
      "step": 66120
    },
    {
      "epoch": 9.163087155327698,
      "grad_norm": 12.357586860656738,
      "learning_rate": 3.3642787861992523e-06,
      "loss": 1.5401,
      "step": 66130
    },
    {
      "epoch": 9.164472772620202,
      "grad_norm": 8.467145919799805,
      "learning_rate": 3.358736317029237e-06,
      "loss": 1.5137,
      "step": 66140
    },
    {
      "epoch": 9.165858389912707,
      "grad_norm": 9.033634185791016,
      "learning_rate": 3.3531938478592215e-06,
      "loss": 1.8085,
      "step": 66150
    },
    {
      "epoch": 9.167244007205209,
      "grad_norm": 19.194351196289062,
      "learning_rate": 3.3476513786892063e-06,
      "loss": 1.7595,
      "step": 66160
    },
    {
      "epoch": 9.168629624497713,
      "grad_norm": 12.0361328125,
      "learning_rate": 3.342108909519191e-06,
      "loss": 1.5726,
      "step": 66170
    },
    {
      "epoch": 9.170015241790217,
      "grad_norm": 16.54547119140625,
      "learning_rate": 3.336566440349176e-06,
      "loss": 1.6786,
      "step": 66180
    },
    {
      "epoch": 9.171400859082722,
      "grad_norm": 22.548070907592773,
      "learning_rate": 3.3310239711791604e-06,
      "loss": 2.2744,
      "step": 66190
    },
    {
      "epoch": 9.172786476375226,
      "grad_norm": 10.963072776794434,
      "learning_rate": 3.325481502009145e-06,
      "loss": 1.6484,
      "step": 66200
    },
    {
      "epoch": 9.174172093667728,
      "grad_norm": 10.780104637145996,
      "learning_rate": 3.31993903283913e-06,
      "loss": 1.9838,
      "step": 66210
    },
    {
      "epoch": 9.175557710960232,
      "grad_norm": 7.11813497543335,
      "learning_rate": 3.314396563669115e-06,
      "loss": 1.7142,
      "step": 66220
    },
    {
      "epoch": 9.176943328252737,
      "grad_norm": 11.431947708129883,
      "learning_rate": 3.3088540944991e-06,
      "loss": 1.8198,
      "step": 66230
    },
    {
      "epoch": 9.17832894554524,
      "grad_norm": 12.011265754699707,
      "learning_rate": 3.303311625329084e-06,
      "loss": 1.6823,
      "step": 66240
    },
    {
      "epoch": 9.179714562837745,
      "grad_norm": 8.00890827178955,
      "learning_rate": 3.2977691561590693e-06,
      "loss": 1.508,
      "step": 66250
    },
    {
      "epoch": 9.181100180130247,
      "grad_norm": 9.180610656738281,
      "learning_rate": 3.292226686989054e-06,
      "loss": 1.8617,
      "step": 66260
    },
    {
      "epoch": 9.182485797422752,
      "grad_norm": 8.046051025390625,
      "learning_rate": 3.286684217819039e-06,
      "loss": 1.7511,
      "step": 66270
    },
    {
      "epoch": 9.183871414715256,
      "grad_norm": 11.770506858825684,
      "learning_rate": 3.2811417486490234e-06,
      "loss": 1.3951,
      "step": 66280
    },
    {
      "epoch": 9.18525703200776,
      "grad_norm": 24.93375015258789,
      "learning_rate": 3.275599279479008e-06,
      "loss": 2.0116,
      "step": 66290
    },
    {
      "epoch": 9.186642649300262,
      "grad_norm": 26.469444274902344,
      "learning_rate": 3.270056810308993e-06,
      "loss": 1.7432,
      "step": 66300
    },
    {
      "epoch": 9.188028266592767,
      "grad_norm": 14.835737228393555,
      "learning_rate": 3.264514341138978e-06,
      "loss": 1.4557,
      "step": 66310
    },
    {
      "epoch": 9.18941388388527,
      "grad_norm": 8.678077697753906,
      "learning_rate": 3.2589718719689623e-06,
      "loss": 1.6397,
      "step": 66320
    },
    {
      "epoch": 9.190799501177775,
      "grad_norm": 12.11579418182373,
      "learning_rate": 3.253429402798947e-06,
      "loss": 1.7331,
      "step": 66330
    },
    {
      "epoch": 9.19218511847028,
      "grad_norm": 12.263514518737793,
      "learning_rate": 3.247886933628932e-06,
      "loss": 2.0667,
      "step": 66340
    },
    {
      "epoch": 9.193570735762782,
      "grad_norm": 15.02627182006836,
      "learning_rate": 3.2423444644589167e-06,
      "loss": 1.7854,
      "step": 66350
    },
    {
      "epoch": 9.194956353055286,
      "grad_norm": 9.172256469726562,
      "learning_rate": 3.236801995288901e-06,
      "loss": 2.0294,
      "step": 66360
    },
    {
      "epoch": 9.19634197034779,
      "grad_norm": 7.472373008728027,
      "learning_rate": 3.231259526118886e-06,
      "loss": 1.8822,
      "step": 66370
    },
    {
      "epoch": 9.197727587640294,
      "grad_norm": 10.29238224029541,
      "learning_rate": 3.2257170569488712e-06,
      "loss": 1.5679,
      "step": 66380
    },
    {
      "epoch": 9.199113204932798,
      "grad_norm": 13.56387996673584,
      "learning_rate": 3.220174587778856e-06,
      "loss": 1.9048,
      "step": 66390
    },
    {
      "epoch": 9.2004988222253,
      "grad_norm": 15.10205078125,
      "learning_rate": 3.2146321186088404e-06,
      "loss": 1.7517,
      "step": 66400
    },
    {
      "epoch": 9.201884439517805,
      "grad_norm": 13.574606895446777,
      "learning_rate": 3.2090896494388253e-06,
      "loss": 1.9352,
      "step": 66410
    },
    {
      "epoch": 9.20327005681031,
      "grad_norm": 13.874475479125977,
      "learning_rate": 3.20354718026881e-06,
      "loss": 1.9174,
      "step": 66420
    },
    {
      "epoch": 9.204655674102813,
      "grad_norm": 16.111310958862305,
      "learning_rate": 3.198004711098795e-06,
      "loss": 1.7835,
      "step": 66430
    },
    {
      "epoch": 9.206041291395316,
      "grad_norm": 10.580105781555176,
      "learning_rate": 3.1924622419287793e-06,
      "loss": 1.344,
      "step": 66440
    },
    {
      "epoch": 9.20742690868782,
      "grad_norm": 15.260266304016113,
      "learning_rate": 3.186919772758764e-06,
      "loss": 1.8731,
      "step": 66450
    },
    {
      "epoch": 9.208812525980324,
      "grad_norm": 23.216222763061523,
      "learning_rate": 3.181377303588749e-06,
      "loss": 1.7559,
      "step": 66460
    },
    {
      "epoch": 9.210198143272828,
      "grad_norm": 18.08812713623047,
      "learning_rate": 3.175834834418734e-06,
      "loss": 2.1372,
      "step": 66470
    },
    {
      "epoch": 9.211583760565333,
      "grad_norm": 9.789887428283691,
      "learning_rate": 3.170292365248718e-06,
      "loss": 1.7102,
      "step": 66480
    },
    {
      "epoch": 9.212969377857835,
      "grad_norm": 12.920794486999512,
      "learning_rate": 3.164749896078703e-06,
      "loss": 1.922,
      "step": 66490
    },
    {
      "epoch": 9.21435499515034,
      "grad_norm": 21.09244155883789,
      "learning_rate": 3.1592074269086883e-06,
      "loss": 1.5031,
      "step": 66500
    },
    {
      "epoch": 9.215740612442843,
      "grad_norm": 15.103784561157227,
      "learning_rate": 3.153664957738673e-06,
      "loss": 1.6911,
      "step": 66510
    },
    {
      "epoch": 9.217126229735348,
      "grad_norm": 6.67549991607666,
      "learning_rate": 3.1481224885686575e-06,
      "loss": 1.5227,
      "step": 66520
    },
    {
      "epoch": 9.218511847027852,
      "grad_norm": 13.100357055664062,
      "learning_rate": 3.1425800193986423e-06,
      "loss": 1.7829,
      "step": 66530
    },
    {
      "epoch": 9.219897464320354,
      "grad_norm": 14.186407089233398,
      "learning_rate": 3.137037550228627e-06,
      "loss": 1.4811,
      "step": 66540
    },
    {
      "epoch": 9.221283081612858,
      "grad_norm": 13.521400451660156,
      "learning_rate": 3.131495081058612e-06,
      "loss": 1.4548,
      "step": 66550
    },
    {
      "epoch": 9.222668698905363,
      "grad_norm": 14.633076667785645,
      "learning_rate": 3.125952611888597e-06,
      "loss": 1.7471,
      "step": 66560
    },
    {
      "epoch": 9.224054316197867,
      "grad_norm": 18.06125831604004,
      "learning_rate": 3.120410142718581e-06,
      "loss": 1.5426,
      "step": 66570
    },
    {
      "epoch": 9.22543993349037,
      "grad_norm": 9.919264793395996,
      "learning_rate": 3.114867673548566e-06,
      "loss": 2.1065,
      "step": 66580
    },
    {
      "epoch": 9.226825550782873,
      "grad_norm": 7.506730079650879,
      "learning_rate": 3.109325204378551e-06,
      "loss": 1.8108,
      "step": 66590
    },
    {
      "epoch": 9.228211168075378,
      "grad_norm": 19.101863861083984,
      "learning_rate": 3.103782735208536e-06,
      "loss": 1.6526,
      "step": 66600
    },
    {
      "epoch": 9.229596785367882,
      "grad_norm": 12.933606147766113,
      "learning_rate": 3.09824026603852e-06,
      "loss": 1.4349,
      "step": 66610
    },
    {
      "epoch": 9.230982402660386,
      "grad_norm": 9.108674049377441,
      "learning_rate": 3.0926977968685053e-06,
      "loss": 1.904,
      "step": 66620
    },
    {
      "epoch": 9.232368019952888,
      "grad_norm": 16.975887298583984,
      "learning_rate": 3.08715532769849e-06,
      "loss": 1.7642,
      "step": 66630
    },
    {
      "epoch": 9.233753637245393,
      "grad_norm": 16.84674072265625,
      "learning_rate": 3.081612858528475e-06,
      "loss": 1.7866,
      "step": 66640
    },
    {
      "epoch": 9.235139254537897,
      "grad_norm": 11.687253952026367,
      "learning_rate": 3.0760703893584594e-06,
      "loss": 1.7015,
      "step": 66650
    },
    {
      "epoch": 9.236524871830401,
      "grad_norm": 20.440338134765625,
      "learning_rate": 3.0705279201884442e-06,
      "loss": 1.4603,
      "step": 66660
    },
    {
      "epoch": 9.237910489122903,
      "grad_norm": 8.098030090332031,
      "learning_rate": 3.064985451018429e-06,
      "loss": 1.7844,
      "step": 66670
    },
    {
      "epoch": 9.239296106415408,
      "grad_norm": 12.514114379882812,
      "learning_rate": 3.059442981848414e-06,
      "loss": 1.8821,
      "step": 66680
    },
    {
      "epoch": 9.240681723707912,
      "grad_norm": 13.672746658325195,
      "learning_rate": 3.0539005126783983e-06,
      "loss": 1.4168,
      "step": 66690
    },
    {
      "epoch": 9.242067341000416,
      "grad_norm": 20.707462310791016,
      "learning_rate": 3.048358043508383e-06,
      "loss": 1.6408,
      "step": 66700
    },
    {
      "epoch": 9.24345295829292,
      "grad_norm": 11.986981391906738,
      "learning_rate": 3.042815574338368e-06,
      "loss": 1.6125,
      "step": 66710
    },
    {
      "epoch": 9.244838575585423,
      "grad_norm": 14.488992691040039,
      "learning_rate": 3.0372731051683527e-06,
      "loss": 2.0278,
      "step": 66720
    },
    {
      "epoch": 9.246224192877927,
      "grad_norm": 14.212279319763184,
      "learning_rate": 3.031730635998337e-06,
      "loss": 1.8559,
      "step": 66730
    },
    {
      "epoch": 9.247609810170431,
      "grad_norm": 9.221482276916504,
      "learning_rate": 3.026188166828322e-06,
      "loss": 1.2457,
      "step": 66740
    },
    {
      "epoch": 9.248995427462935,
      "grad_norm": 18.179916381835938,
      "learning_rate": 3.0206456976583072e-06,
      "loss": 1.4852,
      "step": 66750
    },
    {
      "epoch": 9.25038104475544,
      "grad_norm": 14.250393867492676,
      "learning_rate": 3.015103228488292e-06,
      "loss": 2.2954,
      "step": 66760
    },
    {
      "epoch": 9.251766662047942,
      "grad_norm": 13.456816673278809,
      "learning_rate": 3.0095607593182765e-06,
      "loss": 1.82,
      "step": 66770
    },
    {
      "epoch": 9.253152279340446,
      "grad_norm": 19.12261199951172,
      "learning_rate": 3.0040182901482613e-06,
      "loss": 1.5546,
      "step": 66780
    },
    {
      "epoch": 9.25453789663295,
      "grad_norm": 13.965290069580078,
      "learning_rate": 2.998475820978246e-06,
      "loss": 2.2312,
      "step": 66790
    },
    {
      "epoch": 9.255923513925454,
      "grad_norm": 13.848047256469727,
      "learning_rate": 2.992933351808231e-06,
      "loss": 1.5558,
      "step": 66800
    },
    {
      "epoch": 9.257309131217959,
      "grad_norm": 8.097354888916016,
      "learning_rate": 2.9873908826382153e-06,
      "loss": 1.9101,
      "step": 66810
    },
    {
      "epoch": 9.258694748510461,
      "grad_norm": 13.194329261779785,
      "learning_rate": 2.9818484134682e-06,
      "loss": 1.9494,
      "step": 66820
    },
    {
      "epoch": 9.260080365802965,
      "grad_norm": 17.392040252685547,
      "learning_rate": 2.976305944298185e-06,
      "loss": 1.9628,
      "step": 66830
    },
    {
      "epoch": 9.26146598309547,
      "grad_norm": 22.72651481628418,
      "learning_rate": 2.97076347512817e-06,
      "loss": 1.613,
      "step": 66840
    },
    {
      "epoch": 9.262851600387974,
      "grad_norm": 15.554862022399902,
      "learning_rate": 2.965221005958155e-06,
      "loss": 1.3454,
      "step": 66850
    },
    {
      "epoch": 9.264237217680476,
      "grad_norm": 13.908403396606445,
      "learning_rate": 2.959678536788139e-06,
      "loss": 1.885,
      "step": 66860
    },
    {
      "epoch": 9.26562283497298,
      "grad_norm": 10.496825218200684,
      "learning_rate": 2.9541360676181243e-06,
      "loss": 1.5333,
      "step": 66870
    },
    {
      "epoch": 9.267008452265484,
      "grad_norm": 5.602433204650879,
      "learning_rate": 2.948593598448109e-06,
      "loss": 1.6572,
      "step": 66880
    },
    {
      "epoch": 9.268394069557989,
      "grad_norm": 15.967414855957031,
      "learning_rate": 2.943051129278094e-06,
      "loss": 1.938,
      "step": 66890
    },
    {
      "epoch": 9.269779686850493,
      "grad_norm": 16.972585678100586,
      "learning_rate": 2.9375086601080783e-06,
      "loss": 1.6727,
      "step": 66900
    },
    {
      "epoch": 9.271165304142995,
      "grad_norm": 18.072492599487305,
      "learning_rate": 2.931966190938063e-06,
      "loss": 1.606,
      "step": 66910
    },
    {
      "epoch": 9.2725509214355,
      "grad_norm": 13.716604232788086,
      "learning_rate": 2.926423721768048e-06,
      "loss": 1.7062,
      "step": 66920
    },
    {
      "epoch": 9.273936538728003,
      "grad_norm": 11.706952095031738,
      "learning_rate": 2.920881252598033e-06,
      "loss": 1.8346,
      "step": 66930
    },
    {
      "epoch": 9.275322156020508,
      "grad_norm": 20.27359390258789,
      "learning_rate": 2.9153387834280172e-06,
      "loss": 1.8361,
      "step": 66940
    },
    {
      "epoch": 9.27670777331301,
      "grad_norm": 26.779674530029297,
      "learning_rate": 2.909796314258002e-06,
      "loss": 1.8937,
      "step": 66950
    },
    {
      "epoch": 9.278093390605514,
      "grad_norm": 6.698850154876709,
      "learning_rate": 2.904253845087987e-06,
      "loss": 1.6614,
      "step": 66960
    },
    {
      "epoch": 9.279479007898018,
      "grad_norm": 19.587120056152344,
      "learning_rate": 2.898711375917972e-06,
      "loss": 2.6869,
      "step": 66970
    },
    {
      "epoch": 9.280864625190523,
      "grad_norm": 16.382946014404297,
      "learning_rate": 2.893168906747956e-06,
      "loss": 2.1566,
      "step": 66980
    },
    {
      "epoch": 9.282250242483027,
      "grad_norm": 11.415743827819824,
      "learning_rate": 2.8876264375779413e-06,
      "loss": 1.2498,
      "step": 66990
    },
    {
      "epoch": 9.28363585977553,
      "grad_norm": 18.664165496826172,
      "learning_rate": 2.882083968407926e-06,
      "loss": 2.2532,
      "step": 67000
    },
    {
      "epoch": 9.285021477068033,
      "grad_norm": 9.616737365722656,
      "learning_rate": 2.876541499237911e-06,
      "loss": 2.0016,
      "step": 67010
    },
    {
      "epoch": 9.286407094360538,
      "grad_norm": 16.744056701660156,
      "learning_rate": 2.8709990300678954e-06,
      "loss": 1.6862,
      "step": 67020
    },
    {
      "epoch": 9.287792711653042,
      "grad_norm": 31.491779327392578,
      "learning_rate": 2.8654565608978802e-06,
      "loss": 1.9073,
      "step": 67030
    },
    {
      "epoch": 9.289178328945546,
      "grad_norm": 26.293193817138672,
      "learning_rate": 2.859914091727865e-06,
      "loss": 1.3606,
      "step": 67040
    },
    {
      "epoch": 9.290563946238048,
      "grad_norm": 8.54922103881836,
      "learning_rate": 2.85437162255785e-06,
      "loss": 1.3994,
      "step": 67050
    },
    {
      "epoch": 9.291949563530553,
      "grad_norm": 11.103321075439453,
      "learning_rate": 2.8488291533878343e-06,
      "loss": 1.7853,
      "step": 67060
    },
    {
      "epoch": 9.293335180823057,
      "grad_norm": 13.486903190612793,
      "learning_rate": 2.843286684217819e-06,
      "loss": 1.7479,
      "step": 67070
    },
    {
      "epoch": 9.294720798115561,
      "grad_norm": 8.726603507995605,
      "learning_rate": 2.837744215047804e-06,
      "loss": 1.791,
      "step": 67080
    },
    {
      "epoch": 9.296106415408063,
      "grad_norm": 15.168392181396484,
      "learning_rate": 2.8322017458777888e-06,
      "loss": 1.8598,
      "step": 67090
    },
    {
      "epoch": 9.297492032700568,
      "grad_norm": 15.640686988830566,
      "learning_rate": 2.826659276707773e-06,
      "loss": 1.4687,
      "step": 67100
    },
    {
      "epoch": 9.298877649993072,
      "grad_norm": 17.421531677246094,
      "learning_rate": 2.821116807537758e-06,
      "loss": 1.4135,
      "step": 67110
    },
    {
      "epoch": 9.300263267285576,
      "grad_norm": 6.503532409667969,
      "learning_rate": 2.8155743383677432e-06,
      "loss": 1.8954,
      "step": 67120
    },
    {
      "epoch": 9.30164888457808,
      "grad_norm": 20.182676315307617,
      "learning_rate": 2.810031869197728e-06,
      "loss": 1.7017,
      "step": 67130
    },
    {
      "epoch": 9.303034501870583,
      "grad_norm": 15.170470237731934,
      "learning_rate": 2.8044894000277125e-06,
      "loss": 1.6721,
      "step": 67140
    },
    {
      "epoch": 9.304420119163087,
      "grad_norm": 8.312515258789062,
      "learning_rate": 2.7989469308576973e-06,
      "loss": 1.6494,
      "step": 67150
    },
    {
      "epoch": 9.305805736455591,
      "grad_norm": 15.810124397277832,
      "learning_rate": 2.793404461687682e-06,
      "loss": 1.6318,
      "step": 67160
    },
    {
      "epoch": 9.307191353748095,
      "grad_norm": 18.732772827148438,
      "learning_rate": 2.787861992517667e-06,
      "loss": 1.6592,
      "step": 67170
    },
    {
      "epoch": 9.3085769710406,
      "grad_norm": 19.859718322753906,
      "learning_rate": 2.7823195233476518e-06,
      "loss": 2.0999,
      "step": 67180
    },
    {
      "epoch": 9.309962588333102,
      "grad_norm": 18.432720184326172,
      "learning_rate": 2.776777054177636e-06,
      "loss": 1.5737,
      "step": 67190
    },
    {
      "epoch": 9.311348205625606,
      "grad_norm": 8.921004295349121,
      "learning_rate": 2.771234585007621e-06,
      "loss": 1.4058,
      "step": 67200
    },
    {
      "epoch": 9.31273382291811,
      "grad_norm": 9.79816722869873,
      "learning_rate": 2.765692115837606e-06,
      "loss": 1.8079,
      "step": 67210
    },
    {
      "epoch": 9.314119440210614,
      "grad_norm": 11.606148719787598,
      "learning_rate": 2.760149646667591e-06,
      "loss": 1.9047,
      "step": 67220
    },
    {
      "epoch": 9.315505057503117,
      "grad_norm": 18.375591278076172,
      "learning_rate": 2.754607177497575e-06,
      "loss": 1.7152,
      "step": 67230
    },
    {
      "epoch": 9.316890674795621,
      "grad_norm": 18.171974182128906,
      "learning_rate": 2.7490647083275603e-06,
      "loss": 2.1541,
      "step": 67240
    },
    {
      "epoch": 9.318276292088125,
      "grad_norm": 10.359493255615234,
      "learning_rate": 2.743522239157545e-06,
      "loss": 1.9035,
      "step": 67250
    },
    {
      "epoch": 9.31966190938063,
      "grad_norm": 10.818325996398926,
      "learning_rate": 2.73797976998753e-06,
      "loss": 1.5293,
      "step": 67260
    },
    {
      "epoch": 9.321047526673134,
      "grad_norm": 23.910362243652344,
      "learning_rate": 2.7324373008175144e-06,
      "loss": 1.7255,
      "step": 67270
    },
    {
      "epoch": 9.322433143965636,
      "grad_norm": 14.699313163757324,
      "learning_rate": 2.726894831647499e-06,
      "loss": 1.5814,
      "step": 67280
    },
    {
      "epoch": 9.32381876125814,
      "grad_norm": 5.94358491897583,
      "learning_rate": 2.721352362477484e-06,
      "loss": 1.6126,
      "step": 67290
    },
    {
      "epoch": 9.325204378550644,
      "grad_norm": 10.121825218200684,
      "learning_rate": 2.715809893307469e-06,
      "loss": 1.546,
      "step": 67300
    },
    {
      "epoch": 9.326589995843149,
      "grad_norm": 12.851313591003418,
      "learning_rate": 2.7102674241374532e-06,
      "loss": 1.7618,
      "step": 67310
    },
    {
      "epoch": 9.327975613135653,
      "grad_norm": 13.005721092224121,
      "learning_rate": 2.704724954967438e-06,
      "loss": 1.5159,
      "step": 67320
    },
    {
      "epoch": 9.329361230428155,
      "grad_norm": 10.049413681030273,
      "learning_rate": 2.699182485797423e-06,
      "loss": 1.796,
      "step": 67330
    },
    {
      "epoch": 9.33074684772066,
      "grad_norm": 20.055208206176758,
      "learning_rate": 2.693640016627408e-06,
      "loss": 1.5206,
      "step": 67340
    },
    {
      "epoch": 9.332132465013164,
      "grad_norm": 13.31963062286377,
      "learning_rate": 2.688097547457392e-06,
      "loss": 1.7062,
      "step": 67350
    },
    {
      "epoch": 9.333518082305668,
      "grad_norm": 9.365676879882812,
      "learning_rate": 2.6825550782873774e-06,
      "loss": 1.9367,
      "step": 67360
    },
    {
      "epoch": 9.33490369959817,
      "grad_norm": 16.930721282958984,
      "learning_rate": 2.677012609117362e-06,
      "loss": 1.9052,
      "step": 67370
    },
    {
      "epoch": 9.336289316890674,
      "grad_norm": 16.344688415527344,
      "learning_rate": 2.671470139947347e-06,
      "loss": 1.6658,
      "step": 67380
    },
    {
      "epoch": 9.337674934183179,
      "grad_norm": 27.784042358398438,
      "learning_rate": 2.6659276707773314e-06,
      "loss": 2.1006,
      "step": 67390
    },
    {
      "epoch": 9.339060551475683,
      "grad_norm": 8.274096488952637,
      "learning_rate": 2.6603852016073162e-06,
      "loss": 1.647,
      "step": 67400
    },
    {
      "epoch": 9.340446168768187,
      "grad_norm": 14.336280822753906,
      "learning_rate": 2.654842732437301e-06,
      "loss": 1.6936,
      "step": 67410
    },
    {
      "epoch": 9.34183178606069,
      "grad_norm": 22.158170700073242,
      "learning_rate": 2.649300263267286e-06,
      "loss": 1.4695,
      "step": 67420
    },
    {
      "epoch": 9.343217403353194,
      "grad_norm": 7.148260116577148,
      "learning_rate": 2.6437577940972703e-06,
      "loss": 1.8567,
      "step": 67430
    },
    {
      "epoch": 9.344603020645698,
      "grad_norm": 11.070027351379395,
      "learning_rate": 2.638215324927255e-06,
      "loss": 1.9239,
      "step": 67440
    },
    {
      "epoch": 9.345988637938202,
      "grad_norm": 13.848243713378906,
      "learning_rate": 2.63267285575724e-06,
      "loss": 1.7795,
      "step": 67450
    },
    {
      "epoch": 9.347374255230704,
      "grad_norm": 16.062358856201172,
      "learning_rate": 2.6271303865872248e-06,
      "loss": 1.6538,
      "step": 67460
    },
    {
      "epoch": 9.348759872523209,
      "grad_norm": 17.372005462646484,
      "learning_rate": 2.621587917417209e-06,
      "loss": 1.3353,
      "step": 67470
    },
    {
      "epoch": 9.350145489815713,
      "grad_norm": 11.277702331542969,
      "learning_rate": 2.616045448247194e-06,
      "loss": 1.8629,
      "step": 67480
    },
    {
      "epoch": 9.351531107108217,
      "grad_norm": 10.904560089111328,
      "learning_rate": 2.6105029790771792e-06,
      "loss": 1.7317,
      "step": 67490
    },
    {
      "epoch": 9.352916724400721,
      "grad_norm": 18.98324203491211,
      "learning_rate": 2.604960509907164e-06,
      "loss": 1.4801,
      "step": 67500
    },
    {
      "epoch": 9.354302341693224,
      "grad_norm": 8.500821113586426,
      "learning_rate": 2.599418040737149e-06,
      "loss": 1.6324,
      "step": 67510
    },
    {
      "epoch": 9.355687958985728,
      "grad_norm": 9.262721061706543,
      "learning_rate": 2.5938755715671333e-06,
      "loss": 1.7293,
      "step": 67520
    },
    {
      "epoch": 9.357073576278232,
      "grad_norm": 6.715632915496826,
      "learning_rate": 2.588333102397118e-06,
      "loss": 1.9908,
      "step": 67530
    },
    {
      "epoch": 9.358459193570736,
      "grad_norm": 14.107194900512695,
      "learning_rate": 2.582790633227103e-06,
      "loss": 2.1446,
      "step": 67540
    },
    {
      "epoch": 9.35984481086324,
      "grad_norm": 12.65040111541748,
      "learning_rate": 2.5772481640570878e-06,
      "loss": 1.9759,
      "step": 67550
    },
    {
      "epoch": 9.361230428155743,
      "grad_norm": 13.255461692810059,
      "learning_rate": 2.571705694887072e-06,
      "loss": 1.7319,
      "step": 67560
    },
    {
      "epoch": 9.362616045448247,
      "grad_norm": 13.25434684753418,
      "learning_rate": 2.566163225717057e-06,
      "loss": 1.5806,
      "step": 67570
    },
    {
      "epoch": 9.364001662740751,
      "grad_norm": 18.462404251098633,
      "learning_rate": 2.560620756547042e-06,
      "loss": 1.8477,
      "step": 67580
    },
    {
      "epoch": 9.365387280033255,
      "grad_norm": 12.727807998657227,
      "learning_rate": 2.555078287377027e-06,
      "loss": 1.4402,
      "step": 67590
    },
    {
      "epoch": 9.366772897325758,
      "grad_norm": 14.010512351989746,
      "learning_rate": 2.549535818207011e-06,
      "loss": 1.9317,
      "step": 67600
    },
    {
      "epoch": 9.368158514618262,
      "grad_norm": 16.07622528076172,
      "learning_rate": 2.5439933490369963e-06,
      "loss": 1.6457,
      "step": 67610
    },
    {
      "epoch": 9.369544131910766,
      "grad_norm": 11.39292049407959,
      "learning_rate": 2.538450879866981e-06,
      "loss": 1.7112,
      "step": 67620
    },
    {
      "epoch": 9.37092974920327,
      "grad_norm": 9.830755233764648,
      "learning_rate": 2.532908410696966e-06,
      "loss": 1.5148,
      "step": 67630
    },
    {
      "epoch": 9.372315366495775,
      "grad_norm": 5.636091709136963,
      "learning_rate": 2.5273659415269504e-06,
      "loss": 1.3705,
      "step": 67640
    },
    {
      "epoch": 9.373700983788277,
      "grad_norm": 11.948308944702148,
      "learning_rate": 2.521823472356935e-06,
      "loss": 1.5569,
      "step": 67650
    },
    {
      "epoch": 9.375086601080781,
      "grad_norm": 16.01006317138672,
      "learning_rate": 2.51628100318692e-06,
      "loss": 1.7208,
      "step": 67660
    },
    {
      "epoch": 9.376472218373285,
      "grad_norm": 16.164955139160156,
      "learning_rate": 2.510738534016905e-06,
      "loss": 1.6747,
      "step": 67670
    },
    {
      "epoch": 9.37785783566579,
      "grad_norm": 12.913830757141113,
      "learning_rate": 2.5051960648468892e-06,
      "loss": 1.9884,
      "step": 67680
    },
    {
      "epoch": 9.379243452958294,
      "grad_norm": 7.199874401092529,
      "learning_rate": 2.499653595676874e-06,
      "loss": 1.3252,
      "step": 67690
    },
    {
      "epoch": 9.380629070250796,
      "grad_norm": 23.893617630004883,
      "learning_rate": 2.494111126506859e-06,
      "loss": 1.8113,
      "step": 67700
    },
    {
      "epoch": 9.3820146875433,
      "grad_norm": 14.211806297302246,
      "learning_rate": 2.4885686573368437e-06,
      "loss": 2.0025,
      "step": 67710
    },
    {
      "epoch": 9.383400304835805,
      "grad_norm": 9.080230712890625,
      "learning_rate": 2.4830261881668285e-06,
      "loss": 1.2757,
      "step": 67720
    },
    {
      "epoch": 9.384785922128309,
      "grad_norm": 19.1757755279541,
      "learning_rate": 2.4774837189968134e-06,
      "loss": 1.9103,
      "step": 67730
    },
    {
      "epoch": 9.386171539420811,
      "grad_norm": 10.22397232055664,
      "learning_rate": 2.471941249826798e-06,
      "loss": 1.7832,
      "step": 67740
    },
    {
      "epoch": 9.387557156713315,
      "grad_norm": 15.896622657775879,
      "learning_rate": 2.4663987806567826e-06,
      "loss": 2.0989,
      "step": 67750
    },
    {
      "epoch": 9.38894277400582,
      "grad_norm": 20.087467193603516,
      "learning_rate": 2.4608563114867674e-06,
      "loss": 1.7111,
      "step": 67760
    },
    {
      "epoch": 9.390328391298324,
      "grad_norm": 24.53907585144043,
      "learning_rate": 2.4553138423167523e-06,
      "loss": 1.6592,
      "step": 67770
    },
    {
      "epoch": 9.391714008590828,
      "grad_norm": 16.190250396728516,
      "learning_rate": 2.449771373146737e-06,
      "loss": 1.968,
      "step": 67780
    },
    {
      "epoch": 9.39309962588333,
      "grad_norm": 13.904244422912598,
      "learning_rate": 2.444228903976722e-06,
      "loss": 2.0451,
      "step": 67790
    },
    {
      "epoch": 9.394485243175835,
      "grad_norm": 23.099699020385742,
      "learning_rate": 2.4386864348067067e-06,
      "loss": 2.1891,
      "step": 67800
    },
    {
      "epoch": 9.395870860468339,
      "grad_norm": 12.439154624938965,
      "learning_rate": 2.4331439656366916e-06,
      "loss": 1.4782,
      "step": 67810
    },
    {
      "epoch": 9.397256477760843,
      "grad_norm": 11.400409698486328,
      "learning_rate": 2.427601496466676e-06,
      "loss": 1.8149,
      "step": 67820
    },
    {
      "epoch": 9.398642095053347,
      "grad_norm": 12.80328369140625,
      "learning_rate": 2.4220590272966608e-06,
      "loss": 1.5547,
      "step": 67830
    },
    {
      "epoch": 9.40002771234585,
      "grad_norm": 15.057202339172363,
      "learning_rate": 2.4165165581266456e-06,
      "loss": 2.1925,
      "step": 67840
    },
    {
      "epoch": 9.401413329638354,
      "grad_norm": 10.959067344665527,
      "learning_rate": 2.4109740889566304e-06,
      "loss": 1.7297,
      "step": 67850
    },
    {
      "epoch": 9.402798946930858,
      "grad_norm": 8.701483726501465,
      "learning_rate": 2.4054316197866153e-06,
      "loss": 1.8575,
      "step": 67860
    },
    {
      "epoch": 9.404184564223362,
      "grad_norm": 5.25,
      "learning_rate": 2.3998891506166e-06,
      "loss": 1.2191,
      "step": 67870
    },
    {
      "epoch": 9.405570181515865,
      "grad_norm": 11.28533935546875,
      "learning_rate": 2.3943466814465845e-06,
      "loss": 1.8419,
      "step": 67880
    },
    {
      "epoch": 9.406955798808369,
      "grad_norm": 17.35472297668457,
      "learning_rate": 2.3888042122765693e-06,
      "loss": 1.9371,
      "step": 67890
    },
    {
      "epoch": 9.408341416100873,
      "grad_norm": 18.72846031188965,
      "learning_rate": 2.383261743106554e-06,
      "loss": 1.8949,
      "step": 67900
    },
    {
      "epoch": 9.409727033393377,
      "grad_norm": 28.39548683166504,
      "learning_rate": 2.377719273936539e-06,
      "loss": 1.9805,
      "step": 67910
    },
    {
      "epoch": 9.411112650685881,
      "grad_norm": 13.28221607208252,
      "learning_rate": 2.372176804766524e-06,
      "loss": 1.6725,
      "step": 67920
    },
    {
      "epoch": 9.412498267978384,
      "grad_norm": 15.013026237487793,
      "learning_rate": 2.3666343355965086e-06,
      "loss": 2.266,
      "step": 67930
    },
    {
      "epoch": 9.413883885270888,
      "grad_norm": 18.142087936401367,
      "learning_rate": 2.361091866426493e-06,
      "loss": 1.9024,
      "step": 67940
    },
    {
      "epoch": 9.415269502563392,
      "grad_norm": 12.055336952209473,
      "learning_rate": 2.355549397256478e-06,
      "loss": 1.7933,
      "step": 67950
    },
    {
      "epoch": 9.416655119855896,
      "grad_norm": 17.40725326538086,
      "learning_rate": 2.3500069280864627e-06,
      "loss": 1.7186,
      "step": 67960
    },
    {
      "epoch": 9.418040737148399,
      "grad_norm": 15.232105255126953,
      "learning_rate": 2.3444644589164475e-06,
      "loss": 1.9672,
      "step": 67970
    },
    {
      "epoch": 9.419426354440903,
      "grad_norm": 21.86258316040039,
      "learning_rate": 2.3389219897464323e-06,
      "loss": 1.9267,
      "step": 67980
    },
    {
      "epoch": 9.420811971733407,
      "grad_norm": 15.994950294494629,
      "learning_rate": 2.333379520576417e-06,
      "loss": 2.0554,
      "step": 67990
    },
    {
      "epoch": 9.422197589025911,
      "grad_norm": 22.499187469482422,
      "learning_rate": 2.3278370514064016e-06,
      "loss": 2.1775,
      "step": 68000
    },
    {
      "epoch": 9.423583206318416,
      "grad_norm": 11.759477615356445,
      "learning_rate": 2.3222945822363864e-06,
      "loss": 2.0807,
      "step": 68010
    },
    {
      "epoch": 9.424968823610918,
      "grad_norm": 13.622416496276855,
      "learning_rate": 2.316752113066371e-06,
      "loss": 1.7124,
      "step": 68020
    },
    {
      "epoch": 9.426354440903422,
      "grad_norm": 14.800407409667969,
      "learning_rate": 2.311209643896356e-06,
      "loss": 1.7472,
      "step": 68030
    },
    {
      "epoch": 9.427740058195926,
      "grad_norm": 13.126873016357422,
      "learning_rate": 2.305667174726341e-06,
      "loss": 1.9884,
      "step": 68040
    },
    {
      "epoch": 9.42912567548843,
      "grad_norm": 12.145452499389648,
      "learning_rate": 2.3001247055563257e-06,
      "loss": 1.6926,
      "step": 68050
    },
    {
      "epoch": 9.430511292780935,
      "grad_norm": 10.056081771850586,
      "learning_rate": 2.29458223638631e-06,
      "loss": 1.8663,
      "step": 68060
    },
    {
      "epoch": 9.431896910073437,
      "grad_norm": 14.248281478881836,
      "learning_rate": 2.289039767216295e-06,
      "loss": 1.6396,
      "step": 68070
    },
    {
      "epoch": 9.433282527365941,
      "grad_norm": 19.47400665283203,
      "learning_rate": 2.2834972980462797e-06,
      "loss": 1.6089,
      "step": 68080
    },
    {
      "epoch": 9.434668144658445,
      "grad_norm": 11.9051513671875,
      "learning_rate": 2.2779548288762646e-06,
      "loss": 1.333,
      "step": 68090
    },
    {
      "epoch": 9.43605376195095,
      "grad_norm": 9.512203216552734,
      "learning_rate": 2.2724123597062494e-06,
      "loss": 1.8134,
      "step": 68100
    },
    {
      "epoch": 9.437439379243452,
      "grad_norm": 8.644986152648926,
      "learning_rate": 2.2668698905362342e-06,
      "loss": 1.9973,
      "step": 68110
    },
    {
      "epoch": 9.438824996535956,
      "grad_norm": 19.02734375,
      "learning_rate": 2.261327421366219e-06,
      "loss": 1.6237,
      "step": 68120
    },
    {
      "epoch": 9.44021061382846,
      "grad_norm": 9.448467254638672,
      "learning_rate": 2.2557849521962034e-06,
      "loss": 1.2995,
      "step": 68130
    },
    {
      "epoch": 9.441596231120965,
      "grad_norm": 12.387216567993164,
      "learning_rate": 2.2502424830261883e-06,
      "loss": 1.8934,
      "step": 68140
    },
    {
      "epoch": 9.442981848413469,
      "grad_norm": 16.266971588134766,
      "learning_rate": 2.244700013856173e-06,
      "loss": 1.9198,
      "step": 68150
    },
    {
      "epoch": 9.444367465705971,
      "grad_norm": 29.58755874633789,
      "learning_rate": 2.239157544686158e-06,
      "loss": 1.6316,
      "step": 68160
    },
    {
      "epoch": 9.445753082998475,
      "grad_norm": 8.518198013305664,
      "learning_rate": 2.2336150755161427e-06,
      "loss": 1.6115,
      "step": 68170
    },
    {
      "epoch": 9.44713870029098,
      "grad_norm": 15.980525016784668,
      "learning_rate": 2.2280726063461276e-06,
      "loss": 1.6245,
      "step": 68180
    },
    {
      "epoch": 9.448524317583484,
      "grad_norm": 11.562141418457031,
      "learning_rate": 2.222530137176112e-06,
      "loss": 1.3107,
      "step": 68190
    },
    {
      "epoch": 9.449909934875988,
      "grad_norm": 13.844152450561523,
      "learning_rate": 2.216987668006097e-06,
      "loss": 1.742,
      "step": 68200
    },
    {
      "epoch": 9.45129555216849,
      "grad_norm": 13.148719787597656,
      "learning_rate": 2.2114451988360816e-06,
      "loss": 1.8229,
      "step": 68210
    },
    {
      "epoch": 9.452681169460995,
      "grad_norm": 9.266082763671875,
      "learning_rate": 2.2059027296660664e-06,
      "loss": 1.6344,
      "step": 68220
    },
    {
      "epoch": 9.454066786753499,
      "grad_norm": 9.93191909790039,
      "learning_rate": 2.2003602604960513e-06,
      "loss": 1.8752,
      "step": 68230
    },
    {
      "epoch": 9.455452404046003,
      "grad_norm": 13.124106407165527,
      "learning_rate": 2.194817791326036e-06,
      "loss": 2.0149,
      "step": 68240
    },
    {
      "epoch": 9.456838021338505,
      "grad_norm": 13.89909839630127,
      "learning_rate": 2.1892753221560205e-06,
      "loss": 1.488,
      "step": 68250
    },
    {
      "epoch": 9.45822363863101,
      "grad_norm": 17.25808334350586,
      "learning_rate": 2.1837328529860053e-06,
      "loss": 1.6311,
      "step": 68260
    },
    {
      "epoch": 9.459609255923514,
      "grad_norm": 22.61467933654785,
      "learning_rate": 2.17819038381599e-06,
      "loss": 1.9286,
      "step": 68270
    },
    {
      "epoch": 9.460994873216018,
      "grad_norm": 19.5063419342041,
      "learning_rate": 2.172647914645975e-06,
      "loss": 1.9692,
      "step": 68280
    },
    {
      "epoch": 9.462380490508522,
      "grad_norm": 22.68056297302246,
      "learning_rate": 2.16710544547596e-06,
      "loss": 1.868,
      "step": 68290
    },
    {
      "epoch": 9.463766107801025,
      "grad_norm": 10.359465599060059,
      "learning_rate": 2.1615629763059446e-06,
      "loss": 1.1303,
      "step": 68300
    },
    {
      "epoch": 9.465151725093529,
      "grad_norm": 10.435007095336914,
      "learning_rate": 2.156020507135929e-06,
      "loss": 1.916,
      "step": 68310
    },
    {
      "epoch": 9.466537342386033,
      "grad_norm": 21.18502426147461,
      "learning_rate": 2.150478037965914e-06,
      "loss": 1.8616,
      "step": 68320
    },
    {
      "epoch": 9.467922959678537,
      "grad_norm": 12.476606369018555,
      "learning_rate": 2.1449355687958987e-06,
      "loss": 1.6762,
      "step": 68330
    },
    {
      "epoch": 9.469308576971041,
      "grad_norm": 18.32129669189453,
      "learning_rate": 2.1393930996258835e-06,
      "loss": 1.8615,
      "step": 68340
    },
    {
      "epoch": 9.470694194263544,
      "grad_norm": 8.765186309814453,
      "learning_rate": 2.1338506304558683e-06,
      "loss": 1.9047,
      "step": 68350
    },
    {
      "epoch": 9.472079811556048,
      "grad_norm": 12.136178970336914,
      "learning_rate": 2.128308161285853e-06,
      "loss": 1.767,
      "step": 68360
    },
    {
      "epoch": 9.473465428848552,
      "grad_norm": 9.119804382324219,
      "learning_rate": 2.1227656921158376e-06,
      "loss": 1.8413,
      "step": 68370
    },
    {
      "epoch": 9.474851046141056,
      "grad_norm": 16.39373207092285,
      "learning_rate": 2.1172232229458224e-06,
      "loss": 2.2091,
      "step": 68380
    },
    {
      "epoch": 9.476236663433559,
      "grad_norm": 11.402225494384766,
      "learning_rate": 2.1116807537758072e-06,
      "loss": 1.9778,
      "step": 68390
    },
    {
      "epoch": 9.477622280726063,
      "grad_norm": 20.972679138183594,
      "learning_rate": 2.106138284605792e-06,
      "loss": 1.8035,
      "step": 68400
    },
    {
      "epoch": 9.479007898018567,
      "grad_norm": 5.961535453796387,
      "learning_rate": 2.100595815435777e-06,
      "loss": 1.7084,
      "step": 68410
    },
    {
      "epoch": 9.480393515311071,
      "grad_norm": 15.118314743041992,
      "learning_rate": 2.0950533462657617e-06,
      "loss": 2.1645,
      "step": 68420
    },
    {
      "epoch": 9.481779132603576,
      "grad_norm": 13.64991283416748,
      "learning_rate": 2.0895108770957465e-06,
      "loss": 1.9046,
      "step": 68430
    },
    {
      "epoch": 9.483164749896078,
      "grad_norm": 16.327009201049805,
      "learning_rate": 2.083968407925731e-06,
      "loss": 1.7884,
      "step": 68440
    },
    {
      "epoch": 9.484550367188582,
      "grad_norm": 17.514141082763672,
      "learning_rate": 2.078425938755716e-06,
      "loss": 2.236,
      "step": 68450
    },
    {
      "epoch": 9.485935984481086,
      "grad_norm": 7.830520153045654,
      "learning_rate": 2.0728834695857006e-06,
      "loss": 1.7976,
      "step": 68460
    },
    {
      "epoch": 9.48732160177359,
      "grad_norm": 22.949325561523438,
      "learning_rate": 2.0673410004156854e-06,
      "loss": 1.4495,
      "step": 68470
    },
    {
      "epoch": 9.488707219066093,
      "grad_norm": 16.540895462036133,
      "learning_rate": 2.0617985312456702e-06,
      "loss": 1.4454,
      "step": 68480
    },
    {
      "epoch": 9.490092836358597,
      "grad_norm": 10.97111988067627,
      "learning_rate": 2.056256062075655e-06,
      "loss": 1.4454,
      "step": 68490
    },
    {
      "epoch": 9.491478453651101,
      "grad_norm": 10.95628547668457,
      "learning_rate": 2.0507135929056395e-06,
      "loss": 1.788,
      "step": 68500
    },
    {
      "epoch": 9.492864070943606,
      "grad_norm": 16.255111694335938,
      "learning_rate": 2.0451711237356243e-06,
      "loss": 1.829,
      "step": 68510
    },
    {
      "epoch": 9.49424968823611,
      "grad_norm": 9.336655616760254,
      "learning_rate": 2.039628654565609e-06,
      "loss": 1.7359,
      "step": 68520
    },
    {
      "epoch": 9.495635305528612,
      "grad_norm": 17.27899169921875,
      "learning_rate": 2.034086185395594e-06,
      "loss": 1.9972,
      "step": 68530
    },
    {
      "epoch": 9.497020922821116,
      "grad_norm": 13.71491813659668,
      "learning_rate": 2.0285437162255788e-06,
      "loss": 1.8778,
      "step": 68540
    },
    {
      "epoch": 9.49840654011362,
      "grad_norm": 19.226463317871094,
      "learning_rate": 2.0230012470555636e-06,
      "loss": 2.0587,
      "step": 68550
    },
    {
      "epoch": 9.499792157406125,
      "grad_norm": 7.475240230560303,
      "learning_rate": 2.017458777885548e-06,
      "loss": 1.9217,
      "step": 68560
    },
    {
      "epoch": 9.501177774698629,
      "grad_norm": 15.359003067016602,
      "learning_rate": 2.011916308715533e-06,
      "loss": 1.9427,
      "step": 68570
    },
    {
      "epoch": 9.502563391991131,
      "grad_norm": 16.857839584350586,
      "learning_rate": 2.0063738395455176e-06,
      "loss": 1.3766,
      "step": 68580
    },
    {
      "epoch": 9.503949009283636,
      "grad_norm": 16.462568283081055,
      "learning_rate": 2.0008313703755025e-06,
      "loss": 1.8132,
      "step": 68590
    },
    {
      "epoch": 9.50533462657614,
      "grad_norm": 15.6526517868042,
      "learning_rate": 1.9952889012054873e-06,
      "loss": 1.5125,
      "step": 68600
    },
    {
      "epoch": 9.506720243868644,
      "grad_norm": 22.737611770629883,
      "learning_rate": 1.989746432035472e-06,
      "loss": 1.6792,
      "step": 68610
    },
    {
      "epoch": 9.508105861161148,
      "grad_norm": 9.989775657653809,
      "learning_rate": 1.9842039628654565e-06,
      "loss": 1.505,
      "step": 68620
    },
    {
      "epoch": 9.50949147845365,
      "grad_norm": 12.873047828674316,
      "learning_rate": 1.9786614936954413e-06,
      "loss": 1.4511,
      "step": 68630
    },
    {
      "epoch": 9.510877095746155,
      "grad_norm": 4.906518459320068,
      "learning_rate": 1.973119024525426e-06,
      "loss": 1.9382,
      "step": 68640
    },
    {
      "epoch": 9.512262713038659,
      "grad_norm": 14.18810749053955,
      "learning_rate": 1.967576555355411e-06,
      "loss": 1.9635,
      "step": 68650
    },
    {
      "epoch": 9.513648330331163,
      "grad_norm": 9.327604293823242,
      "learning_rate": 1.962034086185396e-06,
      "loss": 1.4565,
      "step": 68660
    },
    {
      "epoch": 9.515033947623666,
      "grad_norm": 16.462064743041992,
      "learning_rate": 1.9564916170153806e-06,
      "loss": 1.6364,
      "step": 68670
    },
    {
      "epoch": 9.51641956491617,
      "grad_norm": 19.61707305908203,
      "learning_rate": 1.950949147845365e-06,
      "loss": 1.5084,
      "step": 68680
    },
    {
      "epoch": 9.517805182208674,
      "grad_norm": 21.466472625732422,
      "learning_rate": 1.94540667867535e-06,
      "loss": 1.8334,
      "step": 68690
    },
    {
      "epoch": 9.519190799501178,
      "grad_norm": 18.28133773803711,
      "learning_rate": 1.9398642095053347e-06,
      "loss": 1.7051,
      "step": 68700
    },
    {
      "epoch": 9.520576416793682,
      "grad_norm": 13.03486442565918,
      "learning_rate": 1.9343217403353195e-06,
      "loss": 1.6809,
      "step": 68710
    },
    {
      "epoch": 9.521962034086185,
      "grad_norm": 13.940348625183105,
      "learning_rate": 1.9287792711653043e-06,
      "loss": 1.5229,
      "step": 68720
    },
    {
      "epoch": 9.523347651378689,
      "grad_norm": 10.511218070983887,
      "learning_rate": 1.923236801995289e-06,
      "loss": 1.3105,
      "step": 68730
    },
    {
      "epoch": 9.524733268671193,
      "grad_norm": 8.997940063476562,
      "learning_rate": 1.917694332825274e-06,
      "loss": 1.5124,
      "step": 68740
    },
    {
      "epoch": 9.526118885963697,
      "grad_norm": 20.55425453186035,
      "learning_rate": 1.9121518636552584e-06,
      "loss": 1.5771,
      "step": 68750
    },
    {
      "epoch": 9.5275045032562,
      "grad_norm": 13.377250671386719,
      "learning_rate": 1.9066093944852434e-06,
      "loss": 1.9417,
      "step": 68760
    },
    {
      "epoch": 9.528890120548704,
      "grad_norm": 19.469457626342773,
      "learning_rate": 1.901066925315228e-06,
      "loss": 1.8966,
      "step": 68770
    },
    {
      "epoch": 9.530275737841208,
      "grad_norm": 14.052434921264648,
      "learning_rate": 1.8955244561452129e-06,
      "loss": 2.0892,
      "step": 68780
    },
    {
      "epoch": 9.531661355133712,
      "grad_norm": 13.816620826721191,
      "learning_rate": 1.8899819869751975e-06,
      "loss": 1.6917,
      "step": 68790
    },
    {
      "epoch": 9.533046972426217,
      "grad_norm": 5.188144683837891,
      "learning_rate": 1.8844395178051825e-06,
      "loss": 1.4079,
      "step": 68800
    },
    {
      "epoch": 9.534432589718719,
      "grad_norm": 15.845030784606934,
      "learning_rate": 1.8788970486351671e-06,
      "loss": 1.8926,
      "step": 68810
    },
    {
      "epoch": 9.535818207011223,
      "grad_norm": 20.237995147705078,
      "learning_rate": 1.873354579465152e-06,
      "loss": 1.7618,
      "step": 68820
    },
    {
      "epoch": 9.537203824303727,
      "grad_norm": 10.469057083129883,
      "learning_rate": 1.8678121102951366e-06,
      "loss": 1.389,
      "step": 68830
    },
    {
      "epoch": 9.538589441596232,
      "grad_norm": 18.00612449645996,
      "learning_rate": 1.8622696411251214e-06,
      "loss": 1.527,
      "step": 68840
    },
    {
      "epoch": 9.539975058888736,
      "grad_norm": 9.540977478027344,
      "learning_rate": 1.856727171955106e-06,
      "loss": 1.7957,
      "step": 68850
    },
    {
      "epoch": 9.541360676181238,
      "grad_norm": 13.977685928344727,
      "learning_rate": 1.851184702785091e-06,
      "loss": 1.8813,
      "step": 68860
    },
    {
      "epoch": 9.542746293473742,
      "grad_norm": 16.12718963623047,
      "learning_rate": 1.8456422336150757e-06,
      "loss": 1.7698,
      "step": 68870
    },
    {
      "epoch": 9.544131910766247,
      "grad_norm": 10.648573875427246,
      "learning_rate": 1.8400997644450605e-06,
      "loss": 1.8026,
      "step": 68880
    },
    {
      "epoch": 9.54551752805875,
      "grad_norm": 13.046194076538086,
      "learning_rate": 1.8345572952750451e-06,
      "loss": 1.7323,
      "step": 68890
    },
    {
      "epoch": 9.546903145351253,
      "grad_norm": 10.794537544250488,
      "learning_rate": 1.82901482610503e-06,
      "loss": 1.7176,
      "step": 68900
    },
    {
      "epoch": 9.548288762643757,
      "grad_norm": 15.832504272460938,
      "learning_rate": 1.8234723569350146e-06,
      "loss": 1.5244,
      "step": 68910
    },
    {
      "epoch": 9.549674379936262,
      "grad_norm": 21.527027130126953,
      "learning_rate": 1.8179298877649996e-06,
      "loss": 1.6552,
      "step": 68920
    },
    {
      "epoch": 9.551059997228766,
      "grad_norm": 21.58504867553711,
      "learning_rate": 1.8123874185949842e-06,
      "loss": 1.3671,
      "step": 68930
    },
    {
      "epoch": 9.55244561452127,
      "grad_norm": 16.440263748168945,
      "learning_rate": 1.806844949424969e-06,
      "loss": 2.0112,
      "step": 68940
    },
    {
      "epoch": 9.553831231813772,
      "grad_norm": 14.1388578414917,
      "learning_rate": 1.8013024802549536e-06,
      "loss": 1.4414,
      "step": 68950
    },
    {
      "epoch": 9.555216849106277,
      "grad_norm": 10.85770320892334,
      "learning_rate": 1.7957600110849385e-06,
      "loss": 1.8969,
      "step": 68960
    },
    {
      "epoch": 9.55660246639878,
      "grad_norm": 15.367526054382324,
      "learning_rate": 1.790217541914923e-06,
      "loss": 1.5863,
      "step": 68970
    },
    {
      "epoch": 9.557988083691285,
      "grad_norm": 19.669485092163086,
      "learning_rate": 1.7846750727449081e-06,
      "loss": 2.338,
      "step": 68980
    },
    {
      "epoch": 9.559373700983787,
      "grad_norm": 14.572075843811035,
      "learning_rate": 1.7791326035748927e-06,
      "loss": 2.2083,
      "step": 68990
    },
    {
      "epoch": 9.560759318276292,
      "grad_norm": 11.33873462677002,
      "learning_rate": 1.7735901344048776e-06,
      "loss": 1.8878,
      "step": 69000
    },
    {
      "epoch": 9.562144935568796,
      "grad_norm": 7.530355930328369,
      "learning_rate": 1.7680476652348622e-06,
      "loss": 1.632,
      "step": 69010
    },
    {
      "epoch": 9.5635305528613,
      "grad_norm": 9.268798828125,
      "learning_rate": 1.762505196064847e-06,
      "loss": 1.3875,
      "step": 69020
    },
    {
      "epoch": 9.564916170153804,
      "grad_norm": 36.33496856689453,
      "learning_rate": 1.7569627268948316e-06,
      "loss": 1.773,
      "step": 69030
    },
    {
      "epoch": 9.566301787446307,
      "grad_norm": 16.842805862426758,
      "learning_rate": 1.7514202577248167e-06,
      "loss": 1.6632,
      "step": 69040
    },
    {
      "epoch": 9.56768740473881,
      "grad_norm": 10.725693702697754,
      "learning_rate": 1.7458777885548015e-06,
      "loss": 1.5124,
      "step": 69050
    },
    {
      "epoch": 9.569073022031315,
      "grad_norm": 9.42366886138916,
      "learning_rate": 1.740335319384786e-06,
      "loss": 2.0992,
      "step": 69060
    },
    {
      "epoch": 9.57045863932382,
      "grad_norm": 10.685068130493164,
      "learning_rate": 1.734792850214771e-06,
      "loss": 1.8404,
      "step": 69070
    },
    {
      "epoch": 9.571844256616323,
      "grad_norm": 5.22342586517334,
      "learning_rate": 1.7292503810447555e-06,
      "loss": 1.3873,
      "step": 69080
    },
    {
      "epoch": 9.573229873908826,
      "grad_norm": 9.263443946838379,
      "learning_rate": 1.7237079118747404e-06,
      "loss": 1.8288,
      "step": 69090
    },
    {
      "epoch": 9.57461549120133,
      "grad_norm": 10.660429000854492,
      "learning_rate": 1.718165442704725e-06,
      "loss": 2.0466,
      "step": 69100
    },
    {
      "epoch": 9.576001108493834,
      "grad_norm": 23.649309158325195,
      "learning_rate": 1.71262297353471e-06,
      "loss": 1.5759,
      "step": 69110
    },
    {
      "epoch": 9.577386725786338,
      "grad_norm": 13.171649932861328,
      "learning_rate": 1.7070805043646946e-06,
      "loss": 1.5099,
      "step": 69120
    },
    {
      "epoch": 9.578772343078843,
      "grad_norm": 15.873425483703613,
      "learning_rate": 1.7015380351946795e-06,
      "loss": 2.1609,
      "step": 69130
    },
    {
      "epoch": 9.580157960371345,
      "grad_norm": 5.0451436042785645,
      "learning_rate": 1.695995566024664e-06,
      "loss": 1.6979,
      "step": 69140
    },
    {
      "epoch": 9.58154357766385,
      "grad_norm": 11.901421546936035,
      "learning_rate": 1.6904530968546489e-06,
      "loss": 1.7005,
      "step": 69150
    },
    {
      "epoch": 9.582929194956353,
      "grad_norm": 14.169928550720215,
      "learning_rate": 1.6849106276846335e-06,
      "loss": 1.6029,
      "step": 69160
    },
    {
      "epoch": 9.584314812248858,
      "grad_norm": 16.788602828979492,
      "learning_rate": 1.6793681585146185e-06,
      "loss": 2.017,
      "step": 69170
    },
    {
      "epoch": 9.58570042954136,
      "grad_norm": 23.742515563964844,
      "learning_rate": 1.6738256893446032e-06,
      "loss": 1.9257,
      "step": 69180
    },
    {
      "epoch": 9.587086046833864,
      "grad_norm": 8.468307495117188,
      "learning_rate": 1.668283220174588e-06,
      "loss": 1.5317,
      "step": 69190
    },
    {
      "epoch": 9.588471664126368,
      "grad_norm": 13.955917358398438,
      "learning_rate": 1.6627407510045726e-06,
      "loss": 1.965,
      "step": 69200
    },
    {
      "epoch": 9.589857281418873,
      "grad_norm": 22.607349395751953,
      "learning_rate": 1.6571982818345574e-06,
      "loss": 1.7668,
      "step": 69210
    },
    {
      "epoch": 9.591242898711377,
      "grad_norm": 20.007671356201172,
      "learning_rate": 1.651655812664542e-06,
      "loss": 2.2108,
      "step": 69220
    },
    {
      "epoch": 9.59262851600388,
      "grad_norm": 22.839008331298828,
      "learning_rate": 1.646113343494527e-06,
      "loss": 1.473,
      "step": 69230
    },
    {
      "epoch": 9.594014133296383,
      "grad_norm": 15.260383605957031,
      "learning_rate": 1.6405708743245117e-06,
      "loss": 1.8532,
      "step": 69240
    },
    {
      "epoch": 9.595399750588887,
      "grad_norm": 5.5039777755737305,
      "learning_rate": 1.6350284051544965e-06,
      "loss": 1.6064,
      "step": 69250
    },
    {
      "epoch": 9.596785367881392,
      "grad_norm": 7.720849514007568,
      "learning_rate": 1.6294859359844811e-06,
      "loss": 1.1892,
      "step": 69260
    },
    {
      "epoch": 9.598170985173894,
      "grad_norm": 13.584612846374512,
      "learning_rate": 1.623943466814466e-06,
      "loss": 1.7915,
      "step": 69270
    },
    {
      "epoch": 9.599556602466398,
      "grad_norm": 14.126324653625488,
      "learning_rate": 1.6184009976444506e-06,
      "loss": 1.711,
      "step": 69280
    },
    {
      "epoch": 9.600942219758902,
      "grad_norm": 15.748468399047852,
      "learning_rate": 1.6128585284744356e-06,
      "loss": 1.6428,
      "step": 69290
    },
    {
      "epoch": 9.602327837051407,
      "grad_norm": 17.49494171142578,
      "learning_rate": 1.6073160593044202e-06,
      "loss": 1.3044,
      "step": 69300
    },
    {
      "epoch": 9.60371345434391,
      "grad_norm": 12.1569242477417,
      "learning_rate": 1.601773590134405e-06,
      "loss": 1.8174,
      "step": 69310
    },
    {
      "epoch": 9.605099071636413,
      "grad_norm": 20.076684951782227,
      "learning_rate": 1.5962311209643897e-06,
      "loss": 1.9525,
      "step": 69320
    },
    {
      "epoch": 9.606484688928917,
      "grad_norm": 9.94826889038086,
      "learning_rate": 1.5906886517943745e-06,
      "loss": 1.2679,
      "step": 69330
    },
    {
      "epoch": 9.607870306221422,
      "grad_norm": 10.798405647277832,
      "learning_rate": 1.585146182624359e-06,
      "loss": 2.0045,
      "step": 69340
    },
    {
      "epoch": 9.609255923513926,
      "grad_norm": 11.302234649658203,
      "learning_rate": 1.5796037134543441e-06,
      "loss": 1.811,
      "step": 69350
    },
    {
      "epoch": 9.61064154080643,
      "grad_norm": 13.796902656555176,
      "learning_rate": 1.5740612442843288e-06,
      "loss": 1.5971,
      "step": 69360
    },
    {
      "epoch": 9.612027158098932,
      "grad_norm": 17.685791015625,
      "learning_rate": 1.5685187751143136e-06,
      "loss": 1.6821,
      "step": 69370
    },
    {
      "epoch": 9.613412775391437,
      "grad_norm": 14.256769180297852,
      "learning_rate": 1.5629763059442984e-06,
      "loss": 1.343,
      "step": 69380
    },
    {
      "epoch": 9.61479839268394,
      "grad_norm": 12.449943542480469,
      "learning_rate": 1.557433836774283e-06,
      "loss": 1.7145,
      "step": 69390
    },
    {
      "epoch": 9.616184009976445,
      "grad_norm": 10.323090553283691,
      "learning_rate": 1.551891367604268e-06,
      "loss": 1.9417,
      "step": 69400
    },
    {
      "epoch": 9.617569627268947,
      "grad_norm": 18.388940811157227,
      "learning_rate": 1.5463488984342527e-06,
      "loss": 2.1309,
      "step": 69410
    },
    {
      "epoch": 9.618955244561452,
      "grad_norm": 10.018526077270508,
      "learning_rate": 1.5408064292642375e-06,
      "loss": 1.7973,
      "step": 69420
    },
    {
      "epoch": 9.620340861853956,
      "grad_norm": 8.262479782104492,
      "learning_rate": 1.5352639600942221e-06,
      "loss": 1.7032,
      "step": 69430
    },
    {
      "epoch": 9.62172647914646,
      "grad_norm": 12.503800392150879,
      "learning_rate": 1.529721490924207e-06,
      "loss": 1.3933,
      "step": 69440
    },
    {
      "epoch": 9.623112096438964,
      "grad_norm": 17.46520233154297,
      "learning_rate": 1.5241790217541915e-06,
      "loss": 1.7705,
      "step": 69450
    },
    {
      "epoch": 9.624497713731467,
      "grad_norm": 14.076037406921387,
      "learning_rate": 1.5186365525841764e-06,
      "loss": 1.8314,
      "step": 69460
    },
    {
      "epoch": 9.62588333102397,
      "grad_norm": 11.16456413269043,
      "learning_rate": 1.513094083414161e-06,
      "loss": 1.782,
      "step": 69470
    },
    {
      "epoch": 9.627268948316475,
      "grad_norm": 11.48939323425293,
      "learning_rate": 1.507551614244146e-06,
      "loss": 1.8462,
      "step": 69480
    },
    {
      "epoch": 9.62865456560898,
      "grad_norm": 13.834498405456543,
      "learning_rate": 1.5020091450741306e-06,
      "loss": 1.9471,
      "step": 69490
    },
    {
      "epoch": 9.630040182901482,
      "grad_norm": 19.859172821044922,
      "learning_rate": 1.4964666759041155e-06,
      "loss": 1.8234,
      "step": 69500
    },
    {
      "epoch": 9.631425800193986,
      "grad_norm": 19.599515914916992,
      "learning_rate": 1.4909242067341e-06,
      "loss": 1.9806,
      "step": 69510
    },
    {
      "epoch": 9.63281141748649,
      "grad_norm": 22.11698341369629,
      "learning_rate": 1.485381737564085e-06,
      "loss": 1.9907,
      "step": 69520
    },
    {
      "epoch": 9.634197034778994,
      "grad_norm": 11.483336448669434,
      "learning_rate": 1.4798392683940695e-06,
      "loss": 2.0326,
      "step": 69530
    },
    {
      "epoch": 9.635582652071498,
      "grad_norm": 27.7550106048584,
      "learning_rate": 1.4742967992240546e-06,
      "loss": 1.8174,
      "step": 69540
    },
    {
      "epoch": 9.636968269364,
      "grad_norm": 9.108022689819336,
      "learning_rate": 1.4687543300540392e-06,
      "loss": 1.5877,
      "step": 69550
    },
    {
      "epoch": 9.638353886656505,
      "grad_norm": 24.938411712646484,
      "learning_rate": 1.463211860884024e-06,
      "loss": 2.0008,
      "step": 69560
    },
    {
      "epoch": 9.63973950394901,
      "grad_norm": 10.357312202453613,
      "learning_rate": 1.4576693917140086e-06,
      "loss": 1.7013,
      "step": 69570
    },
    {
      "epoch": 9.641125121241513,
      "grad_norm": 10.081796646118164,
      "learning_rate": 1.4521269225439934e-06,
      "loss": 2.0198,
      "step": 69580
    },
    {
      "epoch": 9.642510738534018,
      "grad_norm": 10.376021385192871,
      "learning_rate": 1.446584453373978e-06,
      "loss": 1.684,
      "step": 69590
    },
    {
      "epoch": 9.64389635582652,
      "grad_norm": 10.965691566467285,
      "learning_rate": 1.441041984203963e-06,
      "loss": 1.7886,
      "step": 69600
    },
    {
      "epoch": 9.645281973119024,
      "grad_norm": 13.637353897094727,
      "learning_rate": 1.4354995150339477e-06,
      "loss": 1.7177,
      "step": 69610
    },
    {
      "epoch": 9.646667590411528,
      "grad_norm": 20.65730857849121,
      "learning_rate": 1.4299570458639325e-06,
      "loss": 1.595,
      "step": 69620
    },
    {
      "epoch": 9.648053207704033,
      "grad_norm": 14.231332778930664,
      "learning_rate": 1.4244145766939171e-06,
      "loss": 1.1018,
      "step": 69630
    },
    {
      "epoch": 9.649438824996537,
      "grad_norm": 21.28005599975586,
      "learning_rate": 1.418872107523902e-06,
      "loss": 1.9091,
      "step": 69640
    },
    {
      "epoch": 9.65082444228904,
      "grad_norm": 13.678833961486816,
      "learning_rate": 1.4133296383538866e-06,
      "loss": 1.9392,
      "step": 69650
    },
    {
      "epoch": 9.652210059581543,
      "grad_norm": 25.488773345947266,
      "learning_rate": 1.4077871691838716e-06,
      "loss": 1.7415,
      "step": 69660
    },
    {
      "epoch": 9.653595676874048,
      "grad_norm": 10.095813751220703,
      "learning_rate": 1.4022447000138562e-06,
      "loss": 1.4312,
      "step": 69670
    },
    {
      "epoch": 9.654981294166552,
      "grad_norm": 11.101680755615234,
      "learning_rate": 1.396702230843841e-06,
      "loss": 1.5323,
      "step": 69680
    },
    {
      "epoch": 9.656366911459054,
      "grad_norm": 9.526224136352539,
      "learning_rate": 1.3911597616738259e-06,
      "loss": 1.5974,
      "step": 69690
    },
    {
      "epoch": 9.657752528751558,
      "grad_norm": 17.745162963867188,
      "learning_rate": 1.3856172925038105e-06,
      "loss": 1.8092,
      "step": 69700
    },
    {
      "epoch": 9.659138146044063,
      "grad_norm": 25.460355758666992,
      "learning_rate": 1.3800748233337955e-06,
      "loss": 1.8767,
      "step": 69710
    },
    {
      "epoch": 9.660523763336567,
      "grad_norm": 14.908638954162598,
      "learning_rate": 1.3745323541637801e-06,
      "loss": 1.9613,
      "step": 69720
    },
    {
      "epoch": 9.661909380629071,
      "grad_norm": 18.01066780090332,
      "learning_rate": 1.368989884993765e-06,
      "loss": 1.8172,
      "step": 69730
    },
    {
      "epoch": 9.663294997921573,
      "grad_norm": 10.102869033813477,
      "learning_rate": 1.3634474158237496e-06,
      "loss": 1.3527,
      "step": 69740
    },
    {
      "epoch": 9.664680615214078,
      "grad_norm": 14.245462417602539,
      "learning_rate": 1.3579049466537344e-06,
      "loss": 1.6419,
      "step": 69750
    },
    {
      "epoch": 9.666066232506582,
      "grad_norm": 13.089523315429688,
      "learning_rate": 1.352362477483719e-06,
      "loss": 1.7821,
      "step": 69760
    },
    {
      "epoch": 9.667451849799086,
      "grad_norm": 15.763270378112793,
      "learning_rate": 1.346820008313704e-06,
      "loss": 1.2236,
      "step": 69770
    },
    {
      "epoch": 9.668837467091588,
      "grad_norm": 21.515975952148438,
      "learning_rate": 1.3412775391436887e-06,
      "loss": 1.3963,
      "step": 69780
    },
    {
      "epoch": 9.670223084384093,
      "grad_norm": 12.73032283782959,
      "learning_rate": 1.3357350699736735e-06,
      "loss": 1.7609,
      "step": 69790
    },
    {
      "epoch": 9.671608701676597,
      "grad_norm": 9.765506744384766,
      "learning_rate": 1.3301926008036581e-06,
      "loss": 1.6195,
      "step": 69800
    },
    {
      "epoch": 9.672994318969101,
      "grad_norm": 12.3916015625,
      "learning_rate": 1.324650131633643e-06,
      "loss": 1.8381,
      "step": 69810
    },
    {
      "epoch": 9.674379936261605,
      "grad_norm": 11.085165023803711,
      "learning_rate": 1.3191076624636276e-06,
      "loss": 1.3827,
      "step": 69820
    },
    {
      "epoch": 9.675765553554108,
      "grad_norm": 13.815390586853027,
      "learning_rate": 1.3135651932936124e-06,
      "loss": 1.6407,
      "step": 69830
    },
    {
      "epoch": 9.677151170846612,
      "grad_norm": 12.597990989685059,
      "learning_rate": 1.308022724123597e-06,
      "loss": 1.5474,
      "step": 69840
    },
    {
      "epoch": 9.678536788139116,
      "grad_norm": 13.994425773620605,
      "learning_rate": 1.302480254953582e-06,
      "loss": 1.6498,
      "step": 69850
    },
    {
      "epoch": 9.67992240543162,
      "grad_norm": 19.607070922851562,
      "learning_rate": 1.2969377857835667e-06,
      "loss": 1.901,
      "step": 69860
    },
    {
      "epoch": 9.681308022724124,
      "grad_norm": 10.383027076721191,
      "learning_rate": 1.2913953166135515e-06,
      "loss": 1.7941,
      "step": 69870
    },
    {
      "epoch": 9.682693640016627,
      "grad_norm": 8.032952308654785,
      "learning_rate": 1.285852847443536e-06,
      "loss": 1.6043,
      "step": 69880
    },
    {
      "epoch": 9.684079257309131,
      "grad_norm": 9.146198272705078,
      "learning_rate": 1.280310378273521e-06,
      "loss": 1.5685,
      "step": 69890
    },
    {
      "epoch": 9.685464874601635,
      "grad_norm": 18.15529441833496,
      "learning_rate": 1.2747679091035055e-06,
      "loss": 1.8499,
      "step": 69900
    },
    {
      "epoch": 9.68685049189414,
      "grad_norm": 19.41303825378418,
      "learning_rate": 1.2692254399334906e-06,
      "loss": 1.7363,
      "step": 69910
    },
    {
      "epoch": 9.688236109186644,
      "grad_norm": 15.736209869384766,
      "learning_rate": 1.2636829707634752e-06,
      "loss": 1.6886,
      "step": 69920
    },
    {
      "epoch": 9.689621726479146,
      "grad_norm": 8.720155715942383,
      "learning_rate": 1.25814050159346e-06,
      "loss": 1.4967,
      "step": 69930
    },
    {
      "epoch": 9.69100734377165,
      "grad_norm": 20.385236740112305,
      "learning_rate": 1.2525980324234446e-06,
      "loss": 1.654,
      "step": 69940
    },
    {
      "epoch": 9.692392961064154,
      "grad_norm": 9.286321640014648,
      "learning_rate": 1.2470555632534294e-06,
      "loss": 2.0149,
      "step": 69950
    },
    {
      "epoch": 9.693778578356659,
      "grad_norm": 11.851957321166992,
      "learning_rate": 1.2415130940834143e-06,
      "loss": 2.1574,
      "step": 69960
    },
    {
      "epoch": 9.695164195649161,
      "grad_norm": 13.420350074768066,
      "learning_rate": 1.235970624913399e-06,
      "loss": 2.2575,
      "step": 69970
    },
    {
      "epoch": 9.696549812941665,
      "grad_norm": 21.406208038330078,
      "learning_rate": 1.2304281557433837e-06,
      "loss": 1.6929,
      "step": 69980
    },
    {
      "epoch": 9.69793543023417,
      "grad_norm": 15.454015731811523,
      "learning_rate": 1.2248856865733685e-06,
      "loss": 1.8524,
      "step": 69990
    },
    {
      "epoch": 9.699321047526674,
      "grad_norm": 9.326025009155273,
      "learning_rate": 1.2193432174033534e-06,
      "loss": 1.785,
      "step": 70000
    },
    {
      "epoch": 9.700706664819178,
      "grad_norm": 12.97040843963623,
      "learning_rate": 1.213800748233338e-06,
      "loss": 1.5631,
      "step": 70010
    },
    {
      "epoch": 9.70209228211168,
      "grad_norm": 15.074176788330078,
      "learning_rate": 1.2082582790633228e-06,
      "loss": 2.1671,
      "step": 70020
    },
    {
      "epoch": 9.703477899404184,
      "grad_norm": 14.100014686584473,
      "learning_rate": 1.2027158098933076e-06,
      "loss": 1.6414,
      "step": 70030
    },
    {
      "epoch": 9.704863516696689,
      "grad_norm": 13.42286205291748,
      "learning_rate": 1.1971733407232922e-06,
      "loss": 1.8626,
      "step": 70040
    },
    {
      "epoch": 9.706249133989193,
      "grad_norm": 12.805304527282715,
      "learning_rate": 1.191630871553277e-06,
      "loss": 1.6068,
      "step": 70050
    },
    {
      "epoch": 9.707634751281695,
      "grad_norm": 13.724370956420898,
      "learning_rate": 1.186088402383262e-06,
      "loss": 1.8601,
      "step": 70060
    },
    {
      "epoch": 9.7090203685742,
      "grad_norm": 17.949310302734375,
      "learning_rate": 1.1805459332132465e-06,
      "loss": 1.8257,
      "step": 70070
    },
    {
      "epoch": 9.710405985866704,
      "grad_norm": 18.902006149291992,
      "learning_rate": 1.1750034640432313e-06,
      "loss": 2.0351,
      "step": 70080
    },
    {
      "epoch": 9.711791603159208,
      "grad_norm": 13.936068534851074,
      "learning_rate": 1.1694609948732162e-06,
      "loss": 1.789,
      "step": 70090
    },
    {
      "epoch": 9.713177220451712,
      "grad_norm": 28.64010238647461,
      "learning_rate": 1.1639185257032008e-06,
      "loss": 1.5545,
      "step": 70100
    },
    {
      "epoch": 9.714562837744214,
      "grad_norm": 13.706573486328125,
      "learning_rate": 1.1583760565331856e-06,
      "loss": 1.5425,
      "step": 70110
    },
    {
      "epoch": 9.715948455036719,
      "grad_norm": 16.685009002685547,
      "learning_rate": 1.1528335873631704e-06,
      "loss": 1.6668,
      "step": 70120
    },
    {
      "epoch": 9.717334072329223,
      "grad_norm": 20.634811401367188,
      "learning_rate": 1.147291118193155e-06,
      "loss": 1.8257,
      "step": 70130
    },
    {
      "epoch": 9.718719689621727,
      "grad_norm": 11.470279693603516,
      "learning_rate": 1.1417486490231399e-06,
      "loss": 2.0148,
      "step": 70140
    },
    {
      "epoch": 9.720105306914231,
      "grad_norm": 16.404573440551758,
      "learning_rate": 1.1362061798531247e-06,
      "loss": 1.6926,
      "step": 70150
    },
    {
      "epoch": 9.721490924206734,
      "grad_norm": 11.85421085357666,
      "learning_rate": 1.1306637106831095e-06,
      "loss": 1.8424,
      "step": 70160
    },
    {
      "epoch": 9.722876541499238,
      "grad_norm": 22.40642547607422,
      "learning_rate": 1.1251212415130941e-06,
      "loss": 1.6925,
      "step": 70170
    },
    {
      "epoch": 9.724262158791742,
      "grad_norm": 9.870291709899902,
      "learning_rate": 1.119578772343079e-06,
      "loss": 2.2421,
      "step": 70180
    },
    {
      "epoch": 9.725647776084246,
      "grad_norm": 10.021828651428223,
      "learning_rate": 1.1140363031730638e-06,
      "loss": 1.7388,
      "step": 70190
    },
    {
      "epoch": 9.727033393376749,
      "grad_norm": 10.946937561035156,
      "learning_rate": 1.1084938340030484e-06,
      "loss": 1.5668,
      "step": 70200
    },
    {
      "epoch": 9.728419010669253,
      "grad_norm": 12.287796020507812,
      "learning_rate": 1.1029513648330332e-06,
      "loss": 1.815,
      "step": 70210
    },
    {
      "epoch": 9.729804627961757,
      "grad_norm": 18.269807815551758,
      "learning_rate": 1.097408895663018e-06,
      "loss": 1.6156,
      "step": 70220
    },
    {
      "epoch": 9.731190245254261,
      "grad_norm": 9.586369514465332,
      "learning_rate": 1.0918664264930027e-06,
      "loss": 1.3764,
      "step": 70230
    },
    {
      "epoch": 9.732575862546765,
      "grad_norm": 12.29288387298584,
      "learning_rate": 1.0863239573229875e-06,
      "loss": 1.7708,
      "step": 70240
    },
    {
      "epoch": 9.733961479839268,
      "grad_norm": 13.033992767333984,
      "learning_rate": 1.0807814881529723e-06,
      "loss": 1.7105,
      "step": 70250
    },
    {
      "epoch": 9.735347097131772,
      "grad_norm": 25.68021583557129,
      "learning_rate": 1.075239018982957e-06,
      "loss": 1.9122,
      "step": 70260
    },
    {
      "epoch": 9.736732714424276,
      "grad_norm": 13.95425033569336,
      "learning_rate": 1.0696965498129418e-06,
      "loss": 1.8558,
      "step": 70270
    },
    {
      "epoch": 9.73811833171678,
      "grad_norm": 19.893325805664062,
      "learning_rate": 1.0641540806429266e-06,
      "loss": 1.9544,
      "step": 70280
    },
    {
      "epoch": 9.739503949009283,
      "grad_norm": 10.819453239440918,
      "learning_rate": 1.0586116114729112e-06,
      "loss": 1.7829,
      "step": 70290
    },
    {
      "epoch": 9.740889566301787,
      "grad_norm": 9.029548645019531,
      "learning_rate": 1.053069142302896e-06,
      "loss": 1.8939,
      "step": 70300
    },
    {
      "epoch": 9.742275183594291,
      "grad_norm": 23.862035751342773,
      "learning_rate": 1.0475266731328808e-06,
      "loss": 2.0704,
      "step": 70310
    },
    {
      "epoch": 9.743660800886795,
      "grad_norm": 16.61170196533203,
      "learning_rate": 1.0419842039628655e-06,
      "loss": 1.9701,
      "step": 70320
    },
    {
      "epoch": 9.7450464181793,
      "grad_norm": 9.67358684539795,
      "learning_rate": 1.0364417347928503e-06,
      "loss": 1.7555,
      "step": 70330
    },
    {
      "epoch": 9.746432035471802,
      "grad_norm": 15.360368728637695,
      "learning_rate": 1.0308992656228351e-06,
      "loss": 1.7659,
      "step": 70340
    },
    {
      "epoch": 9.747817652764306,
      "grad_norm": 13.11561393737793,
      "learning_rate": 1.0253567964528197e-06,
      "loss": 2.1801,
      "step": 70350
    },
    {
      "epoch": 9.74920327005681,
      "grad_norm": 29.563159942626953,
      "learning_rate": 1.0198143272828046e-06,
      "loss": 1.8357,
      "step": 70360
    },
    {
      "epoch": 9.750588887349315,
      "grad_norm": 9.187397003173828,
      "learning_rate": 1.0142718581127894e-06,
      "loss": 1.6738,
      "step": 70370
    },
    {
      "epoch": 9.751974504641819,
      "grad_norm": 27.43351936340332,
      "learning_rate": 1.008729388942774e-06,
      "loss": 2.0167,
      "step": 70380
    },
    {
      "epoch": 9.753360121934321,
      "grad_norm": 15.6108980178833,
      "learning_rate": 1.0031869197727588e-06,
      "loss": 2.0465,
      "step": 70390
    },
    {
      "epoch": 9.754745739226825,
      "grad_norm": 8.916783332824707,
      "learning_rate": 9.976444506027436e-07,
      "loss": 1.8264,
      "step": 70400
    },
    {
      "epoch": 9.75613135651933,
      "grad_norm": 14.391830444335938,
      "learning_rate": 9.921019814327283e-07,
      "loss": 1.9433,
      "step": 70410
    },
    {
      "epoch": 9.757516973811834,
      "grad_norm": 10.785684585571289,
      "learning_rate": 9.86559512262713e-07,
      "loss": 2.2224,
      "step": 70420
    },
    {
      "epoch": 9.758902591104338,
      "grad_norm": 14.829018592834473,
      "learning_rate": 9.81017043092698e-07,
      "loss": 1.6729,
      "step": 70430
    },
    {
      "epoch": 9.76028820839684,
      "grad_norm": 15.633889198303223,
      "learning_rate": 9.754745739226825e-07,
      "loss": 1.7996,
      "step": 70440
    },
    {
      "epoch": 9.761673825689344,
      "grad_norm": 12.707889556884766,
      "learning_rate": 9.699321047526673e-07,
      "loss": 1.7805,
      "step": 70450
    },
    {
      "epoch": 9.763059442981849,
      "grad_norm": 21.59792137145996,
      "learning_rate": 9.643896355826522e-07,
      "loss": 2.1187,
      "step": 70460
    },
    {
      "epoch": 9.764445060274353,
      "grad_norm": 16.053874969482422,
      "learning_rate": 9.58847166412637e-07,
      "loss": 1.7269,
      "step": 70470
    },
    {
      "epoch": 9.765830677566855,
      "grad_norm": 13.20162582397461,
      "learning_rate": 9.533046972426217e-07,
      "loss": 1.8498,
      "step": 70480
    },
    {
      "epoch": 9.76721629485936,
      "grad_norm": 8.053985595703125,
      "learning_rate": 9.477622280726064e-07,
      "loss": 1.3949,
      "step": 70490
    },
    {
      "epoch": 9.768601912151864,
      "grad_norm": 9.76935863494873,
      "learning_rate": 9.422197589025913e-07,
      "loss": 1.4495,
      "step": 70500
    },
    {
      "epoch": 9.769987529444368,
      "grad_norm": 7.812654495239258,
      "learning_rate": 9.36677289732576e-07,
      "loss": 2.3468,
      "step": 70510
    },
    {
      "epoch": 9.771373146736872,
      "grad_norm": 18.72902488708496,
      "learning_rate": 9.311348205625607e-07,
      "loss": 1.8065,
      "step": 70520
    },
    {
      "epoch": 9.772758764029374,
      "grad_norm": 14.161513328552246,
      "learning_rate": 9.255923513925455e-07,
      "loss": 1.6819,
      "step": 70530
    },
    {
      "epoch": 9.774144381321879,
      "grad_norm": 13.177703857421875,
      "learning_rate": 9.200498822225303e-07,
      "loss": 1.6688,
      "step": 70540
    },
    {
      "epoch": 9.775529998614383,
      "grad_norm": 13.240982055664062,
      "learning_rate": 9.14507413052515e-07,
      "loss": 1.4665,
      "step": 70550
    },
    {
      "epoch": 9.776915615906887,
      "grad_norm": 14.835309982299805,
      "learning_rate": 9.089649438824998e-07,
      "loss": 1.4765,
      "step": 70560
    },
    {
      "epoch": 9.77830123319939,
      "grad_norm": 12.867478370666504,
      "learning_rate": 9.034224747124845e-07,
      "loss": 1.6777,
      "step": 70570
    },
    {
      "epoch": 9.779686850491894,
      "grad_norm": 9.61563491821289,
      "learning_rate": 8.978800055424692e-07,
      "loss": 1.6625,
      "step": 70580
    },
    {
      "epoch": 9.781072467784398,
      "grad_norm": 12.66744613647461,
      "learning_rate": 8.923375363724541e-07,
      "loss": 1.8039,
      "step": 70590
    },
    {
      "epoch": 9.782458085076902,
      "grad_norm": 13.723897933959961,
      "learning_rate": 8.867950672024388e-07,
      "loss": 1.6456,
      "step": 70600
    },
    {
      "epoch": 9.783843702369406,
      "grad_norm": 14.575887680053711,
      "learning_rate": 8.812525980324235e-07,
      "loss": 1.9818,
      "step": 70610
    },
    {
      "epoch": 9.785229319661909,
      "grad_norm": 7.110925197601318,
      "learning_rate": 8.757101288624083e-07,
      "loss": 1.9814,
      "step": 70620
    },
    {
      "epoch": 9.786614936954413,
      "grad_norm": 18.260534286499023,
      "learning_rate": 8.70167659692393e-07,
      "loss": 1.5478,
      "step": 70630
    },
    {
      "epoch": 9.788000554246917,
      "grad_norm": 7.359407901763916,
      "learning_rate": 8.646251905223778e-07,
      "loss": 1.7157,
      "step": 70640
    },
    {
      "epoch": 9.789386171539421,
      "grad_norm": 11.511087417602539,
      "learning_rate": 8.590827213523625e-07,
      "loss": 1.5196,
      "step": 70650
    },
    {
      "epoch": 9.790771788831925,
      "grad_norm": 16.527490615844727,
      "learning_rate": 8.535402521823473e-07,
      "loss": 1.8386,
      "step": 70660
    },
    {
      "epoch": 9.792157406124428,
      "grad_norm": 22.068538665771484,
      "learning_rate": 8.47997783012332e-07,
      "loss": 1.6174,
      "step": 70670
    },
    {
      "epoch": 9.793543023416932,
      "grad_norm": 19.76578140258789,
      "learning_rate": 8.424553138423168e-07,
      "loss": 1.8532,
      "step": 70680
    },
    {
      "epoch": 9.794928640709436,
      "grad_norm": 17.5921688079834,
      "learning_rate": 8.369128446723016e-07,
      "loss": 1.6048,
      "step": 70690
    },
    {
      "epoch": 9.79631425800194,
      "grad_norm": 22.176105499267578,
      "learning_rate": 8.313703755022863e-07,
      "loss": 2.2255,
      "step": 70700
    },
    {
      "epoch": 9.797699875294443,
      "grad_norm": 9.636879920959473,
      "learning_rate": 8.25827906332271e-07,
      "loss": 1.7507,
      "step": 70710
    },
    {
      "epoch": 9.799085492586947,
      "grad_norm": 15.45743465423584,
      "learning_rate": 8.202854371622558e-07,
      "loss": 1.6222,
      "step": 70720
    },
    {
      "epoch": 9.800471109879451,
      "grad_norm": 17.10251235961914,
      "learning_rate": 8.147429679922406e-07,
      "loss": 1.3497,
      "step": 70730
    },
    {
      "epoch": 9.801856727171955,
      "grad_norm": 15.103700637817383,
      "learning_rate": 8.092004988222253e-07,
      "loss": 1.9809,
      "step": 70740
    },
    {
      "epoch": 9.80324234446446,
      "grad_norm": 18.260889053344727,
      "learning_rate": 8.036580296522101e-07,
      "loss": 2.1851,
      "step": 70750
    },
    {
      "epoch": 9.804627961756962,
      "grad_norm": 8.489361763000488,
      "learning_rate": 7.981155604821948e-07,
      "loss": 1.7188,
      "step": 70760
    },
    {
      "epoch": 9.806013579049466,
      "grad_norm": 14.31036376953125,
      "learning_rate": 7.925730913121795e-07,
      "loss": 1.6722,
      "step": 70770
    },
    {
      "epoch": 9.80739919634197,
      "grad_norm": 14.387653350830078,
      "learning_rate": 7.870306221421644e-07,
      "loss": 1.9171,
      "step": 70780
    },
    {
      "epoch": 9.808784813634475,
      "grad_norm": 8.149399757385254,
      "learning_rate": 7.814881529721492e-07,
      "loss": 1.6558,
      "step": 70790
    },
    {
      "epoch": 9.810170430926977,
      "grad_norm": 10.492286682128906,
      "learning_rate": 7.75945683802134e-07,
      "loss": 1.6404,
      "step": 70800
    },
    {
      "epoch": 9.811556048219481,
      "grad_norm": 9.157719612121582,
      "learning_rate": 7.704032146321187e-07,
      "loss": 1.5277,
      "step": 70810
    },
    {
      "epoch": 9.812941665511985,
      "grad_norm": 5.2356133460998535,
      "learning_rate": 7.648607454621035e-07,
      "loss": 1.6643,
      "step": 70820
    },
    {
      "epoch": 9.81432728280449,
      "grad_norm": 13.979439735412598,
      "learning_rate": 7.593182762920882e-07,
      "loss": 1.6555,
      "step": 70830
    },
    {
      "epoch": 9.815712900096994,
      "grad_norm": 12.38845157623291,
      "learning_rate": 7.53775807122073e-07,
      "loss": 1.417,
      "step": 70840
    },
    {
      "epoch": 9.817098517389496,
      "grad_norm": 16.348037719726562,
      "learning_rate": 7.482333379520577e-07,
      "loss": 1.6848,
      "step": 70850
    },
    {
      "epoch": 9.818484134682,
      "grad_norm": 16.782777786254883,
      "learning_rate": 7.426908687820425e-07,
      "loss": 1.8801,
      "step": 70860
    },
    {
      "epoch": 9.819869751974505,
      "grad_norm": 10.635272026062012,
      "learning_rate": 7.371483996120273e-07,
      "loss": 1.8084,
      "step": 70870
    },
    {
      "epoch": 9.821255369267009,
      "grad_norm": 19.74321174621582,
      "learning_rate": 7.31605930442012e-07,
      "loss": 1.8766,
      "step": 70880
    },
    {
      "epoch": 9.822640986559513,
      "grad_norm": 12.092008590698242,
      "learning_rate": 7.260634612719967e-07,
      "loss": 2.275,
      "step": 70890
    },
    {
      "epoch": 9.824026603852015,
      "grad_norm": 27.105504989624023,
      "learning_rate": 7.205209921019815e-07,
      "loss": 1.9861,
      "step": 70900
    },
    {
      "epoch": 9.82541222114452,
      "grad_norm": 15.224441528320312,
      "learning_rate": 7.149785229319663e-07,
      "loss": 2.0971,
      "step": 70910
    },
    {
      "epoch": 9.826797838437024,
      "grad_norm": 14.064488410949707,
      "learning_rate": 7.09436053761951e-07,
      "loss": 2.3129,
      "step": 70920
    },
    {
      "epoch": 9.828183455729528,
      "grad_norm": 10.701167106628418,
      "learning_rate": 7.038935845919358e-07,
      "loss": 1.5097,
      "step": 70930
    },
    {
      "epoch": 9.829569073022032,
      "grad_norm": 10.691686630249023,
      "learning_rate": 6.983511154219205e-07,
      "loss": 1.8958,
      "step": 70940
    },
    {
      "epoch": 9.830954690314535,
      "grad_norm": 12.803829193115234,
      "learning_rate": 6.928086462519052e-07,
      "loss": 1.5476,
      "step": 70950
    },
    {
      "epoch": 9.832340307607039,
      "grad_norm": 24.20540428161621,
      "learning_rate": 6.872661770818901e-07,
      "loss": 1.8709,
      "step": 70960
    },
    {
      "epoch": 9.833725924899543,
      "grad_norm": 10.399876594543457,
      "learning_rate": 6.817237079118748e-07,
      "loss": 1.6905,
      "step": 70970
    },
    {
      "epoch": 9.835111542192047,
      "grad_norm": 18.62196922302246,
      "learning_rate": 6.761812387418595e-07,
      "loss": 1.6493,
      "step": 70980
    },
    {
      "epoch": 9.83649715948455,
      "grad_norm": 11.314421653747559,
      "learning_rate": 6.706387695718443e-07,
      "loss": 1.6551,
      "step": 70990
    },
    {
      "epoch": 9.837882776777054,
      "grad_norm": 12.332291603088379,
      "learning_rate": 6.650963004018291e-07,
      "loss": 1.6172,
      "step": 71000
    },
    {
      "epoch": 9.839268394069558,
      "grad_norm": 8.405961036682129,
      "learning_rate": 6.595538312318138e-07,
      "loss": 1.9089,
      "step": 71010
    },
    {
      "epoch": 9.840654011362062,
      "grad_norm": 14.110248565673828,
      "learning_rate": 6.540113620617985e-07,
      "loss": 1.7463,
      "step": 71020
    },
    {
      "epoch": 9.842039628654566,
      "grad_norm": 24.941919326782227,
      "learning_rate": 6.484688928917833e-07,
      "loss": 1.5201,
      "step": 71030
    },
    {
      "epoch": 9.843425245947069,
      "grad_norm": 14.311807632446289,
      "learning_rate": 6.42926423721768e-07,
      "loss": 1.7383,
      "step": 71040
    },
    {
      "epoch": 9.844810863239573,
      "grad_norm": 16.485322952270508,
      "learning_rate": 6.373839545517528e-07,
      "loss": 1.934,
      "step": 71050
    },
    {
      "epoch": 9.846196480532077,
      "grad_norm": 18.282360076904297,
      "learning_rate": 6.318414853817376e-07,
      "loss": 1.5393,
      "step": 71060
    },
    {
      "epoch": 9.847582097824581,
      "grad_norm": 9.802228927612305,
      "learning_rate": 6.262990162117223e-07,
      "loss": 1.9033,
      "step": 71070
    },
    {
      "epoch": 9.848967715117084,
      "grad_norm": 17.433258056640625,
      "learning_rate": 6.207565470417071e-07,
      "loss": 1.5706,
      "step": 71080
    },
    {
      "epoch": 9.850353332409588,
      "grad_norm": 11.35959243774414,
      "learning_rate": 6.152140778716919e-07,
      "loss": 1.8754,
      "step": 71090
    },
    {
      "epoch": 9.851738949702092,
      "grad_norm": 10.653535842895508,
      "learning_rate": 6.096716087016767e-07,
      "loss": 1.6376,
      "step": 71100
    },
    {
      "epoch": 9.853124566994596,
      "grad_norm": 11.878519058227539,
      "learning_rate": 6.041291395316614e-07,
      "loss": 1.759,
      "step": 71110
    },
    {
      "epoch": 9.8545101842871,
      "grad_norm": 26.50868797302246,
      "learning_rate": 5.985866703616461e-07,
      "loss": 1.794,
      "step": 71120
    },
    {
      "epoch": 9.855895801579603,
      "grad_norm": 23.948379516601562,
      "learning_rate": 5.93044201191631e-07,
      "loss": 1.7353,
      "step": 71130
    },
    {
      "epoch": 9.857281418872107,
      "grad_norm": 16.092378616333008,
      "learning_rate": 5.875017320216157e-07,
      "loss": 1.4808,
      "step": 71140
    },
    {
      "epoch": 9.858667036164611,
      "grad_norm": 10.692893028259277,
      "learning_rate": 5.819592628516004e-07,
      "loss": 1.6927,
      "step": 71150
    },
    {
      "epoch": 9.860052653457116,
      "grad_norm": 15.05268383026123,
      "learning_rate": 5.764167936815852e-07,
      "loss": 1.6532,
      "step": 71160
    },
    {
      "epoch": 9.86143827074962,
      "grad_norm": 11.496173858642578,
      "learning_rate": 5.708743245115699e-07,
      "loss": 2.0398,
      "step": 71170
    },
    {
      "epoch": 9.862823888042122,
      "grad_norm": 8.52410888671875,
      "learning_rate": 5.653318553415548e-07,
      "loss": 1.8662,
      "step": 71180
    },
    {
      "epoch": 9.864209505334626,
      "grad_norm": 9.168957710266113,
      "learning_rate": 5.597893861715395e-07,
      "loss": 1.8881,
      "step": 71190
    },
    {
      "epoch": 9.86559512262713,
      "grad_norm": 18.15249252319336,
      "learning_rate": 5.542469170015242e-07,
      "loss": 1.5391,
      "step": 71200
    },
    {
      "epoch": 9.866980739919635,
      "grad_norm": 16.31536293029785,
      "learning_rate": 5.48704447831509e-07,
      "loss": 1.7935,
      "step": 71210
    },
    {
      "epoch": 9.868366357212139,
      "grad_norm": 10.321258544921875,
      "learning_rate": 5.431619786614937e-07,
      "loss": 1.9989,
      "step": 71220
    },
    {
      "epoch": 9.869751974504641,
      "grad_norm": 13.542824745178223,
      "learning_rate": 5.376195094914785e-07,
      "loss": 1.8985,
      "step": 71230
    },
    {
      "epoch": 9.871137591797146,
      "grad_norm": 18.468311309814453,
      "learning_rate": 5.320770403214633e-07,
      "loss": 1.4499,
      "step": 71240
    },
    {
      "epoch": 9.87252320908965,
      "grad_norm": 11.673783302307129,
      "learning_rate": 5.26534571151448e-07,
      "loss": 2.0782,
      "step": 71250
    },
    {
      "epoch": 9.873908826382154,
      "grad_norm": 12.099309921264648,
      "learning_rate": 5.209921019814327e-07,
      "loss": 1.9645,
      "step": 71260
    },
    {
      "epoch": 9.875294443674656,
      "grad_norm": 20.75649642944336,
      "learning_rate": 5.154496328114176e-07,
      "loss": 1.8701,
      "step": 71270
    },
    {
      "epoch": 9.87668006096716,
      "grad_norm": 13.584110260009766,
      "learning_rate": 5.099071636414023e-07,
      "loss": 2.0738,
      "step": 71280
    },
    {
      "epoch": 9.878065678259665,
      "grad_norm": 11.547327041625977,
      "learning_rate": 5.04364694471387e-07,
      "loss": 1.5412,
      "step": 71290
    },
    {
      "epoch": 9.879451295552169,
      "grad_norm": 12.513465881347656,
      "learning_rate": 4.988222253013718e-07,
      "loss": 1.5659,
      "step": 71300
    },
    {
      "epoch": 9.880836912844671,
      "grad_norm": 17.06499481201172,
      "learning_rate": 4.932797561313565e-07,
      "loss": 1.8218,
      "step": 71310
    },
    {
      "epoch": 9.882222530137176,
      "grad_norm": 16.654340744018555,
      "learning_rate": 4.877372869613413e-07,
      "loss": 1.5348,
      "step": 71320
    },
    {
      "epoch": 9.88360814742968,
      "grad_norm": 10.226766586303711,
      "learning_rate": 4.821948177913261e-07,
      "loss": 1.8766,
      "step": 71330
    },
    {
      "epoch": 9.884993764722184,
      "grad_norm": 12.28305721282959,
      "learning_rate": 4.7665234862131086e-07,
      "loss": 2.2356,
      "step": 71340
    },
    {
      "epoch": 9.886379382014688,
      "grad_norm": 18.482925415039062,
      "learning_rate": 4.7110987945129563e-07,
      "loss": 2.0784,
      "step": 71350
    },
    {
      "epoch": 9.88776499930719,
      "grad_norm": 11.3240966796875,
      "learning_rate": 4.6556741028128035e-07,
      "loss": 1.609,
      "step": 71360
    },
    {
      "epoch": 9.889150616599695,
      "grad_norm": 8.933137893676758,
      "learning_rate": 4.600249411112651e-07,
      "loss": 1.2158,
      "step": 71370
    },
    {
      "epoch": 9.890536233892199,
      "grad_norm": 12.115071296691895,
      "learning_rate": 4.544824719412499e-07,
      "loss": 1.5557,
      "step": 71380
    },
    {
      "epoch": 9.891921851184703,
      "grad_norm": 17.73365592956543,
      "learning_rate": 4.489400027712346e-07,
      "loss": 1.4806,
      "step": 71390
    },
    {
      "epoch": 9.893307468477207,
      "grad_norm": 10.458446502685547,
      "learning_rate": 4.433975336012194e-07,
      "loss": 1.9851,
      "step": 71400
    },
    {
      "epoch": 9.89469308576971,
      "grad_norm": 18.906444549560547,
      "learning_rate": 4.3785506443120416e-07,
      "loss": 1.9574,
      "step": 71410
    },
    {
      "epoch": 9.896078703062214,
      "grad_norm": 17.6090145111084,
      "learning_rate": 4.323125952611889e-07,
      "loss": 2.1974,
      "step": 71420
    },
    {
      "epoch": 9.897464320354718,
      "grad_norm": 10.068655967712402,
      "learning_rate": 4.2677012609117366e-07,
      "loss": 1.5183,
      "step": 71430
    },
    {
      "epoch": 9.898849937647222,
      "grad_norm": 16.68724250793457,
      "learning_rate": 4.212276569211584e-07,
      "loss": 1.8796,
      "step": 71440
    },
    {
      "epoch": 9.900235554939727,
      "grad_norm": 18.79935646057129,
      "learning_rate": 4.1568518775114315e-07,
      "loss": 1.9134,
      "step": 71450
    },
    {
      "epoch": 9.901621172232229,
      "grad_norm": 9.922050476074219,
      "learning_rate": 4.101427185811279e-07,
      "loss": 1.5262,
      "step": 71460
    },
    {
      "epoch": 9.903006789524733,
      "grad_norm": 18.07642364501953,
      "learning_rate": 4.0460024941111264e-07,
      "loss": 1.3454,
      "step": 71470
    },
    {
      "epoch": 9.904392406817237,
      "grad_norm": 15.067742347717285,
      "learning_rate": 3.990577802410974e-07,
      "loss": 1.5292,
      "step": 71480
    },
    {
      "epoch": 9.905778024109742,
      "grad_norm": 14.799211502075195,
      "learning_rate": 3.935153110710822e-07,
      "loss": 1.7072,
      "step": 71490
    },
    {
      "epoch": 9.907163641402244,
      "grad_norm": 22.191898345947266,
      "learning_rate": 3.87972841901067e-07,
      "loss": 1.9466,
      "step": 71500
    },
    {
      "epoch": 9.908549258694748,
      "grad_norm": 9.409029006958008,
      "learning_rate": 3.8243037273105173e-07,
      "loss": 1.8509,
      "step": 71510
    },
    {
      "epoch": 9.909934875987252,
      "grad_norm": 21.10394859313965,
      "learning_rate": 3.768879035610365e-07,
      "loss": 1.7677,
      "step": 71520
    },
    {
      "epoch": 9.911320493279757,
      "grad_norm": 9.472365379333496,
      "learning_rate": 3.713454343910212e-07,
      "loss": 1.6145,
      "step": 71530
    },
    {
      "epoch": 9.91270611057226,
      "grad_norm": 14.666275024414062,
      "learning_rate": 3.65802965221006e-07,
      "loss": 1.997,
      "step": 71540
    },
    {
      "epoch": 9.914091727864763,
      "grad_norm": 10.548608779907227,
      "learning_rate": 3.6026049605099077e-07,
      "loss": 1.8518,
      "step": 71550
    },
    {
      "epoch": 9.915477345157267,
      "grad_norm": 5.616037368774414,
      "learning_rate": 3.547180268809755e-07,
      "loss": 2.0226,
      "step": 71560
    },
    {
      "epoch": 9.916862962449772,
      "grad_norm": 11.984661102294922,
      "learning_rate": 3.4917555771096026e-07,
      "loss": 1.6306,
      "step": 71570
    },
    {
      "epoch": 9.918248579742276,
      "grad_norm": 11.90195369720459,
      "learning_rate": 3.4363308854094504e-07,
      "loss": 1.8584,
      "step": 71580
    },
    {
      "epoch": 9.919634197034778,
      "grad_norm": 16.852352142333984,
      "learning_rate": 3.3809061937092976e-07,
      "loss": 2.0273,
      "step": 71590
    },
    {
      "epoch": 9.921019814327282,
      "grad_norm": 9.57042121887207,
      "learning_rate": 3.3254815020091453e-07,
      "loss": 1.9059,
      "step": 71600
    },
    {
      "epoch": 9.922405431619786,
      "grad_norm": 8.387527465820312,
      "learning_rate": 3.2700568103089925e-07,
      "loss": 1.4719,
      "step": 71610
    },
    {
      "epoch": 9.92379104891229,
      "grad_norm": 21.204193115234375,
      "learning_rate": 3.21463211860884e-07,
      "loss": 1.6031,
      "step": 71620
    },
    {
      "epoch": 9.925176666204795,
      "grad_norm": 20.740324020385742,
      "learning_rate": 3.159207426908688e-07,
      "loss": 2.0755,
      "step": 71630
    },
    {
      "epoch": 9.926562283497297,
      "grad_norm": 11.7852783203125,
      "learning_rate": 3.1037827352085357e-07,
      "loss": 1.7348,
      "step": 71640
    },
    {
      "epoch": 9.927947900789801,
      "grad_norm": 12.914244651794434,
      "learning_rate": 3.0483580435083834e-07,
      "loss": 2.1072,
      "step": 71650
    },
    {
      "epoch": 9.929333518082306,
      "grad_norm": 8.124021530151367,
      "learning_rate": 2.9929333518082306e-07,
      "loss": 1.4653,
      "step": 71660
    },
    {
      "epoch": 9.93071913537481,
      "grad_norm": 11.920662879943848,
      "learning_rate": 2.9375086601080783e-07,
      "loss": 1.695,
      "step": 71670
    },
    {
      "epoch": 9.932104752667314,
      "grad_norm": 12.216764450073242,
      "learning_rate": 2.882083968407926e-07,
      "loss": 1.4257,
      "step": 71680
    },
    {
      "epoch": 9.933490369959816,
      "grad_norm": 15.978202819824219,
      "learning_rate": 2.826659276707774e-07,
      "loss": 1.6963,
      "step": 71690
    },
    {
      "epoch": 9.93487598725232,
      "grad_norm": 14.385436058044434,
      "learning_rate": 2.771234585007621e-07,
      "loss": 1.6435,
      "step": 71700
    },
    {
      "epoch": 9.936261604544825,
      "grad_norm": 27.855682373046875,
      "learning_rate": 2.7158098933074687e-07,
      "loss": 1.6815,
      "step": 71710
    },
    {
      "epoch": 9.937647221837329,
      "grad_norm": 7.00715446472168,
      "learning_rate": 2.6603852016073165e-07,
      "loss": 1.7384,
      "step": 71720
    },
    {
      "epoch": 9.939032839129833,
      "grad_norm": 10.26565933227539,
      "learning_rate": 2.6049605099071637e-07,
      "loss": 1.5617,
      "step": 71730
    },
    {
      "epoch": 9.940418456422336,
      "grad_norm": 20.038009643554688,
      "learning_rate": 2.5495358182070114e-07,
      "loss": 1.4477,
      "step": 71740
    },
    {
      "epoch": 9.94180407371484,
      "grad_norm": 14.788007736206055,
      "learning_rate": 2.494111126506859e-07,
      "loss": 1.9913,
      "step": 71750
    },
    {
      "epoch": 9.943189691007344,
      "grad_norm": 7.928048133850098,
      "learning_rate": 2.444228903976722e-07,
      "loss": 1.9083,
      "step": 71760
    },
    {
      "epoch": 9.944575308299848,
      "grad_norm": 21.44306182861328,
      "learning_rate": 2.388804212276569e-07,
      "loss": 1.672,
      "step": 71770
    },
    {
      "epoch": 9.94596092559235,
      "grad_norm": 9.209049224853516,
      "learning_rate": 2.3333795205764168e-07,
      "loss": 2.1333,
      "step": 71780
    },
    {
      "epoch": 9.947346542884855,
      "grad_norm": 20.252952575683594,
      "learning_rate": 2.2779548288762646e-07,
      "loss": 2.1319,
      "step": 71790
    },
    {
      "epoch": 9.948732160177359,
      "grad_norm": 22.29323959350586,
      "learning_rate": 2.2225301371761123e-07,
      "loss": 1.7766,
      "step": 71800
    },
    {
      "epoch": 9.950117777469863,
      "grad_norm": 19.147441864013672,
      "learning_rate": 2.1671054454759598e-07,
      "loss": 1.8324,
      "step": 71810
    },
    {
      "epoch": 9.951503394762367,
      "grad_norm": 21.661725997924805,
      "learning_rate": 2.1116807537758075e-07,
      "loss": 2.04,
      "step": 71820
    },
    {
      "epoch": 9.95288901205487,
      "grad_norm": 9.815688133239746,
      "learning_rate": 2.056256062075655e-07,
      "loss": 1.6072,
      "step": 71830
    },
    {
      "epoch": 9.954274629347374,
      "grad_norm": 7.673615455627441,
      "learning_rate": 2.0008313703755024e-07,
      "loss": 1.7183,
      "step": 71840
    },
    {
      "epoch": 9.955660246639878,
      "grad_norm": 14.958210945129395,
      "learning_rate": 1.94540667867535e-07,
      "loss": 1.988,
      "step": 71850
    },
    {
      "epoch": 9.957045863932382,
      "grad_norm": 13.050516128540039,
      "learning_rate": 1.8899819869751976e-07,
      "loss": 1.5439,
      "step": 71860
    },
    {
      "epoch": 9.958431481224885,
      "grad_norm": 12.304810523986816,
      "learning_rate": 1.834557295275045e-07,
      "loss": 1.7154,
      "step": 71870
    },
    {
      "epoch": 9.959817098517389,
      "grad_norm": 11.960938453674316,
      "learning_rate": 1.779132603574893e-07,
      "loss": 1.3061,
      "step": 71880
    },
    {
      "epoch": 9.961202715809893,
      "grad_norm": 9.936639785766602,
      "learning_rate": 1.7237079118747405e-07,
      "loss": 1.777,
      "step": 71890
    },
    {
      "epoch": 9.962588333102397,
      "grad_norm": 12.99637508392334,
      "learning_rate": 1.668283220174588e-07,
      "loss": 1.7999,
      "step": 71900
    },
    {
      "epoch": 9.963973950394902,
      "grad_norm": 18.009302139282227,
      "learning_rate": 1.6128585284744354e-07,
      "loss": 1.754,
      "step": 71910
    },
    {
      "epoch": 9.965359567687404,
      "grad_norm": 16.861568450927734,
      "learning_rate": 1.5574338367742832e-07,
      "loss": 1.641,
      "step": 71920
    },
    {
      "epoch": 9.966745184979908,
      "grad_norm": 9.748063087463379,
      "learning_rate": 1.5020091450741306e-07,
      "loss": 1.7492,
      "step": 71930
    },
    {
      "epoch": 9.968130802272412,
      "grad_norm": 14.382523536682129,
      "learning_rate": 1.4465844533739784e-07,
      "loss": 1.9096,
      "step": 71940
    },
    {
      "epoch": 9.969516419564917,
      "grad_norm": 17.2869930267334,
      "learning_rate": 1.3911597616738258e-07,
      "loss": 2.3794,
      "step": 71950
    },
    {
      "epoch": 9.97090203685742,
      "grad_norm": 11.742592811584473,
      "learning_rate": 1.3357350699736733e-07,
      "loss": 2.2862,
      "step": 71960
    },
    {
      "epoch": 9.972287654149923,
      "grad_norm": 21.188032150268555,
      "learning_rate": 1.280310378273521e-07,
      "loss": 1.5879,
      "step": 71970
    },
    {
      "epoch": 9.973673271442427,
      "grad_norm": 14.4320707321167,
      "learning_rate": 1.2248856865733685e-07,
      "loss": 1.9998,
      "step": 71980
    },
    {
      "epoch": 9.975058888734932,
      "grad_norm": 12.35987663269043,
      "learning_rate": 1.1694609948732161e-07,
      "loss": 1.926,
      "step": 71990
    },
    {
      "epoch": 9.976444506027436,
      "grad_norm": 14.496160507202148,
      "learning_rate": 1.1140363031730637e-07,
      "loss": 1.5626,
      "step": 72000
    },
    {
      "epoch": 9.977830123319938,
      "grad_norm": 11.433377265930176,
      "learning_rate": 1.0586116114729111e-07,
      "loss": 1.9824,
      "step": 72010
    },
    {
      "epoch": 9.979215740612442,
      "grad_norm": 13.916341781616211,
      "learning_rate": 1.0031869197727589e-07,
      "loss": 1.5184,
      "step": 72020
    },
    {
      "epoch": 9.980601357904947,
      "grad_norm": 17.002073287963867,
      "learning_rate": 9.477622280726065e-08,
      "loss": 2.2156,
      "step": 72030
    },
    {
      "epoch": 9.98198697519745,
      "grad_norm": 14.08128547668457,
      "learning_rate": 8.923375363724539e-08,
      "loss": 1.7751,
      "step": 72040
    },
    {
      "epoch": 9.983372592489955,
      "grad_norm": 12.670425415039062,
      "learning_rate": 8.369128446723015e-08,
      "loss": 1.7366,
      "step": 72050
    },
    {
      "epoch": 9.984758209782457,
      "grad_norm": 13.486283302307129,
      "learning_rate": 7.814881529721493e-08,
      "loss": 2.0455,
      "step": 72060
    },
    {
      "epoch": 9.986143827074962,
      "grad_norm": 9.655299186706543,
      "learning_rate": 7.260634612719967e-08,
      "loss": 1.8165,
      "step": 72070
    },
    {
      "epoch": 9.987529444367466,
      "grad_norm": 15.4603271484375,
      "learning_rate": 6.706387695718443e-08,
      "loss": 1.6322,
      "step": 72080
    },
    {
      "epoch": 9.98891506165997,
      "grad_norm": 14.610941886901855,
      "learning_rate": 6.152140778716919e-08,
      "loss": 1.8015,
      "step": 72090
    },
    {
      "epoch": 9.990300678952472,
      "grad_norm": 20.528940200805664,
      "learning_rate": 5.5978938617153944e-08,
      "loss": 2.0735,
      "step": 72100
    },
    {
      "epoch": 9.991686296244977,
      "grad_norm": 20.239574432373047,
      "learning_rate": 5.0436469447138704e-08,
      "loss": 2.0967,
      "step": 72110
    },
    {
      "epoch": 9.99307191353748,
      "grad_norm": 11.359028816223145,
      "learning_rate": 4.4894000277123457e-08,
      "loss": 1.9357,
      "step": 72120
    },
    {
      "epoch": 9.994457530829985,
      "grad_norm": 18.23033332824707,
      "learning_rate": 3.935153110710822e-08,
      "loss": 1.6933,
      "step": 72130
    },
    {
      "epoch": 9.99584314812249,
      "grad_norm": 5.863268852233887,
      "learning_rate": 3.380906193709298e-08,
      "loss": 1.9694,
      "step": 72140
    },
    {
      "epoch": 9.997228765414992,
      "grad_norm": 18.570730209350586,
      "learning_rate": 2.826659276707774e-08,
      "loss": 1.9834,
      "step": 72150
    },
    {
      "epoch": 9.998614382707496,
      "grad_norm": 9.684898376464844,
      "learning_rate": 2.2724123597062495e-08,
      "loss": 1.697,
      "step": 72160
    },
    {
      "epoch": 10.0,
      "grad_norm": 24.6923770904541,
      "learning_rate": 1.718165442704725e-08,
      "loss": 1.9972,
      "step": 72170
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.5446292446292447,
      "eval_bert_f1": 0.9882574081420898,
      "eval_bert_precision": 0.9898381233215332,
      "eval_bert_recall": 0.9870777130126953,
      "eval_f1": 0.08412026163475071,
      "eval_loss": 2.074575424194336,
      "eval_runtime": 283.0906,
      "eval_samples_per_second": 50.973,
      "eval_steps_per_second": 6.373,
      "eval_synonym_accuracy": 0.5578655578655579,
      "step": 72170
    }
  ],
  "logging_steps": 10,
  "max_steps": 72170,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.27974348356736e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
